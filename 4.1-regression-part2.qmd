```{r}
#| echo: false
source("_common.R")
```


# Regression Part 2 (L4) {.unnumbered}

## Introduction

Now that we have a satisfactory model, we can start to use it. In the following material, you will learn:

* How to measure how good is the regression (correlation and $R^2$).
* How to test if the parameter estimates are compatible with some specific value ($t$-test).
* How to find the range of parameters values are compatible with the data (confidence intervals).
* How to find the regression lines compatible with the data (confidence band).
* How to calculate plausible values of newly collected data (prediction band).



## How good is the regression model?

What would a good regression model look like? What would a bad one look like? One could say that a good regression model is one that explains the dependent variable well. But what could we mean by "explains the data well"?

Take these two examples.

```{r fig.width = 7, fig.height = 4}
#| echo: false
## two side by sides graphs, one with a good fit, one with a bad fit
x <- 1:10
y1 <- 2*x + rnorm(10,0,1)
y2 <- 2*x + rnorm(10,0,5)
m1 <- lm(y1 ~ x)
m2 <- lm(y2 ~ x)
y1_predicted <- predict(m1)
y2_predicted <- predict(m2)
p1 <- ggplot() +
  geom_point(aes(x=x,y=y1)) +
  geom_line(aes(x=x,y=y1_predicted)) +
  theme_minimal()
p2 <- ggplot() +
  geom_point(aes(x=x,y=y2)) +
  geom_line(aes(x=x,y=y2_predicted)) +
  theme_minimal()
p1 + p2

```


**Think-Pair-Share** (#tps-better-model) In which of these two would you say the model is better, and in which is it worse?



The first model seems to fit the data well, while the second one does not. But how can we quantify this?

Let's say that we will measure the goodness of the model by the amount of variability of the dependent variable that is explained by the independent variable. To do this we need to do the following:

1. Measure the total variability of the dependent variable (total sum of squares, $SST$).
2. Measure the amount of variability of the dependent variable that is explained by the independent variable (model sum of squares, $SSM$).
3. Measure the variability of the dependent variable that is not explained by the independent variable (error sum of squares, $SSE$).
4. Calculate the proportion of variability of the dependent variable that is explained by the independent variable ($R^2$, pronounced as "r-squared") (also known as the coefficient of determination) ($R^2$ = $SSM/SST$).

**Importantly, note that we will calculate $SSM$ and $SSE$ so that they sum up to $SST$. I.e., $SST = SSM + SSE$. That is, the total variability is the sum of what is explained by the model and what remains unexplained.**

Let's take each in turn:

### $SST$

**1. The total variability of the dependent variable is the sum of the squared differences between the dependent variable and its mean. This is called the total sum of squares ($SST$).**

$$SST = \sum_{i=1}^{n} (y_i - \bar{y})^2$$

where:
$y_i$ is the dependent variable,
$\bar{y}$ is the mean of the dependent variable,
$n$ is the number of observations.

**Note that sometimes $SST$ is referred to as $SSY$ (sum of squares of $y$).**

Graphically, this is the sum of the square of the blue residuals as shown in the following graph, where the horizontal dashed line is at the value of the mean of the dependent variable.

```{r}
#| echo: false
ggplot() +
  geom_hline(yintercept = mean(y1), linetype = "dashed") +
  geom_segment(aes(x=x, y=y1, xend=x, yend=mean(y1)), col = "blue", linewidth = 2) +
  geom_point(aes(x=x,y=y1), size = 5) +
  theme_minimal()
```

We can calculate this in R as follows:

```{r}
#| echo: true
SST <- sum((y1 - mean(y1))^2)
```

### SSM and SSE

Now the next two steps, that is getting the model sum of squares (SSM) and the error sum of squares (SSE) are a bit more complicated. To do this we need to fit a regression model to the data. Let's see this graphically, and divide the data into the explained and unexplained parts.

Make a graph with vertical lines connecting the data to the mean of the data, but with each line two parts, one from the mean to the data, and one from the data to the predicted value.

```{r}
#| echo: false
ggplot() +
  geom_hline(yintercept = mean(y1), linetype = "dashed") +
  geom_segment(aes(x=x, y=y1_predicted, xend=x, yend=mean(y1)), col = "green", linewidth = 3) +
  geom_line(aes(x=x,y=y1_predicted)) +
  ## show the residuals
  geom_segment(aes(x=x, y=y1, xend=x, yend=y1_predicted), col = "red", linewidth = 1.5) +
    geom_point(aes(x=x,y=y1), size = 5) +
  theme_minimal()
```

In this graph, the square of the length of the green lines is the model sum of squares ($SST$). The square of the length of the red lines is the error sum of squares ($SSE$).

In a better model the length of the green lines will be **longer** (the square of these gives the $SMM$, the variability explained by the model). And the length of the red lines will be **shorter** (the square of these gives the $SSE$, the variability not explained by the model).


### $SSM$

Next we will do the second step, that is calculate the model sum of squares ($SSM$).

**2. The amount of variability of the dependent variable that is explained by the independent variable is called the model sum of squares ($SSM$).**

This is the difference between the predicted value of the dependent variable and the mean of the dependent variable, squared and summed:

$$SSE = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2$$

where:
$\hat{y}_i$ is the predicted value of the dependent variable,

In R, we calculate this as follows:

```{r}
#| echo: true
m1 <- lm(y1 ~ x)
y1_predicted <- predict(m1)
SSM <- sum((y1_predicted - mean(y1))^2)
SSM
```

### $SSE$

Third, we calculate the error sum of squares ($SSE$) with either of two methods. We could calculate it as the sum of the squared residuals, or as the difference between the total sum of squares and the model sum of squares:

$$SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = SST - SSM$$
Let's calculate this in R uses both approaches:

```{r}
#| echo: true
SSE <- sum((y1 - y1_predicted)^2)
SSE
```

Or...

```{r}
#| echo: true
SSE <- SST - SSM
SSE
```

### $R^2$

Finally, we calculate the proportion of variability of the dependent variable that is explained by the independent variable ($R^2$):    

$$R^2 = \frac{SSM}{SST}$$

```{r}
R.squared <- SSM/SST
R.squared
```

### Is my R squared good?

What value of $R^2$ is considered good? In ecological research, $R^2$ values are often low (less than 0.3), because ecological systems are complex and many factors influence the dependent variable. However, in other fields, such as physiology, $R^2$ values are often higher. Therefore, the answer of what values of $R^2$ are good depends on the field of research.

Here are the four examples and their r-squared.

```{r fig.width = 7, fig.height = 7}
#| echo: false
## Make a graph of x and y1 and the regression line and the r-squared value
set.seed(1233)
x <- 1:10
y1 <- 2*x + rnorm(10,0,1)
y2 <- 2*x + rnorm(10,0,5)
y3 <- 2*x + rnorm(10,0,10)
y4 <- 2*x + rnorm(10,0,20)
m1 <- lm(y1 ~ x)
m2 <- lm(y2 ~ x)
m3 <- lm(y3 ~ x)
m4 <- lm(y4 ~ x)
ylims <- range(c(y1, y2, y3, y4))
#ylims <- c(-30, 50)
p1 <- ggplot(data.frame(x = x, y1 = y1), aes(x = x, y = y1)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  annotate("text", x = 0, y = -20, label = paste("R^2 = ", round(summary(m1)$r.squared, 2)), hjust = 0, vjust = 0) +
  theme_minimal() +
  ylim(ylims)
p2 <- ggplot(data.frame(x = x, y2 = y2), aes(x = x, y = y2)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  annotate("text", x = 0, y = -20, label = paste("R^2 = ", round(summary(m2)$r.squared, 2)), hjust = 0, vjust = 0) +
  theme_minimal() +
  ylim(ylims)
p3 <- ggplot(data.frame(x = x, y3 = y3), aes(x = x, y = y3)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  annotate("text", x = 0, y = -20,
                label = paste("R^2 = ", round(summary(m3)$r.squared, 2)),
            hjust = 0, vjust = 0) +
  theme_minimal() +
  ylim(ylims)
p4 <- ggplot(data.frame(x = x, y4 = y4), aes(x = x, y = y4)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  annotate("text", x = 0, y = -20, label = paste("R^2 = ", round(summary(m4)$r.squared, 2)), hjust = 0, vjust = 0) +
  theme_minimal() +
  ylim(ylims)
(p1 + p2) / (p3 + p4)
```



**Think-Pair-Share** (#tps-what-minimised) What is minimised when we fit a regression model? And therefore what is maximised?


## How unlikey is the observed data given the null hypothesis?

We often hear this expressed as "is the relationship significant?" And maybe we heard that the relationship is significant if the p-value is less than 0.05. But what does all this actually mean? In this section we'll figure all this out. The first step to is to formulate a null hypothesis.

What is a meaningful null hypothesis for a regression model?

As mentioned, often we're interested in whether there is a relationship between the dependent (response) and independent (explanatory) variable. Therefore, the null hypothesis is that there is no relationship between the dependent and independent variable. This means that the null hypothesis is that the slope of the regression line is zero.

Recall the regression model:
$$y = \beta_0 + \beta_1 x + \epsilon$$

**Think-Pair-Share** (#tps-null-hypothesis) Write down the null hypothesis of no relationship between $x$ and $y$ in terms of a $\beta$ parameter.


The null hypothesis is that the slope of the regression line is zero:
$$H_0: \beta_1 = 0$$

What is the alternative hypothesis?

$$H_1: \beta_1 \neq 0$$

So, how do we test the null hypothesis? More precisely, we are going to calculate the probability of observing the data we have, given that the null hypothesis is true. If this probability is very low, then we can reject the null hypothesis.

Does that make sense? Does it seem a bit convoluted? It is a bit!!!

But this is how hypothesis testing works. We never prove the null hypothesis is true. Instead, we calculate the probability of observing our data given that the null hypothesis is true. If this probability is very low, we reject the null hypothesis.

To make the calculation we can use the fact that the slope of the regression line is an estimate of the true slope. This estimate has uncertainty associated with it. We can use this uncertainty to calculate the probability of observing the data we have, given the null hypothesis is true.

We can see that the slope estimate (the $x$ row) has uncertainty by looking at the regression output:

```{r}
summary(m1)$coefficients[1:2, 1:2]
```

The estimate is the mean of the distribution of the parameter (slope) and the standard error is a measure of the uncertainty of the estimate.

The standard error is calculated as:

$$\sigma^{(\beta_1)} = \sqrt{ \frac{\hat\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}}$$

Where $\hat\sigma^2$ is the expected residual variance of the model. This is calculated as:

$$\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n-2}$$

Where $\hat y_i$ is the predicted value of $y_i$ from the regression model.

OK, let's take a look at this intuitively. We have the estimate of the slope and the standard error of the estimate.

Here is a graph of the value of the slope estimate versus the standard error of the estimate:

```{r}
#| echo: false
slope_estimate_value <- seq(-1, 1, 0.01)
standard_error_value <- seq(0.01, 0.5, along = slope_estimate_value)
ggplot(data.frame(slope_estimate = slope_estimate_value,
                  standard_error = standard_error_value),
       aes(x = slope_estimate, y = standard_error)) +
  #geom_line() +
  theme_minimal()
```





**Think-Pair-Share** (#tps-chance-area) In what areas of the graph is the slope estimate more likely to have been observed by chance? And what regions is it less likely to have been observed by chance? Think about this before you look at the end of this chapter for an answer (Section @sec-visual-p-values-regress).




When the slope estimate is larger, it is less likely to have been observed by chance. And when the standard error is larger, it is more likely to have been observed by chance. How can we put these together into a single measure?

If we divide the slope estimate by the standard error, we get a measure of how many standard errors the slope estimate is from the null hypothesis slope of zero. This is the $t$-statistic:

$$t = \frac{\hat\beta_1 - \beta_{1,H_0}}{\sigma^{(\beta_1)}}$$

Where $\beta_{1,H_0}$ is the null hypothesis value of the slope, usually zero, so that

$$t = \frac{\hat\beta_1}{\sigma^{(\beta_1)}}$$

**The $t$-statistic is a measure of how many standard errors the slope estimate is from the null hypothesis value of the slope. The larger the $t$-statistic, the less likely the slope estimate was observed by chance.**

How can we transform the value of a $t$-statistic into a p-value? We can use the **$t$-distribution**, which quantifies the probability of observing a value of the $t$-statistic under the null hypothesis.

But what is the $t$-distribution? It is a distribution of the $t$-statistic under the null hypothesis. It is a bell-shaped distribution that is centered on zero. The shape of the distribution is determined by the degrees of freedom, which is $n-2$ for a simple linear regression model.

```{r}
#| echo: false
ggplot(data.frame(x = seq(-4, 4, 0.01)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = 98)) +
  theme_minimal()
```


::: {.callout-tip}
By the way, it is named the $t$-distribution by it's developer, William Sealy Gosset, who worked for the Guinness brewery in Dublin, Ireland. In his 1908 paper, Gosset introduced the $t$-distribution but he didn't explicitly explain his choice of the letter $t$. The choice of the letter $t$ could be to indicate "Test", as the $t$-distribution was developed specifically for hypothesis testing.
:::

Now, recall that the p-value is the probability of observing the value of the test statistic (so here the $t$-statistic) at least as extreme as the one we have, given the null hypothesis is true. We can calculate this probability by integrating the $t$-distribution from the observed $t$-statistic to the tails of the distribution.

Here is a graph of the $t$-distribution with 100 degrees of freedom with the tails of the distribution shaded so that the area of the shaded region is 0.05 (i.e., 5% of the total area).



```{r}
#| echo: false
n <- 100
t_95perc <- qt(0.975, df = n)
df <- data.frame(x = seq(-4, 4, 0.01), 
                dt = dt(seq(-4, 4, 0.01), df = n-2))
df_95perc_lower <- df[df$x < -t_95perc, ]
df_95perc_upper <- df[df$x > t_95perc, ]
ggplot(df, aes(x = x, y = dt)) +
  geom_line(data = df, mapping = aes(x = x, y = dt)) +
  geom_area(data = df_95perc_lower,
            mapping = aes(x = x, y = dt),
            fill = "lightblue") +
  geom_area(data = df_95perc_upper,
            mapping = aes(x = x, y = dt),
            fill = "lightblue") +
  theme_minimal()
```

And here's a graph of the $t$-distribution with 1000 degrees of freedom (blue line) and the normal distribution (green dashed line):

```{r}
#| echo: false
n <- 1000
ggplot(data.frame(x = seq(-4, 4, 0.01)), aes(x = x)) +
  stat_function(fun = dt, args = list(df = n-2),
                lwd = 1.5, lty = "solid", col = "blue") +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1),
                lwd = 1.3, lty = "solid", col = "darkorange") +
  theme_minimal()
```


So, with a large number of observations, the $t$-distribution approaches the normal distribution. For the normal distribution, the 95% area is between -1.96 and 1.96.

```{r}
#| echo: true
## x value for 95% area of normal distribution
x_value <- qnorm(0.975)
x_value
```

`qnorm` is a function that calculates the $x$ value for a given quantile (probability) of the normal distribution. In simpler terms, it finds the value $x$ at which the area under the normal curve (up to $x$) equals the given probability 
$p$ (0.975 in the example immediately above here).

```{r}
#| eval: false
#| echo: false
ggplot(data.frame(x = seq(-4, 4, 0.01)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) +
  geom_vline(xintercept = x_value, linetype = "dashed")
```





Let's go back to the age - blood pressure data and calculate the p-value for the slope estimate.

Read in the data:

```{r}
bp_data <- read_csv(here::here("datasets/Simulated_Blood_Pressure_and_Age_Data.csv"))
```

Make a graph:

```{r}
ggplot(bp_data, aes(x = Age, y = Systolic_BP)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x)
```

Here's the model:

```{r}
#| echo: true
mod1 <- lm(Systolic_BP ~ Age, data = bp_data)
```

Here we calculate the $t$-statistic for the slope estimate:

```{r}
t_stat <- mod1$coefficients[2] / summary(mod1)$coefficients[2, 2]
```

And here we calculate the one-tailed and two-tailed $p$-values:

```{r}
one_tailed_p_value <- pt(-abs(t_stat), df = nrow(bp_data) - 2)
two_tailed_p_value <- 2 * one_tailed_p_value
one_tailed_p_value
two_tailed_p_value
```

We can get the $p$-value directly from the `summary` function:

```{r}
summary(mod1)$coefficients[2, ]
```


Conclusion: there is __very strong evidence__ that the blood pressure is associated with age, because the $p$-value is extremely small (thus it is very unlikely that the observed slope value or a large one would be seen if there was really no association). Thus, we can reject the null hypothesis that the slope is zero.

This basically answers question 1: "Are the parameters compatible with some specific value?"

### Recap: Formal definition of the $p$-value


**The formal definition of $p$-value is the probability to observe a data summary (e.g., an average or a slope) that is at least as extreme as the one observed, given that the null hypothesis is correct.**
 

Example (normal distribution): Assume that we calculated that $t$-value = -1.96  

$\Rightarrow$ $Pr(|t|\geq 1.96)=0.05$ (two-tailed) and $Pr(t\leq-1.96)=0.025$ (one-tailed).

And here is a graph showing this:

```{r fig.width=8, fig.height=4}
#| echo: false
par(mfrow=c(1,2))

zz1 <- qnorm(0.025)
zz2 <- qnorm(0.975)
zz3 <- qnorm(0.025)

cord.x1 <- c(-4,seq(-4,zz1,0.01),zz1)
cord.y1 <- c(0,dnorm(seq(-4,zz1,0.01)),0)

cord.x2 <- c(zz2,seq(zz2,4,0.01),4)
cord.y2 <- c(0,dnorm(seq(zz2,4,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="Two-tailed p-value (0.05)",xlab="")
polygon(cord.x1,cord.y1,col='gray')
polygon(cord.x2,cord.y2,col='gray')
text(-3,0.05,labels="2.5%")
text(3,0.05,labels="2.5%")

cord.x3 <- c(-4,seq(-4,zz3,0.01),zz3)
cord.y3 <- c(0,dnorm(seq(-4,zz3,0.01)),0)

curve(dnorm(x,0,1),-4,4,ylab="density",main="One-tailed p-value (0.025)",xlab="")
polygon(cord.x3,cord.y3,col='gray')
text(-3,0.05,labels="2.5%")
```



### A cautionary note on the use of $p$-values

Maybe you have seen that in statistical testing, often the criterion $p\leq 0.05$ is used to test whether $H_0$ should be rejected. This is often done in a black-or-white manner. However, we will put a lot of attention to a more reasonable and cautionary interpretation of $p$-values in this course! 



## How strong is the relationship?

The actual value of the slope has practical meaning. The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit. The slope is one measure of the strength of the relationship between the two variables.

We can ask what values of a parameter estimate are compatible with the data (confidence intervals)? To answer this question, we can determine the confidence intervals of the regression parameters.

The confidence interval of a parameter estimate is defined as the interval that contains the true parameter value with a certain probability. So the 95% confidence interval of the slope is the interval that contains the true slope with a probability of 95%.

We can then imagine two cases. The 95% confidence interval of the slope includes 0:

```{r fig.width=10, fig.height=2}
#| echo: false
## a horizontal line showing the confidence interval overlapping 0
## only an x-axis
x <- seq(-1, 1, 0.01)
y <- rep(0, length(x))
ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
  geom_line(lty = "dotted") +
  geom_errorbarh(aes(y = 0, xmin = -0.1, xmax = 0.5), height = 0.05) +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  #geom_vline(xintercept = c(-1, 1), linetype = "dashed") +
  theme_minimal() +
  ylim(-0.1, 0.1) +
  xlim(-1, 1) +
  ## no y axis
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

Or where the confidence interval does not include zero:

```{r fig.width=10, fig.height=2}
#| echo: false
## a horizontal line showing the confidence interval overlapping 0
## only an x-axis
x <- seq(-1, 1, 0.01)
y <- rep(0, length(x))
ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
  geom_line(lty = "dotted") +
  geom_errorbarh(aes(y = 0, xmin = 0.1, xmax = 0.7), height = 0.05) +
  theme_minimal() +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  #geom_vline(xintercept = c(-1, 1), linetype = "dashed") +
  theme_minimal() +
  ylim(-0.1, 0.1) +
  xlim(-1, 1) +
  ## no y axis
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

How do we calculate the lower and upper limits of the 95% confidence interval of the slope?

Recall that the $t$-value for a null hypothesis of slope of zero is defined as:

$$t = \frac{\hat\beta_1}{\hat\sigma^{(\beta_1)}}$$

The first step is to calculate the $t$-value that corresponds to a p-value of 0.05. This is the $t$-value that corresponds to the 97.5% quantile of the $t$-distribution with $n-2$ degrees of freedom.

$t_{0.975} = t_{0.025} = 1.96$, for large $n$.

The 95% confidence interval of the slope is then given by:

$$\hat\beta_1 \pm t_{0.975} \cdot \hat\sigma^{(\beta_1)}$$

In our blood pressure example the estimated slope is `r coef(mod1)[2]` and the standard error of the slope is `r summary(mod1)$coef[2,2]`. We can calculate the 95% confidence interval of the slope in *R* as follows:

```{r}
#| echo: true
n <- 100
t_0975 <- qt(0.975, df = n - 2)
half_interval <- t_0975 * summary(mod1)$coef[2,2]
lower_limit <- coef(mod1)[2] - half_interval
upper_limit <- coef(mod1)[2] + half_interval
ci_slope <- c(lower_limit, upper_limit)
slope <- coef(mod1)[2]
slope
ci_slope
```

Or, using the `confint` function:

```{r}
## 95% confidence interval of the slope of mod1
ci_slope_2 <- confint(mod1, level=c(0.95))[2,]
ci_slope_2

```


Or we can do it using values from the coefficients table:

```{r echo = TRUE, eval = TRUE}
coefs <- summary(mod1)$coef
beta <- coefs[2,1]
sdbeta <- coefs[2,2] 
beta + c(-1,1) * qt(0.975,241) * sdbeta 
```


*Interpretation*: for an increase in the age by one year, roughly 0.82 mmHg increase in blood pressure is expected, and all true values for $\beta_1$ between 0.77 and 0.88 are compatible with the observed data.

## Confidence and Prediction Bands

* Remember: If another sample from the same population was taken, the regression line would look slightly different.  

* There are two questions to be asked:  

1. Which other regression lines are compatible with the observed data? This leads to the *confidence band*.
 
2. Where do future observations ($y$)  with a given $x$ coordinate lie? This leads to the *prediction band*.


Note: The prediction band is much broader than the confidence band.

## Calculation of the confidence band

Given a fixed value of $x$, say $x_0$. The question is: 


Where does $\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0$ lie with a certain confidence (i.e., 95%)? 


This question is not trivial, because both $\hat\beta_0$ and $\hat\beta_1$ are estimates from the data and contain uncertainty. 

The details of the calculation are given in Stahel 2.4b. 

Plotting the confidence interval around all $\hat y_0$ values one obtains the *confidence band* or *confidence band for the expected values* of $y$.

Note: For the confidence band, only the uncertainty in the estimates $\hat\beta_0$ and $\hat\beta_1$ matters.

Here is the confidence band for the blood pressure data:

```{r}
#| echo: false
## calculate the confidence band for the expected values of y
new_data <- data.frame(Age = seq(20, 80, by = 1))
conf_band <- cbind(new_data,
                   predict(mod1, newdata = new_data, interval = "confidence", level = 0.95))
ggplot(bp_data, aes(x = Age, y = Systolic_BP)) +
  geom_point() +
  geom_line(aes(y = fit), data = conf_band, color = "purple", lwd = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, y = fit), data = conf_band, alpha = 0.3) +
  labs(title = "Confidence band for the expected values of y",
       subtitle = "95% confidence band",
       x = "Age", y = "Systolic BP")
  
```

Very narrow confidence bands indicate that the estimates are very precise. In this case the estimated intercept and slope are precise because the sample size is large and the data points are close to the regression line.






```{r eval=FALSE, fig.width=5,fig.height=5, out.width='7cm', echo = FALSE, fig.align='center'}
t.range <- range(bp_data$Systolic_BP)
t.xwerte <- seq(t.range[1]-1,t.range[2]+1,by=1)
t.vert <- predict(mod1,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="confidence")$fit
t.vorh <- predict(mod1,se.fit=T,newdata=data.frame(bmi=t.xwerte),
interval ="prediction")$fit
plot(bp_data$Systolic_BP,d.bodyfat$bodyfat,main="Confidence band",xlab="BMI",ylab="bodyfat",xlim=range(t.xwerte),ylim=c(-5,50),cex=0.8)
abline(mod1,lwd=2)
lines(x=t.xwerte,y=t.vert[,2],lty=8,lwd=2,col=2)
lines(x=t.xwerte,y=t.vert[,3],lty=8,lwd=2,col=2)
legend("bottomright", c("confidence band (95%)"),
lty=8, cex=1,col=c(2),lwd=2)
```


## Calculations of the prediction band

We can easily predicted an expected value of $y$ for a given $x$ value. But we can also ask w where does a *future observation* lie with a certain confidence (i.e., 95\%)?

To answer this question, we have to *consider not only the uncertainty in the predicted value caused by uncertainty in the parameter estimates* $\hat y_0 =  \hat\beta_0 + \hat\beta_1 x_0$, but also the *error term* $\epsilon_i \sim N(0,\sigma^2)$}.  

This is the reason why the **prediction band** is wider than the confidence band.

Here's a graph showing the prediction band for the blood pressure data:

```{r}
#| echo: false
prediction_band <- cbind(new_data,
                   predict(mod1, newdata = new_data, interval = "prediction", level = 0.95))
ggplot(bp_data, aes(x = Age, y = Systolic_BP)) +
  geom_point() +
  geom_line(aes(y = fit), data = prediction_band, color = "blue", lwd = 1) +
  geom_ribbon(aes(ymin = lwr, ymax = upr, y = fit), data = prediction_band, alpha = 0.3, fill = "green") +
  geom_ribbon(aes(ymin = lwr, ymax = upr, y = fit), data = conf_band, alpha = 0.3,
              fill = "purple") +
  labs(title = "Prediction band for future observations (green)\nand confidence band for expected values (purple)",
       x = "Age", y = "Systolic BP")
```

Another way to think of the 95% confidence band is that it is where we would expect 95% of the regression lines to lie if we were to collect many samples from the same population. The 95% prediction band is where we would expect 95% of the future observations to lie.


**Think–Pair–Share** (#a_prediction_vs_estimation) Which interval answers: "What is the mean response?" Which answers: "What might a new observation look like?"


## That is regression done (at least for our current purposes)

* Why use (linear) regression?
* Fitting the line (= parameter estimation)
* Is linear regression good enough model to use?
* What to do when things go wrong?
* Transformation of variables/the response.
* Handling of outliers.
* Goodness of the model: Correlation and $R^2$
* Tests and confidence intervals
* Confidence and prediction bands



## Extras

### Randomisation test for the slope of a regression line

Let's use randomisation as another method to understand how likely we are to observe the data we have, given the null hypothesis is true.

If the null hypothesis is true, we expect no relationship between $x$ and $y$. Therefore, we can shuffle the $y$ values and fit a regression model to the shuffled data. We can repeat this many times and calculate the slope of the regression line each time. This will give us a distribution of slopes we would expect to observe if the null hypothesis is true.

First, we'll make some data and get the slope of the regression line. Here is the observed slope and relationship:

```{r}
set.seed(123)
n <- 100
x <- 1:n
y <- 0.1*x + rnorm(n, 0, 10)
m <- lm(y ~ x)
coef(m)[2]
```

```{r}
ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +
  theme_minimal()
```

Now we'll use randomisation to test the null hypothesis. We can create lots of examples where the relationship is expected to have a slope of zero by shuffling randomly the $y$ values. Here are 20:

```{r fig.width=10, fig.height=10}
par(mfrow = c(5, 4))
for (i in 1:20) {
  y_rand <- sample(y)
  m_rand <- lm(y_rand ~ x)
  plot(x, y_rand, main = paste("Slope = ", round(coef(m_rand)[2], 2)))
  abline(m_rand)
}
```

Now let's create 19 and put the real one in there somewhere random. Here's a case where the real data has a quite strong relationship:

```{r fig.width=10, fig.height=10}
#| echo: false
y <- 0.5*x + rnorm(n, 0, 15)
par(mfrow = c(5, 4))
this_one <- round(runif(1, 1, 20),0)
for (i in 1:20) {
  if (i == this_one) {
    plot(x, y, main = paste(i))
    #abline(m)
  } else {
    y_rand <- sample(y)
    m_rand <- lm(y_rand ~ x)
    plot(x, y_rand, main = paste(i))
    #abline(m_rand)
  }
}

```

We can confidently find the real data amount the shuffled data. But what if the relationship is weaker?

```{r fig.width=10, fig.height=10}
#| echo: false
y <- 0.15*x + rnorm(n, 0, 15)
par(mfrow = c(5, 4))
this_one <- round(runif(1, 1, 20),0)
for (i in 1:20) {
  if (i == this_one) {
    plot(x, y, main = paste(i), ylab = "y")
    #abline(m)
  } else {
    y_rand <- sample(y)
    m_rand <- lm(y_rand ~ x)
    plot(x, y_rand, main = paste(i), ylab = "y")
    #abline(m_rand)
  }
}
```

Now its less clear which is the real data. We can use this idea to test the null hypothesis.

We do the same procedure of but instead of just looking at the graphs, we calculate the slope of the regression line each time. This gives us a distribution of slopes we would expect to observe if the null hypothesis is true. We can then see where the observed slope lies in this distribution of null hypothesis slopes.

```{r}
# repeat 10000 time a randomisation test
y <- 0.15*x + rnorm(n, 0, 15)
rand_slopes <- replicate(10000, {
  y_rand <- sample(y)
  m_rand <- lm(y_rand ~ x)
  coef(m_rand)[2]
})

ggplot(data.frame(slopes = rand_slopes), aes(x = slopes)) +
  geom_histogram(bins = 50) +
  geom_vline(xintercept = coef(m)[2], color = "red") +
  theme_minimal()
```

We can now calculate the probability of observing the data we have, given the null hypothesis is true.

```{r}
p_value <- mean(abs(rand_slopes) >= abs(coef(m)[2]))
p_value
```



### Visualising p-values for regression slopes {#sec-visual-p-values-regress}

```{r}
#| echo: false
slope_estimate_value <- seq(-1, 1, 0.01)
standard_error_value <- seq(0.01, 0.5, along = slope_estimate_value)
ggplot(data.frame(slope_estimate = slope_estimate_value,
                  standard_error = standard_error_value),
       aes(x = slope_estimate, y = standard_error)) +
  annotate("text", x = 0, y = 0.4, label = "Likely\nto happen\nby chance", color = "red", size = 6) +
  annotate("text", x = 0.7, y = 0.1, label = "Unlikely\nto happen\nby chance", color = "blue", size = 6) +
  annotate("text", x = -0.7, y = 0.1, label = "Unlikely\nto happen\nby chance", color = "blue", size = 6) +
  theme_minimal() +
  xlim(-1, 1) +
  ylim(0, 0.5) +
  geom_abline(intercept = 0, slope = 0.5) +
  geom_abline(intercept = 0, slope = -0.5)


```

