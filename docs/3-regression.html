<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>BIO144 Course Book - Regression (L3&amp;4)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./4-multiple-regression.html" rel="next">
<link href="./2-data-wrangling.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3-regression.html">Regression (L3&amp;4)</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">BIO144 Course Book</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction (L1)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2-data-wrangling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Data wrangling (L2)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3-regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Regression (L3&amp;4)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4-multiple-regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Multiple regression (L4)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5-categorical-explanatory-variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Categorical explanatory variables (L5)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6-interactions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interactions (L5)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7-ANOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ANOVA (L6)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8-ANCOVA.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">ANCOVA (L7)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./9-linear-algebra.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Linear algebra (L7)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./10-variable-selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Variable selection (L8)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./11-interpretation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Intepretation, causality, and cautionary notes (L9)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./12-count-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Count data (L10)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./13-binary-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Binary data (L11)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./14-measurement-error.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Measurement error (L12)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./15-mixed-models.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Mixed models (L12)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./16-what-next.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">What next (L12)</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#why-use-linear-regression" id="toc-why-use-linear-regression" class="nav-link" data-scroll-target="#why-use-linear-regression">Why use linear regression?</a></li>
  <li><a href="#an-example---blood-pressure-and-age" id="toc-an-example---blood-pressure-and-age" class="nav-link" data-scroll-target="#an-example---blood-pressure-and-age">An example - blood pressure and age</a></li>
  <li><a href="#regression-from-a-mathematical-perspective" id="toc-regression-from-a-mathematical-perspective" class="nav-link" data-scroll-target="#regression-from-a-mathematical-perspective">Regression from a mathematical perspective</a></li>
  <li><a href="#but-a-straight-line-cannot-perfectly-fit-the-data" id="toc-but-a-straight-line-cannot-perfectly-fit-the-data" class="nav-link" data-scroll-target="#but-a-straight-line-cannot-perfectly-fit-the-data">But a straight line cannot perfectly fit the data</a></li>
  <li><a href="#dealing-with-the-error" id="toc-dealing-with-the-error" class="nav-link" data-scroll-target="#dealing-with-the-error">Dealing with the error</a></li>
  <li><a href="#back-to-blood-pressure-and-age" id="toc-back-to-blood-pressure-and-age" class="nav-link" data-scroll-target="#back-to-blood-pressure-and-age">Back to blood pressure and age</a></li>
  </ul></li>
  <li><a href="#finding-the-intercept-and-the-slope" id="toc-finding-the-intercept-and-the-slope" class="nav-link" data-scroll-target="#finding-the-intercept-and-the-slope">Finding the intercept and the slope</a>
  <ul class="collapse">
  <li><a href="#least-squares" id="toc-least-squares" class="nav-link" data-scroll-target="#least-squares">Least squares</a></li>
  <li><a href="#least-squares-estimates" id="toc-least-squares-estimates" class="nav-link" data-scroll-target="#least-squares-estimates">Least squares estimates</a></li>
  <li><a href="#why-division-by-n-2-ensures-an-unbiased-estimator" id="toc-why-division-by-n-2-ensures-an-unbiased-estimator" class="nav-link" data-scroll-target="#why-division-by-n-2-ensures-an-unbiased-estimator">Why division by <span class="math inline">\(n-2\)</span> ensures an unbiased estimator</a></li>
  <li><a href="#lets-do-it-in-r" id="toc-lets-do-it-in-r" class="nav-link" data-scroll-target="#lets-do-it-in-r">Let’s do it in R</a></li>
  </ul></li>
  <li><a href="#is-the-model-good-enough-to-use" id="toc-is-the-model-good-enough-to-use" class="nav-link" data-scroll-target="#is-the-model-good-enough-to-use">Is the model good enough to use?</a>
  <ul class="collapse">
  <li><a href="#what-assumptions-do-we-make" id="toc-what-assumptions-do-we-make" class="nav-link" data-scroll-target="#what-assumptions-do-we-make">What assumptions do we make?</a></li>
  <li><a href="#a-normally-distributed-residuals" id="toc-a-normally-distributed-residuals" class="nav-link" data-scroll-target="#a-normally-distributed-residuals">(a) Normally distributed residuals</a></li>
  <li><a href="#a-normally-distributed-residuals-the-qq-plot" id="toc-a-normally-distributed-residuals-the-qq-plot" class="nav-link" data-scroll-target="#a-normally-distributed-residuals-the-qq-plot">(a) Normally distributed residuals: The QQ-plot</a></li>
  <li><a href="#b-equal-variance-homoscedasticity" id="toc-b-equal-variance-homoscedasticity" class="nav-link" data-scroll-target="#b-equal-variance-homoscedasticity">(b) Equal variance (homoscedasticity)</a></li>
  <li><a href="#c-independence-the-epsilon_i-are-independent-of-each-other" id="toc-c-independence-the-epsilon_i-are-independent-of-each-other" class="nav-link" data-scroll-target="#c-independence-the-epsilon_i-are-independent-of-each-other">(c) Independence (the <span class="math inline">\(\epsilon_i\)</span> are independent of each other)</a></li>
  <li><a href="#d-linearity-assumption" id="toc-d-linearity-assumption" class="nav-link" data-scroll-target="#d-linearity-assumption">(d) Linearity assumption</a></li>
  <li><a href="#e-no-outliers" id="toc-e-no-outliers" class="nav-link" data-scroll-target="#e-no-outliers">(e) No outliers</a></li>
  </ul></li>
  <li><a href="#what-can-go-wrong-during-the-modeling-process" id="toc-what-can-go-wrong-during-the-modeling-process" class="nav-link" data-scroll-target="#what-can-go-wrong-during-the-modeling-process">What can go “wrong” during the modeling process?</a>
  <ul class="collapse">
  <li><a href="#what-to-do-when-things-go-wrong" id="toc-what-to-do-when-things-go-wrong" class="nav-link" data-scroll-target="#what-to-do-when-things-go-wrong">What to do when things “go wrong”?</a></li>
  <li><a href="#dealing-with-non-linearity" id="toc-dealing-with-non-linearity" class="nav-link" data-scroll-target="#dealing-with-non-linearity">Dealing with non-linearity</a></li>
  <li><a href="#common-transformations" id="toc-common-transformations" class="nav-link" data-scroll-target="#common-transformations">Common transformations</a></li>
  <li><a href="#outliers" id="toc-outliers" class="nav-link" data-scroll-target="#outliers">Outliers</a></li>
  <li><a href="#removing-outliers" id="toc-removing-outliers" class="nav-link" data-scroll-target="#removing-outliers">Removing outliers</a></li>
  </ul></li>
  <li><a href="#sec-kind-of-magic" id="toc-sec-kind-of-magic" class="nav-link" data-scroll-target="#sec-kind-of-magic">Its a kind of magic…</a>
  <ul class="collapse">
  <li><a href="#so-what-is-a-linear-model" id="toc-so-what-is-a-linear-model" class="nav-link" data-scroll-target="#so-what-is-a-linear-model">So what is a linear model?</a></li>
  </ul></li>
  <li><a href="#end-of-l3-start-of-l4" id="toc-end-of-l3-start-of-l4" class="nav-link" data-scroll-target="#end-of-l3-start-of-l4">End of L3 / Start of L4</a></li>
  <li><a href="#regression-continued" id="toc-regression-continued" class="nav-link" data-scroll-target="#regression-continued">Regression continued</a></li>
  <li><a href="#overview-of-this-week-l4" id="toc-overview-of-this-week-l4" class="nav-link" data-scroll-target="#overview-of-this-week-l4">Overview of this week (L4)</a>
  <ul class="collapse">
  <li><a href="#accompanying-reading-material" id="toc-accompanying-reading-material" class="nav-link" data-scroll-target="#accompanying-reading-material">Accompanying reading material</a></li>
  </ul></li>
  <li><a href="#how-good-is-the-regression-model" id="toc-how-good-is-the-regression-model" class="nav-link" data-scroll-target="#how-good-is-the-regression-model">How good is the regression model?</a>
  <ul class="collapse">
  <li><a href="#sst" id="toc-sst" class="nav-link" data-scroll-target="#sst"><span class="math inline">\(SST\)</span></a></li>
  <li><a href="#ssm-and-sse" id="toc-ssm-and-sse" class="nav-link" data-scroll-target="#ssm-and-sse">SSM and SSE</a></li>
  <li><a href="#ssm" id="toc-ssm" class="nav-link" data-scroll-target="#ssm"><span class="math inline">\(SSM\)</span></a></li>
  <li><a href="#sse" id="toc-sse" class="nav-link" data-scroll-target="#sse"><span class="math inline">\(SSE\)</span></a></li>
  <li><a href="#r2" id="toc-r2" class="nav-link" data-scroll-target="#r2"><span class="math inline">\(R^2\)</span></a></li>
  <li><a href="#is-my-r-squared-good" id="toc-is-my-r-squared-good" class="nav-link" data-scroll-target="#is-my-r-squared-good">Is my R squared good?</a></li>
  <li><a href="#questions" id="toc-questions" class="nav-link" data-scroll-target="#questions">Questions</a></li>
  </ul></li>
  <li><a href="#how-unlikey-is-the-observed-data-given-the-null-hypothesis" id="toc-how-unlikey-is-the-observed-data-given-the-null-hypothesis" class="nav-link" data-scroll-target="#how-unlikey-is-the-observed-data-given-the-null-hypothesis">How unlikey is the observed data given the null hypothesis?</a>
  <ul class="collapse">
  <li><a href="#recap-formal-definition-of-the-p-value" id="toc-recap-formal-definition-of-the-p-value" class="nav-link" data-scroll-target="#recap-formal-definition-of-the-p-value">Recap: Formal definition of the <span class="math inline">\(p\)</span>-value</a></li>
  <li><a href="#a-cautionary-note-on-the-use-of-p-values" id="toc-a-cautionary-note-on-the-use-of-p-values" class="nav-link" data-scroll-target="#a-cautionary-note-on-the-use-of-p-values">A cautionary note on the use of <span class="math inline">\(p\)</span>-values</a></li>
  </ul></li>
  <li><a href="#how-strong-is-the-relationship" id="toc-how-strong-is-the-relationship" class="nav-link" data-scroll-target="#how-strong-is-the-relationship">How strong is the relationship?</a></li>
  <li><a href="#confidence-and-prediction-bands" id="toc-confidence-and-prediction-bands" class="nav-link" data-scroll-target="#confidence-and-prediction-bands">Confidence and Prediction Bands</a></li>
  <li><a href="#calculation-of-the-confidence-band" id="toc-calculation-of-the-confidence-band" class="nav-link" data-scroll-target="#calculation-of-the-confidence-band">Calculation of the confidence band</a></li>
  <li><a href="#calculations-of-the-prediction-band" id="toc-calculations-of-the-prediction-band" class="nav-link" data-scroll-target="#calculations-of-the-prediction-band">Calculations of the prediction band</a></li>
  <li><a href="#that-is-regression-done-at-least-for-our-current-purposes" id="toc-that-is-regression-done-at-least-for-our-current-purposes" class="nav-link" data-scroll-target="#that-is-regression-done-at-least-for-our-current-purposes">That is regression done (at least for our current purposes)</a></li>
  <li><a href="#additional-reading-material" id="toc-additional-reading-material" class="nav-link" data-scroll-target="#additional-reading-material">Additional reading material</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Regression (L3&amp;4)</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>This chapter contains the content of the third and fourth Monday lecture of the course BIO144 Data Analysis in Biology at the University of Zurich. When</p>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Linear regression is a common statistical method that models the relationship between a dependent (response) variable and one or more independent (explanatory) variables. The relationship is modeled with the equation for a straight line (<span class="math inline">\(y = a + bx\)</span>).</p>
<p>With linear regression we can answer questions such as:</p>
<ul>
<li>How does the dependent (response) variable change with respect to the independent (explanatory) variable?</li>
<li>What amount of variation in the dependent variable can be explained by the independent variable?</li>
<li>Is there a statistically significant relationship between the dependent variable and the independent variable?</li>
<li>Does the linear model fit the data well?</li>
</ul>
<p>In this chapter / lesson we will explore what is linear regression and how to use it to answer these questions. We’ll cover the following topics:</p>
<ul>
<li>Why use linear regression?</li>
<li>What is the linear regression model?</li>
<li>Fitting the regression model (= finding the intercept and the slope).</li>
<li>Is linear regression a good enough model to use?</li>
<li>What do we do when things go wrong?</li>
<li>Transformation of variables/the response.</li>
<li>Identifying and handling odd data points (aka outliers).</li>
</ul>
<p>In this chapter / lesson we will not discuss the statistical significance of the model. We will cover this topic in the next chapter / lesson.</p>
<section id="why-use-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="why-use-linear-regression">Why use linear regression?</h3>
<ul>
<li>It’s a good starting point because it is a relatively simple model.</li>
<li>Relationships are sometimes close enough to linear.</li>
<li>It’s easy to interpret.</li>
<li>It’s easy to use.</li>
<li>It’s actually quite flexible (e.g.&nbsp;can be used for non-linear relationships, e.g., a quadratic model is still a linear model!!! See <a href="#sec-kind-of-magic"><span>Section&nbsp;5</span></a>.).</li>
</ul>
</section>
<section id="an-example---blood-pressure-and-age" class="level3">
<h3 class="anchored" data-anchor-id="an-example---blood-pressure-and-age">An example - blood pressure and age</h3>
<p>There are lots of situations in which linear regression can be useful. For example, consider hypertension. Hypertension is a condition in which the blood pressure in the arteries is persistently elevated. Hypertension is a major risk factor for heart disease, stroke, and kidney disease. It is estimated that hypertension affects about 1 billion people worldwide. Hypertension is a complex condition that is influenced by many factors, including age. In fact, it is well known that blood pressure increases with age. But how much does blood pressure increase with age? This is a question that can be answered using linear regression.</p>
<p>Here is an example of a study that used linear regression to answer this question: https://journals.lww.com/jhypertension/fulltext/2021/06000/association_of_age_and_blood_pressure_among_3_3.15.aspx</p>
<p>In this study, the authors used linear regression to model the relationship between age and blood pressure. They found that systolic blood pressure increased by 0.28–0.85 mmHg/year. This is a small increase, but it is statistically significant. This means that the observed relationship between age and blood pressure is unlikely to be due to chance.</p>
<p>Lets look at some simulated example data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Well, that is pretty conclusive. We hardly need statistics. There is a clear positive relationship between age and systolic blood pressure. But how can we quantify this relationship? And in less clear-cut cases what is the strength of evidence for a relationship? This is where linear regression comes in. Linear regression models the relationship between age and systolic blood pressure. With linear regression we can answer the following questions:</p>
<ul>
<li>What is a good mathematical representation of the relationship?</li>
<li>Is the relationship different from what we would expect if there were no relationship?</li>
<li>How well does the mathematical representation match the observed values?</li>
<li>How much uncertainty is there in any predictions?</li>
</ul>
<p>Lets try to figure some of these out from the visualisation.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#guess-params)
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>Make a guess of the slope.</li>
<li>Make a guess of the intercept (hint be careful, lots of people get this wrong).</li>
</ul>
</div>
</div>
</section>
<section id="regression-from-a-mathematical-perspective" class="level3">
<h3 class="anchored" data-anchor-id="regression-from-a-mathematical-perspective">Regression from a mathematical perspective</h3>
<p>Given an <strong>independent/explanatory variable</strong> (<span class="math inline">\(X\)</span>) and a <strong>dependent/response variable</strong> (<span class="math inline">\(Y\)</span>) all points <span class="math inline">\((x_i,y_i)\)</span>, <span class="math inline">\(i= 1,\ldots, n\)</span>, on a straight line follow the equation</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i\ .\]</span></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the <strong>intercept</strong> - the value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x_i = 0\)</span></li>
<li><span class="math inline">\(\beta_1\)</span> the <strong>slope</strong> of the line, also known as the regression coefficient of <span class="math inline">\(X\)</span>.</li>
<li>If <span class="math inline">\(\beta_0=0\)</span> the line goes through the origin <span class="math inline">\((x,y)=(0,0)\)</span>.</li>
<li><strong>Interpretation</strong> of linear dependency: proportional increase in <span class="math inline">\(y\)</span> with increase (decrease) in <span class="math inline">\(x\)</span>.</li>
</ul>
</section>
<section id="but-a-straight-line-cannot-perfectly-fit-the-data" class="level3">
<h3 class="anchored" data-anchor-id="but-a-straight-line-cannot-perfectly-fit-the-data">But a straight line cannot perfectly fit the data</h3>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>The line is not a perfect fit to the data. There is scatter around the line.</p>
<p>Some of this scatter could be caused by other factors that influence blood pressure, such as diet, exercise, and genetics. Also, the there could be differences due to the measurement instrument (i.e., some measurement error).</p>
<p>These other factors are not included in the model (only age is in the model), so they create variation that can only appear in error term.</p>
<p>In the linear regression model the dependent variable <span class="math inline">\(Y\)</span> is related to the independent variable <span class="math inline">\(x\)</span> as</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 x + \epsilon \ \]</span> where</p>
<ul>
<li><span class="math inline">\(\epsilon\)</span> is the error term</li>
<li><span class="math inline">\(\beta_0\)</span> is the intercept</li>
<li><span class="math inline">\(\beta_1\)</span> is the slope</li>
<li><span class="math inline">\(\epsilon\)</span> is the error term.</li>
</ul>
<p>The error term captures the difference between the observed value of the dependent variable and the value predicted by the model. The error term includes the effects of other factors that influence the dependent variable, as well as measurement error.</p>
<p><span class="math display">\[Y \quad= \quad \underbrace{\text{expected value}}_{E(Y) = \beta_0 + \beta_1 x} \quad + \quad \underbrace{\text{random error}}_{\epsilon}  \ .\]</span></p>
<p>Graphically the error term is the vertical distance between the observed value of the dependent variable and the value predicted by the model.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>The error term is also known as the residual. It is the variation that <em>resides</em> (is left over / is left unexplained) after accounting for the relationship between the dependent and independent variables.</p>
</section>
<section id="dealing-with-the-error" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-the-error">Dealing with the error</h3>
<p>If we use each of the observed residuals as the error for the respective data point, we end up with a perfect fit to the data. This is because the model is fitted to the data, so the residuals are the difference between the observed value of the dependent variable and the value predicted by the model. We then have a model with no error – everything is explained by the model. This is known as over-fitting. In fact, we have gained nothing by fitting the model, because we have not learned anything about the relationship between the dependent and independent variables. We have simply memorized / copied the data!!!</p>
<p>In order to avoid this, we need to assume something about the residuals – we need to <em>model</em> the error term. The most common model for the error term is normally distributed with mean 0 and constant variance.</p>
<p><span class="math display">\[\epsilon \sim N(0,\sigma^2)\]</span></p>
<p><strong>This is known as the normality assumption.</strong> The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.</p>
<p>The linear regression model then becomes:</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 x + N(0,\sigma^2) \ \]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> is the variance of the error term. The variance of the error term is the amount of variation in the dependent variable that is not explained by the independent variable. The variance of the error term is also known as the residual variance.</p>
<p>An alternate and equivalent formulation is that <span class="math inline">\(Y\)</span> is a random variable that follows a normal distribution with mean <span class="math inline">\(\beta_0 + \beta_1 x\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display">\[Y \sim N(\beta_0 + \beta_1 x, \sigma^2)\]</span></p>
<p>So, the answer to the question “how do we deal with the error term” is that we model the error term as normally distributed with mean 0 and constant variance. Put another way, the error term is assumed to be normally distributed with mean 0 and constant variance.</p>
</section>
<section id="back-to-blood-pressure-and-age" class="level3">
<h3 class="anchored" data-anchor-id="back-to-blood-pressure-and-age">Back to blood pressure and age</h3>
<p>The mathematical model in this case is:</p>
<p><span class="math display">\[SystolicBP = \beta_0 + \beta_1 \times Age + \epsilon\]</span></p>
<p>where: <em>SystolicBP</em> is the dependent (response) variable, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1\)</span> is the coefficient of Age, <em>Age</em> is the independent (explanatory) variable, <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<p>Let’s ensure we understand this, by thinking about the units of the variables in this model. This can be very useful because it can help us to understand the model better and to check that the model makes sense.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, pair, share (#what-units)
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>What are the units of blood pressure?</li>
<li>What are the units of age?</li>
<li>What are the units of the intercept?</li>
<li>What are the units of the coefficient of Age?</li>
<li>What are the units of the error term?</li>
</ul>
</div>
</div>
<p>We can easily guess that the coefficient of Age is positive, because blood pressure tends to increase with age. But what is the most likely value of this coefficient? We need to estimate the coefficient of Age from the data. This is what linear regression does. It estimates the coefficients (intercept and slope) of the independent variables in the model.</p>
</section>
</section>
<section id="finding-the-intercept-and-the-slope" class="level2">
<h2 class="anchored" data-anchor-id="finding-the-intercept-and-the-slope">Finding the intercept and the slope</h2>
<p>In a regression analysis, one task is to estimate the intercept and the slope. These are known as the <strong>regression coefficients</strong> <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span>. (We also estimate the <strong>residual variance</strong> <span class="math inline">\(\sigma^2\)</span>.)</p>
<ul>
<li><p><strong>Problem</strong>: For more than two points <span class="math inline">\((x_i,y_i)\)</span>, <span class="math inline">\(i=1,\ldots, n\)</span>, there is generally no perfectly fitting line.</p></li>
<li><p><strong>Aim:</strong> We want to estimate the parameters <span class="math inline">\((\beta_0,\beta_1)\)</span> of the <strong>best fitting</strong> line <span class="math inline">\(Y = \beta_0 + \beta_1 x\)</span>.</p></li>
<li><p><strong>Idea:</strong> Find the <strong>best fitting line</strong> by minimizing the deviations between the data points <span class="math inline">\((x_i,y_i)\)</span> and the regression line. I.e., minimising the residuals.</p></li>
</ul>
<p>But which deviations?</p>
<p>These ones?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Or these?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Or maybe even these?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Well, actually its none of these!!!</p>
<section id="least-squares" class="level3">
<h3 class="anchored" data-anchor-id="least-squares">Least squares</h3>
<p>For multiple reasons (theoretical aspects and mathematical convenience), the intercept and slope are estimated using the <strong>least squares</strong> approach. In this, yet something else is minimized:</p>
<p>The parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are estimated such that the <strong>sum of squared vertical distances</strong> (sum of squared residuals / errors) is minimised.</p>
<p><strong>SSE</strong> means <strong>S</strong>um of <strong>S</strong>quared <strong>E</strong>rrors:</p>
<p><span class="math display">\[SSE = \sum_{i=1}^n e_i^2 \]</span></p>
<p>where,</p>
<p><span class="math display">\[e_i = y_i - \underbrace{(\beta_0 + \beta_1 x_i)}_{=\hat{y}_i} \]</span> <strong>Note:</strong> <span class="math inline">\(\hat y_i = \beta_0 + \beta_1 x_i\)</span> are the <em>predicted values</em>.</p>
<p>In the graph just below, one of these squares is shown in red.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="480"></p>
</div>
</div>
</section>
<section id="least-squares-estimates" class="level3">
<h3 class="anchored" data-anchor-id="least-squares-estimates">Least squares estimates</h3>
<p>With a linear model, we can calculate the least squares estimates of the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> directly using the following formulas.</p>
<p>For a given sample of data <span class="math inline">\((x_i,y_i), i=1,..,n\)</span>, with mean values <span class="math inline">\(\overline{x}\)</span> and <span class="math inline">\(\overline{y}\)</span>, the least squares estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are computed as</p>
<p><span class="math display">\[ \hat\beta_1 = \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)}\]</span></p>
<p><span class="math display">\[\hat\beta_0 = \overline{y} - \hat\beta_1 \overline{x}  \]</span></p>
<p>Moreover,</p>
<p><span class="math display">\[ \hat\sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i) \]</span></p>
<p>is an unbiased estimate of the residual variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>(Derivations of the equations above are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)</p>
</section>
<section id="why-division-by-n-2-ensures-an-unbiased-estimator" class="level3">
<h3 class="anchored" data-anchor-id="why-division-by-n-2-ensures-an-unbiased-estimator">Why division by <span class="math inline">\(n-2\)</span> ensures an unbiased estimator</h3>
<p>When estimating parameters (<span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>), the square of the residuals is minimised. This fitting process inherently <em>uses up</em> two <em>degrees of freedom</em>, as the model forces the residuals to sum to zero and aligns the slope to best fit the data. I.e., one degree of freedom is lost due to the estimation of the intercept, and another due to the estimation of the slope.</p>
<p>The adjustment (division by <span class="math inline">\(n-2\)</span> instead of <span class="math inline">\(n\)</span>) compensates for the loss of variability due to parameter estimation, ensuring the estimator of the residual variance is unbiased. Mathematically, dividing by n - 2 adjusts for this loss and gives an accurate estimate of the population variance when working with sample data.</p>
<p>We’ll look at degrees of freedom in more detail later, so don’t worry if this is a bit confusing right now.</p>
</section>
<section id="lets-do-it-in-r" class="level3">
<h3 class="anchored" data-anchor-id="lets-do-it-in-r">Let’s do it in R</h3>
<p>First we read in the dataset:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>bp_age_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"data/Simulated_Blood_Pressure_and_Age_Data.csv"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The we make a graph of the data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Then we make the linear model, using the <code>lm()</code> function:</p>
<p>Then we can look at the summary of the model. It contains a lot of information, so can be a bit confusing at first.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = Systolic_BP ~ Age, data = bp_age_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.2195  -3.4434  -0.0808   3.1383  12.6025 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 98.96874    1.46102   67.74   &lt;2e-16 ***
Age          0.82407    0.02771   29.74   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.971 on 98 degrees of freedom
Multiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 
F-statistic: 884.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>How do our guesses of the intercept and slope compare to the guesses we made earlier?</p>
<p>Recal that the units of the <em>Age</em> coefficient are in mmHg per year. This means that for each additional year of age, the systolic blood pressure increases by ´r round(coef(bp_age_model)[2],2)´ mmHg.</p>
</section>
</section>
<section id="is-the-model-good-enough-to-use" class="level2">
<h2 class="anchored" data-anchor-id="is-the-model-good-enough-to-use">Is the model good enough to use?</h2>
<ul>
<li>All models are wrong, but is ours good enough to be useful?</li>
<li>Are the assumption of the model justified?</li>
<li>It would be very unwise to use the model before we know if it is good enough to use.</li>
<li>Don’t jump out of an aeroplane until you know your parachute is good enough!</li>
</ul>
<section id="what-assumptions-do-we-make" class="level3">
<h3 class="anchored" data-anchor-id="what-assumptions-do-we-make">What assumptions do we make?</h3>
<p>We already heard about one. We assume that the residuals follow a <span class="math inline">\(N(0,\sigma^2)\)</span> distribution. We make this assumption because it is often well enough met, and it gives great mathematical tractability.</p>
<p>This assumption implies that:</p>
<ol type="a">
<li>The <span class="math inline">\(\epsilon_i\)</span> are normally distributed.</li>
<li>All <span class="math inline">\(\epsilon_i\)</span> have the same variance: <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span>.</li>
<li>The <span class="math inline">\(\epsilon_i\)</span> are independent of each other.</li>
</ol>
<p>Furthermore:</p>
<ol start="4" type="a">
<li>we assumed a linear relationship.</li>
<li>implies there are no outliers (implied by (a) above)</li>
</ol>
<p>Lets go through each five assumptions.</p>
</section>
<section id="a-normally-distributed-residuals" class="level3">
<h3 class="anchored" data-anchor-id="a-normally-distributed-residuals">(a) Normally distributed residuals</h3>
<p>What does this mean? How can we check it?</p>
<p>A normal distribution is symmetric and bell-shaped…</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Lets look at the frequency distribution of the residuals of the linear regression of blood pressure and age:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>The normal distribution assumption (a) seems ok as well.</p>
</section>
<section id="a-normally-distributed-residuals-the-qq-plot" class="level3">
<h3 class="anchored" data-anchor-id="a-normally-distributed-residuals-the-qq-plot">(a) Normally distributed residuals: The QQ-plot</h3>
<p>Usually, not the histogram of the residuals is plotted, but the so-called <strong>quantile-quantile</strong> (QQ) plot. The quantiles of the observed distribution are plotted against the quantiles of the respective theoretical (normal) distribution:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>If the points lie approximately on a straight line, the data is fairly normally distributed.</p>
<p>This is often “tested” by eye, and needs some experience.</p>
<p><em>But what on earth is a quantile???</em></p>
<p>Imagine we make 21 measures of something, say 21 blood pressures:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The median of these is 127.8. The median is the 50% or 0.5 quantile, because half the data points are above it, and half below.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  50% 
127.8 </code></pre>
</div>
</div>
<p>The <em>theoretical quantiles</em> come from the normal distribution. The <em>sample quantiles</em> come from the distribution of our residuals.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<section id="how-do-i-know-if-a-qq-plot-looks-good" class="level4">
<h4 class="anchored" data-anchor-id="how-do-i-know-if-a-qq-plot-looks-good">How do I know if a QQ-plot looks “good”?</h4>
<p>There is <strong>no quantitative rule</strong> to answer this question. Instead experience is needed. You can gain this experience from simulations. To this end, we can generate the same number of data points of a normally distributed variable and compare this simulated qqplot to our observed one.</p>
<p>Example: Generate 100 points <span class="math inline">\(\epsilon_i \sim N(0,1)\)</span> each time:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Each of the graphs above has data points that are randomly generated from a normal distribution. In all cases the data points are close to the line. This is what we would expect if the data were normally distributed. The amount of deviation from the line is what we would expect from random variation, and so seeing this amount of variation in a QQ-plot of your model should not be cause for concern.</p>
</section>
</section>
<section id="b-equal-variance-homoscedasticity" class="level3">
<h3 class="anchored" data-anchor-id="b-equal-variance-homoscedasticity">(b) Equal variance (homoscedasticity)</h3>
<p>Basically, we’re interested if the size of the residuals tends to show a pattern with the fitted values. By <em>size</em> of the residuals we mean the <em>absolute value</em> of the residuals.</p>
<p>We have assumed that there is no relationship between the residuals and the fitted values.</p>
<p>That is, variance of the residuals is a constant: <span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2\)</span>. And not, for example <span class="math inline">\(\text{Var}(\epsilon_i) = \sigma^2 \cdot x_i\)</span>.</p>
<p>So we’re going to plot the size of the residuals against the fitted values.</p>
<p>This graph is known as the scale-location plot. It is particularly suited to check the assumption of equal variances (<strong>homoscedasticity / Homoskedastizität</strong>).</p>
<p>The idea is to plot the square root of the (standardized) residuals <span class="math inline">\(\sqrt{|R_i|}\)</span> against the fitted values <span class="math inline">\(\hat{y_i}\)</span>. There should be <strong>no trend</strong>:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<section id="how-it-looks-with-the-variance-increasing-with-the-fitted-values" class="level4">
<h4 class="anchored" data-anchor-id="how-it-looks-with-the-variance-increasing-with-the-fitted-values">How it looks with the variance increasing with the fitted values</h4>
<p>Here’s a graphical example of how it would look if the variance of the residuals increases with the fitted values.</p>
<p>First here is a graph of the relationship:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>And here the scale-location plot for a linear model of that data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="480"></p>
</div>
</div>
</section>
</section>
<section id="c-independence-the-epsilon_i-are-independent-of-each-other" class="level3">
<h3 class="anchored" data-anchor-id="c-independence-the-epsilon_i-are-independent-of-each-other">(c) Independence (the <span class="math inline">\(\epsilon_i\)</span> are independent of each other)</h3>
<p>We assume that the residuals are independent of each other. This means that the value of one residual is not somehow related to the value of another.</p>
<p>The dataset about blood pressure we looked at contained 100 observations, each one made from a different person. In such a study design, we could be safe in the assumption that the people are independent, and therefore the assumption that the residuals are independent.</p>
<p>Imagine, however, if we had 100 observations of blood pressure collected from 50 people, because we measured the blood pressure of each person twice. In this case, the residuals would not be independent, because two measures of the blood pressure of the same person are likely to be similar. A person is likely to have a high blood pressure in both measurements, or a low blood pressure in both measurements. This would mean they have a high residual in both measurements, or a low residual in both measurements.</p>
<p>In this case, we would need to account for the fact that the residuals are not independent. We would need to use a more complex model, such as a mixed effects model, to account for the fact that the residuals are not independent. We will talk about this again in the last week of this course.</p>
<p>In general, you should always think about the study design when you are analysing data. You should always think about whether the residuals are likely to be independent of each other. If they are not, you should think about how you can account for this in your analysis.</p>
<p>A good way to assess if there could be dependencies in the residuals is to be critical about what is the unit of observation in the data. In the blood pressure example, the unit of observation is the person. Count the number of persons in the study. If there are fewer persons than observations, then at least some people must have been measured at least twice. Repeating measures on the same person is a common way to get dependent residuals.</p>
<p>So, to check the assumption of independence, you should:</p>
<ul>
<li>Think carefully about the study design.</li>
<li>Think carefully about the unit of observation in the data.</li>
<li>Compare the number of observations to the number of units of observation.</li>
</ul>
</section>
<section id="d-linearity-assumption" class="level3">
<h3 class="anchored" data-anchor-id="d-linearity-assumption">(d) Linearity assumption</h3>
<p>The linearity assumption states that the relationship between the independent variable and the dependent variable is linear. This means that the dependent variable changes by a constant amount for a one-unit change in the independent variable. And that this slope is does not change with the value of the independent variable.</p>
<p>The blood pressure data seems to be linear:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>In contrast, look at this linear regression through data that appears non-linear:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>And with the residuals shown as red lines:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>At low values of <span class="math inline">\(y\)</span>, the residuals are positive, at intermediate values of <span class="math inline">\(y\)</span> the residuals are negative, and at high values of <span class="math inline">\(y\)</span> the residuals are positive. This pattern in the residuals is a sign that the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is not linear.</p>
<p>We can plot the value of the residuals against the <span class="math inline">\(y\)</span> value directly, instead of looking at the pattern in the graph above. This is called a <strong>Tukey-Anscombe plot</strong>. It is a graph of the residuals versus the fitted <span class="math inline">\(y\)</span> values.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>We can very clearly see pattern in the residuals in this Tukey-Anscombe plot. The residuals are positive, then negative, then positive, as the fitted <span class="math inline">\(y\)</span> value gets larger.</p>
<p>The red line in the Tukey-Anscombe plot is a loess smooth. It is automatically added to the plot. It is a way of estimating the pattern in the residuals. If the red line is not flat, then there is a pattern in the residuals. However, the loess smooth is not always reliable. It is a good idea to look at the residuals directly, without this smooth.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>The data here is simulated to show a very clear pattern in the residuals. In real data, the pattern might not be so clear. But if you suspect you see a pattern in the residuals, it could be a sign that the relationship between the independent and dependent variable is not linear.</p>
<p>Here is the Tukey-Anscombe plot for the blood pressure data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>There is very little evidence of any pattern in the residuals. This data is simulated with a truly linear relationship, so we would not expect to see any pattern in the residuals.</p>
</section>
<section id="e-no-outliers" class="level3">
<h3 class="anchored" data-anchor-id="e-no-outliers">(e) No outliers</h3>
<p>An outlier is a data point that is very different from the other data points. Outliers can have a big effect on the results of a regression analysis. They can pull the line of best fit towards them, and make the line of best fit a poor representation of the data.</p>
<p>Lets again look at the blood pressure versus age data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>There are no obvious outliers in this data. The data points are all close to the line of best fit. This is a good sign that the line of best fit is a good representation of the data.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#odd-data)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Where on this graph would you expect to see particularly influential outliers? Influential in the sense that they would have a large effect on the slope of the line of best fit.</p>
</div>
</div>
<p>Data points that are far from the mean of the independent variable have a large effect on the value of the slope. These data points have a large leverage. They are data points that are far from the other data points in the <span class="math inline">\(x\)</span> direction.</p>
<p>We can think of this with the analogy of a seesaw. The slope of the line of best fit is like the pivot point of a seesaw. Data points that are far from the pivot point have a large effect on the slope. Data points that are close to the pivot point have a small effect on the slope.</p>
<p>A measure of distance from the pivot point is called the <span class="math inline">\(leverage\)</span> of a data point. In simple regression, the leverage of individual <span class="math inline">\(i\)</span> is defined as</p>
<p><span class="math inline">\(h_{i} = (1/n) + (x_i-\overline{x})^2 / SSX\)</span>.</p>
<p>where <span class="math inline">\(SSX = \sum_{i=1}^n (x_i - \overline{x})^2\)</span>. (<strong>S</strong>um of <strong>S</strong>quares of <strong><span class="math inline">\(X\)</span></strong>)</p>
<p>So, the leverage of a data point is inversely related to <span class="math inline">\(n\)</span> (the number of data points). The leverage of a data point is also inversely related to the sum of the squares of the <span class="math inline">\(x\)</span> values. The leverage of a data point is directly related to the square of the distance of the <span class="math inline">\(x\)</span> value from the mean of the <span class="math inline">\(x\)</span> values.</p>
<p>More intuitively perhaps, the leverage of a data point will be greater when the are fewer other data points. It will also be greater when the distance from the mean value of <span class="math inline">\(x\)</span> is greater.</p>
<p>Going back to the analogy of a seesaw, with data points as children on the seesaw, the leverage of a data point is like the distance from the pivot a child sits. But we also have children of different weights. A lighter child will have less effect on the tilt of the seesaw. A heavier one will have a greater effect on the tilt. A heavier child sitting far from the pivot will have a very large effect.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#like-weight)
</div>
</div>
<div class="callout-body-container callout-body">
<p>What quantity that we already experienced is like the weight of the child?</p>
</div>
</div>
<p>The size of the residuals are like the weight of the child. Data points with large residuals have a large effect on the slope of the line of best fit. Data points with small residuals have a small effect on the slope of the line of best fit.</p>
<p>So the overall effect of a data point on the slope of the line of best fit is a combination of the leverage and the residual. This quantity is called the <span class="math inline">\(influence\)</span> of a data point.</p>
<p>Lets add a rather extreme data point to the blood pressure versus age data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-30-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>This is a bit ridiculous, but it is a good example of an outlier. The data point is far from the other data points. It has a large residual. And it is a long way from the pivot (the middle of the <span class="math inline">\(x\)</span> data) so has large leverage.</p>
<p>We can make a histogram of the residuals and see that the outlier has a large residual:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-31-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>And we can see that the leverage is large.</p>
<p>There is a graph that we can look at to see the influence of a data point. This is called a <span class="math inline">\(Cook's\)</span> <span class="math inline">\(distance\)</span> plot. The Cook’s distance of a data point is a measure of how much the slope of the line of best fit changes when that data point is removed. The Cook’s distance of a data point is defined as</p>
<p><span class="math inline">\(D_i = \sum_{j=1}^n (\hat{y}_j - \hat{y}_{j(i)})^2 / (p \times MSE)\)</span>.</p>
<p>where <span class="math inline">\(\hat{y}_j\)</span> is the predicted value of the dependent variable for data point <span class="math inline">\(j\)</span>, <span class="math inline">\(\hat{y}_{j(i)}\)</span> is the predicted value of the dependent variable for data point <span class="math inline">\(j\)</span> when data point <span class="math inline">\(i\)</span> is removed, <span class="math inline">\(p\)</span> is the number of parameters in the model (2 in this case), <span class="math inline">\(MSE\)</span> is the mean squared error of the model.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-32-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>But does it have a large influence on the value of the slope?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-33-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>The outlier has a large leverage. It is far from the pivot. But it does not have such a large effect (influence) on the slope. This is in large part because there are a lot data points (100) that are quite tightly arranged around the regression line.</p>
<section id="graphical-illustration-of-the-leverage-effect" class="level4">
<h4 class="anchored" data-anchor-id="graphical-illustration-of-the-leverage-effect">Graphical illustration of the leverage effect</h4>
<p>Data points with <span class="math inline">\(x_i\)</span> values far from the mean have a stronger leverage effect than when <span class="math inline">\(x_i\approx \overline{x}\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-34-1.png" class="img-fluid" width="768"></p>
</div>
</div>
<p>The outlier in the middle plot “pulls” the regression line in its direction and has large influence on the slope.</p>
</section>
<section id="leverage-plot-hebelarm-diagramm" class="level4">
<h4 class="anchored" data-anchor-id="leverage-plot-hebelarm-diagramm">Leverage plot (Hebelarm-Diagramm)</h4>
<p>In the leverage plot, (standardized) residuals <span class="math inline">\(\tilde{R_i}\)</span> are plotted against the leverage <span class="math inline">\(H_{ii}\)</span> :</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-35-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Critical ranges are the top and bottom right corners!!</p>
<p>Here, observations 71, 85, and 87 are labelled as potential outliers.</p>
<p>Some texts will give a rule of thumb that points with Cook’s distances greater than 1 should be considered influential, while others claim a reasonable rule of thumb is <span class="math inline">\(4 / ( n - p - 1 )\)</span> where <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(p\)</span> is the number of <span class="math inline">\(beta\)</span> parameters.</p>
</section>
</section>
</section>
<section id="what-can-go-wrong-during-the-modeling-process" class="level2">
<h2 class="anchored" data-anchor-id="what-can-go-wrong-during-the-modeling-process">What can go “wrong” during the modeling process?</h2>
<p>Answer: a lot of things!</p>
<ul>
<li>Non-linearity. We assumed a linear relationship between the response and the explanatory variables. But this is not always the case in practice. We might find that the relationship is curved and not well represtented by a straight line.</li>
<li>Non-normal distribution of residuals. The QQ-plot data might deviate from the straight line so much that we get worried!</li>
<li>Heteroscadisticity (non-constant variance). We assumed homoscadisticity, but the residuals might show a pattern.</li>
<li>Data point with high influence. We might have a data point that has a large influence on the slope of the line of best fit.</li>
</ul>
<section id="what-to-do-when-things-go-wrong" class="level3">
<h3 class="anchored" data-anchor-id="what-to-do-when-things-go-wrong">What to do when things “go wrong”?</h3>
<ol type="1">
<li>Now: Transform the response and/or explanatory variables.</li>
<li>Now: Take care of outliers.</li>
<li>Later in the course: Improve the model, e.g., by adding additional terms or interactions.</li>
<li>Later in the course: Use another model family (generalized or nonlinear regression model).</li>
</ol>
</section>
<section id="dealing-with-non-linearity" class="level3">
<h3 class="anchored" data-anchor-id="dealing-with-non-linearity">Dealing with non-linearity</h3>
<p>Here’s another example of <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> that are not linearly related:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-36-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>One way to deal with this is to transform the response variable <span class="math inline">\(Y\)</span>. Here we try two different transformations: <span class="math inline">\(\log_{10}(Y)\)</span> and <span class="math inline">\(\sqrt{Y}\)</span>.</p>
<p>Square root transform of the response variable <span class="math inline">\(Y\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-37-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Not great.</p>
<p>Log transformation of the response variable <span class="math inline">\(Y\)</span>:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-38-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Nope. Still some evidence of non-linearity.</p>
<p>What about transforming the explanatory variable <span class="math inline">\(X\)</span> as well?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-39-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s look at the four diagnostic plots for the log-log-transformed data:</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="3-regression_files/figure-html/unnamed-chunk-40-1.png" class="img-fluid figure-img" style="width:7cm"></p>
</figure>
</div>
</div>
</div>
<p>All looks pretty good. Pat on the back for us!</p>
<p>But… how to know which transformation to use…? It’s a bit of trial and error. But we can use the diagnostic plots to help us.</p>
<p><strong>Very very important</strong> is that we do this trial and error before we start using the model. E.g., we don’t want to jump from the aeroplane and then find out that our parachute is not working properly! And then try to fix the parachute while we are falling….</p>
<p>Likewise, we must not start using the model and then try to fix it. We need to make sure our model is in good working order before we start using it.</p>
<p>One of the traps we could fall into is called “p-hacking”. This is when we try different transformation until we find one that gives us the <strong>result we want</strong>, for example significant relationship. This is a big no-no in statistics. We need to decide on the model (including any transformations) before we start using it.</p>
</section>
<section id="common-transformations" class="level3">
<h3 class="anchored" data-anchor-id="common-transformations">Common transformations</h3>
<p>Which transformations could be considered? There is no simple answer. But some guidelines. E.g. if we see non-linearity and increasing variance with increasing fitted values, then a log transform may improve matter.</p>
<p>Some common and useful transformations are:</p>
<ul>
<li>The log transformation for concentrations and absolute values.</li>
<li>The square-root transformation for count data.</li>
<li>The arcsin square-root <span class="math inline">\(\arcsin(\sqrt{\cdot})\)</span> transformation for proportions/percentages.</li>
</ul>
<p>Transformations can also be applied on explanatory variables, as we saw in the example above.</p>
</section>
<section id="outliers" class="level3">
<h3 class="anchored" data-anchor-id="outliers">Outliers</h3>
<p>What do we do when we identify the presence of one or more outliers?</p>
<ol type="1">
<li>Start by checking the “correctness” of the data. Is there a typo or a decimal point that was shifted by mistake? Check both the response and explanatory variables.</li>
<li>If not, ask whether the model could be improved. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?</li>
<li>Sometimes, an outlier may be the most interesting observation in a dataset! Was the outlier created by some interesting but different process from the other data points?</li>
<li>Consider that outliers can also occur just by chance!</li>
<li>Only if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.</li>
</ol>
</section>
<section id="removing-outliers" class="level3">
<h3 class="anchored" data-anchor-id="removing-outliers">Removing outliers</h3>
<p>It might seem tempting to remove observations that apparently don’t fit into the picture. However:</p>
<ul>
<li>Do this <strong>only with greatest care</strong> e.g., if an observation has extremely implausible values!<br>
</li>
<li>Before deleting outliers, check points 1-5 above.</li>
<li>When removing outliers, <strong>you must mention this in your report</strong>.</li>
</ul>
<p>During the course we’ll see many more examples of things going at least a bit wrong. And we’ll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do in the next lesson. But before we wrap up, some good news…</p>
</section>
</section>
<section id="sec-kind-of-magic" class="level2">
<h2 class="anchored" data-anchor-id="sec-kind-of-magic">Its a kind of magic…</h2>
<p>Above, we learned about linear regression, the equation for it, how to estimate the coefficients, and how to check the assumptions. There was a lot of information, and it might seem a bit overwhelming.</p>
<p>You might also be aware that there are quite a few other types of statistical model, such as multiple regression, t-test, ANOVA, two-way ANOVA, and ANCOVA. It could be worrying to think that you need to learn so much new information for each of these types of tests.</p>
<p>But this is where the kind-of-magic happens. The good news is that the linear regression model is a special case of what is called a <em>general linear model</em>, or just <em>linear model</em> for short. And that all the tests mentioned above are also types of <em>linear model</em>. So, once you have learned about linear regression, you have learned a lot about linear models, and therefore also a lot about all of these other tests as well.</p>
<p>Moreover, the same function in R ‘lm’ is used to make all those statistical models Awesome.</p>
<section id="so-what-is-a-linear-model" class="level3">
<h3 class="anchored" data-anchor-id="so-what-is-a-linear-model">So what is a linear model?</h3>
<p>A linear model is a model where the relationship between the dependent variable and the independent variables is linear. That is, the dependent variable can be expressed as a linear combination of the independent variables. An example of a linear model is:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon\]</span></p>
<p>where: <span class="math inline">\(y\)</span> is the dependent variable, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1, \beta_2, \ldots, \beta_p\)</span> are the coefficients of the independent variables, <span class="math inline">\(x_1, x_2, \ldots, x_p\)</span> are the independent variables, <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<p>In contrast, a non-linear model is a model where the relationship between the dependent variable and the independent variables is non-linear. An example of a non-linear model is the exponential growth model:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 e^{\beta_2 x} + \epsilon\]</span></p>
<p>where: y is the dependent variable, <span class="math inline">\(\beta_0\)</span> is the intercept, <span class="math inline">\(\beta_1, \beta_2\)</span> are the coefficients of the independent variables, <span class="math inline">\(x\)</span> is the independent variable, <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<p>Keep in mind that a model with a quadratic term is still a linear model. For example:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x^2 + \epsilon\]</span></p>
<p>is still a linear model. We can see this if we substitute <span class="math inline">\(x^2\)</span> with a new variable <span class="math inline">\(x_2\)</span>, where <span class="math inline">\(x_2 = x^2\)</span>. The model then becomes:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\]</span></p>
<p>This is clearly a linear model.</p>
</section>
</section>
<section id="end-of-l3-start-of-l4" class="level2">
<h2 class="anchored" data-anchor-id="end-of-l3-start-of-l4">End of L3 / Start of L4</h2>
</section>
<section id="regression-continued" class="level2">
<h2 class="anchored" data-anchor-id="regression-continued">Regression continued</h2>
</section>
<section id="overview-of-this-week-l4" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-this-week-l4">Overview of this week (L4)</h2>
<p>Now that we have a satisfactory model, we can start to use it. In the following material, you will learn:</p>
<ul>
<li>How to measure how good is the regression (correlation and <span class="math inline">\(R^2\)</span>).</li>
<li>How to test if the parameter estimates are compatible with some specific value (<span class="math inline">\(t\)</span>-test).</li>
<li>How to find the range of parameters values are compatible with the data (confidence intervals).</li>
<li>How to find the regression lines compatible with the data (confidence band).</li>
<li>How to calculate plausible values of newly collected data (prediction band).</li>
</ul>
<section id="accompanying-reading-material" class="level3">
<h3 class="anchored" data-anchor-id="accompanying-reading-material">Accompanying reading material</h3>
</section>
</section>
<section id="how-good-is-the-regression-model" class="level2">
<h2 class="anchored" data-anchor-id="how-good-is-the-regression-model">How good is the regression model?</h2>
<p>What would a good regression model look like? What would a bad one look like? One could say that a good regression model is one that explains the dependent variable well. But what could we mean by “explains the data well”?</p>
<p>Take these two examples.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-41-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#better-model)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In which of these two would you say the model is better, and in which is it worse?</p>
</div>
</div>
<p>The first model seems to fit the data well, while the second one does not. But how can we quantify this?</p>
<p>Let’s say that we will measure the goodness of the model by the amount of variability of the dependent variable that is explained by the independent variable. To do this we need to do the following:</p>
<ol type="1">
<li>Measure the total variability of the dependent variable (total sum of squares, <span class="math inline">\(SST\)</span>).</li>
<li>Measure the amount of variability of the dependent variable that is explained by the independent variable (model sum of squares, <span class="math inline">\(SSM\)</span>).</li>
<li>Measure the variability of the dependent variable that is not explained by the independent variable (error sum of squares, <span class="math inline">\(SSE\)</span>).</li>
<li>Calculate the proportion of variability of the dependent variable that is explained by the independent variable (<span class="math inline">\(R^2\)</span>, pronounced as “r-squared”) (also known as the coefficient of determination) (<span class="math inline">\(R^2\)</span> = <span class="math inline">\(SSM/SST\)</span>).</li>
</ol>
<p><strong>Importantly, note that we will calculate <span class="math inline">\(SSM\)</span> and <span class="math inline">\(SSE\)</span> so that they sum up to <span class="math inline">\(SST\)</span>. I.e., <span class="math inline">\(SST = SSM + SSE\)</span>. That is, the total variability is the sum of what is explained by the model and what remains unexplained.</strong></p>
<p>Let’s take each in turn:</p>
<section id="sst" class="level3">
<h3 class="anchored" data-anchor-id="sst"><span class="math inline">\(SST\)</span></h3>
<p><strong>1. The total variability of the dependent variable is the sum of the squared differences between the dependent variable and its mean. This is called the total sum of squares (<span class="math inline">\(SST\)</span>).</strong></p>
<p><span class="math display">\[SST = \sum_{i=1}^{n} (y_i - \bar{y})^2\]</span></p>
<p>where: <span class="math inline">\(y_i\)</span> is the dependent variable, <span class="math inline">\(\bar{y}\)</span> is the mean of the dependent variable, <span class="math inline">\(n\)</span> is the number of observations.</p>
<p><strong>Note that sometimes <span class="math inline">\(SST\)</span> is referred to as <span class="math inline">\(SSY\)</span> (sum of squares of <span class="math inline">\(y\)</span>).</strong></p>
<p>Graphically, this is the sum of the square of the blue residuals as shown in the following graph, where the horizontal dashed line is at the value of the mean of the dependent variable.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-42-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>We can calculate this in R as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>SST <span class="ot">&lt;-</span> <span class="fu">sum</span>((y1 <span class="sc">-</span> <span class="fu">mean</span>(y1))<span class="sc">^</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="ssm-and-sse" class="level3">
<h3 class="anchored" data-anchor-id="ssm-and-sse">SSM and SSE</h3>
<p>Now the next two steps, that is getting the model sum of squares (SSM) and the error sum of squares (SSE) are a bit more complicated. To do this we need to fit a regression model to the data. Let’s see this graphically, and divide the data into the explained and unexplained parts.</p>
<p>Make a graph with vertical lines connecting the data to the mean of the data, but with each line two parts, one from the mean to the data, and one from the data to the predicted value.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-44-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>In this graph, the square of the length of the green lines is the model sum of squares (<span class="math inline">\(SST\)</span>). The square of the length of the red lines is the error sum of squares (<span class="math inline">\(SSE\)</span>).</p>
<p>In a better model the length of the green lines will be <strong>longer</strong> (the square of these gives the <span class="math inline">\(SMM\)</span>, the variability explained by the model). And the length of the red lines will be <strong>shorter</strong> (the square of these gives the <span class="math inline">\(SSE\)</span>, the variability not explained by the model).</p>
</section>
<section id="ssm" class="level3">
<h3 class="anchored" data-anchor-id="ssm"><span class="math inline">\(SSM\)</span></h3>
<p>Next we will do the second step, that is calculate the model sum of squares (<span class="math inline">\(SSM\)</span>).</p>
<p><strong>2. The amount of variability of the dependent variable that is explained by the independent variable is called the model sum of squares (<span class="math inline">\(SSM\)</span>).</strong></p>
<p>This is the difference between the predicted value of the dependent variable and the mean of the dependent variable, squared and summed:</p>
<p><span class="math display">\[SSE = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2\]</span></p>
<p>where: <span class="math inline">\(\hat{y}_i\)</span> is the predicted value of the dependent variable,</p>
<p>In R, we calculate this as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>y1_predicted <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>SSM <span class="ot">&lt;-</span> <span class="fu">sum</span>((y1_predicted <span class="sc">-</span> <span class="fu">mean</span>(y1))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>SSM</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 306.4576</code></pre>
</div>
</div>
</section>
<section id="sse" class="level3">
<h3 class="anchored" data-anchor-id="sse"><span class="math inline">\(SSE\)</span></h3>
<p>Third, we calculate the error sum of squares (<span class="math inline">\(SSE\)</span>) with either of two methods. We could calculate it as the sum of the squared residuals, or as the difference between the total sum of squares and the model sum of squares:</p>
<p><span class="math display">\[SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = SST - SSM\]</span> Let’s calculate this in R uses both approaches:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>SSE <span class="ot">&lt;-</span> <span class="fu">sum</span>((y1 <span class="sc">-</span> y1_predicted)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>SSE</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 8.163797</code></pre>
</div>
</div>
<p>Or…</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>SSE <span class="ot">&lt;-</span> SST <span class="sc">-</span> SSM</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>SSE</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 8.163797</code></pre>
</div>
</div>
</section>
<section id="r2" class="level3">
<h3 class="anchored" data-anchor-id="r2"><span class="math inline">\(R^2\)</span></h3>
<p>Finally, we calculate the proportion of variability of the dependent variable that is explained by the independent variable (<span class="math inline">\(R^2\)</span>):</p>
<p><span class="math display">\[R^2 = \frac{SSM}{SST}\]</span></p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.974052</code></pre>
</div>
</div>
</section>
<section id="is-my-r-squared-good" class="level3">
<h3 class="anchored" data-anchor-id="is-my-r-squared-good">Is my R squared good?</h3>
<p>What value of <span class="math inline">\(R^2\)</span> is considered good? In ecological research, <span class="math inline">\(R^2\)</span> values are often low (less than 0.3), because ecological systems are complex and many factors influence the dependent variable. However, in other fields, such as physiology, <span class="math inline">\(R^2\)</span> values are often higher. Therefore, the answer of what values of <span class="math inline">\(R^2\)</span> are good depends on the field of research.</p>
<p>Here are the four examples and their r-squared.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-49-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="questions" class="level3">
<h3 class="anchored" data-anchor-id="questions">Questions</h3>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#what-minimised)
</div>
</div>
<div class="callout-body-container callout-body">
<p>What is minimised when we fit a regression model? And therefore what is maximised?</p>
</div>
</div>
</section>
</section>
<section id="how-unlikey-is-the-observed-data-given-the-null-hypothesis" class="level2">
<h2 class="anchored" data-anchor-id="how-unlikey-is-the-observed-data-given-the-null-hypothesis">How unlikey is the observed data given the null hypothesis?</h2>
<p>(We often hear this expressed as “is the relationship significant?”)</p>
<p>What is a meaningful null hypothesis for a regression model?</p>
<p>Often we are interested in whether there is a relationship between the dependent and independent variable. Therefore, the null hypothesis is that there is no relationship between the dependent and independent variable. This means that the null hypothesis is that the slope of the regression line is zero.</p>
<p>Recall the regression model: <span class="math display">\[y = \beta_0 + \beta_1 x + \epsilon\]</span></p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#null-hypothesis)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Write down the null hypothesis of no relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
</div>
</div>
<p>The null hypothesis is that the slope of the regression line is zero: <span class="math display">\[H_0: \beta_1 = 0\]</span></p>
<p>What is the alternative hypothesis?</p>
<p><span class="math display">\[H_1: \beta_1 \neq 0\]</span></p>
<p>So, how do we test the null hypothesis?</p>
<p>Let’s use randomisation as a method to understand how likely we are to observe the data we have, given the null hypothesis is true.</p>
<p>If the null hypothesis is true, we expect no relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. Therefore, we can shuffle the <span class="math inline">\(y\)</span> values and fit a regression model to the shuffled data. We can repeat this many times and calculate the slope of the regression line each time. This will give us a distribution of slopes we would expect to observe if the null hypothesis is true.</p>
<p>First, we’ll make some data and get the slope of the regression line. Here is the observed slope and relationship:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>        x 
0.1251108 </code></pre>
</div>
</div>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-51-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Now we’ll use randomisation to test the null hypothesis. We can create lots of examples where the relationship is expected to have a slope of zero by shuffling randomly the <span class="math inline">\(y\)</span> values. Here are 20:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-52-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>Now let’s create 19 and put the real one in there somewhere random. Here’s a case where the real data has a quite strong relationship:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-53-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>We can confidently find the real data amont the shuffled data. But what if the relationship is weaker?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-54-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>Now its less clear which is the real data. We can use this idea to test the null hypothesis.</p>
<p>We do the same procedure of but instead of just looking at the graphs, we calculate the slope of the regression line each time. This gives us a distribution of slopes we would expect to observe if the null hypothesis is true. We can then see where the observed slope lies in this distribution of null hypothesis slopes.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-55-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>We can now calculate the probability of observing the data we have, given the null hypothesis is true.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.0172</code></pre>
</div>
</div>
<p>When we do linear regression we usually don’t use this randomisation approach. Instead, we can use the fact that the the slope of the regression line is an estimate of the true slope. This estimate has uncertainty associated with it. We can use this uncertainty to calculate the probability of observing the data we have, given the null hypothesis is true.</p>
<p>We can see that the slope estimate (the <span class="math inline">\(x\)</span> row) has uncertainty by looking at the regression output:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>              Estimate Std. Error
(Intercept) -0.3640385   1.842874
x            0.1251108   0.031682</code></pre>
</div>
</div>
<p>The estimate is the mean of the distribution of the parameter (slope) and the standard error is a measure of the uncertainty of the estimate.</p>
<p>The standard error is calculated as:</p>
<p><span class="math display">\[\sigma^{(\beta_1)} = \sqrt{ \frac{\hat\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}}\]</span></p>
<p>Where <span class="math inline">\(\hat\sigma^2\)</span> is the expected residual variance of the model. This is calculated as:</p>
<p><span class="math display">\[\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n-2}\]</span></p>
<p>Where <span class="math inline">\(\hat y_i\)</span> is the predicted value of <span class="math inline">\(y_i\)</span> from the regression model.</p>
<p>OK, let’s take a look at this intuitively. We have the the estimate of the slope and the standard error of the estimate.</p>
<p>Here is a graph of the value of the slope estimate versus the standard error of the estimate:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-58-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Think, Pair, Share (#chance-area)
</div>
</div>
<div class="callout-body-container callout-body">
<p>In what areas of the graph is the slope estimate more likely to have been observed by chance? And what regions is it less likely to have been observed by chance?</p>
</div>
</div>
<p>When the slope estimate is larger, it is less likely to have been observed by chance. And when the standard error is larger, it is more likely to have been observed by chance. How can we put these together into a single measure?</p>
<p>If we divide the slope estimate by the standard error, we get a measure of how many standard errors the slope estimate is from the null hypothesis slope of zero. This is the <span class="math inline">\(t\)</span>-statistic:</p>
<p><span class="math display">\[t = \frac{\hat\beta_1 - \beta_{1,H_0}}{\sigma^{(\beta_1)}}\]</span></p>
<p>Where <span class="math inline">\(\beta_{1,H_0}\)</span> is the null hypothesis value of the slope, usually zero, so that</p>
<p><span class="math display">\[t = \frac{\hat\beta_1}{\sigma^{(\beta_1)}}\]</span></p>
<p><strong>The <span class="math inline">\(t\)</span>-statistic is a measure of how many standard errors the slope estimate is from the null hypothesis value of the slope. The larger the <span class="math inline">\(t\)</span>-statistic, the less likely the slope estimate was observed by chance.</strong></p>
<p>How can we transform the value of a <span class="math inline">\(t\)</span>-statistic into a p-value? We can use the <strong><span class="math inline">\(t\)</span>-distribution</strong>, which quantifies the probability of observing a value of the <span class="math inline">\(t\)</span>-statistic under the null hypothesis.</p>
<p>But what is the <span class="math inline">\(t\)</span>-distribution? It is a distribution of the <span class="math inline">\(t\)</span>-statistic under the null hypothesis. It is a bell-shaped distribution that is centered on zero. The shape of the distribution is determined by the degrees of freedom, which is <span class="math inline">\(n-2\)</span> for a simple linear regression model.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-59-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>By the way, it is named the <span class="math inline">\(t\)</span>-distribution by it’s developer, William Sealy Gosset, who worked for the Guinness brewery in Dublin, Ireland. In his 1908 paper, Gosset introduced the <span class="math inline">\(t\)</span>-distribution but he didn’t explicitly explain his choice of the letter <span class="math inline">\(t\)</span>. The choice of the letter <span class="math inline">\(t\)</span> could be to indicate “Test”, as the <span class="math inline">\(t\)</span>-distribution was developed specifically for hypothesis testing.</p>
<p>Now, recall that the p-value is the probability of observing the value of the test statistic (so here the <span class="math inline">\(t\)</span>-statistic) at least as extreme as the one we have, given the null hypothesis is true. We can calculate this probability by integrating the <span class="math inline">\(t\)</span>-distribution from the observed <span class="math inline">\(t\)</span>-statistic to the tails of the distribution.</p>
<p>Here is a graph of the <span class="math inline">\(t\)</span>-distribution with 100 degrees of freedom with the tails of the distribution shaded so that the area of the shaded region is 0.05 (i.e., 5% of the total area).</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-60-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Graph of the <span class="math inline">\(t\)</span>-distribution with 1000 degrees of freedom (blue line) and the normal distribution (green dashed line):</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-61-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>So, with a large number of observations, the <span class="math inline">\(t\)</span>-distribution approaches the normal distribution. For the normal distribution, the 95% area is between -1.96 and 1.96.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="do">## x value for 95% area of normal distribution</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>x_value <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x_value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.959964</code></pre>
</div>
</div>
<p><code>qnorm</code> is a function that calculates the <span class="math inline">\(x\)</span> value for a given quantile (probability) of the normal distribution. In simpler terms, it finds the value <span class="math inline">\(x\)</span> at which the area under the normal curve (up to <span class="math inline">\(x\)</span>) equals the given probability <span class="math inline">\(p\)</span> (0.975 in the example immediately above here).</p>
<p>Let’s go back to the age - blood pressure data and calculate the p-value for the slope estimate.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-65-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Here’s the model:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>mod1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Systolic_BP <span class="sc">~</span> Age, <span class="at">data =</span> bp_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here we calculate the <span class="math inline">\(t\)</span>-statistic for the slope estimate:</p>
<p>And here we calculate the one-tailed and two-tailed <span class="math inline">\(p\)</span>-values:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>         Age 
3.746958e-51 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>         Age 
7.493917e-51 </code></pre>
</div>
</div>
<p>We can get the <span class="math inline">\(p\)</span>-value directly from the <code>summary</code> function:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>    Estimate   Std. Error      t value     Pr(&gt;|t|) 
8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 </code></pre>
</div>
</div>
<p>Conclusion: there is <strong>very strong evidence</strong> that the blood pressure is associated with age, because the <span class="math inline">\(p\)</span>-value is extremely small (thus it is very unlikely that the observed slope value or a large one would be seen if there was really no association). Thus, we can reject the null hypothesis that the slope is zero.</p>
<p>This basically answers question 1: “Are the parameters compatible with some specific value?”</p>
<section id="recap-formal-definition-of-the-p-value" class="level3">
<h3 class="anchored" data-anchor-id="recap-formal-definition-of-the-p-value">Recap: Formal definition of the <span class="math inline">\(p\)</span>-value</h3>
<p>The formal definition of <span class="math inline">\(p\)</span>-value is the probability to observe a data summary (e.g., an average or a slope) that is at least as extreme as the one observed, given that the null hypothesis is correct.</p>
<p>Example (normal distribution): Assume that we calculated that <span class="math inline">\(t\)</span>-value = -1.96</p>
<p><span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(Pr(|t|\geq 1.96)=0.05\)</span> and <span class="math inline">\(Pr(t\leq-1.96)=0.025\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-70-1.png" class="img-fluid" width="768"></p>
</div>
</div>
</section>
<section id="a-cautionary-note-on-the-use-of-p-values" class="level3">
<h3 class="anchored" data-anchor-id="a-cautionary-note-on-the-use-of-p-values">A cautionary note on the use of <span class="math inline">\(p\)</span>-values</h3>
<p>Maybe you have seen that in statistical testing, often the criterion <span class="math inline">\(p\leq 0.05\)</span> is used to test whether <span class="math inline">\(H_0\)</span> should be rejected. This is often done in a black-or-white manner. However, we will put a lot of attention to a more reasonable and cautionary interpretation of <span class="math inline">\(p\)</span>-values in this course!</p>
</section>
</section>
<section id="how-strong-is-the-relationship" class="level2">
<h2 class="anchored" data-anchor-id="how-strong-is-the-relationship">How strong is the relationship?</h2>
<p>The actual value of the parameter has practical meaning. The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit. The slope is one measure of the strength of the relationship between the two variables.</p>
<p>We can ask what values of a parameter estimate are compatible with the data (confidence intervals)? To answer this question, we can determine the confidence intervals of the regression parameters.</p>
<p>The confidence interval of a parameter estimate is defined as the interval that contains the true parameter value with a certain probability. So the 95% confidence interval of the slope is the interval that contains the true slope with a probability of 95%.</p>
<p>We can then imagine two cases. The 95% confidence interval of the slope includes 0:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-71-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>Or where the confidence interval does not include zero:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-72-1.png" class="img-fluid" width="960"></p>
</div>
</div>
<p>How do we calculate the lower and upper limits of the 95% confidence interval of the slope?</p>
<p>Recall that the <span class="math inline">\(t\)</span>-value for a null hypothesis of slope of zero is defined as:</p>
<p><span class="math display">\[t = \frac{\hat\beta_1}{\hat\sigma^{(\beta_1)}}\]</span></p>
<p>The first step is to calculate the <span class="math inline">\(t\)</span>-value that corresponds to a p-value of 0.05. This is the <span class="math inline">\(t\)</span>-value that corresponds to the 97.5% quantile of the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<p><span class="math inline">\(t_{0.975} = t_{0.025} = 1.96\)</span>, for large <span class="math inline">\(n\)</span>.</p>
<p>The 95% confidence interval of the slope is then given by:</p>
<p><span class="math display">\[\hat\beta_1 \pm t_{0.975} \cdot \hat\sigma^{(\beta_1)}\]</span></p>
<p>In our blood pressure example the estimated slope is 0.8240678 and the standard error of the slope is 0.0277096. We can calculate the 95% confidence interval of the slope as follows:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>t_0975 <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> n <span class="sc">-</span> <span class="dv">2</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>half_interval <span class="ot">&lt;-</span> t_0975 <span class="sc">*</span> <span class="fu">summary</span>(mod1)<span class="sc">$</span>coef[<span class="dv">2</span>,<span class="dv">2</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>lower_limit <span class="ot">&lt;-</span> <span class="fu">coef</span>(mod1)[<span class="dv">2</span>] <span class="sc">-</span> half_interval</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>upper_limit <span class="ot">&lt;-</span> <span class="fu">coef</span>(mod1)[<span class="dv">2</span>] <span class="sc">+</span> half_interval</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>ci_slope <span class="ot">&lt;-</span> <span class="fu">c</span>(lower_limit, upper_limit)</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>slope <span class="ot">&lt;-</span> <span class="fu">coef</span>(mod1)[<span class="dv">2</span>]</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>slope</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Age 
0.8240678 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>ci_slope</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Age       Age 
0.7690791 0.8790565 </code></pre>
</div>
</div>
<p>R can do all this for us:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>    2.5 %    97.5 % 
0.7690791 0.8790565 </code></pre>
</div>
</div>
<p>Or we can do it using values from the coefficients table:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>coefs <span class="ot">&lt;-</span> <span class="fu">summary</span>(mod1)<span class="sc">$</span>coef</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> coefs[<span class="dv">2</span>,<span class="dv">1</span>]</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>sdbeta <span class="ot">&lt;-</span> coefs[<span class="dv">2</span>,<span class="dv">2</span>] </span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>beta <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>) <span class="sc">*</span> <span class="fu">qt</span>(<span class="fl">0.975</span>,<span class="dv">241</span>) <span class="sc">*</span> sdbeta </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.7694840 0.8786516</code></pre>
</div>
</div>
<p><em>Interpretation</em>: for an increase in the age by one year, roughly 0.82 mmHg increase in blood pressure is expected, and all true values for <span class="math inline">\(\beta_1\)</span> between 0.77 and 0.88 are compatible with the observed data.</p>
</section>
<section id="confidence-and-prediction-bands" class="level2">
<h2 class="anchored" data-anchor-id="confidence-and-prediction-bands">Confidence and Prediction Bands</h2>
<ul>
<li><p>Remember: If another sample from the same population was taken, the regression line would look slightly different.</p></li>
<li><p>There are two questions to be asked:</p></li>
</ul>
<ol type="1">
<li><p>Which other regression lines are compatible with the observed data? This leads to the <em>confidence band</em>.</p></li>
<li><p>Where do future observations (<span class="math inline">\(y\)</span>) with a given <span class="math inline">\(x\)</span> coordinate lie? This leads to the <em>prediction band</em>.</p></li>
</ol>
<p>Note: The prediction band is much broader than the confidence band.</p>
</section>
<section id="calculation-of-the-confidence-band" class="level2">
<h2 class="anchored" data-anchor-id="calculation-of-the-confidence-band">Calculation of the confidence band</h2>
<p>Given a fixed value of <span class="math inline">\(x\)</span>, say <span class="math inline">\(x_0\)</span>. The question is:</p>
<p>Where does <span class="math inline">\(\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0\)</span> lie with a certain confidence (i.e., 95%)?</p>
<p>This question is not trivial, because both <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> are estimates from the data and contain uncertainty.</p>
<p>The details of the calculation are given in Stahel 2.4b.</p>
<p>Plotting the confidence interval around all <span class="math inline">\(\hat y_0\)</span> values one obtains the <em>confidence band</em> or <em>confidence band for the expected values</em> of <span class="math inline">\(y\)</span>.</p>
<p>Note: For the confidence band, only the uncertainty in the estimates <span class="math inline">\(\hat\beta_0\)</span> and <span class="math inline">\(\hat\beta_1\)</span> matters.</p>
<p>Here is the confidence band for the blood pressure data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-76-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Very narrow confidence bands indicate that the estimates are very precise. In this case the estimated intercept and slope are precise because the sample size is large and the data points are close to the regression line.</p>
</section>
<section id="calculations-of-the-prediction-band" class="level2">
<h2 class="anchored" data-anchor-id="calculations-of-the-prediction-band">Calculations of the prediction band</h2>
<p>We can easily predicted an expected value of <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span> value. But we can also ask w where does a <em>future observation</em> lie with a certain confidence (i.e., 95%)?</p>
<p>To answer this question, we have to <em>consider not only the uncertainty in the predicted value caused by uncertainty in the parameter estimates</em> <span class="math inline">\(\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0\)</span>, but also the <em>error term</em> <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span>}.</p>
<p>This is the reason why the <strong>prediction band</strong> is wider than the confidence band.</p>
<p>Here’s a graph showing the prediction band for the blood pressure data:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="3-regression_files/figure-html/unnamed-chunk-78-1.png" class="img-fluid" width="480"></p>
</div>
</div>
<p>Another way to think of the 95% confidence band is that it is where we would expect 95% of the regression lines to lie if we were to collect many samples from the same population. The 95% prediction band is where we would expect 95% of the future observations to lie.</p>
</section>
<section id="that-is-regression-done-at-least-for-our-current-purposes" class="level2">
<h2 class="anchored" data-anchor-id="that-is-regression-done-at-least-for-our-current-purposes">That is regression done (at least for our current purposes)</h2>
<ul>
<li>Why use (linear) regression?</li>
<li>Fitting the line (= parameter estimation)</li>
<li>Is linear regression good enough model to use?</li>
<li>What to do when things go wrong?</li>
<li>Transformation of variables/the response.</li>
<li>Handling of outliers.</li>
<li>Goodness of the model: Correlation and <span class="math inline">\(R^2\)</span></li>
<li>Tests and confidence intervals</li>
<li>Confidence and prediction bands</li>
</ul>
</section>
<section id="additional-reading-material" class="level2">
<h2 class="anchored" data-anchor-id="additional-reading-material">Additional reading material</h2>
<p>If you’d like another perspective and a deeper delve into some of the mathematical details, please look at Chapter 2 of <em>Lineare Regression</em>, p.7-20 (Stahel script), Chapters 3.1, 3.2a-q of <em>Lineare Regression</em>, and Chapters 4.1 4.2f, 4.3a-e of <em>Lineare Regression</em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./2-data-wrangling.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Data wrangling (L2)</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./4-multiple-regression.html" class="pagination-link">
        <span class="nav-page-text">Multiple regression (L4)</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>