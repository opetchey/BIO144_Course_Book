[
  {
    "objectID": "1.1-intro.html",
    "href": "1.1-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Notation and some definitions\nThe first lecture of the course introduces it, gives some important information, and sets the stage for the rest of the course. Some of the time in the lecture will be used to create a dataset for use during the course. It also gives an opportunity to review some of the things about R and statistics that it is very useful to already know.\nThe lecture includes:\nThroughout the course, we will use the following notation:",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1.1-intro.html#notation-and-some-definitions",
    "href": "1.1-intro.html#notation-and-some-definitions",
    "title": "Introduction",
    "section": "",
    "text": "\\(x\\) for a variable. Typically this variable contains a set of observations. These observations are said to represent a sample of all the possible observations that could be made of a population.\n\\(x_1, x_2, \\ldots\\) for the values of a variable\n\\(x_i\\) for the \\(i\\)th value of a scalar variable. This is often spoken as “x sub i” or the “i-th value of x”.\n\\(x^{(1)}\\) for variable 1, \\(x^{(2)}\\) for variable 2, etc.\nThe mean of the sample \\(x\\) is \\(\\bar{x}\\). This is usually spoken as “x-bar”.\nThe mean of \\(x\\) is calculated as \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\).\n\\(n\\) is the number of observations in a sample.\nThe summation symbol \\(\\sum\\) is used to indicate that the values of \\(x\\) are summed over all values of \\(i\\) from 1 to \\(n\\).\nThe standard deviation of the sample is \\(s\\). The standard deviation of the population is \\(\\sigma\\).\nThe variance is \\(s^2\\). The variance of the population is \\(\\sigma^2\\).\nThe variance of the sample is calculated as \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\).\nThe standard deviation of the sample is calculated as \\(s = \\sqrt{s^2}\\).\n\\(y\\) is usually used to represent a dependent / response variable.\n\\(x\\) is usually used to represent an independent / predictor / explanatory variable.\n\\(\\beta_0\\) is usually used to denote the intercept of a linear model.\n\\(\\beta_1\\), \\(\\beta_2\\), etc. are usually used to denote the coefficients of the independent variables in a linear model.\nEstimates are denoted with a hat, so \\(\\hat{\\beta}_0\\) is the estimate of the intercept of a linear model.\nHence, the estimated value of \\(y_i\\) in a linear regression model is \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^{(1)}\\).\n\\(e_i\\) is the residual for the \\(i\\)th observation in a linear model. The residual is the difference between the observed value of \\(y_i\\) and the predicted value of \\(y_i\\) (\\(\\hat{y_i}\\)).\nOften we assume errors are normally distributed with mean 0 and variance \\(\\sigma^2\\). This is written as \\(e_i \\sim N(0, \\sigma^2)\\).\nSST is the total sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\).\nSSM is the model sum of squares. It is the sum of the squared differences between the predicted values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\).\nSSE is the error sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the predicted values of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\hat{y_i})^2\\).\nThe variance of \\(x\\) can be written as \\(Var(x)\\). The covariance between \\(x\\) and \\(y\\) can be written as \\(Cov(x, y)\\).\nCovariance is calculated as \\(Cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\).\n\\(H_0\\) is the null hypothesis.\n\\(\\alpha\\) is the significance level.\ndf is the degrees of freedom.\n\\(p\\) is the p-value.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "1.1-intro.html#using-generative-ai-in-r-and-data-analysis-guidance-and-good-practice",
    "href": "1.1-intro.html#using-generative-ai-in-r-and-data-analysis-guidance-and-good-practice",
    "title": "Introduction",
    "section": "Using Generative AI in R and Data Analysis: Guidance and Good Practice",
    "text": "Using Generative AI in R and Data Analysis: Guidance and Good Practice\n\n\n\n\n\n\nImportant\n\n\n\nFor the final examination, you will use your own computer, but the test will run inside the Safe Exam Browser, which will be configured to block all access to generative AI tools, browser-based assistants, external software, online services, and any AI code copilots inside RStudio or other IDEs. This means no form of generative AI will be available during the exam. Because of this, please avoid becoming overly reliant on GenAI—such as ChatGPT, Claude, Gemini, Copilot, or similar tools for answering quiz questions, explaining results, fixing errors, guiding your analysis, or writing code. You must be able to perform all tasks independently. We also strongly recommend that you do not use RStudio with Copilot integration during the course, as it will not function in the exam environment and may leave you under prepared. Throughout the semester, be sure to practice writing your own R code, interpreting outputs yourself, and applying statistical reasoning without AI assistance, as your exam performance will depend entirely on your own knowledge and skills.\n\n\nGenerative AI (GenAI) tools can support learning, exploration, and coding in R. They can be powerful assistants, but they must be used with care. This section introduces the types of tools available, provides guidelines for responsible use, highlights red flags for problematic usage, and gives examples of good and poor practice.\nTypical uses:\n\nasking conceptual questions\n\nsummarizing methods\n\ngenerating example code\n\nexplaining error messages\n\nStrengths:\n\nflexible and conversational\n\ngood for brainstorming\n\ncan generate starter code\n\nLimitations:\n\noften wrong in subtle ways\n\nmay hallucinate functions\n\ncannot see your working R session\n\n\nGuidelines for Good Use of Generative AI\n\nUse GenAI as a Helper, Not a Source of Truth\nBest uses:\n\ndrafting\n\nexplanation\n\nsyntax reminders\n\nscaffolding\n\nNot reliable for:\n\nmodel choice\n\nstatistical inference\n\ninterpreting coefficients\n\ndesigning analysis workflows\n\nchecking assumptions\n\n\n\nAlways Verify AI-Generated Code and Explanations\nCheck:\n\ndoes the code run?\n\ndo variable names match?\n\nis the model appropriate?\n\nare assumptions addressed?\n\nis the explanation logically correct?\n\n\n\nKeep Human Judgement Central\nGenAI cannot:\n\nunderstand scientific questions\n\nevaluate model assumptions\n\nknow ecological/biological reasoning\n\ndetermine appropriate models\n\n\n\nProvide Context Carefully\nWhen asking GenAI:\n\ndescribe variables\n\nprovide example data\n\nspecify your goal\n\nshow your existing code\n\nBetter context = better answers.\n\n\nUse GenAI to Improve Understanding, Not Bypass It\nHelpful:\n\n“Explain logistic regression.”\n\n“Why do residuals fan out?”\n\nNot helpful:\n\n“Do my assignment for me.”\n\n\n\n\nIndicators of Problematic Usage\n\nCode That Does Not Reflect Ability\nSigns:\n\nunfamiliar advanced syntax\n\nunexplained packages\n\ninconsistent style\n\n\n\nHallucinated Functions or Nonsensical Code\nExamples:\n\nslope(x) in mixed models\n\nmissing arguments\n\nfabricated packages\n\n\n\nStatistical Errors Typical of AI\nCommon issues:\n\nwrong model family\n\nwrong inference logic\n\ninvented assumptions\n\nincorrect explanation of coefficients\n\n\n\nLack of Understanding\nIndicators:\n\ncannot explain model\n\ninconsistent interpretations\n\nidentical phrasing to AI output\n\n\n\nOver-Reliance on AI\nSigns:\n\nusing AI for every step\n\nno debugging effort\n\nstagnation in skill development\n\n\n\n\nExamples of Good and Problematic Use\n\nGood Use Examples\nA. Syntax help\n“How do I specify a random slope in lme4?”\nB. Clarification\n“How does adding an interaction change interpretation?”\nC. Debugging\n“What does ‘object not found’ usually mean?”\nD. Brainstorming\n“How can I visualise a logistic regression?”\n\n\nProblematic Use Examples\nA. Blindly copying model code\nlm(y ~ x1 + x2 * x3 * x1)\nB. Incorrect statistical logic\nAI code labelled as a bootstrap but is actually a permutation test.\nC. Misleading interpretation\nClaims that coefficients assume predictor independence.\nD. Presenting AI-generated plots without understanding\nE. Outsourcing entire workflow\n“Write a script that loads data, cleans it, runs models, interprets, and writes the report.”\n\n\nSummary\nGenerative AI can:\n\nhelp learning\n\nsupport debugging\n\nprovide code scaffolds\n\nexplain concepts\n\nBut it can also:\n\nhallucinate\n\nproduce incorrect models\n\nmisinterpret statistics\n\nUse GenAI as a supportive tool—never as an unquestioned authority.\nGood use of GenAI supports learning. Problematic use replaces it.\n\n\n\nCommon GenAI Errors in R and Statistical Modelling\nGenerative AI tools can be helpful for writing R code, exploring ideas, and learning syntax.\nHowever, they sometimes produce plausible but incorrect code or explanations.\nThis section provides real examples of typical GenAI mistakes, with correct solutions and teaching points.\nWhy this matters: GenAI is a pattern-matching system, not a statistical reasoning engine. It does not understand assumptions, inference, or modelling logic.Therefore, students should never accept code or explanations without checking them.\n\n\nIncorrect formula structure in lm()\nPrompt: Fit a linear model with main effects and a two-way interaction between x2 and x3.\nIncorrect GenAI output:\nlm(y ~ x1 + x2 * x3 * x1, data = df)\nThis includes an unintended three-way interaction and extra terms.\nCorrect:\nlm(y ~ x1 + x2 * x3, data = df)\nTeaching point: Always check model formulas carefully. AI often adds or removes interactions.\n\n\nConfusing bootstrap and permutation tests\nDocumented case: GenAI was asked for a bootstrap t-test.\nIncorrect GenAI code (actually a permutation test):\nt_stats &lt;- replicate(1000, {\n  perm &lt;- sample(df$group)\n  t.test(df$value ~ perm)$statistic\n})\nCorrect bootstrap approach:\nt_stats &lt;- replicate(1000, {\n  sample_df &lt;- df[sample(nrow(df), replace = TRUE), ]\n  t.test(value ~ group, data = sample_df)$statistic\n})\nTeaching point: The logic of inference matters. Code that runs is not necessarily correct.\n\n\n\nIncorrect explanation of linear-model coefficients\nIncorrect claim: “Coefficients assume independence among predictors.”\nThis is false. Linear model coefficients describe conditional effects within the model, regardless of collinearity.\nTeaching point: Interpretations come from the model structure, not from simplistic assumptions GenAI sometimes invents.\n\n\n\nHallucinated functions in mixed models\nIncorrect GenAI output:\nlmer(y ~ x + (slope(x) | group), data = df)\nslope() does not exist.\nCorrect random-slope specification:\nlmer(y ~ x + (x | group), data = df)\nTeaching point: Always verify syntax in package documentation.\n\n\n\nWrong variable names\nThe dataset has variables height and age.\nIncorrect GenAI output:\nlm(Height ~ Age, data = df)\nCorrect:\nlm(height ~ age, data = df)\nTeaching point: GenAI often guesses variable names. Check against your data.\n\n\n\nWrong model family for binary data\nIncorrect GenAI output (linear regression):\nlm(y ~ x, data = df)\nCorrect logistic regression:\nglm(y ~ x, data = df, family = binomial)\nTeaching point: For binary response variables, specify the model family explicitly.\n\n\n\nIncorrect explanation of random intercepts\nIncorrect claim:\n“Random intercepts eliminate correlation among repeated measures.”\nIncorrect — they model correlation, not eliminate it.\nTeaching point: Random effects structure determines the implied correlation. AI explanations are often vague or wrong here.\n\n\n\nOmitting interaction terms in ANOVA\nPrompt: Two-way ANOVA with interaction.\nIncorrect:\naov(y ~ factor1 + factor2, data = df)\nCorrect:\naov(y ~ factor1 * factor2, data = df)\nTeaching point: Confirm that the model matches the experimental design.\n\n\n\nIncorrect use of predict()\nPrompt: Predict for new x values.\nIncorrect GenAI output:\npredict(model)\nThis gives in-sample fitted values, not predictions for new data.\nCorrect:\npredict(model, newdata = data.frame(x = c(1, 2, 3)))\nTeaching point: Always specify newdata for predictions.\n\n\n\nPoor explanations of multicollinearity\nIncorrect GenAI claim:\n“Multicollinearity is indicated when the model p-value is low but the individual predictor p-values are high.”\nThis is an unreliable and incomplete diagnostic.\nBetter diagnostics:\ncar::vif(model)\ncor(df)\nmodel.matrix(model)\nTeaching point: AI often repeats common internet tropes rather than robust statistical principles.\n\n\n\nSummary\nGenAI can:\n\nwrite useful scaffolding code,\n\nprovide quick reminders,\n\nhelp with simple tasks.\n\nBut it can also:\n\nhallucinate functions,\n\ngive subtly incorrect models,\n\ninvent statistical logic,\n\nprovide plausible but wrong explanations.\n\nAdvice for students:\nUse GenAI as a starting point, not an authority.\nAlways check:\n\nfunction names,\n\nmodel formulas,\n\nassumptions,\n\ninterpretations,\n\nand logic.\n\nIn statistics, clarity of reasoning matters more than code that merely runs.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html",
    "href": "2.1-R-and-RStudio.html",
    "title": "R and RStudio (L2)",
    "section": "",
    "text": "Getting R and RStudio\nR is a programming language and software environment for statistical computing and graphics. RStudio is an integrated development environment (IDE) for R. RStudio provides a user-friendly interface for working with R, including a console, a script editor, and tools for managing packages and projects.\nWe highly recommend using RStudio to work with R.\nThere are two ways to use RStudio:\nWhat do we recommend? Try the cloud first. If you like it then continue to use it.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-using-r-and-rstudio",
    "href": "2.1-R-and-RStudio.html#sec-using-r-and-rstudio",
    "title": "R and RStudio (L2)",
    "section": "",
    "text": "RStudio Desktop: a standalone application that you can install on your computer. If you choose this option, you will need to install R first, and then install RStudio. This usually requires administrator privileges on your computer. If you have problems installing add-on packages, they will have to be fixed by you or with our help (rarely we cannot find a solution). Follow the instructions on this website about how to install R and RStudio: https://posit.co/download/rstudio-desktop/.\nRstudio Cloud: a web-based version of RStudio that you can use in your web browser. You don’t need to install anything on your computer, and you can access your work from any computer with an internet connection. The Faculty of Science has a RStudio Cloud here that you can use (and will have to use during the final exam).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhether you use the RStudio application on your computer, or use RStudio on the Cloud, you are responsible for the safety and persistence of your files (data, code, etc.). Just because you’re using RStudio on the Cloud does not mean your files are automatically saved forever. Make sure to download and back up your important files regularly!",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-getting-to-know-the-rstudio-ide",
    "href": "2.1-R-and-RStudio.html#sec-getting-to-know-the-rstudio-ide",
    "title": "R and RStudio (L2)",
    "section": "Getting to know the RStudio IDE",
    "text": "Getting to know the RStudio IDE\nWhen you open RStudio, you will see a window with four main panes:\n\nSource pane: where you can write and edit R scripts, R Markdown documents, and other files. This pane can have multiple tabs, so you can have several files open at the same time.\nConsole pane: where you can type and execute R commands directly. This pane has multiple tabs, including: Console, Terminal, and Jobs. During this course we will mostly use the Console tab.\nEnvironment pane: where you can see the objects (data frames, vectors, etc.) that are currently in your R session. This pane has multiple tabs, including: Environment, History, Connections, and Tutorial. During this course we will mostly use the Environment tab.\nFiles/Plots/Packages/Help pane: where you can manage files, view plots, manage packages, and access help documentation. This pane has multiple tabs, including: Files, Plots, Packages, Help, and Viewer. During this course we will mostly use the Files, Plots, Packages, and Help tabs.\n\nOur Scripts are in the Source pane tabs. The code / script we write in R is usually saved in a file with the extension .R. This file can be opened and edited in the Source pane. Creating a new R script: File &gt; New File &gt; R Script.\nYou can run code from the script by selecting the code and clicking the “Run” button, or by using the keyboard shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac).\nThere is so much more to learn about the RStudio IDE, but we will cover that as we go along in the course.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-getting-to-know-r",
    "href": "2.1-R-and-RStudio.html#sec-getting-to-know-r",
    "title": "R and RStudio (L2)",
    "section": "Getting to know R",
    "text": "Getting to know R\nIn our newly opened script file, type the following code:\nThen type the following code:\n\n\n[1] 2\n\n\n[1] 7.389056\n\n\n[1] 4\n\n\nSelect all the code and run it (using the “Run” button or Ctrl + Enter / Cmd + Enter). You should see the results of the calculations in the Console pane.\nNow try assigning values to named object:\nAnd then print the value of c:\n\n\n[1] 15\n\n\nYou should see the value 15 printed in the Console pane.\nWe can also create vectors:\n\n\n[1] 1 2 3 4 5\n\n\nAnd do maths on vectors:\n\n\n[1]  2  4  6  8 10\n\n\n[1] 11 12 13 14 15\n\n\n[1]  1  4  9 16 25\n\n\nWe can vectors of strings (text):\n\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n\nAnd can perform operations on strings:\n\n\n[1] \"I like apple\"  \"I like banana\" \"I like cherry\"\n\n\n[1] \"APPLE\"  \"BANANA\" \"CHERRY\"\n\n\nAnd we can create data frames, which are like tables of data\n\n\n     Name Age Height\n1   Alice  25    165\n2     Bob  30    180\n3 Charlie  35    175\n\n\nAbove we have numerous examples of functions: exp(), sqrt(), c(), paste(), toupper(), and data.frame(). Functions are a fundamental part of R programming. They are used to perform specific tasks, such as calculations, data manipulation, and data analysis. All functions have a name and can take arguments (inputs) and return values (outputs). They are called by writing the function name followed by parentheses, with any arguments inside the parentheses.\nYou likely guessed that there is much much more to learn about R, but we will cover that as we go along in the course.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-getting-help",
    "href": "2.1-R-and-RStudio.html#sec-getting-help",
    "title": "R and RStudio (L2)",
    "section": "Getting help",
    "text": "Getting help\nR has a built-in help system that you can use to get information about functions, packages, and other topics. To access the help system, you can use the ? operator followed by the name of the function or topic you want to learn about. For example, to get help on the mean() function, you would type:\nThis will open the help documentation for the mean() function in the Help pane of RStudio. The documentation includes a description of the function, its arguments, and examples of how to use it. Some of the help documentation is very useful and accessible, other is less so. Over time you will learn which functions and packages have good documentation, and you will get better and better at understanding R help files.\nOf course you can use any other resources to get help with R, including online forums, tutorials, and books. Some popular online resources for R help include:\n\nStack Overflow\nRStudio Community\nR-bloggers\nThe R Graph Gallery\n\nYou can also use search engines like Google to find answers to your R questions. Just be sure to include “R” in your search query to get relevant results.\nAI assistants like ChatGPT can also be useful for getting help with R programming. You can ask specific questions about R code, functions, and packages, and get instant responses.\nAnd of course there is always your course instructors and fellow students to help you out when you get stuck.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-add-on-packages",
    "href": "2.1-R-and-RStudio.html#sec-add-on-packages",
    "title": "R and RStudio (L2)",
    "section": "Add-on packages",
    "text": "Add-on packages\nR has a vast ecosystem of add-on packages that extend its functionality. These packages are collections of functions, data, and documentation that can be installed and loaded into your R session. There are thousands of packages available on CRAN (the Comprehensive R Archive Network) and other repositories like Bioconductor and GitHub.\nWe will be using several packages throughout this course. To install a package, you can use the install.packages() function. For example, to install the ggplot2 package, you would type:\nYou can also use the RStudio interface to install packages. Go to the “Packages” tab in the bottom right pane, click on “Install”, type the name of the package you want to install, and click “Install”.\nYou can see which packages are currently installed by looking in the “Packages” tab.\n\n\n\n\n\n\nTip\n\n\n\nYou only need to install a package once. After it is installed, you can load it into your R session using the library() function. Do not install packages every time you want to use them; just load them with library().",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-r-versions",
    "href": "2.1-R-and-RStudio.html#sec-r-versions",
    "title": "R and RStudio (L2)",
    "section": "R Version and add-on package versions",
    "text": "R Version and add-on package versions\n(This section concerns the Desktop version of R and RStudio, and not so much the Cloud version, because version management is handled for you in the Cloud.)\nR and its add-on packages are constantly being updated and improved. This can cause problems when trying to install or use packages that depend on specific versions of R or other packages.\nImagine that the online version of a package has been updated and now only works with the lastest version of R. If you are using an older version of R, you may not be able to install or use that package.\nOr if a package depends on another package that has been updated, you may need to update that package as well to use the first package.\nThis sounds complicated, but there are some simple steps you can take to reduce the chances of running into version-related problems:\n\nKeep your R version up to date. New versions of R are released every 6 months or so, and they often include important bug fixes and new features. You can check your current R version by typing R.version.string in the Console. To update R, you can download the latest version from the CRAN website.\nKeep your add-on packages up to date. You can update all your installed packages by using the update.packages() function. This will check for updates for all installed packages and install the latest versions. You can also use the RStudio interface to update packages by going to the “Packages” tab, clicking on “Update”, selecting the packages you want to update, and clicking “Install Updates”.\nDo this well before critical deadlines or important events (e.g., exams). Updating R and packages can sometimes lead to unexpected issues, so it’s best to do it well in advance of when you need everything to work perfectly.\n\nNevertheless, even with these precautions, you may still encounter version-related issues from time to time. When this happens, don’t panic!\nA common problem you might see is an error message when trying to install or load a package, indicating that the package requires a newer version of R or another package. The error / warning message might look like:\nwarning: package 'xyz' requires R version &gt;= 4.2.0\nWarning in install.packages: package ‘XYZ’ is not available (for R version 4.2.0)\nThese messages indicate that the package you are trying to install or load requires a newer version of R than the one you currently have. To fix this, you will need to update your R installation to the required version or higher. Then its also a good idea to update your packages as well.\n\n\n\n\n\n\nNote\n\n\n\nRStudio is also regularly updated, with new version released every several months or so. Your version of RStudio is independent of your version of R, so you can update RStudio without changing your R version. Note that usually your version of RStudio is not as important as your version of R and the packages you are using. So updating RStudio is usually not a high priority and doesn’t often help solve problems related to add on package versions.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-r-projects",
    "href": "2.1-R-and-RStudio.html#sec-r-projects",
    "title": "R and RStudio (L2)",
    "section": "R Projects",
    "text": "R Projects\nI always work within R Projects. R Projects help you to organise your work and keep all files related to a project in one place. They also make importing data a breeze.\nBut what is an R Project? An R Project is a directory (folder) that contains all the files related to a specific project. When you open an R Project, RStudio automatically sets the working directory to the project directory, so you don’t have to worry about setting the working directory manually.\nTo see if you’re working within an R Project, look at the top right of the RStudio window. If you see the name of your project there, you’re good to go. If you see “Project: (None)”, then you’re not working within an R Project.\nIf you click on the project name, a dropdown menu will appear. From there, you can create a new project, open an existing project, or switch between projects.\nCreate a new R Project: File &gt; New Project &gt; New Directory or Existing Directory &gt; New Project &gt; Choose a name and location for your project &gt; Create Project.\n\n\n\n\n\n\nImportant\n\n\n\nGet organised! Put all files for a project in one folder. For example, I made a folder called BIO144_2026 and put all files related to this course in that folder. Within that folder, I have subfolders for data, scripts, and results. I then create an R Project in the BIO144_2026 folder. This way, all files related to the course are in one place, and I can easily find them later.\n\n\nNow, always open and ensure you’re working within the R Project for your project. As mentioned, you can see the project name at the top right of the RStudio window. And if its not the correct project, click on the name to get the drop-down list of available projects from which you can switch to the correct one.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-importing-data",
    "href": "2.1-R-and-RStudio.html#sec-importing-data",
    "title": "R and RStudio (L2)",
    "section": "Importing data",
    "text": "Importing data\nFirst, get some data sets for us to work with. XYZ You can download them from the course website or use your own data sets. Save the data files in a folder called data within your R Project directory.\nWe will use the readr package to import data into R. The readr package provides functions to read data from various file formats, including CSV (comma-separated values) files, tab-separated values files, and others.\nTo read a CSV file, we can use the read_csv() function from the readr package. For example, to read a CSV file called data.csv, we can use the following code:\nThis code will read the data.csv file from the data folder within the current working directory (which should be the R Project directory) and store it in a data frame called data.\n\n\n\n\n\n\nTip\n\n\n\nEasily getting the file path In RStudio, you can easily get the file path by putting the cursor in the parentheses of the read_csv() function, the press the tab key. A drop-down menu will appear with options to navigate to the file. This way, you don’t have to type the file path manually.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-viewing-data",
    "href": "2.1-R-and-RStudio.html#sec-viewing-data",
    "title": "R and RStudio (L2)",
    "section": "Viewing the data",
    "text": "Viewing the data\nOnce you’ve imported your data, you can view it in several ways:\n\nClick on the data frame in the Environment tab in RStudio to open it in a new tab.\nUse the View() function to open the data frame in a new tab in RStudio.\nUse the head() function to view the first few rows of the data frame.\nUse the str() function to view the structure of the data frame, including the variable names and types.\nUse the summary() function to get a summary of the data frame, including basic statistics for each variable.\n\nAnother useful function is glimpse() from the dplyr package, which provides a quick overview of the data frame.\nThere are many checks you can do to ensure your data was imported correctly. For example checking if there are duplicated values in a variable when there shouldn’t be:\n\n\n[1] FALSE\n\n\nThe function any() will return TRUE if there are any duplicated values in the Name variable, and FALSE otherwise. The function duplicated() returns a logical vector indicating which values are duplicates. We use the dollar sign $ to access a specific variable (column) in the data frame. A logical vector is a vector that contains only TRUE or FALSE values:\n\n\n[1] FALSE FALSE FALSE\n\n\nAll three logicals are FALSE, meaning none of the three are duplicates. If there were duplicates, the corresponding positions in the logical vector would be TRUE. For example:\nWhat do you expect the output of duplicated(example_vector) to be?\nA final check (though not the final one we could do–there are many others). Let us check for missing values and get a count of how many there are in each variable. We can do this with the following tidyverse code:\n\n\n  Name Age Height\n1    0   0      0\n\n\nLooks complicated eh! Well, that’s because it is, for sure. But let’s break it down:\n\nsummarise() creates a new data frame with summary statistics.\nacross(everything(), ~ sum(is.na(.))) applies the function sum(is.na(.)) to every variable in the data frame.\nThe is.na() function returns a logical vector indicating which values are missing (NA), and the sum() function counts the number of TRUE values in that vector (i.e., the number of missing values).\n\n\n\n\n\n\n\nImportant\n\n\n\nLet’s assume your data was imported incorrectly. This means you have to inspect it carefully. Check that the variable names are correct, that the data types are correct (e.g., numeric, character, factor), that there are the correct number of rows and columns. If you find any issues, you need to find out what caused them, fix them, and re-import the data (see below).\n\n\nCommon data import problems:\n\nIncorrect delimiter: If your data file uses a different delimiter (e.g., tab, semicolon), you need to specify it in the read_csv() function using the delim argument (e.g., read_delim(\"data.csv\", delim = \"\\t\") for tab-delimited files).\nMissing values: If your data file uses a specific value to represent missing data (e.g., “NA”, “-999”), you need to specify it in the read_csv() function using the na argument (e.g., read_csv(\"data.csv\", na = c(\"NA\", \"-999\"))).\nOnly one column: If your data file has only one column, it may be because the delimiter is incorrect. Check the delimiter and re-import the data with the correct delimiter.\nYou opened the downloaded file in Excel and then saved it: Excel may have changed the format of the file when you opened and saved it. Always work with the original downloaded file.\nWrong path or file name: Make sure the file path and name are correct. Remember, when you work in an R Project, you can place the cursor in the parentheses of the read_csv() function and press the tab key to navigate to the file.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-data-wrangling",
    "href": "2.1-R-and-RStudio.html#sec-data-wrangling",
    "title": "R and RStudio (L2)",
    "section": "Data wrangling",
    "text": "Data wrangling\nNow we have our data imported and checked, and we’re ready to start working with it. This process is called data wrangling, and it involves cleaning, transforming, and reshaping the data to make it suitable for visualisation and analysis.\n\nClean the variable names\nThe first thing I like to do is standardise and clean up the variable names. I like to use the janitor package for this:\n\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\nThe clean_names() function from the janitor package will convert variable names to a consistent format (lowercase, spaces replaced by underscores, no special characters).\n\n\nManipulate the data frame\nFunctions in the dplyr package are used to manipulate data frames:\n\nselect(): select columns by position, or by name, or by other methods\nfilter(): select rows that meet a logical condition\nslice(): select rows by position\narrange(): reorder rows\nmutate(): add new variables\n\nThe dplyr package also provides functions to group data frames and to summarize data:\n\ngroup_by(): add to a data frame a grouping structure\nsummarize(): summarize data, respecting any grouping structure specified by group_by()\n\nThe pipe operator |&gt; is used to chain together multiple operations on a data frame.\n\n\n\n\n\n\nTip\n\n\n\nNote that you will often see another pipe operator %&gt;% used in examples. The pipe operator |&gt; is a newer version of %&gt;% that is more efficient and easier to use. The pipe operator |&gt; is available in R version 4.1.0 and later.\n\n\nLets work through some examples with a sample data frame:\nHere is the same dataset with 100 rows:\n\n\nSelect columns [#select-columns]\nWe can select columns by name\n\n\n# A tibble: 100 × 2\n   name       score\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Person_001  91.9\n 2 Person_002  87.3\n 3 Person_003  77.8\n 4 Person_004  64.5\n 5 Person_005  69.8\n 6 Person_006  91.2\n 7 Person_007  64.3\n 8 Person_008  91.9\n 9 Person_009  72.6\n10 Person_010  70.3\n# ℹ 90 more rows\n\n\nWe can select columns by position\n\n\n# A tibble: 100 × 2\n   name       score\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Person_001  91.9\n 2 Person_002  87.3\n 3 Person_003  77.8\n 4 Person_004  64.5\n 5 Person_005  69.8\n 6 Person_006  91.2\n 7 Person_007  64.3\n 8 Person_008  91.9\n 9 Person_009  72.6\n10 Person_010  70.3\n# ℹ 90 more rows\n\n\nWe can select columns by a condition, for example select only the numeric columns:\n\n\n# A tibble: 100 × 2\n     age score\n   &lt;int&gt; &lt;dbl&gt;\n 1    50  91.9\n 2    34  87.3\n 3    38  77.8\n 4    33  64.5\n 5    22  69.8\n 6    29  91.2\n 7    37  64.3\n 8    41  91.9\n 9    30  72.6\n10    24  70.3\n# ℹ 90 more rows\n\n\nWe can select a column by pattern matching, using helper functions, for example select columns that contain the letter “a”:\n\n\n# A tibble: 100 × 2\n   name         age\n   &lt;chr&gt;      &lt;int&gt;\n 1 Person_001    50\n 2 Person_002    34\n 3 Person_003    38\n 4 Person_004    33\n 5 Person_005    22\n 6 Person_006    29\n 7 Person_007    37\n 8 Person_008    41\n 9 Person_009    30\n10 Person_010    24\n# ℹ 90 more rows\n\n\nOther helpers include starts_with(), ends_with(), matches(), and everything().\n\n\nFilter: Getting particular rows of data [#filter-rows]\nTo get particular rows of data, we can use the filter() function. This function takes a logical condition as an argument and returns only the rows that meet that condition. For example, to get all rows where the Age is greater than 30:\n\n\n# A tibble: 66 × 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_002    34  87.3\n 3 Person_003    38  77.8\n 4 Person_004    33  64.5\n 5 Person_007    37  64.3\n 6 Person_008    41  91.9\n 7 Person_011    39  67.3\n 8 Person_012    33  96.5\n 9 Person_013    41  61.7\n10 Person_014    44  80.0\n# ℹ 56 more rows\n\n\nHere, the logical condition is age &gt; 30.\nWe can combine multiple conditions using the logical operators & (and), | (or), and ! (not). For example, to get all rows where the Age is greater than 30 and the Score is less than 90:\n\n\n# A tibble: 60 × 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_002    34  87.3\n 2 Person_003    38  77.8\n 3 Person_004    33  64.5\n 4 Person_007    37  64.3\n 5 Person_011    39  67.3\n 6 Person_013    41  61.7\n 7 Person_014    44  80.0\n 8 Person_015    45  87.3\n 9 Person_016    46  81.3\n10 Person_018    38  82.9\n# ℹ 50 more rows\n\n\nOther logical operators include == (equal to), != (not equal to), &lt;= (less than or equal to), and &gt;= (greater than or equal to).\n\n\nSlice: Getting rows by position [#slice-rows]\nThe slice() function allows us to get rows by their position in the data frame. For example, to get the first two rows:\n\n\n# A tibble: 2 × 3\n  name         age score\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n1 Person_001    50  91.9\n2 Person_002    34  87.3\n\n\nI very rarely use this function, as I prefer to use filter() with logical conditions. I can’t think of a good use case for this function right now! Perhaps you can?\n\n\nArrange: Reordering rows [#arrange-rows]\nThe arrange() function allows us to reorder the rows of a data frame based on the values in one or more columns. For example, to reorder the rows by Age in ascending order:\n\n\n# A tibble: 100 × 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_064    20  88.8\n 2 Person_056    21  79.5\n 3 Person_091    21  67.4\n 4 Person_005    22  69.8\n 5 Person_025    22  74.9\n 6 Person_033    23  67.3\n 7 Person_092    23  72.4\n 8 Person_010    24  70.3\n 9 Person_017    24  79.1\n10 Person_058    24  72.7\n# ℹ 90 more rows\n\n\nI f we want to reorder the rows by Age in descending order, we can use the desc() function:\n\n\n# A tibble: 100 × 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_061    50  79.9\n 3 Person_075    50  67.3\n 4 Person_085    50  50.1\n 5 Person_096    50  86.8\n 6 Person_031    49  71.8\n 7 Person_078    49  72.6\n 8 Person_024    48  81.2\n 9 Person_057    48  80.3\n10 Person_021    47  66.0\n# ℹ 90 more rows\n\n\nIt’s unusual to need the rows of a dataset to be arranged in a specific order, but it can be useful when looking at the data directly.\n\n\n\n\n\n\nTip\n\n\n\nNote that when you view the data in RStudio, it will always be arranged by the row number. In the viewer you can sort by clicking on the column headers.\n\n\n\n\nMutate: Adding new variables [#mutate-variables]\nThe mutate() function allows us to add new variables to a data frame. For example, to add a new variable called Age_in_5_years that is the Age plus 5:\n\n\n# A tibble: 100 × 4\n   name         age score age_in_5_years\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 Person_001    50  91.9             55\n 2 Person_002    34  87.3             39\n 3 Person_003    38  77.8             43\n 4 Person_004    33  64.5             38\n 5 Person_005    22  69.8             27\n 6 Person_006    29  91.2             34\n 7 Person_007    37  64.3             42\n 8 Person_008    41  91.9             46\n 9 Person_009    30  72.6             35\n10 Person_010    24  70.3             29\n# ℹ 90 more rows\n\n\nWe can add multiple new variables at once:\n\n\n# A tibble: 100 × 5\n   name         age score age_in_5_years percentage_score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n 1 Person_001    50  91.9             55            0.919\n 2 Person_002    34  87.3             39            0.873\n 3 Person_003    38  77.8             43            0.778\n 4 Person_004    33  64.5             38            0.645\n 5 Person_005    22  69.8             27            0.698\n 6 Person_006    29  91.2             34            0.912\n 7 Person_007    37  64.3             42            0.643\n 8 Person_008    41  91.9             46            0.919\n 9 Person_009    30  72.6             35            0.726\n10 Person_010    24  70.3             29            0.703\n# ℹ 90 more rows\n\n\n\n\nWorking with categorical variables [#sec-categorical-variables]\nVariables in a data frame in R have a type. The most common types of variables are numeric and categorical. Numeric variables are variables that take on numerical values, such as age or score. Categorical variables are variables that take on a limited number of values, often representing categories or groups. In R, categorical variables are typically have type &lt;chr&gt; which is character. Or they can be of type &lt;fct&gt; which is factor.\nWhen we import data categorical variable is usually imported as a character variable. For example, the variable name in our example dataset is a categorical variable of type character. Look at the first few rows of the dataset again, and see that below the variable name it says &lt;chr&gt; for the name variable:\n\n\n# A tibble: 100 × 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_002    34  87.3\n 3 Person_003    38  77.8\n 4 Person_004    33  64.5\n 5 Person_005    22  69.8\n 6 Person_006    29  91.2\n 7 Person_007    37  64.3\n 8 Person_008    41  91.9\n 9 Person_009    30  72.6\n10 Person_010    24  70.3\n# ℹ 90 more rows\n\n\nThis is all totally fine. There are, however, use cases where we might want to convert a character variable to a factor variable. Factors are useful when we have a categorical variable with a fixed number of levels, and we want to specify the order of those levels. For example, if we had a variable called education_level with the values “High School”, “Bachelor’s”, “Master’s”, and “PhD”, we might want to convert this variable to a factor and specify the order of the levels.\nLet’s make a new dataset to illustrate this:\nLook at the structure of this new dataset:\n\n\n# A tibble: 5 × 3\n  name    education_level   age\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n1 Alice   Bachelor's         19\n2 Bob     Master's           23\n3 Charlie PhD                25\n4 David   High School        16\n5 Eve     Bachelor's         20\n\n\nWe can see that the education_level variable is of type &lt;chr&gt;, which is character.\nWe can convert the education_level variable to a factor:\n\n\n# A tibble: 5 × 3\n  name    education_level   age\n  &lt;chr&gt;   &lt;fct&gt;           &lt;dbl&gt;\n1 Alice   Bachelor's         19\n2 Bob     Master's           23\n3 Charlie PhD                25\n4 David   High School        16\n5 Eve     Bachelor's         20\n\n\nNow, the education_level variable is of type &lt;fct&gt;, which is factor.\nHere is a graph of age by education level:\n\n\n\n\n\n\n\n\n\nWe have a problem here: the education levels are not in a sensible order. The first level is “Bachelor’s”, followed by “High School”, “Master’s”, and “PhD”.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think the levels are in this order? We didn’t tell R to order them like this! The answer is that R orders factor levels alphabetically by default. So when we convert a character variable to a factor without specifying the order of the levels, R will order them alphabetically.\n\n\nIt would be much better to have the levels ordered as “High School”, “Bachelor’s”, “Master’s”, and “PhD”.\nWe can fix this by specifying the order of the levels when we convert the variable to a factor:\nNow when we plot the data again, the education levels are in the correct order:\n\n\n\n\n\n\n\n\n\nAnother use case is when we are making a linear model and want to specify the reference level for a categorical variable. We will look at this when we get to linear models. If you want to skip ahead, you can see how this works in a section at the end of this chapter (Section Setting a reference level in a linear model).",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#sec-visualisation",
    "href": "2.1-R-and-RStudio.html#sec-visualisation",
    "title": "R and RStudio (L2)",
    "section": "Visualisation",
    "text": "Visualisation\nThere are many many many types of data visualisation. We will not explore them all in this course! In fact, we will use only a few basic types of visualisation, but we will use them well and critically. The three types of visualisation we will focus on are scatter plots, histograms, and box and whisker plots.\n\nThree basic types of visualisation [#three-basic-visualisations]\nScatterplots are used to visualise the relationship between two continuous variables. Here is an example of a scatterplot:\n\n\n\n\n\n\n\n\n\nHistograms are used to visualise the distribution of a single continuous variable. The axiss are different to scatterplots: the x-axis is the variable being measured, and the y-axis is the count (or frequency) of observations in each bin. A bin is a range of values. Here is an esample of a histogram:\n\n\n\n\n\n\n\n\n\nBox and whisker plots are used to visualise the distribution of a continuous variable across different categories. Here is an example of a box and whisker plot. First we add a new variable that is age group:\nThe new variable age_group is a categorical variable with three levels: “20-29”, “30-39”, and “40-49”. We make this using the case_when() function. This function works by checking each condition (which are given as the arguments to the function) in turn, and assigning the corresponding value when the condition is true. Now we can make the box and whisker plot:\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding ggplot2 syntax [#understanding-ggplot2]\nWe have used the ggplot2 package to create visualisations. The ggplot2 package is based on the grammar of graphics, which provides a consistent way to create visualisations. It is amazing, and when it was created it revolutionised data visualisation in R.\nYou can see that for each of the three visualisations, we use the ggplot() function to create the base plot, and then we add layers to the plot using the + operator.\nThe first argument to the ggplot() function is the data frame that we want to visualise. The layers that we add to the plot each have two main components. The first component is the aesthetic mappings, which specify how the variables in the data frame are mapped to the visual properties of the plot (e.g., x-axis, y-axis, color, size). The second component is the geometric object, which defines how the data is represented in the plot (e.g., points, lines, bars).\nThe aesthetic mappings are specified using the aes() function, which takes arguments that define the mappings. Inside the aes() function, we specify the variables from the data frame that we want to map to the visual properties of the plot. For example, in the scatterplot, we map the age variable to the x-axis and the score variable to the y-axis using aes(x = age, y = score).\nThe geometric object is specified using functions that start with geom_, such as geom_point(), geom_histogram(), and geom_boxplot().\nYou will notice that for the scatterplot and the box and whisker plot, we specify both an x- and a y-variable, but for the histogram we only specify an x-variable. This is because histograms only have one variable, which is the variable being measured. The y-axis is automatically calculated as the count (or frequency) of observations in each bin.\nWe can customise many features of the graph using additional arguments to the ggplot() function and the geom_ functions. For example, we can add titles and labels to the axes using the labs() function:\n\n\n\n\n\n\n\n\n\nWe can also change the theme of the plot using the theme_ functions. For example, to use a minimal theme, and add it the customisations we already made:\n\n\n\n\n\n\n\n\n\nThere are a million and one ways to customise visualisations in ggplot2. We will explore many of them during the course in a rather ad-hoc way. In this course we do not assess your skill and competence in making clear and beautiful visualisations. We will, however, be very happy to help you make beautiful and effective visualisations for your assignments and projects. And please be sure that making beautiful and effective visualisations is a skill that is very highly valued in the workplace.\n\n\nSaving ggplot visualisations [#saving-ggplot]\nAnother feature that is very useful is to save ggplot visualisations to objects and then save to a file (for example a pdf). First, here is how we save a ggplot to an object:\nNow we can save the plot to a file using the ggsave() function:\nNote two things about the ggsave() function. First, the first argument is the file name (including the file extension). The file extension determines the file type (e.g., pdf, png, jpeg). Second, we can specify the width and height of the plot in inches.\nAlso note that the file is saved to the current working directory. When you’re working in an R project, this is usually the base directory of the project. If you want to save your plots in a folder named plots you would first need to create the folder (if it doesn’t already exist) and then specify the path in the file name:\n\n\nWarning in dir.create(\"plots\"): 'plots' already exists",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#extras",
    "href": "2.1-R-and-RStudio.html#extras",
    "title": "R and RStudio (L2)",
    "section": "Extras",
    "text": "Extras\n\nMaking reports directly using Quarto [#quarto-reports]\nWe don’t explicitly ask you to make reports using Quarto in this course, but it is a very useful skill to have, and I highly recommend you explore it further in your own time. Here are a few basics to get you started.\nOne of the great features of R and RStudio is the ability to create reports that combine text, code, and visualisations. One of the most popular tools for this is Quarto (https://quarto.org/), which allows you to create documents in various formats (HTML, PDF, Word, etc.) using a combination of Markdown and R code.\n**Why is this so great???* If you want to show someone your analysis and visualisation, say a team member or supervisor, it is often good to prepare a report that explains what you did, perhaps shows the code you used, and presents the results (including visualisations). One way to go about this is to prepare a powerpoint presentation or a word document, and then copy and paste code and visualisations into the document. Its what I used to do. It works. But it is tedious, error prone, and when you change something in your code or data, you have to remember to go back and update the powerpoint or word document.\nWith Quarto, you can create a report that automatically includes the code and visualisations directly from your R script. This way, if you change the code or data, you can simply re-render the report and everything is automatically updated. It takes away a lot of the tediousness and potential for errors. And it makes updating reports much easier.\nIf you’d like to get started with Quarto, check out the Quarto website (https://quarto.org/) and the RStudio Quarto documentation (https://quarto.org/docs/get-started/). There are also many tutorials and resources available online to help you learn how to use Quarto effectively.\nIf you have questions about Quarto, feel free to ask me or TAs during the practicals, though note that any particular TAs may or may not be experienced with Quarto themselves.\n\n\nCombining ggplots with patchwork [#combining-ggplots]\nWe often make multiple ggplots in our analyses. Sometimes it is useful to combine multiple plots into a single figure for easier comparison or presentation. We can do with ggplots and the lovely add-on package called patchwork. The patchwork package allows us to combine multiple ggplots into a single plot layout. Here is an example of how to use patchwork to combine the three plots we made earlier (scatterplot, histogram, and boxplot):\nFirst, load the patchwork package:\nNext make the first plot and assign it to an object:\nNow make the second plot and assign it to an object:\nNow make the third plot and assign it to an object:\nNow we can combine the three plots into a single layout using the patchwork syntax. Here, we arrange plot1 on the top row, and plot2 and plot3 side by side on the bottom row:\n\n\n\n\n\n\n\n\n\nAmazing eh! OK, lets leave it there for now. We’ll use ggplot2 throughout the course, and explore more features as we go along.\n\n\nSetting a reference level in a linear model\nSometimes when fitting linear models with categorical explanatory (independent) variables, it is useful to set a specific reference level for the categorical variable. This can help in interpreting the model coefficients. In R, we can set the reference level using the relevel() function or by using the factor() function with the levels argument.\nFirst, let’s create a simple dataset:\nBy default, R will set the first level of the factor (in alphabetical order) as the reference level. In this case, “Aspirin” would be the reference level. Therefore when we visualise the data:\n\n\n\n\n\n\n\n\n\nIt would be nicer to have the “Control” group as the first level on the left of the x-axis.\nLikewise, when we make a linear model:\n\n\n\nCall:\nlm(formula = response ~ treatment, data = my_data)\n\nResiduals:\n   1    2    3    4    5    6 \n 0.5 -0.5 -0.5 -0.5  0.5  0.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.5000     0.5000  15.000 0.000643 ***\ntreatmentControl    -3.0000     0.7071  -4.243 0.023981 *  \ntreatmentIbuprofen  -1.0000     0.7071  -1.414 0.252215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7071 on 3 degrees of freedom\nMultiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 \nF-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152\n\n\nThe (Intercept) term corresponds to the “Aspirin” group, and the coefficients for “Control” and “Ibuprofen” are relative to “Aspirin”. R has done this because in the factor levels, “Aspirin” comes first alphabetically and was therefore set as the reference level when the factor variable was created.\nIf we want to set “Control” as the reference level, we can do so using relevel():\nNow when we visualise the data again:\n\n\n\n\n\n\n\n\n\nMagic! The “Control” group is now the first level on the left of the x-axis.\nAnd when we fit the linear model again:\n\n\n\nCall:\nlm(formula = response ~ treatment, data = my_data)\n\nResiduals:\n   1    2    3    4    5    6 \n 0.5 -0.5 -0.5 -0.5  0.5  0.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          4.5000     0.5000   9.000   0.0029 **\ntreatmentAspirin     3.0000     0.7071   4.243   0.0240 * \ntreatmentIbuprofen   2.0000     0.7071   2.828   0.0663 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7071 on 3 degrees of freedom\nMultiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 \nF-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152\n\n\nThe (Intercept) term now corresponds to the “Control” group, and the coefficients for “Aspirin” and “Ibuprofen” are relative to “Control”. This makes interpretation of the model coefficients more intuitive.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "3.1-study-design.html",
    "href": "3.1-study-design.html",
    "title": "Study design (L3)",
    "section": "",
    "text": "Different studies, different goals, different designs, different analyses",
    "crumbs": [
      "Study design (L3)"
    ]
  },
  {
    "objectID": "3.1-study-design.html#paradigms-of-study-design",
    "href": "3.1-study-design.html#paradigms-of-study-design",
    "title": "Study design (L3)",
    "section": "Paradigms of study design",
    "text": "Paradigms of study design\n\nObservational studies\nObservational studies are used to observe and analyze the effects of interventions or exposures without manipulating the study environment.\n\n\nExperimental studies\nExperimental studies involve the manipulation of one or more variables to determine their effect on a response variable. The manipulation is typically done through random assignment of subjects to different treatment groups, which means that on average the only thing that differs among the subjects in the different groups is the treatment they receive. This helps to ensure that any differences in outcomes can be attributed to the treatment rather than other factors that might affect the subjects.\nKey aspects of experimental studies:\n\nA treatment is applied to one or more groups of subjects.\nSubjects are randomly assigned to treatment groups, which helps to control for confounding variables.\nThere are typically one or more control groups that do not receive the treatment, allowing for comparison of outcomes between treated and untreated groups.\nThere is replication, meaning that there are multiple subjects in each treatment group to ensure that estimates of within group variability can be made.\n\n\n\nOther terms\nUnfortunately, the terms “observational” and “experimental” are not always used consistently. In particular, the term “experiment” is often used to refer to any study that involves a comparison of two or more groups, regardless of whether the groups were assigned randomly or not. This can lead to confusion, as some studies that are called “experiments” are actually observational studies. Some studies use the term “mensurative experiment” to refer to observational studies (e.g., Burdon et al 2018 Global Change Biology).",
    "crumbs": [
      "Study design (L3)"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html",
    "href": "4.1-regression-part1.html",
    "title": "Regression Part 1",
    "section": "",
    "text": "Introduction\nHow Owen will structure the lecture time\nThe chapter content below is the reference for what students are expected to know. During the lecture time Owen will talk through and explain hopefully most of the content of this chapter. He will write on a tablet, show figures and other content of this chapter, and may live-code in RStudio. He will also present questions and ask students to Think, Pair, Share. The Share part will sometime be via clicker, sometimes by telling the class. The same will happen in lecture 3-6.\nLinear regression is a common statistical method that models the relationship between a dependent (response) variable and one or more independent (explanatory) variables. The relationship is modeled with the equation for a straight line (\\(y = a + bx\\)).\nWith linear regression we can answer questions such as:\nIn this chapter / lesson we will explore what is linear regression and how to use it to answer these questions. We’ll cover the following topics:\nIn this chapter / lesson we will not discuss the statistical significance of the model. We will cover this topic in the next chapter / lesson.",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#introduction",
    "href": "4.1-regression-part1.html#introduction",
    "title": "Regression Part 1",
    "section": "",
    "text": "How does the dependent (response) variable change with respect to the independent (explanatory) variable?\nWhat amount of variation in the dependent variable can be explained by the independent variable?\nIs there a statistically significant relationship between the dependent variable and the independent variable?\nDoes the linear model fit the data well?\n\n\n\nWhy use linear regression?\nWhat is the linear regression model?\nFitting the regression model (= finding the intercept and the slope).\nIs linear regression a good enough model to use?\nWhat do we do when things go wrong?\nTransformation of variables/the response.\nIdentifying and handling odd data points (aka outliers).\n\n\n\nWhy use linear regression?\n\nIt’s a good starting point because it is a relatively simple model.\nRelationships are sometimes close enough to linear.\nIt’s easy to interpret.\nIt’s easy to use.\nIt’s actually quite flexible (e.g. can be used for non-linear relationships, e.g., a quadratic model is still a linear model!!! See Its a kind of magic….).\n\n\n\nAn example - blood pressure and age\nThere are lots of situations in which linear regression can be useful. For example, consider hypertension. Hypertension is a condition in which the blood pressure in the arteries is persistently elevated. Hypertension is a major risk factor for heart disease, stroke, and kidney disease. It is estimated that hypertension affects about 1 billion people worldwide. Hypertension is a complex condition that is influenced by many factors, including age. In fact, it is well known that blood pressure increases with age. But how much does blood pressure increase with age? This is a question that can be answered using linear regression.\nHere is an example of a study that used linear regression to answer this question: https://journals.lww.com/jhypertension/fulltext/2021/06000/association_of_age_and_blood_pressure_among_3_3.15.aspx\nIn this study, the authors used linear regression to model the relationship between age and blood pressure. They found that systolic blood pressure increased by 0.28–0.85 mmHg/year. This is a small increase, but it is statistically significant. This means that the observed relationship between age and blood pressure is unlikely to be due to chance.\nLets look at some simulated example data:\n\n\n\n\n\n\n\n\n\nWell, that is pretty conclusive. We hardly need statistics. There is a clear positive relationship between age and systolic blood pressure. But how can we quantify this relationship? And in less clear-cut cases what is the strength of evidence for a relationship? This is where linear regression comes in. Linear regression models the relationship between age and systolic blood pressure. With linear regression we can answer the following questions:\n\nWhat is the value of the intercept and slope of the relationship?\nIs the relationship different from what we would expect if there were no relationship?\nHow well does the mathematical representation match the observed values?\nHow much uncertainty is there in predictions?\n\nLets try to figure some of these out from the visualisation.\n\n\n\n\n\n\nThink, Pair, Share (#guess-params)\n\n\n\n\nMake a guess of the slope.\nMake a guess of the intercept (hint be careful, lots of people get this wrong).",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#calculating-the-intercept-and-slope",
    "href": "4.1-regression-part1.html#calculating-the-intercept-and-slope",
    "title": "Regression Part 1",
    "section": "Calculating the intercept and slope",
    "text": "Calculating the intercept and slope\n\nRegression from a mathematical perspective\nGiven an independent/explanatory variable (\\(X\\)) and a dependent/response variable (\\(Y\\)) all points \\((x_i,y_i)\\), \\(i= 1,\\ldots, n\\), on a straight line follow the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i\\ .\\]\n\n\\(\\beta_0\\) is the intercept - the value of \\(Y\\) when \\(x_i = 0\\)\n\\(\\beta_1\\) the slope of the line, also known as the regression coefficient of \\(X\\).\nIf \\(\\beta_0=0\\) the line goes through the origin \\((x,y)=(0,0)\\).\nInterpretation of linear dependency: proportional increase in \\(y\\) with increase (decrease) in \\(x\\).\n\n\n\nFinding the intercept and the slope\nIn a regression analysis, one task is to estimate the intercept and the slope. These are known as the regression coefficients \\(\\beta_0\\), \\(\\beta_1\\).\n\nProblem: For more than two points \\((x_i,y_i)\\), \\(i=1,\\ldots, n\\), there is generally no perfectly fitting line.\nAim: We want to estimate the parameters \\((\\beta_0,\\beta_1)\\) of the best fitting line \\(Y = \\beta_0 + \\beta_1 x\\).\nIdea: Find the best fitting line by minimizing the deviations between the data points \\((x_i,y_i)\\) and the regression line. I.e., minimising the residuals.\n\nBut which deviations?\nThese ones?\n\n\n\n\n\n\n\n\n\nOr these?\n\n\n\n\n\n\n\n\n\nOr maybe even these?\n\n\n\n\n\n\n\n\n\nWell, actually its none of these!!!\n\n\nLeast squares\nFor multiple reasons (theoretical aspects and mathematical convenience), the intercept and slope are estimated using the least squares approach. In this, yet something else is minimized:\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated such that the sum of squared vertical distances (sum of squared residuals / errors) is minimised.\nSSE means Sum of Squared Errors:\n\\[SSE = \\sum_{i=1}^n e_i^2 \\]\nwhere,\n\\[e_i = y_i - \\underbrace{(\\beta_0 + \\beta_1 x_i)}_{=\\hat{y}_i} \\] Note: \\(\\hat y_i = \\beta_0 + \\beta_1 x_i\\) are the predicted values.\nIn the graph just below, one of these squares is shown in red.\n\n\n\n\n\n\n\n\n\n\n\nLeast squares estimates\nWith a linear model, we can calculate the least squares estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) directly using the following formulas.\nFor a given sample of data \\((x_i,y_i), i=1,..,n\\), with mean values \\(\\overline{x}\\) and \\(\\overline{y}\\), the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are computed as\n\\[ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n  (y_i - \\overline{y}) (x_i - \\overline{x})}{ \\sum_{i=1}^n (x_i - \\overline{x})^2 } = \\frac{cov(x,y)}{var(x)}\\]\n\\[\\hat\\beta_0 = \\overline{y} - \\hat\\beta_1 \\overline{x}  \\]\nMoreover,\n\\[ \\hat\\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2 \\quad \\text{with residuals  } e_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i) \\]\nis an unbiased estimate of the residual variance \\(\\sigma^2\\).\n(Derivations of the equations above are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)\n\n\nWhy division by \\(n-2\\) ensures an unbiased estimator\nWhen estimating parameters (\\(\\beta_0\\) and \\(\\beta_1\\)), the square of the residuals is minimised. This fitting process inherently uses up two degrees of freedom, as the model forces the residuals to sum to zero and aligns the slope to best fit the data. I.e., one degree of freedom is lost due to the estimation of the intercept, and another due to the estimation of the slope.\nThe adjustment (division by \\(n-2\\) instead of \\(n\\)) compensates for the loss of variability due to parameter estimation, ensuring the estimator of the residual variance is unbiased. Mathematically, dividing by n - 2 adjusts for this loss and gives an accurate estimate of the population variance when working with sample data.\nWe’ll look at degrees of freedom in more detail later, so don’t worry if this is a bit confusing right now.\n\n\nLet’s do it in R\nFirst we read in the dataset:\n\nbp_age_data &lt;- read.csv(\"data/Simulated_Blood_Pressure_and_Age_Data.csv\")\n\nThe we make a graph of the data:\n\n\n\n\n\n\n\n\n\nThen we make the linear model, using the lm() function:\nThen we can look at the summary of the model. It contains a lot of information, so can be a bit confusing at first.\n\n\n\nCall:\nlm(formula = Systolic_BP ~ Age, data = bp_age_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2195  -3.4434  -0.0808   3.1383  12.6025 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 98.96874    1.46102   67.74   &lt;2e-16 ***\nAge          0.82407    0.02771   29.74   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.971 on 98 degrees of freedom\nMultiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 \nF-statistic: 884.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nHow do our guesses of the intercept and slope compare to the guesses we made earlier?\nRecal that the units of the Age coefficient are in mmHg per year. This means that for each additional year of age, the systolic blood pressure increases by ´r round(coef(bp_age_model)[2],2)´ mmHg.",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#dealing-with-the-error",
    "href": "4.1-regression-part1.html#dealing-with-the-error",
    "title": "Regression Part 1",
    "section": "Dealing with the error",
    "text": "Dealing with the error\n\n\n\n\n\n\n\n\n\nThe line is not a perfect fit to the data. There is scatter around the line.\nSome of this scatter could be caused by other factors that influence blood pressure, such as diet, exercise, and genetics. Also, the there could be differences due to the measurement instrument (i.e., some measurement error).\nThese other factors are not included in the model (only age is in the model), so they create variation that can only appear in error term.\nIn the linear regression model the dependent variable \\(Y\\) is related to the independent variable \\(x\\) as\n\\[Y = \\beta_0 + \\beta_1 x + \\epsilon \\ \\] where\n\n\\(\\epsilon\\) is the error term\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope\n\\(\\epsilon\\) is the error term.\n\nThe error term captures the difference between the observed value of the dependent variable and the value predicted by the model. The error term includes the effects of other factors that influence the dependent variable, as well as measurement error.\n\\[Y \\quad= \\quad \\underbrace{\\text{expected value}}_{E(Y) = \\beta_0 + \\beta_1 x} \\quad + \\quad \\underbrace{\\text{random error}}_{\\epsilon}  \\ .\\]\nGraphically the error term is the vertical distance between the observed value of the dependent variable and the value predicted by the model.\n\n\n\n\n\n\n\n\n\nThe error term is also known as the residual. It is the variation that resides (is left over / is left unexplained) after accounting for the relationship between the dependent and independent variables.\n\nExample in R\nLet’s look at observed values, expected (predicted) values, and residuals (error) in R.\nThe observed values of the response (dependent) variable are already in the dataset:\n\n\n[1] 150.3277 170.0801 139.7174 135.4089 151.9041 122.0296\n\n\nTo get the expected values, we need to find the intercept and slope of the linear model. We can do this using the lm() function in R.\nAnd we can get the intercept and slope using the coef() function:\n\n\n(Intercept)         Age \n 98.9687381   0.8240678 \n\n\nWe can then use the mutate function from the dplyr package to add the expected values to the dataset:\nAnd we can get the residuals by subtracting the expected values from the observed values:\n\n\n\n\n\n\nTip\n\n\n\nWe can also get the expected values and residuals directly from the lm object using the fitted() (or predicted()) and residuals() functions:\n\n\nNow we have a model that gives the expected values (on the regression line) and that gives us a residual. Because the expected value plus the residual equals the observed value, if we use each of the residuals as the error for each respective data point, we end up with a perfect fit to the data. All we are doing is describing the observed data in a different way. This is known as over-fitting. In fact, we have gained very little by fitting the model. We have simply memorized / copied the data!!!\nIn order to avoid this, we need to assume something about the residuals – we need to model the residuals. The most common model for the residuals is a normal distribution with mean 0 and constant variance.\n\\[\\epsilon \\sim N(0,\\sigma^2)\\]\nThis is known as the normality assumption. The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.\nThe linear regression model then becomes:\n\\[Y = \\beta_0 + \\beta_1 x + N(0,\\sigma^2) \\ \\]\nwhere \\(\\sigma^2\\) is the variance of the error term. The variance of the error term is the amount of variation in the dependent variable that is not explained by the independent variable. The variance of the error term is also known as the residual variance.\nAn alternate and equivalent formulation is that \\(Y\\) is a random variable that follows a normal distribution with mean \\(\\beta_0 + \\beta_1 x\\) and variance \\(\\sigma^2\\).\n\\[Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\]\nSo, the answer to the question “how do we deal with the error term” is that we model the error term as normally distributed with mean 0 and constant variance. Put another way, the error term is assumed to be normally distributed with mean 0 and constant variance.\n\n\nBack to blood pressure and age\nThe mathematical model in this case is:\n\\[SystolicBP = \\beta_0 + \\beta_1 \\times Age + \\epsilon\\]\nwhere: SystolicBP is the dependent (response) variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient of Age, Age is the independent (explanatory) variable, \\(\\epsilon\\) is the error term.\nLet’s ensure we understand this, by thinking about the units of the variables in this model. This can be very useful because it can help us to understand the model better and to check that the model makes sense.\n\n\n\n\n\n\nThink, pair, share (#what-units)\n\n\n\n\nWhat are the units of blood pressure?\nWhat are the units of age?\nWhat are the units of the intercept?\nWhat are the units of the coefficient of Age?\nWhat are the units of the error term?",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#is-the-model-good-enough-to-use",
    "href": "4.1-regression-part1.html#is-the-model-good-enough-to-use",
    "title": "Regression Part 1",
    "section": "Is the model good enough to use?",
    "text": "Is the model good enough to use?\n\nAll models are wrong, but is ours good enough to be useful?\nAre the assumption of the model justified?\nIt would be very unwise to use the model before we know if it is good enough to use.\nDon’t jump out of an aeroplane until you know your parachute is good enough!\n\n\nWhat assumptions do we make?\nWe already heard about one. We assume that the residuals follow a \\(N(0,\\sigma^2)\\) distribution (that is, a Gaussian / Normal distrution with mean of zero and variance of \\(\\sigma^2\\)). We make this assumption because it is often well enough met, and it gives great mathematical tractability.\nThis assumption implies that:\n\nThe \\(\\epsilon_i\\) are normally distributed.\n\\(\\epsilon_i\\) has constant variance: \\(Var(\\epsilon_i)=\\sigma^2\\).\nThe \\(\\epsilon_i\\) are independent of each other.\n\nFurthermore:\n\nwe assumed a linear relationship.\nimplies there are no outliers (implied by (a) above)\n\nLets go through each five assumptions.\n\n\n(a) Normally distributed residuals\nRecall that we make the assumption that the residuals are normally distributed with mean 0 and constant variance:\n\\[\\epsilon \\sim N(0,\\sigma^2)\\]\nHere we are concerned with the first part of this assumption, that the residuals are normally distributed.\nWhat does this mean? How can we check it?\nA normal distribution is symmetric and bell-shaped…\n\n\n\n\n\n\n\n\n\nLets look at the frequency distribution of the residuals of the linear regression of blood pressure and age:\n\n\n\n\n\n\n\n\n\nThe normal distribution assumption (a) seems ok as well.\n\n\n(a) Normally distributed residuals: The QQ-plot\nUsually, not the histogram of the residuals is plotted, but the so-called quantile-quantile (QQ) plot. The quantiles of the observed distribution are plotted against the quantiles of the respective theoretical (normal) distribution:\n\n\n\n\n\n\n\n\n\nIf the points lie approximately on a straight line, the data is fairly normally distributed.\nThis is often “tested” by eye, and needs some experience.\nBut what on earth is a quantile???\nImagine we make 21 measures of something, say 21 blood pressures:\n\n\n\n\n\n\n\n\n\nThe median of these is 127.8. The median is the 50% or 0.5 quantile, because half the data points are above it, and half below.\n\n\n  50% \n127.8 \n\n\nThe theoretical quantiles come from the normal distribution. The sample quantiles come from the distribution of our residuals.\n\n\n\n\n\n\n\n\n\n\nHow do I know if a QQ-plot looks “good”?\nThere is no quantitative rule to answer this question. Instead experience is needed. You can gain this experience from simulations. To this end, we can generate the same number of data points of a normally distributed variable and compare this simulated qqplot to our observed one.\nExample: Generate 100 points \\(\\epsilon_i \\sim N(0,1)\\) each time:\n\n\n\n\n\n\n\n\n\nEach of the graphs above has data points that are randomly generated from a normal distribution. In all cases the data points are close to the line. This is what we would expect if the data were normally distributed. The amount of deviation from the line is what we would expect from random variation, and so seeing this amount of variation in a QQ-plot of your model should not be cause for concern.\n\n\n\n(b) Constant error variance (homoscedasticity)\nRecall that we assume the errors are normally distributed with constant variance \\(\\sigma^2\\):\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nHere we’re concerned with the second part of this assumption, that the variance is constant.That is, variance of the residuals is a constant: \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). And not, for example \\(\\text{Var}(\\epsilon_i) = \\sigma^2 \\cdot x_i\\).\nPut another way, we’re interested if the size of the residuals tends to show a pattern with the fitted values. By size of the residuals we mean the absolute value of the residuals. In fact, we often look at the square root of the absolute value of the standardized residuals:\n\\[R_i = \\frac{\\epsilon_i}{\\hat{\\sigma}}\\] Where \\(\\hat{\\sigma}\\) is the estimated standard deviation of the residuals:\n\\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n \\epsilon_i^2}\\]\nSo that the full equation of the square root of the standardised residuals is:\n\\[\\sqrt{|R_i|} = \\sqrt{\\left|\\frac{\\epsilon_i}{\\hat{\\sigma}}\\right|}\\]\nTo look to see if the variance of the residuals is constant, we need to see if there is any relationship between the size of the residuals and the fitted values. A commonly used visualistion for this is a plot of the size of the residuals against the fitted values.\nLets first calculated the \\(\\sqrt{|R_i|}\\) values for our blood pressure model:\nAnd now visualise the relationship between the fitted values and the size of the residuals:\n\n\n\n\n\n\n\n\n\nThis graph is known as the scale-location plot. It is particularly suited to check the assumption of equal variances (homoscedasticity / Homoskedastizität). There should be no trend or pattern.\n\n\n\n\n\n\nTip\n\n\n\nWe can also use the built-in plot function for linear models to create this plot. It is the third plot in the set of diagnostic plots.\n\n\n\n\n\n\n\n\n\n\n\n\nHow it looks with the variance increasing with the fitted values\nHere’s a graphical example of how it would look if the variance of the residuals increases with the fitted values.\nFirst here is a graph of the relationship:\n\n\n\n\n\n\n\n\n\nAnd here the scale-location plot for a linear model of that data:\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Independence (residuals are independent of each other)\nWe assume that the residuals (\\(\\epsilon_i\\)) are independent of each other. This means that the value of one residual is not somehow related to the value of another.\nThe dataset about blood pressure we looked at contained 100 observations, each one made from a different person. In such a study design, we could be safe in the assumption that the people are independent, and therefore the assumption that the residuals are independent.\nImagine, however, if we had 100 observations of blood pressure collected from 50 people, because we measured the blood pressure of each person twice. In this case, the residuals would not be independent, because two measures of the blood pressure of the same person are likely to be similar. A person is likely to have a high blood pressure in both measurements, or a low blood pressure in both measurements. This would mean they have a high residual in both measurements, or a low residual in both measurements.\nIn this case, we would need to account for the fact that the residuals are not independent. We would need to use a more complex model, such as a mixed effects model, to account for the fact that the residuals are not independent. We will talk about this again in the last week of this course.\nIn general, you should always think about the study design when you are analysing data. You should always think about whether the residuals are likely to be independent of each other. If they are not, you should think about how you can account for this in your analysis.\nA good way to assess if there could be dependencies in the residuals is to be critical about what is the unit of observation in the data. In the blood pressure example, the unit of observation is the person. Count the number of persons in the study. If there are fewer persons than observations, then at least some people must have been measured at least twice. Repeating measures on the same person is a common way to get dependent residuals.\nSo, to check the assumption of independence, you should:\n\nThink carefully about the study design.\nThink carefully about the unit of observation in the data.\nCompare the number of observations to the number of units of observation.\n\n\n\n(d) Linearity assumption\nThe linearity assumption states that the relationship between the independent variable and the dependent variable is linear. This means that the dependent variable changes by a constant amount for a one-unit change in the independent variable. And that this slope is does not change with the value of the independent variable.\nThe blood pressure data seems to be linear:\n\n\nWarning: `fortify(&lt;lm&gt;)` was deprecated in ggplot2 3.6.0.\nℹ Please use `broom::augment(&lt;lm&gt;)` instead.\nℹ The deprecated feature was likely used in the ggplot2 package.\n  Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\n\n\n\n\nIn contrast, look at this linear regression through data that appears non-linear:\n\n\n\n\n\n\n\n\n\nAnd with the residuals shown as red lines:\n\n\n\n\n\n\n\n\n\nAt low values of \\(y\\), the residuals are positive, at intermediate values of \\(y\\) the residuals are negative, and at high values of \\(y\\) the residuals are positive. This pattern in the residuals is a sign that the relationship between \\(x\\) and \\(y\\) is not linear.\nWe can plot the value of the residuals against the \\(y\\) value directly, instead of looking at the pattern in the graph above. This is called a Tukey-Anscombe plot. It is a graph of the residuals versus the fitted \\(y\\) values:\n\n\n\n\n\n\n\n\n\nWe can very clearly see pattern in the residuals in this Tukey-Anscombe plot. The residuals are positive, then negative, then positive, as the fitted \\(y\\) value gets larger.\nWe can also make this Tukey-Anscombe plot using the built-in plot function for linear models in R:\n\n\n\n\n\n\n\n\n\nThe red line in the Tukey-Anscombe plot is a loess smooth. It is automatically added to the plot. It is a way of estimating the pattern in the residuals. If the red line is not flat, then there is a pattern in the residuals. However, the loess smooth is not always reliable. It is a good idea to look at the residuals directly, without this smooth.\n\n\n\n\n\n\n\n\n\nThe data here is simulated to show a very clear pattern in the residuals. In real data, the pattern might not be so clear. But if you suspect you see a pattern in the residuals, it could be a sign that the relationship between the independent and dependent variable is not linear.\nHere is the Tukey-Anscombe plot for the blood pressure data:\n\n\n\n\n\n\n\n\n\nThere is very little evidence of any pattern in the residuals. This data is simulated with a truly linear relationship, so we would not expect to see any pattern in the residuals.\n\n\n(e) No outliers\nAn outlier is a data point that is very different from the other data points. Outliers can have a big effect on the results of a regression analysis. They can pull the line of best fit towards them, and make the line of best fit a poor representation of the data.\nLets again look at the blood pressure versus age data:\n\n\n\n\n\n\n\n\n\nThere are no obvious outliers in this data. The data points are all close to the line of best fit. This is a good sign that the line of best fit is a good representation of the data.\n\n\n\n\n\n\nThink, Pair, Share (#odd-data)\n\n\n\nWhere on this graph would you expect to see particularly influential outliers? Influential in the sense that they would have a large effect on the slope of the line of best fit.\n\n\nData points that are far from the mean of the independent variable have a large effect on the value of the slope. These data points have a large leverage. They are data points that are far from the other data points in the \\(x\\) direction.\nWe can think of this with the analogy of a seesaw. The slope of the line of best fit is like the pivot point of a seesaw. Data points that are far from the pivot point have a large effect on the slope. Data points that are close to the pivot point have a small effect on the slope.\nA measure of distance from the pivot point is called the \\(leverage\\) of a data point. In simple regression, the leverage of individual \\(i\\) is defined as\n\\(h_{i} = (1/n) + (x_i-\\overline{x})^2 / SSX\\).\nwhere \\(SSX = \\sum_{i=1}^n (x_i - \\overline{x})^2\\). (Sum of Squares of \\(X\\))\nSo, the leverage of a data point is inversely related to \\(n\\) (the number of data points). The leverage of a data point is also inversely related to the sum of the squares of the \\(x\\) values. The leverage of a data point is directly related to the square of the distance of the \\(x\\) value from the mean of the \\(x\\) values.\nMore intuitively perhaps, the leverage of a data point will be greater when the are fewer other data points. It will also be greater when the distance from the mean value of \\(x\\) is greater.\nGoing back to the analogy of a seesaw, with data points as children on the seesaw, the leverage of a data point is like the distance from the pivot a child sits. But we also have children of different weights. A lighter child will have less effect on the tilt of the seesaw. A heavier one will have a greater effect on the tilt. A heavier child sitting far from the pivot will have a very large effect.\n\n\n\n\n\n\nThink, Pair, Share (#like-weight)\n\n\n\nWhat quantity that we already experienced is like the weight of the child?\n\n\nThe size of the residuals are like the weight of the child. Data points with large residuals have a large effect on the slope of the line of best fit. Data points with small residuals have a small effect on the slope of the line of best fit.\nSo the overall effect of a data point on the slope of the line of best fit is a combination of the leverage and the residual. This quantity is called the \\(influence\\) of a data point.\nLet’s add a rather extreme data point to the blood pressure versus age data:\n\n\n\n\n\n\n\n\n\nThis is a bit ridiculous, but it is a good example of an outlier. The data point is far from the other data points. It has a large residual. And it is a long way from the pivot (the middle of the \\(x\\) data) so has large leverage.\nWe can make a histogram of the residuals and see that the outlier has a large residual:\n\n\n\n\n\n\n\n\n\nAnd we can see that the leverage is large.\nThere is a graph that we can look at to see the influence of a data point. This is called a \\(Cook's\\) \\(distance\\) plot. The Cook’s distance of a data point is a measure of how much the slope of the line of best fit changes when that data point is removed. The Cook’s distance of a data point is defined as\n\\(D_i = \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 / (p \\times MSE)\\).\nwhere \\(\\hat{y}_j\\) is the predicted value of the dependent variable for data point \\(j\\), \\(\\hat{y}_{j(i)}\\) is the predicted value of the dependent variable for data point \\(j\\) when data point \\(i\\) is removed, \\(p\\) is the number of parameters in the model (2 in this case), \\(MSE\\) is the mean squared error of the model.\n\n\n\n\n\n\n\n\n\nBut does it have a large influence on the value of the slope? In the next graph we show the line of best fit with the outlier (blue line) and without the outlier (red line).\n\n\n\n\n\n\n\n\n\nNo, the outlier doesn’t have much influence on the slope. The outlier has a large leverage. It is far from the pivot. But it does not have such a large effect (influence) on the slope. This is in large part because there are a lot data points (100) that are quite tightly arranged around the regression line.\n\nGraphical illustration of the leverage effect\nData points with \\(x_i\\) values far from the mean have a stronger leverage effect than when \\(x_i\\approx \\overline{x}\\):\n\n\n\n\n\n\n\n\n\nThe outlier (red circle) in the middle plot “pulls” the regression line in its direction and has large influence on the slope. THe outlier (red circle) in the right plot has less influence on the slope because it is closer to the mean of \\(x\\).\n\n\nLeverage plot (Hebelarm-Diagramm)\nIn the leverage plot, (standardized) residuals \\(\\tilde{R_i}\\) are plotted against the leverage \\(H_{ii}\\) :\n\n\n\n\n\n\n\n\n\nCritical ranges are the top and bottom right corners!!\nHere, observations 71, 85, and 87 are labelled as potential outliers.\nSome texts will give a rule of thumb that points with Cook’s distances greater than 1 should be considered influential, while others claim a reasonable rule of thumb is \\(4 / ( n - p - 1 )\\) where \\(n\\) is the sample size, and \\(p\\) is the number of \\(beta\\) parameters.",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#what-can-go-wrong-during-the-modeling-process",
    "href": "4.1-regression-part1.html#what-can-go-wrong-during-the-modeling-process",
    "title": "Regression Part 1",
    "section": "What can go “wrong” during the modeling process?",
    "text": "What can go “wrong” during the modeling process?\nAnswer: a lot of things!\n\nNon-linearity. We assumed a linear relationship between the response and the explanatory variables. But this is not always the case in practice. We might find that the relationship is curved and not well represtented by a straight line.\nNon-normal distribution of residuals. The QQ-plot data might deviate from the straight line so much that we get worried!\nHeteroscadisticity (non-constant variance). We assumed homoscadisticity, but the residuals might show a pattern.\nData point with high influence. We might have a data point that has a large influence on the slope of the line of best fit.\n\n\nWhat to do when things “go wrong”?\n\nNow: Transform the response and/or explanatory variables.\nNow: Take care of outliers.\nLater in the course: Improve the model, e.g., by adding additional terms or interactions.\nLater in the course: Use another model family (generalized or nonlinear regression model).\n\n\n\nDealing with non-linearity\nHere’s another example of \\(y\\) and \\(x\\) that are not linearly related:\n\n\n\n\n\n\n\n\n\nOne way to deal with this is to transform the response variable \\(Y\\). Here we try two different transformations: \\(\\log_{10}(Y)\\) and \\(\\sqrt{Y}\\).\nSquare root transform of the response variable \\(Y\\):\n\n\n\n\n\n\n\n\n\nNot great.\nLog transformation of the response variable \\(Y\\):\n\n\n\n\n\n\n\n\n\nNope. Still some evidence of non-linearity.\nWhat about transforming the explanatory variable \\(X\\) as well?\n\n\n\n\n\n\n\n\n\nLet’s look at the four diagnostic plots for the log-log-transformed data:\n\n\n\n\n\n\n\n\n\nAll looks pretty good except for the scale-location plot, which shows a bit of a pattern. But overall, this looks much better than our original model.\nBut… how to know which transformation to use…? It’s a bit of trial and error. But we can use the diagnostic plots to help us.\nVery very important is that we do this trial and error before we start using the model. E.g., we don’t want to jump from the aeroplane and then find out that our parachute is not working properly! And then try to fix the parachute while we are falling….\nLikewise, we must not start using the model and then try to fix it. We need to make sure our model is in good working order before we start using it.\nOne of the traps we could fall into is called “p-hacking”. This is when we try different transformation until we find one that gives us the result we want, for example significant relationship. This is a big no-no in statistics. We need to decide on the model (including any transformations) before we start using it.\n\n\nCommon transformations\nWhich transformations could be considered? There is no simple answer. But some guidelines. E.g. if we see non-linearity and increasing variance with increasing fitted values, then a log transform may improve matter.\nSome common and useful transformations are:\n\nThe log transformation for concentrations and absolute values.\nThe square-root transformation for count data.\nThe arcsin square-root \\(\\arcsin(\\sqrt{\\cdot})\\) transformation for proportions/percentages.\n\nTransformations can also be applied on explanatory variables, as we saw in the example above.\n\n\nOutliers\nWhat do we do when we identify the presence of one or more outliers?\n\nStart by checking the “correctness” of the data. Is there a typo or a decimal point that was shifted by mistake? Check both the response and explanatory variables.\nIf not, ask whether the model could be improved. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?\nSometimes, an outlier may be the most interesting observation in a dataset! Was the outlier created by some interesting but different process from the other data points?\nConsider that outliers can also occur just by chance!\nOnly if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.\n\n\n\nRemoving outliers\nIt might seem tempting to remove observations that apparently don’t fit into the picture. However:\n\nDo this only with greatest care e.g., if an observation has extremely implausible values!\n\nBefore deleting outliers, check points 1-5 above.\nWhen removing outliers, you must mention this in your report.\n\nDuring the course we’ll see many more examples of things going at least a bit wrong. And we’ll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do in the next lesson. But before we wrap up, some good news…",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "4.1-regression-part1.html#sec-kind-of-magic",
    "href": "4.1-regression-part1.html#sec-kind-of-magic",
    "title": "Regression Part 1",
    "section": "Its a kind of magic…",
    "text": "Its a kind of magic…\nAbove, we learned about linear regression, the equation for it, how to estimate the coefficients, and how to check the assumptions. There was a lot of information, and it might seem a bit overwhelming.\nYou might also be aware that there are quite a few other types of statistical model, such as multiple regression, t-test, ANOVA, two-way ANOVA, and ANCOVA. It could be worrying to think that you need to learn so much new information for each of these types of tests.\nBut this is where the kind-of-magic happens. The good news is that the linear regression model is a special case of what is called a general linear model, or just linear model for short. And that all the tests mentioned above are also types of linear model. So, once you have learned about linear regression, you have learned a lot about linear models, and therefore also a lot about all of these other tests as well.\nMoreover, the same function in R ‘lm’ is used to make all those statistical models Awesome.\n\nSo what is a linear model?\nA linear model is a model where the relationship between the dependent variable and the independent variables is linear. That is, the dependent variable can be expressed as a linear combination of the independent variables. An example of a linear model is:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon\\]\nwhere: \\(y\\) is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients of the independent variables, \\(x_1, x_2, \\ldots, x_p\\) are the independent variables, \\(\\epsilon\\) is the error term.\nIn contrast, a non-linear model is a model where the relationship between the dependent variable and the independent variables is non-linear. An example of a non-linear model is the exponential growth model:\n\\[y = \\beta_0 + \\beta_1 e^{\\beta_2 x} + \\epsilon\\]\nwhere: y is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2\\) are the coefficients of the independent variables, \\(x\\) is the independent variable, \\(\\epsilon\\) is the error term.\nKeep in mind that a model with a quadratic term is still a linear model. For example:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\epsilon\\]\nis still a linear model. We can see this if we substitute \\(x^2\\) with a new variable \\(x_2\\), where \\(x_2 = x^2\\). The model then becomes:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nThis is clearly a linear model.",
    "crumbs": [
      "Regression Part 1"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html",
    "href": "5.1-regression-part2.html",
    "title": "Regression Part 2",
    "section": "",
    "text": "Introduction\nHow Owen will structure the lecture time\nThe chapter content below is the reference for what students are expected to know. During the lecture time Owen will talk through and explain hopefully most of the content of this chapter. He will write on a tablet, show figures and other content of this chapter, and may live-code in RStudio. He will also present questions and ask students to Think, Pair, Share. The Share part will sometime be via clicker, sometimes by telling the class. The same will happen in lecture 3-6.\nNow that we have a satisfactory model, we can start to use it. In the following material, you will learn:",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#introduction",
    "href": "5.1-regression-part2.html#introduction",
    "title": "Regression Part 2",
    "section": "",
    "text": "How to measure how good is the regression (correlation and \\(R^2\\)).\nHow to test if the parameter estimates are compatible with some specific value (\\(t\\)-test).\nHow to find the range of parameters values are compatible with the data (confidence intervals).\nHow to find the regression lines compatible with the data (confidence band).\nHow to calculate plausible values of newly collected data (prediction band).\n\n\nAccompanying reading material",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#how-good-is-the-regression-model",
    "href": "5.1-regression-part2.html#how-good-is-the-regression-model",
    "title": "Regression Part 2",
    "section": "How good is the regression model?",
    "text": "How good is the regression model?\nWhat would a good regression model look like? What would a bad one look like? One could say that a good regression model is one that explains the dependent variable well. But what could we mean by “explains the data well”?\nTake these two examples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#better-model)\n\n\n\nIn which of these two would you say the model is better, and in which is it worse?\n\n\nThe first model seems to fit the data well, while the second one does not. But how can we quantify this?\nLet’s say that we will measure the goodness of the model by the amount of variability of the dependent variable that is explained by the independent variable. To do this we need to do the following:\n\nMeasure the total variability of the dependent variable (total sum of squares, \\(SST\\)).\nMeasure the amount of variability of the dependent variable that is explained by the independent variable (model sum of squares, \\(SSM\\)).\nMeasure the variability of the dependent variable that is not explained by the independent variable (error sum of squares, \\(SSE\\)).\nCalculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\), pronounced as “r-squared”) (also known as the coefficient of determination) (\\(R^2\\) = \\(SSM/SST\\)).\n\nImportantly, note that we will calculate \\(SSM\\) and \\(SSE\\) so that they sum up to \\(SST\\). I.e., \\(SST = SSM + SSE\\). That is, the total variability is the sum of what is explained by the model and what remains unexplained.\nLet’s take each in turn:\n\n\\(SST\\)\n1. The total variability of the dependent variable is the sum of the squared differences between the dependent variable and its mean. This is called the total sum of squares (\\(SST\\)).\n\\[SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nwhere: \\(y_i\\) is the dependent variable, \\(\\bar{y}\\) is the mean of the dependent variable, \\(n\\) is the number of observations.\nNote that sometimes \\(SST\\) is referred to as \\(SSY\\) (sum of squares of \\(y\\)).\nGraphically, this is the sum of the square of the blue residuals as shown in the following graph, where the horizontal dashed line is at the value of the mean of the dependent variable.\n\n\n\n\n\n\n\n\n\nWe can calculate this in R as follows:\n\nSST &lt;- sum((y1 - mean(y1))^2)\n\n\n\nSSM and SSE\nNow the next two steps, that is getting the model sum of squares (SSM) and the error sum of squares (SSE) are a bit more complicated. To do this we need to fit a regression model to the data. Let’s see this graphically, and divide the data into the explained and unexplained parts.\nMake a graph with vertical lines connecting the data to the mean of the data, but with each line two parts, one from the mean to the data, and one from the data to the predicted value.\n\n\n\n\n\n\n\n\n\nIn this graph, the square of the length of the green lines is the model sum of squares (\\(SST\\)). The square of the length of the red lines is the error sum of squares (\\(SSE\\)).\nIn a better model the length of the green lines will be longer (the square of these gives the \\(SMM\\), the variability explained by the model). And the length of the red lines will be shorter (the square of these gives the \\(SSE\\), the variability not explained by the model).\n\n\n\\(SSM\\)\nNext we will do the second step, that is calculate the model sum of squares (\\(SSM\\)).\n2. The amount of variability of the dependent variable that is explained by the independent variable is called the model sum of squares (\\(SSM\\)).\nThis is the difference between the predicted value of the dependent variable and the mean of the dependent variable, squared and summed:\n\\[SSE = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nwhere: \\(\\hat{y}_i\\) is the predicted value of the dependent variable,\nIn R, we calculate this as follows:\n\nm1 &lt;- lm(y1 ~ x)\ny1_predicted &lt;- predict(m1)\nSSM &lt;- sum((y1_predicted - mean(y1))^2)\nSSM\n\n[1] 258.6914\n\n\n\n\n\\(SSE\\)\nThird, we calculate the error sum of squares (\\(SSE\\)) with either of two methods. We could calculate it as the sum of the squared residuals, or as the difference between the total sum of squares and the model sum of squares:\n\\[SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = SST - SSM\\] Let’s calculate this in R uses both approaches:\n\nSSE &lt;- sum((y1 - y1_predicted)^2)\nSSE\n\n[1] 12.38292\n\n\nOr…\n\nSSE &lt;- SST - SSM\nSSE\n\n[1] 12.38292\n\n\n\n\n\\(R^2\\)\nFinally, we calculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\)):\n\\[R^2 = \\frac{SSM}{SST}\\]\n\n\n[1] 0.9543191\n\n\n\n\nIs my R squared good?\nWhat value of \\(R^2\\) is considered good? In ecological research, \\(R^2\\) values are often low (less than 0.3), because ecological systems are complex and many factors influence the dependent variable. However, in other fields, such as physiology, \\(R^2\\) values are often higher. Therefore, the answer of what values of \\(R^2\\) are good depends on the field of research.\nHere are the four examples and their r-squared.\n\n\n\n\n\n\n\n\n\n\n\nQuestions\n\n\n\n\n\n\nThink, Pair, Share (#what-minimised)\n\n\n\nWhat is minimised when we fit a regression model? And therefore what is maximised?",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "href": "5.1-regression-part2.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "title": "Regression Part 2",
    "section": "How unlikey is the observed data given the null hypothesis?",
    "text": "How unlikey is the observed data given the null hypothesis?\nWe often hear this expressed as “is the relationship significant?” And maybe we heard that the relationship is significant if the p-value is less than 0.05. But what does all this actually mean? In this section we’ll figure all this out. The first step to is to formulate a null hypothesis.\nWhat is a meaningful null hypothesis for a regression model?\nAs mentioned, often we’re interested in whether there is a relationship between the dependent (response) and independent (explanatory) variable. Therefore, the null hypothesis is that there is no relationship between the dependent and independent variable. This means that the null hypothesis is that the slope of the regression line is zero.\nRecall the regression model: \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\n\n\n\n\n\nThink, Pair, Share (#null-hypothesis)\n\n\n\nWrite down the null hypothesis of no relationship between \\(x\\) and \\(y\\) in terms of a \\(\\beta\\) parameter.\n\n\nThe null hypothesis is that the slope of the regression line is zero: \\[H_0: \\beta_1 = 0\\]\nWhat is the alternative hypothesis?\n\\[H_1: \\beta_1 \\neq 0\\]\nSo, how do we test the null hypothesis? More precisely, we are going to calculate the probability of observing the data we have, given that the null hypothesis is true. If this probability is very low, then we can reject the null hypothesis.\nDoes that make sense? Does it seem a bit convoluted? It is a bit!!!\nBut this is how hypothesis testing works. We never prove the null hypothesis is true. Instead, we calculate the probability of observing our data given that the null hypothesis is true. If this probability is very low, we reject the null hypothesis.\nTo make the calculation we can use the fact that the slope of the regression line is an estimate of the true slope. This estimate has uncertainty associated with it. We can use this uncertainty to calculate the probability of observing the data we have, given the null hypothesis is true.\nWe can see that the slope estimate (the \\(x\\) row) has uncertainty by looking at the regression output:\n\n\n              Estimate Std. Error\n(Intercept) -0.7638353  0.6652233\nx            2.1161160  0.1072104\n\n\nThe estimate is the mean of the distribution of the parameter (slope) and the standard error is a measure of the uncertainty of the estimate.\nThe standard error is calculated as:\n\\[\\sigma^{(\\beta_1)} = \\sqrt{ \\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}}\\]\nWhere \\(\\hat\\sigma^2\\) is the expected residual variance of the model. This is calculated as:\n\\[\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat y_i)^2}{n-2}\\]\nWhere \\(\\hat y_i\\) is the predicted value of \\(y_i\\) from the regression model.\nOK, let’s take a look at this intuitively. We have the estimate of the slope and the standard error of the estimate.\nHere is a graph of the value of the slope estimate versus the standard error of the estimate:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#chance-area)\n\n\n\nIn what areas of the graph is the slope estimate more likely to have been observed by chance? And what regions is it less likely to have been observed by chance?\nThink about this before you look at the end of this chapter for an answer (Section Visualising p-values for regression slopes).\n\n\nWhen the slope estimate is larger, it is less likely to have been observed by chance. And when the standard error is larger, it is more likely to have been observed by chance. How can we put these together into a single measure?\nIf we divide the slope estimate by the standard error, we get a measure of how many standard errors the slope estimate is from the null hypothesis slope of zero. This is the \\(t\\)-statistic:\n\\[t = \\frac{\\hat\\beta_1 - \\beta_{1,H_0}}{\\sigma^{(\\beta_1)}}\\]\nWhere \\(\\beta_{1,H_0}\\) is the null hypothesis value of the slope, usually zero, so that\n\\[t = \\frac{\\hat\\beta_1}{\\sigma^{(\\beta_1)}}\\]\nThe \\(t\\)-statistic is a measure of how many standard errors the slope estimate is from the null hypothesis value of the slope. The larger the \\(t\\)-statistic, the less likely the slope estimate was observed by chance.\nHow can we transform the value of a \\(t\\)-statistic into a p-value? We can use the \\(t\\)-distribution, which quantifies the probability of observing a value of the \\(t\\)-statistic under the null hypothesis.\nBut what is the \\(t\\)-distribution? It is a distribution of the \\(t\\)-statistic under the null hypothesis. It is a bell-shaped distribution that is centered on zero. The shape of the distribution is determined by the degrees of freedom, which is \\(n-2\\) for a simple linear regression model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBy the way, it is named the \\(t\\)-distribution by it’s developer, William Sealy Gosset, who worked for the Guinness brewery in Dublin, Ireland. In his 1908 paper, Gosset introduced the \\(t\\)-distribution but he didn’t explicitly explain his choice of the letter \\(t\\). The choice of the letter \\(t\\) could be to indicate “Test”, as the \\(t\\)-distribution was developed specifically for hypothesis testing.\n\n\nNow, recall that the p-value is the probability of observing the value of the test statistic (so here the \\(t\\)-statistic) at least as extreme as the one we have, given the null hypothesis is true. We can calculate this probability by integrating the \\(t\\)-distribution from the observed \\(t\\)-statistic to the tails of the distribution.\nHere is a graph of the \\(t\\)-distribution with 100 degrees of freedom with the tails of the distribution shaded so that the area of the shaded region is 0.05 (i.e., 5% of the total area).\n\n\n\n\n\n\n\n\n\nAnd here’s a graph of the \\(t\\)-distribution with 1000 degrees of freedom (blue line) and the normal distribution (green dashed line):\n\n\n\n\n\n\n\n\n\nSo, with a large number of observations, the \\(t\\)-distribution approaches the normal distribution. For the normal distribution, the 95% area is between -1.96 and 1.96.\n\n## x value for 95% area of normal distribution\nx_value &lt;- qnorm(0.975)\nx_value\n\n[1] 1.959964\n\n\nqnorm is a function that calculates the \\(x\\) value for a given quantile (probability) of the normal distribution. In simpler terms, it finds the value \\(x\\) at which the area under the normal curve (up to \\(x\\)) equals the given probability \\(p\\) (0.975 in the example immediately above here).\nLet’s go back to the age - blood pressure data and calculate the p-value for the slope estimate.\n\n\n\n\n\n\n\n\n\nHere’s the model:\n\nmod1 &lt;- lm(Systolic_BP ~ Age, data = bp_data)\n\nHere we calculate the \\(t\\)-statistic for the slope estimate:\nAnd here we calculate the one-tailed and two-tailed \\(p\\)-values:\n\n\n         Age \n3.746958e-51 \n\n\n         Age \n7.493917e-51 \n\n\nWe can get the \\(p\\)-value directly from the summary function:\n\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 \n\n\nConclusion: there is very strong evidence that the blood pressure is associated with age, because the \\(p\\)-value is extremely small (thus it is very unlikely that the observed slope value or a large one would be seen if there was really no association). Thus, we can reject the null hypothesis that the slope is zero.\nThis basically answers question 1: “Are the parameters compatible with some specific value?”\n\nRecap: Formal definition of the \\(p\\)-value\nThe formal definition of \\(p\\)-value is the probability to observe a data summary (e.g., an average or a slope) that is at least as extreme as the one observed, given that the null hypothesis is correct.\nExample (normal distribution): Assume that we calculated that \\(t\\)-value = -1.96\n\\(\\Rightarrow\\) \\(Pr(|t|\\geq 1.96)=0.05\\) (two-tailed) and \\(Pr(t\\leq-1.96)=0.025\\) (one-tailed).\nAnd here is a graph showing this:\n\n\n\n\n\n\n\n\n\n\n\nA cautionary note on the use of \\(p\\)-values\nMaybe you have seen that in statistical testing, often the criterion \\(p\\leq 0.05\\) is used to test whether \\(H_0\\) should be rejected. This is often done in a black-or-white manner. However, we will put a lot of attention to a more reasonable and cautionary interpretation of \\(p\\)-values in this course!",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#how-strong-is-the-relationship",
    "href": "5.1-regression-part2.html#how-strong-is-the-relationship",
    "title": "Regression Part 2",
    "section": "How strong is the relationship?",
    "text": "How strong is the relationship?\nThe actual value of the slope has practical meaning. The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit. The slope is one measure of the strength of the relationship between the two variables.\nWe can ask what values of a parameter estimate are compatible with the data (confidence intervals)? To answer this question, we can determine the confidence intervals of the regression parameters.\nThe confidence interval of a parameter estimate is defined as the interval that contains the true parameter value with a certain probability. So the 95% confidence interval of the slope is the interval that contains the true slope with a probability of 95%.\nWe can then imagine two cases. The 95% confidence interval of the slope includes 0:\n\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\nℹ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\nOr where the confidence interval does not include zero:\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\nHow do we calculate the lower and upper limits of the 95% confidence interval of the slope?\nRecall that the \\(t\\)-value for a null hypothesis of slope of zero is defined as:\n\\[t = \\frac{\\hat\\beta_1}{\\hat\\sigma^{(\\beta_1)}}\\]\nThe first step is to calculate the \\(t\\)-value that corresponds to a p-value of 0.05. This is the \\(t\\)-value that corresponds to the 97.5% quantile of the \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\n\\(t_{0.975} = t_{0.025} = 1.96\\), for large \\(n\\).\nThe 95% confidence interval of the slope is then given by:\n\\[\\hat\\beta_1 \\pm t_{0.975} \\cdot \\hat\\sigma^{(\\beta_1)}\\]\nIn our blood pressure example the estimated slope is 0.8240678 and the standard error of the slope is 0.0277096. We can calculate the 95% confidence interval of the slope in R as follows:\n\nn &lt;- 100\nt_0975 &lt;- qt(0.975, df = n - 2)\nhalf_interval &lt;- t_0975 * summary(mod1)$coef[2,2]\nlower_limit &lt;- coef(mod1)[2] - half_interval\nupper_limit &lt;- coef(mod1)[2] + half_interval\nci_slope &lt;- c(lower_limit, upper_limit)\nslope &lt;- coef(mod1)[2]\nslope\n\n      Age \n0.8240678 \n\nci_slope\n\n      Age       Age \n0.7690791 0.8790565 \n\n\nOr, using the confint function:\n\n\n    2.5 %    97.5 % \n0.7690791 0.8790565 \n\n\nOr we can do it using values from the coefficients table:\n\ncoefs &lt;- summary(mod1)$coef\nbeta &lt;- coefs[2,1]\nsdbeta &lt;- coefs[2,2] \nbeta + c(-1,1) * qt(0.975,241) * sdbeta \n\n[1] 0.7694840 0.8786516\n\n\nInterpretation: for an increase in the age by one year, roughly 0.82 mmHg increase in blood pressure is expected, and all true values for \\(\\beta_1\\) between 0.77 and 0.88 are compatible with the observed data.",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#confidence-and-prediction-bands",
    "href": "5.1-regression-part2.html#confidence-and-prediction-bands",
    "title": "Regression Part 2",
    "section": "Confidence and Prediction Bands",
    "text": "Confidence and Prediction Bands\n\nRemember: If another sample from the same population was taken, the regression line would look slightly different.\nThere are two questions to be asked:\n\n\nWhich other regression lines are compatible with the observed data? This leads to the confidence band.\nWhere do future observations (\\(y\\)) with a given \\(x\\) coordinate lie? This leads to the prediction band.\n\nNote: The prediction band is much broader than the confidence band.",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#calculation-of-the-confidence-band",
    "href": "5.1-regression-part2.html#calculation-of-the-confidence-band",
    "title": "Regression Part 2",
    "section": "Calculation of the confidence band",
    "text": "Calculation of the confidence band\nGiven a fixed value of \\(x\\), say \\(x_0\\). The question is:\nWhere does \\(\\hat y_0 = \\hat\\beta_0 + \\hat\\beta_1 x_0\\) lie with a certain confidence (i.e., 95%)?\nThis question is not trivial, because both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are estimates from the data and contain uncertainty.\nThe details of the calculation are given in Stahel 2.4b.\nPlotting the confidence interval around all \\(\\hat y_0\\) values one obtains the confidence band or confidence band for the expected values of \\(y\\).\nNote: For the confidence band, only the uncertainty in the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) matters.\nHere is the confidence band for the blood pressure data:\n\n\n\n\n\n\n\n\n\nVery narrow confidence bands indicate that the estimates are very precise. In this case the estimated intercept and slope are precise because the sample size is large and the data points are close to the regression line.",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#calculations-of-the-prediction-band",
    "href": "5.1-regression-part2.html#calculations-of-the-prediction-band",
    "title": "Regression Part 2",
    "section": "Calculations of the prediction band",
    "text": "Calculations of the prediction band\nWe can easily predicted an expected value of \\(y\\) for a given \\(x\\) value. But we can also ask w where does a future observation lie with a certain confidence (i.e., 95%)?\nTo answer this question, we have to consider not only the uncertainty in the predicted value caused by uncertainty in the parameter estimates \\(\\hat y_0 =  \\hat\\beta_0 + \\hat\\beta_1 x_0\\), but also the error term \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)}.\nThis is the reason why the prediction band is wider than the confidence band.\nHere’s a graph showing the prediction band for the blood pressure data:\n\n\n\n\n\n\n\n\n\nAnother way to think of the 95% confidence band is that it is where we would expect 95% of the regression lines to lie if we were to collect many samples from the same population. The 95% prediction band is where we would expect 95% of the future observations to lie.",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#that-is-regression-done-at-least-for-our-current-purposes",
    "href": "5.1-regression-part2.html#that-is-regression-done-at-least-for-our-current-purposes",
    "title": "Regression Part 2",
    "section": "That is regression done (at least for our current purposes)",
    "text": "That is regression done (at least for our current purposes)\n\nWhy use (linear) regression?\nFitting the line (= parameter estimation)\nIs linear regression good enough model to use?\nWhat to do when things go wrong?\nTransformation of variables/the response.\nHandling of outliers.\nGoodness of the model: Correlation and \\(R^2\\)\nTests and confidence intervals\nConfidence and prediction bands",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#additional-reading-material",
    "href": "5.1-regression-part2.html#additional-reading-material",
    "title": "Regression Part 2",
    "section": "Additional reading material",
    "text": "Additional reading material\nIf you’d like another perspective and a deeper delve into some of the mathematical details, please look at Chapter 2 of Lineare Regression, p.7-20 (Stahel script), Chapters 3.1, 3.2a-q of Lineare Regression, and Chapters 4.1 4.2f, 4.3a-e of Lineare Regression",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "5.1-regression-part2.html#extras",
    "href": "5.1-regression-part2.html#extras",
    "title": "Regression Part 2",
    "section": "Extras",
    "text": "Extras\n\nRandomisation test for the slope of a regression line\nLet’s use randomisation as another method to understand how likely we are to observe the data we have, given the null hypothesis is true.\nIf the null hypothesis is true, we expect no relationship between \\(x\\) and \\(y\\). Therefore, we can shuffle the \\(y\\) values and fit a regression model to the shuffled data. We can repeat this many times and calculate the slope of the regression line each time. This will give us a distribution of slopes we would expect to observe if the null hypothesis is true.\nFirst, we’ll make some data and get the slope of the regression line. Here is the observed slope and relationship:\n\n\n        x \n0.1251108 \n\n\n\n\n\n\n\n\n\n\n\nNow we’ll use randomisation to test the null hypothesis. We can create lots of examples where the relationship is expected to have a slope of zero by shuffling randomly the \\(y\\) values. Here are 20:\n\n\n\n\n\n\n\n\n\nNow let’s create 19 and put the real one in there somewhere random. Here’s a case where the real data has a quite strong relationship:\n\n\n\n\n\n\n\n\n\nWe can confidently find the real data amount the shuffled data. But what if the relationship is weaker?\n\n\n\n\n\n\n\n\n\nNow its less clear which is the real data. We can use this idea to test the null hypothesis.\nWe do the same procedure of but instead of just looking at the graphs, we calculate the slope of the regression line each time. This gives us a distribution of slopes we would expect to observe if the null hypothesis is true. We can then see where the observed slope lies in this distribution of null hypothesis slopes.\n\n\n\n\n\n\n\n\n\nWe can now calculate the probability of observing the data we have, given the null hypothesis is true.\n\n\n[1] 0.0172\n\n\n\n\nVisualising p-values for regression slopes",
    "crumbs": [
      "Regression Part 2"
    ]
  },
  {
    "objectID": "6.1-anova.html",
    "href": "6.1-anova.html",
    "title": "ANOVA (L6)",
    "section": "",
    "text": "Introduction\nANOVA = ANalysis Of VAriance (Varianzanalyse)\nThe previous two chapters were about linear regression. Linear regression is a type of linear model – recall that in R we used the function lm() to make the regression model. In this chapter we will look at a different type of linear model: analysis of variance (ANOVA).\nRecall that linear regression is a linear model with one continuous explanatory (independent) variable. A continuous explanatory variable is a variable in which values can take any value within a range (e.g., height, weight, temperature).\nIn contrast, analysis of variance (ANOVA) is a linear model with one or more categorical explanatory variables. We will first look at a one-way ANOVA, which has one categorical explanatory variable. Later (in a following chapter) we will look at two-way ANOVA, which has two categorical explanatory variables.\nWhat is a categorical variable? A categorical explanatory variable is a variable that contains values that fall into distinct groups or categories. For example, habitat type (e.g., forest, grassland, wetland), treatment group (e.g., control, low dose, high dose), or diet type (e.g., vegetarian, vegan, omnivore).\nThis means that each observation belongs to one of a limited number of categories or groups. For example, in a study of how blood pressure varies with diet type, diet type is a categorical variable with several levels (e.g., vegetarian, vegan, omnivore). A person can only belong to one diet type category.\nHere are the first several rows of a dataset that contains blood pressure measurements for individuals following different diet types:\nbp_data_diet &lt;- select(bp_data_diet, bp, diet, person_ID)\nbp_data_diet\n\n# A tibble: 50 × 3\n      bp diet          person_ID\n   &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;    \n 1   120 meat heavy    person_1 \n 2    89 vegan         person_2 \n 3    86 vegetarian    person_3 \n 4   116 meat heavy    person_4 \n 5   115 Mediterranean person_5 \n 6   134 meat heavy    person_6 \n 7    99 vegetarian    person_7 \n 8   104 vegetarian    person_8 \n 9   110 Mediterranean person_9 \n10    97 Mediterranean person_10\n# ℹ 40 more rows\nThere are three variables: - bp: blood pressure (continuous response variable) - diet: diet type (categorical explanatory variable) - person_ID: unique identifier for each individual (not used in the analysis)\nNote that the diet variable is of type &lt;chr&gt; which is short for character. In R, categorical variables are often represented as factors.\nAs usual, its a really good idea to visualise the data in as close to “raw” form as possible before doing any analysis. We’ll make a scatterplot of blood pressure versus diet type.\nLooking at this graph it certainly looks like diet type has an effect on blood pressure. But is this effect statistically significant? In other words, are the differences in mean blood pressure between diet types larger than we would expect due to random variation alone?\nAnalysis of variance (ANOVA) is a statistical method that can help us answer this question, and also others.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#introduction",
    "href": "6.1-anova.html#introduction",
    "title": "ANOVA (L6)",
    "section": "",
    "text": "Tip\n\n\n\nWe just used geom_jitter() instead of geom_point() to make a scatterplot. This is because geom_jitter() adds a small amount of random noise to the points, which helps to prevent overplotting when multiple points have the same value (which is common when the x-axis is categorical).\nWhen we use geom_jitter(), we can specify the amount of noise to add in the x and y directions using the width and height arguments, respectively. We must be very careful to not add noise to the y direction if we care about the actual y values (e.g., blood pressure). In this case, we only added noise in the x direction by setting height = 0 to separate the points just enough, but not so much that we could get confused about which of the diets they belong to.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#how-does-it-look-like-in-r",
    "href": "6.1-anova.html#how-does-it-look-like-in-r",
    "title": "ANOVA (L6)",
    "section": "How does it look like in R?",
    "text": "How does it look like in R?\nWe can fit a one-way ANOVA model in R using the same lm() function that we used for linear regression. The only difference is that the explanatory variable is categorical.\nThen instead of using summary() to look at the results, we use the anova() function.\n\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is an ANOVA table. It shows us the sources of variation in the data, along with their associated degrees of freedom (Df), sum of squares (Sum Sq), mean square (Mean Sq), F value, and p-value (Pr(&gt;F)) associate with a getting a F value the same as or greater than the observed F value if the null hypothesis were true.\nThe challenge now is to understand what all of these values mean! Let’s take it step by step.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#what-is-anova",
    "href": "6.1-anova.html#what-is-anova",
    "title": "ANOVA (L6)",
    "section": "What is ANOVA?",
    "text": "What is ANOVA?\nAnalysis of variance is a method to compare whether the observations (e.g., of blood pressure) differ according to some grouping (e.g., diet) that the subjects (e.g., people) belong to.\nWe already know a lot about analysing variance: we compared the total sum of squares (SST), model sum of squares (SSM) and the residual sum of squares (SSE) in the context of linear regression. We used these to calculated the \\(R^2\\) value. The \\(R^2\\) value tells us how much of the total variance in the response variable (e.g., blood pressure) is explained by the explanatory variable (e.g., diet).\nThe same applies to analysis of variance (ANOVA) (as well as regression) because ANOVA is a special case of a linear model, just like regression is also a special case of a linear model.\nThe defining characteristic of ANOVA is that we are comparing the means of groups by analysing variances. Put another way, we will have a single categorical explanatory variable with two or more levels. We will test whether the means of the response variable are the same across all levels of the explanatory variable, and we test this by analysing the variances.\nWhen we have only one categorical explanatory variable, we use a one-way ANOVA. When we have two categorical explanatory variables, we will use a two-way ANOVA (we’ll look at this in a subsequent chapter).",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#anova-as-a-linear-model",
    "href": "6.1-anova.html#anova-as-a-linear-model",
    "title": "ANOVA (L6)",
    "section": "ANOVA as a linear model",
    "text": "ANOVA as a linear model\nJust like linear regression, ANOVA can be expressed as a linear model. The key difference is that in ANOVA, the explanatory variable is categorical rather than continuous.We formulate the linear model as follows:\n\\[y_{ij} = \\mu_j + \\epsilon_{i}\\]\nwhere:\n\n\\(y_{ij}\\) = Blood pressure of individual \\(i\\) with diet \\(j\\)\n\\(\\mu_i\\) = Mean blood pressure of an individual with diet \\(j\\)\n\\(\\epsilon_{i}\\sim N(0,\\sigma^2)\\) is an independent error term.\n\nGraphically, with the blood pressure and diet data, this looks like:\n\n\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is lots of hidden code used to create the data used in the graph above, and to make the graph itself. You can see the code by going to the Github repository for this book.\n\nRewrite the model\nWe usually use a different formulation of the linear model for ANOVA. This is because we usually prefer to express the estimated parameters in terms of differences between means (rather than the means themselves). The reason for this is that then the null hypothesis can be that the differences are zero.\nTo proceed with this formulation, we define one of the groups as the reference group, and make the mean of that equal to the intercept of the model. For example, if we choose the “meat heavy” diet as the reference group, we can write:\n\\[\\mu_{meat} = \\beta_0\\]\nAnd then to express the other group means as deviations from the reference group mean:\n\\[\\mu_{Med} = \\beta_0 + \\beta_1\\] \\[\\mu_{vegan} = \\beta_0 + \\beta_2\\] \\[\\mu_{veggi} = \\beta_0 + \\beta_3\\]\nWhen we write out the entire model, we get:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{1} + \\beta_2 x_i^{2} + \\beta_3 x_i^{3} + \\epsilon_i\\] where: \\(y_i\\) is the blood pressure of individual \\(i\\). \\(x_i^{1}\\) is a binary variable indicating whether individual \\(i\\) is on the Mediterranean diet. \\(x_i^{2}\\) is a binary variable indicating whether individual \\(i\\) is on the vegan diet. \\(x_i^{3}\\) is a binary variable indicating whether individual \\(i\\) is on the vegetarian diet.\nGraphically, the model now looks like this:\n\n\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nInsert a callout box ::: {.callout-hint} Here is something to warp you mind… we described one-way ANOVA as a linear model with one categorical explanatory variable. But as you can see above, we can also describe it as a linear model with multiple binary explanatory variables (one for each group except the reference group). And when we make a linear model in R it really does create multiple binary explanatory variables behind the scenes. So one-way ANOVA and multiple linear regression with multiple binary explanatory variables are really the same thing! And, even more mind-warping, one-way ANOVA and multiple regression (regression with multiple continuous explanatory variables) are also the same thing! So when we look at multiple regression later in the course, you can think of it as just an extension of one-way ANOVA.\n\n\n\n\nThe ANOVA test: The \\(F\\)-test\nAim of ANOVA: to test globally if the groups differ. That is we want to test the null hypothesis that all of the group means are equal:\n\\[H_0: \\mu_1=\\mu_2=\\ldots = \\mu_g\\] This is equivalent to testing if all \\(\\beta\\)s that belong to a categorical variable are = 0.\n\\[H_0: \\beta_1 = \\ldots = \\beta_{g-1} = 0\\] The alternate hypothesis is that \\({H_1}\\): The group means are not all the same.\nA key point is that we are testing a null hypothesis that concerns all the groups. We are not testing if one group is different from another group (which we could do with a \\(t\\)-test on one of the non-intercept \\(\\beta\\)s).\nBecause we are testing a null hypothesis that concerns all the groups, we need to use an \\(F\\)-test. It asks if the model with the group means is better than a model with just the overall mean.\n\n\n\n\n\n\nNote\n\n\n\nThe \\(F\\)-test is called the “\\(F\\)-test” because it is based on the \\(F\\)-distribution, which was named after the statistician Sir Ronald A. Fisher. Fisher developed this statistical method as part of his pioneering work in analysis of variance (ANOVA) and other fields of experimental design and statistical inference.\n\n\nActually, the \\(F\\)-test does not directly test the null hypothesis that all the group means are equal. Instead, it tests whether the model that includes the group means explains significantly more variance in the data than a model that only includes the overall mean (i.e., without considering group differences).\nThe \\(F\\)-test does this by comparing two variance estimates: the variance explained by the group means (between-group variance) and the variance that remains unexplained within each group (within-group variance).\n\n\nInterpretation of the \\(F\\) statistic\nThe \\(F\\)-test involves calculating from the observed data the value of the \\(F\\) statistic, and then computing if that value is large enough to reject the null hypothesis.\nThe \\(F\\) statistic is a ratio of two variances: the variance between groups, and the variance within groups.\nHere is an example with very low within group variability, and high between group variability:\n\n\n\n\n\n\n\n\n\nAnd here’s an example with very high within group variability, and low between group variability:\n\n\n\n\n\n\n\n\n\nSo, when the ratio of between group variance to within group variance is large, the group means are very different compared to the variability within groups. This suggests that the groups are different.\nWhen the ratio is small, the group means are similar compared to the variability within groups. This suggests that the groups are not different.\n\n\\(F\\) increases\n\nwhen the group means become more different, or\nwhen the variability within groups decreases.\n\n\\(F\\) decreases\n\nwhen the group means become more similar, or\nwhen the variability within groups increases.\n\n\n\\(\\rightarrow\\) The larger \\(F\\), the less likely are the data seen under \\(H_0\\).\n\n\nCalculating the \\(F\\) statistic\nRecall that the \\(F\\) statistic is a ratio of two variances. Specifically, it is the ratio of two mean squares (MS):\n\n\\(MS_{model}\\): the variability between groups.\n\\(MS_{residual}\\): the variability within groups.\n\n\\(MS\\) stands for Mean Square, and is a variance estimate.\nThe \\(F\\) statistic is calculated as:\n\\[F = \\frac{MS_{model}}{MS_{residual}}\\]\nTo find the mean squares, we need to calculate the within and the between group sums of squares, and the corresponding degrees of freedom. Let’s go though this step by step.\n\n\nCalculating the sums of squares\nFirst we get the total sum of squares (SST), which quantifies the total variability in the data. This is then split into the explained variability (SSM), and the residual variability (SSE).\nTotal variability: SST = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij}-\\overline{y})^2\\)\nwhere:\n\n\\(y_{ij}\\) is the blood pressure of individual \\(j\\) in group \\(i\\)\n\\(\\overline{y}\\) is the overall mean blood pressure\n\\(n_i\\) is the number of individuals in group \\(i\\)\n\\(k\\) is the number of groups\n\nExplained variability (between group variability): == SSM = \\(\\sum_{i=1}^k n_i (\\overline{y}_{i} - \\overline{y})^2\\)\nwhere:\n\n\\(\\overline{y}_{i}\\) is the mean blood pressure of group \\(i\\)\n\nResidual variability (within group variability): = SSE = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i}  (y_{ij} - \\overline{y}_{i} )^2\\)\n\n\nCalculating the degrees of freedom\nAnd now we need the degrees of freedom for each sum of squares:\nSST degrees of freedom: \\(n - 1\\) (total degrees of freedom is number of observations \\(n\\) minus 1)\nSSM degrees of freedom: \\(k - 1\\) (model degrees of freedom is number of groups \\(k\\) minus 1)\nSSE degrees of freedom: \\(n - k\\) (residual degrees of freedom is total degrees of freedom \\(n - 1\\) minus model degrees of freedom \\(k - 1\\))\n\nTotal degrees of freedom\nThe total degrees of freedom are the degrees of freedom associated with the total sum of squares (\\(SST\\)).\nIn order to calculate the \\(SST\\), we need to calculate the mean of the response variable. This implies that we estimate one parameter (the mean of the response variable). As a consequence, we lose one degree of freedom and so there remain \\(n-1\\) degrees of freedom associated with the total sum of squares (where \\(n\\) is the number of observations).\nWhat do we mean by “lose one degree of freedom”? Imagine we have ten observations. We can calculate the mean of these ten observations. But if we know the mean and nine of the observations, we can calculate the tenth observation. So, in a sense, once we calculate the mean, the value of one of the ten observations is fixed. This is what we mean by “losing one degree of freedom”. When we calculate and use the mean, one of the observations “loses its freedom”.\nFor example, take the numbers 1, 3, 5, 7, 9. The mean is 5. The sum of the squared differences between the observations and the mean is \\((1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2 = 20\\). This is the total sum of squares. The degrees of freedom are \\(5-1 = 4\\).\nThe total degrees of freedom are the total number of observations minus one. That is, the total sum of squares is associated with \\(n-1\\) degrees of freedom.\nAnother perspective in which to think about the total sum of squares and total degrees of freedom is to consider the intercept only model. The intercept only model is a model that only includes the intercept term. The equation of this model would be:\n\\[y_i = \\beta_0 + \\epsilon_i\\] The sum of the square of the residuals for this model is minimised when the predicted value of the response variable is the mean of the response variable. That is, the least squares estimate of \\(\\beta_0\\) is the mean of the response variable:\n\\[\\hat{\\beta}_0 = \\bar{y}\\]\nHence, the predicted value of the response variable is the mean of the response variable. The equation is:\n\\[\\hat{y}_i = \\bar{y} + \\epsilon_i\\]\nThe error term is therefore:\n\\[\\epsilon_i = y_i - \\bar{y}\\] And the total sum of squares is:\n\\[SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\]\nwhere \\(\\hat{y}_i\\) is the predicted value of the response variable for the \\(i\\)th observation, \\(\\bar{y}\\) is the mean of the response variable, and \\(\\epsilon_i\\) is the residual for the \\(i\\)th observation.\nThe intercept only model involves estimating only one parameter, so the total degrees of freedom are the total number of observations minus one \\(n - 1\\).\nTherefore, the total degrees of freedom are the total number of observations minus one.\nBottom line: \\(SST\\) is the residual sum of squares when we fit the intercept only model. The total degrees of freedom are the total number of observations minus one.\n\n\nModel degrees of freedom\nThe model degrees of freedom are the degrees of freedom associated with the model sum of squares (\\(SSM\\)).\nIn the case of the intercept only model, we estimated one parameter, the mean of the response variable.\nIn the case of a categorical variable with \\(k\\) groups, we need \\(k-1\\) parameters (non intercept \\(\\beta\\) parameters), so we lose \\(k-1\\) degrees of freedom. Put another way, when we fit a model with a categorical explanatory variable with \\(k\\) groups, we estimate \\(k-1\\) parameters in addition to the intercept. That is, we estimate the difference between each group and the reference group.\nEach time we estimate a new parameter, we lose a degree of freedom.\n\n\nResidual degrees of freedom\nThe residual degrees of freedom are the total degrees of freedom (\\(n-1\\)) minus the model degrees of freedom (\\(k-1\\)).\nTherefore, the residual degrees of freedom are the degrees of freedom remaining after we estimate the intercept and the other \\(\\beta\\) parameters. There is one intercept and \\(k-1\\) other \\(\\beta\\) parameters, so the residual degrees of freedom are \\(n-1- ( k-1) = n - k\\).\n\n\n\nCalculating the mean square and \\(F\\) statistic\nFrom these sums of squares and degrees of freedom we can calculate the mean squares and \\(F\\)-statistic:\n\\[MS_{model} = \\frac{SS_{\\text{between}}}{k-1} = \\frac{SSM}{k-1}\\]\n\\[MS_{residual} = \\frac{SS_{\\text{within}}}{n-k} = \\frac{SSE}{n-k}\\]\n\\[F = \\frac{MS_{model}}{MS_{residual}}\\]\n\n\n\n\n\n\nNote\n\n\n\nWhy divide by the degrees of freedom? The more observations we have, the greater will be the total sum of squares. The more observations we have, the greater will be the residual sum of squares. So it is not very informative to compare totals. Rather, we need to compare the mean of the sums of squares. Except we don’t calculate the mean by dividing by the number of observations. Rather we divide by the degrees of freedom. The total mean square is an estimate of the variance of the response variable. And the residual mean square is an estimate of the variance of the residuals.\n\n\n\n\n\\(SST\\), \\(SSM\\), \\(SSE\\), and degrees of freedom\nJust a reminder and a summary of some of the material above:\n\n\\(SST\\): degrees of freedom = \\(n-1\\)\n\\(SSM\\): degrees of freedom = \\(k-1\\)\n\\(SSE\\): degrees of freedom = \\(n-k\\)\n\nThe sum of squares add up:\n\\[SST = SSM + SSE\\]\nand the degrees of freedom add up\n\\[(n-1) = (k-1) + (n - k)\\]\n\n\nSource of variance table\nNow we have nearly everything we need. We often express all of this (and a few more quantities) in a convenient table called the sources of variance table (or ANOVA table).\nThe sources of variance table is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the \\(F\\)-statistic.\n\nSources of variance table\n\n\n\n\n\n\n\n\n\nSource\nSum of squares\nDegrees of freedom\nMean square\nF-statistic\n\n\n\n\nModel\n\\(SSM\\)\n\\(k-1\\)\n\\(MSE_{model} = SSM / k-1\\)\n\\(\\frac{MSE_{model}}{MSE_{error}}\\)\n\n\nError\n\\(SSE\\)\n\\(n - 1 - (k-1)\\)\n\\(MSE_{error} = SSE / (n - 1 - (k-1))\\)\n\n\n\nTotal\n\\(SST\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\nBack to the \\(F\\)-test\nOK, so we have calculated the \\(F\\) statistic. But how do we use it to test our hypothesis?\nWe can use the \\(F\\) statistic to calculate a \\(p\\)-value, which tells us how likely our data is under the null hypothesis.\nSome key points:\n\n\\(F\\)-Distribution: The test statistic of the \\(F\\)-test (that is, the \\(F\\)-statistic) follows the \\(F\\)-distribution under the null hypothesis. This distribution arises when comparing the ratio of two independent sample variances (or mean squares).\nRonald Fisher’s Contribution: Fisher introduced the \\(F\\)-distribution in the early 20th century as a way to test hypotheses about the equality of variances and to analyze variance in regression and experimental designs. The “\\(F\\)” in \\(F\\)-distribution honours him.\nVariance Ratio: The test statistic for the \\(F\\)-test is the ratio of two variances (termed mean squares in this case), making the \\(F\\)-distribution the natural choice for modeling this ratio when the null hypothesis is true.\n\nThe \\(F\\)-test is widely used, including when comparing variances, assessing the significance of multiple regression models (see later chapter), conducting ANOVA to test for differences among group means, and for comparing different models.\nRecall that “The \\(F\\)-statistic is calculated as the ratio of the mean square error of the model to the mean square error of the residuals.” And that a large \\(F\\)-statistic is evidence against the null hypothesis that the slopes of the explanatory variables are zero. And that a small \\(F\\)-statistic is evidence to not reject the null hypothesis that the slopes of the explanatory variables are zero.\nBut how big does the F-statistic need to be in order to confidently reject the null hypothesis?\nThe null hypothesis that the explained variance of the model is no greater than would be expected by chance. Here, “by chance” means that the slopes of the explanatory variables are zero.\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0\\]\nThe alternative hypothesis is that the explained variance of the model is greater than would be expected by chance. This would occur if the slopes of some or all of the explanatory variables are not zero.\n\\[H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0 \\text{ or } \\ldots \\text{ or } \\beta_p \\neq 0\\]\nTo test this hypothesis we are going to, as usual, calculate a \\(p\\)-value. The \\(p\\)-value is the probability of observing a test statistic as or more extreme as the one we observed, assuming the null hypothesis is true. To do this, we need to know the distribution of the test statistic under the null hypothesis. The distribution of the test statistic under the null hypothesis is known as the \\(F\\)-distribution.\nThe \\(F\\)-distribution has two degrees of freedom values associated with it: the degrees of freedom of the model and the degrees of freedom of the residuals. The degrees of freedom of the model are the number of parameters estimated by the model corresponding to the null hypothesis. The degrees of freedom of the residuals are the total degrees of freedom minus the degrees of freedom of the model.\nHere is the \\(F\\)-distribution with 2 and 99 degrees of freedom:\n\n\n\n\n\n\n\n\n\nThe F-distribution is skewed to the right and has a long tail. The area to the right of 3.89 is shaded in red. This area represents the probability of observing an F-statistic as or more extreme as 3.89, assuming the null hypothesis is true. This probability is the \\(p\\)-value of the hypothesis test.\nThe \\(F\\)-statistic and \\(F\\)-test is briefly recaptured in 3.1.f) of the Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact that\n\\[\\frac{MSE_{model}}{MSE_{residual}} =  \\frac{SSM/p}{SSE/(n-1-p)} \\sim F_{p,n-1-p}\\]\nfollows an \\(F\\)-distribution with \\(p\\) and \\((n-1-p)\\) degrees of freedom, where \\(p\\) are the number of continuous variables, \\(n\\) the number of data points.\n\n\\(SSE=\\sum_{i=1} ^n(y_i-\\hat{y}_i)^2\\) is the residual sum of squares\n\\(SSM = SST - SSE\\) is the sum of squares of the model\n\\(SST=\\sum_{i=1}^n(y_i-\\overline{y})^2\\) is the total sum of squares\n\\(n\\) is the number of data points\n\\(p\\) is the number of explanatory variables in the regression model\n\nWell, that is ANOVA conceptually. But how does it actually look like in R?",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#doing-anova-in-r",
    "href": "6.1-anova.html#doing-anova-in-r",
    "title": "ANOVA (L6)",
    "section": "Doing ANOVA in R",
    "text": "Doing ANOVA in R\nLet’s go back again the question of how diet effects blood pressure. Here is the data:\n\n\n# A tibble: 6 × 3\n     bp diet          person_ID\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;    \n1   120 meat heavy    person_1 \n2    89 vegan         person_2 \n3    86 vegetarian    person_3 \n4   116 meat heavy    person_4 \n5   115 Mediterranean person_5 \n6   134 meat heavy    person_6 \n\n\n\n\n\n\n\n\n\n\n\nAnd here is how we fit a linear model to this data:\nIMPORTANT: Since ANOVA is a linear model, it is important to check the assumptions of linear models before interpreting the results. These are some of the same assumptions we checked for simple linear regression, including: independence of errors, normality of residuals, and homoscedasticity (constant variance of residuals).\nAs with linear regression, we check the assumptions are not too badly broken by looking at diagnostic plots:\n\n\n\n\n\n\n\n\n\nNothing looks too bad.\n** Think-pair-share**: Which of the four plots above would you use to check each of the three assumptions listed above?\n** Think-pair-share**: Before we look at the ANOVA table, lets figure out the total degrees of freedom, the degrees of freedom for the model, and the degrees of freedom for the residuals. Have a think-pair-share about each of these. Write you ideas down. Chat with you neighbour. Then share with the class.\nNow we can look at the ANOVA table:\n\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows the sum of squares, degrees of freedom, mean square, F value, and p-value for the model and residuals. As we know, the \\(F\\) value (\\(F\\) statistics) is calculated as the mean square of the model divided by the mean square of the residuals. The p-value is calculated based on the F-distribution with the appropriate degrees of freedom.\nA suitable sentence to report our findings would be: “Diet has a significant effect on blood pressure (\\(F(2, 27) = 20.7, p &lt; 0.0001\\))”. This means that the probability of observing such a large \\(F\\) value under the null hypothesis is less than 0.01%.\nThink-pair-share: You know that the \\(R^2\\) value is a measure of how much variance in the response variable is explained by the model. How would you calculate the \\(R^2\\) value from the ANOVA table above?",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#difference-between-pairs-of-groups",
    "href": "6.1-anova.html#difference-between-pairs-of-groups",
    "title": "ANOVA (L6)",
    "section": "Difference between pairs of groups",
    "text": "Difference between pairs of groups\nRecall that the \\(F\\) test is a global test. It tests the null hypothesis that all group means are equal. It does not tell us which groups are different from each other. It just tells us that at least one group mean is different. Sometimes researchers are interested in more specific questions such as:\n\nfinding the actual group(s) that deviate(s) from the others.\nin estimates of the pairwise differences.\n\nThe summary table in R provides some of these comparison, specifically it contains the estimates for \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) (while the reference was set to \\(\\beta_0 = 0\\)).\nFor example, here is the summary table for our diet data:\n\n\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        122.625      2.302  53.260  &lt; 2e-16 ***\ndietMediterranean  -12.688      3.256  -3.897 0.000314 ***\ndietvegan          -26.768      4.173  -6.414 6.92e-08 ***\ndietvegetarian     -23.625      3.607  -6.549 4.33e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n\n\nIn this table we have the intercept (\\(\\beta_0\\)) and the three \\(\\beta\\) values for the diet groups: “dietMeat”, “dietVegetarian”, and “dietVegan”. The Estimate column shows the estimated coefficients for each group. The intercept (\\(\\beta_0\\)) represents the mean blood pressure for the reference group (in this case, the “meat” diet group). The other three coefficients represent the difference in mean blood pressure between each diet group and the reference group.\nAll well and good up to a point. But there are two issues with using the results from this table:\n\nThe greater the number of individual tests, the more likely one will be significant just by chance. This is called the problem of multiple comparisons. Many test can result in a type-I error: rejecting the null hypothesis when it is actually true. The more tests one does, the more likely one is to make a type-I error.\n\n** Think-pair-share**: Imagine that when our threshold p-value each individual test is 0.05 (5%) so that if it is less than 0.05 we call it “significant” and if it is greater than 0.05 we call it “not significant” (this is the standard practice in many fields). When we make 20 hypothesis tests, how many would we expect to be “significant” just by chance (i.e., when we assume that all null hypotheses is true.)\n\nThe summary table does not provide all the possible pairwise comparisons. It does not, for example, provide the comparison between the “vegan” and the “vegetarian” group.\n\nSeveral methods to circumvent the problem of too many “significant” test results (type-I error) have been proposed. The most prominent ones are:\n\nBonferroni correction\nTukey Honest Significant Differences (HSD) approach\nFisher Least Significant Differences (LSD) approach\n\nThe second two when implemented in R also provide all possible pairwise comparisons.\n\nBonferroni correction\nIdea: If a total of \\(m\\) tests are carried out, simply divide the type-I error level \\(\\alpha_0\\) (often 5%) such that\n\\[\\alpha = \\alpha_0 / m \\ .\\]\nBut this still leaves the problem of how to efficiently get all of the possible pairwise comparisons. We can do this using the pairwise.t.test function in R:\n\npairwise.t.test(bp_data_diet$bp,\n                bp_data_diet$diet,\n                p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  bp_data_diet$bp and bp_data_diet$diet \n\n              meat heavy Mediterranean vegan \nMediterranean 0.0019     -             -     \nvegan         4.2e-07    0.0091        -     \nvegetarian    2.6e-07    0.0239        1.0000\n\nP value adjustment method: bonferroni \n\n\nHere we can see that all pairwise comparisons have a p-value less than 0.05, except for the comparison of vegan versus vegetarian, which has a p-value that rounds to 1.0000.\nWe also see in the output the note that “P value adjustment method: bonferroni”, indicating that the Bonferroni correction has been applied to the p-values.\n\n\nTukey HSD approach\nIdea: Take into account the distribution of (max-min) and design a new test.\nIn R we can use the multcomp package to do Tukey HSD tests:\n\nbp_data_diet &lt;- bp_data_diet %&gt;%\n  mutate(diet = as.factor(diet))\nfit &lt;- lm(bp ~ diet, data = bp_data_diet)\nlibrary(multcomp)\n\nLoading required package: mvtnorm\n\n\nLoading required package: survival\n\n\nLoading required package: TH.data\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\nAttaching package: 'TH.data'\n\n\nThe following object is masked from 'package:MASS':\n\n    geyser\n\ntukey_test &lt;- glht(fit, linfct = mcp(diet = \"Tukey\"))\nsummary(tukey_test)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = bp ~ diet, data = bp_data_diet)\n\nLinear Hypotheses:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \nMediterranean - meat heavy == 0  -12.688      3.256  -3.897  0.00168 ** \nvegan - meat heavy == 0          -26.768      4.173  -6.414  &lt; 0.001 ***\nvegetarian - meat heavy == 0     -23.625      3.607  -6.549  &lt; 0.001 ***\nvegan - Mediterranean == 0       -14.080      4.173  -3.374  0.00759 ** \nvegetarian - Mediterranean == 0  -10.938      3.607  -3.032  0.01951 *  \nvegetarian - vegan == 0            3.143      4.453   0.706  0.89305    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe get all the pairwise comparisons, along with their estimates, standard errors, t-values, and p-values. We also get a note Adjusted p values reported -- single-step method, indicating that the Tukey HSD adjustment has been applied to the p-values.\nAgain, all pairwise comparisons have a p-value less than 0.05, except for the comparison of vegan versus vegetarian, which has a p-value of 0.89305.\n\n\nFisher’s LSD approach\nIdea: Adjust the idea of a two-sample test, but use a larger variance (namely the pooled variance of all groups).\n\n\nOther contrasts\nA contrast is a specific comparison between groups. So far we have only considered pairwise contrasts (i.e., comparing two groups at a time). But we can also design more complex contrasts. For example: are diets that contain meat different from diets that do not contain meat?\n\n\n# A tibble: 6 × 4\n     bp diet          person_ID meat_or_no_meat\n  &lt;dbl&gt; &lt;fct&gt;         &lt;chr&gt;     &lt;chr&gt;          \n1   120 meat heavy    person_1  no meat        \n2    89 vegan         person_2  no meat        \n3    86 vegetarian    person_3  no meat        \n4   116 meat heavy    person_4  no meat        \n5   115 Mediterranean person_5  meat           \n6   134 meat heavy    person_6  no meat        \n\n\nHere we defined a new explanatory variable that groups the meat heavy and Mediterranean diet together into a single “meat” group and vegetarian and vegan into a single “no meat” group. We then fit a model with this explanatory variable:\n\nfit_mnm &lt;- lm(bp ~ meat_or_no_meat, data = bp_data_diet)\n\n(We should not look at model diagnostics here, before using the model. But let us continue as if the assumptions are sufficiently met.)\nWe now do something a bit more complicated: we compare the variance explained by the model with four diets to the model with two diets. This is done by comparing the two models using an \\(F\\)-test. We are testing the null hypothesis that the two models are equally good at explaining the data, in which case the two diet model will explain as much variance as the four diet model.\nLet’s look at the ANOVA table of the model comparison:\n\nanova(fit, fit_mnm)\n\nAnalysis of Variance Table\n\nModel 1: bp ~ diet\nModel 2: bp ~ meat_or_no_meat\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 3901.5                                  \n2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see the residual sum of squares of the model with meat or no meat is over 9’000, while that of the four diet model is less than 4’000. That is, the four diet model explains much more variance in the data than the two diet model. The \\(F\\)-test is highly significant, so we reject the null hypothesis that the two models are equally good at explaining the data. And we conclude that its not just whether people eat meat or not, but rather what kind of diet they eat that affects their blood pressure.\nIdeally we do not make a lot of contrasts after we have collected and looked at our data. Rather, we would specify the contrasts we are interested in before we collect the data. This is called a priori contrasts. But sometimes we do exploratory data analysis and then we can make post hoc contrasts. In this case we should be careful to adjust for multiple comparisons.\n\n\nChoosing the reference category\nQuestion: Why was the “heavy meat” diet chosen as the reference (intercept) category?\nAnswer: Because R orders the categories alphabetically and takes the first level alphabetically as reference category.\nSometimes we may want to override this, for example if we have a treatment that is experimentally the control, then it will usually be useful to set this as the reference / intercept level.\nIn R we can set the reference level using the relevel function:\nAnd now make the model and look at the estimated coefficients:\n\n\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         95.857      3.481  27.538  &lt; 2e-16 ***\ndietmeat heavy      26.768      4.173   6.414 6.92e-08 ***\ndietMediterranean   14.080      4.173   3.374  0.00151 ** \ndietvegetarian       3.143      4.453   0.706  0.48386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n\n\nNow we see the estimated coefficients for all diets except the vegan diet. The intercept is the mean individuals with vegan diet.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#communicating-the-results-of-anova",
    "href": "6.1-anova.html#communicating-the-results-of-anova",
    "title": "ANOVA (L6)",
    "section": "Communicating the results of ANOVA",
    "text": "Communicating the results of ANOVA\nWhen communicating the results of an ANOVA, we usually report the \\(F\\)-statistic, the degrees of freedom of the numerator and denominator, and the p-value. For example, we could say:\n\nBlood pressure differed significantly between groups, with the mean of a meat heavy diet being 123 mmHg, while the mean blood pressure of the vegan group was 27 mmHg lower (One-way ANOVA, \\(F(3, 46) = 20.7\\), \\(p &lt; 0.0001\\).\n\nAnd we would make a nice graph, in this case showing each individual observation since there are not too many to cause overplotting. We can also add the estimated means of each group if we like:\n\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_jitter(width = 0.1, height = 0) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n  labs(title = \"Blood Pressure by Diet\",\n       x = \"Diet\",\n       y = \"Blood Pressure (mmHg)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSome people like to see error bars as well, for example showing the 95% confidence intervals of the means:\n\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_jitter(width = 0.1, height = 0, col = \"grey\") +\n  stat_summary(fun = mean, geom = \"point\", color = \"black\", size = 3) +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.2, color = \"black\") +\n  labs(title = \"Blood Pressure by Diet\\nBlack points and error bars show mean ± 95% CI\",\n       x = \"Diet\",\n       y = \"Blood Pressure (mmHg)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are many many plotting styles and preferences. The important thing is to clearly communicate the results, and to not mislead the reader. I find that plotting the individual data points is often a good idea, especially when the sample size is not too large.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#summary-of-what-you-have-learned",
    "href": "6.1-anova.html#summary-of-what-you-have-learned",
    "title": "ANOVA (L6)",
    "section": "Summary of what you have learned",
    "text": "Summary of what you have learned\nPlease referring to the learning objectives document and the learning objectives for this chapter.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#end",
    "href": "6.1-anova.html#end",
    "title": "ANOVA (L6)",
    "section": "end",
    "text": "end",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#end-1",
    "href": "6.1-anova.html#end-1",
    "title": "ANOVA (L6)",
    "section": "end",
    "text": "end",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#two-way-anova-zweiweg-varianzanalyse",
    "href": "6.1-anova.html#two-way-anova-zweiweg-varianzanalyse",
    "title": "ANOVA (L6)",
    "section": "Two-way ANOVA (Zweiweg-Varianzanalyse)",
    "text": "Two-way ANOVA (Zweiweg-Varianzanalyse)\nTwo-way ANOVA is used to analyse a specific type of study design. When we have a study with two categorical treatments and all possible combinations of them, we can use a two-way ANOVA.\nFor example, take the question of how diet and exercise affect blood pressure. Let’s say we can have three levels of diet: meat heavy, Mediterranean, and vegetarian. And that we have two levels of exercise: low and high. And that we have all possible combinations of these two treatments: i.e., we have a total of \\(3 \\times 2 = 6\\) treatment combinations.\nWe can also represent this study design in a table:\n\n\n\n\n\n\n\nExercise (G)\n\n\n\n\n\n\nDiet (B)\n\n\nLow (1)\n\n\nHigh (2)\n\n\n\n\nMeat heavy (1)\n\n\n\n\n\n\n\n\nMediterranean (2)\n\n\n\n\n\n\n\n\nVegetarian (3)\n\n\n\n\n\n\n\n\nThe six empty cells in the table represent the six treatment combinations.\nThis type of study, with all possible combinations, is known as a factorial design. The two treatments are called factors, and the levels of the factors are called factor levels. A fully factorial design is one where all possible combinations of the factor levels are present.\nLet’s look at example data:\n\n\n# A tibble: 6 × 5\n  diet          exercise  reps    bp  error\n  &lt;chr&gt;         &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 Mediterranean high         1  110.  4.59 \n2 Mediterranean low          1  119.  6.79 \n3 meat heavy    high         1  102. -3.13 \n4 meat heavy    low          1  128.  7.56 \n5 vegetarian    high         1  104. -0.823\n6 vegetarian    low          1  111.  1.99 \n\n\nWe can use the xtabs function to create a table of the data, by cross-tabulating the two treatments diet and exercise:\n\nxtabs(~diet + exercise, data = bp_data_2cat)\n\n               exercise\ndiet            low high\n  meat heavy     10   10\n  Mediterranean  10   10\n  vegetarian     10   10\n\n\nThis tells us there are 10 replicates in each of the six treatment combinations.\nAnd a visualisation of the data:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#twoway-plot)\n\n\n\nWhat do you conclude from this plot?\n\n\n\nThe model for 2-way ANOVA\nAssume we have a factorial design with two treatments (factors), factor \\(B\\) and factor \\(G\\).\nAnd that we can label the levels of factor \\(B\\) as \\(i=1,2...\\) and factor \\(G\\) as \\(j=1,2...\\).\nThen we can denote a particular treatment combination as \\(B_iG_j\\).\nAnd let us set the one of the treatment combinations as the intercept of the model, and the let the intercept be equal to the mean of the observations in the treatment combination \\(B1G1\\).\n\\[intercept = \\frac{1}{n_{11}}\\sum_{k=1}^{n_{11}} y_{1,1,k}\\]\nwhere:\n\n\\(y_{1,1,k}\\) is the \\(k\\)th observation in the treatment combination \\(B1G1\\)\n\\(n_{11}\\) is the number of observations in the treatment combination \\(B1G1\\).\n\nAnd we will let all of the other treatment combinations be represented by the effects \\(\\beta_i\\) and \\(\\gamma_j\\).\nThe resulting linear model is:\n\\[y_{ijk} = intercept + \\beta_i + \\gamma_j + (\\beta\\gamma)_{ij} + \\epsilon_{ijk} \\quad \\text{with} \\quad \\epsilon_{ijk} \\sim N(0,\\sigma^2)\\]\nwhere\n\n\\(y_{ijk}\\) is the \\(k\\)th observation in the treatment combination of \\(i\\) and \\(j\\).\n\\((\\beta\\gamma)_{ij}\\) is the interaction effect between the \\(i\\)th level of factor \\(\\beta\\) and the \\(j\\)th level of factor \\(\\gamma\\).\n\nIn this model, we set \\(\\beta_1=\\gamma_1=0\\) and \\((\\beta\\gamma)_{11}=0\\) because they are already included in the intercept.\n\n\nUsing R for 2-way ANOVA\nIn R, a two-way ANOVA is as simple as one-way ANOVA, just add another variable:\n\nmod1 &lt;- lm(bp ~ diet * exercise, data = bp_data_2cat)\n\nNote that, as we saw in the chapter about interactions, we include the main effects of diet and exercise and the interaction term with the short hand diet * exercise.\nOf course we next check the model diagnostics:\n\n\n\n\n\n\n\n\n\nNo clear patterns: all is good.\n\n\nHypothesis testing\nAs is implied by the name “Analysis of variance” we analyse variances, here mean squares, to test hypotheses. And as before we use an \\(F\\)-test to do this. Remember that the \\(F\\)-test is a ratio of two mean squares (where mean squares are a kind of variance).\n\n\n\n\n\n\nThink, Pair, Share (#full-degrees)\n\n\n\nHow many degrees of freedom for error will there be when we fit this model with both main effects and the interaction term? Hint: remember that the degrees of freedom for error is the number of observations minus the number of parameters estimated.\n\n\nWe can have have a null hypothesis of no effect for each of the two main effects and for the interaction. So we can do an \\(F\\)-test for each of these null hypotheses.\nHere is the ANOVA table:\n\n\nAnalysis of Variance Table\n\nResponse: bp\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet           2  390.74  195.37  9.9672 0.0002069 ***\nexercise       1 1297.52 1297.52 66.1966 5.952e-11 ***\ndiet:exercise  2  340.67  170.33  8.6901 0.0005346 ***\nResiduals     54 1058.46   19.60                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: All three of the null hypotheses are rejected. Importantly, we see that the interaction term is significant, which means that the effect of one treatment is different depending on the level of the other treatment. This means that even though the main effects are significant, we cannot interpret them without considering the interaction term. I.e., we cannot say anything general about the effect of diet or exercise alone on blood pressure. We have to qualify any statement about the effect of diet or exercise with “depending on the level of the other treatment”. Or state something like “the exercise reduces blood pressure greatly for people with a meat heavy diet, but reduces blood pressure only slightly for people with a vegetarian diet or Mediterranean diet”.\n\n\nInterpreting coefficients\nWe can look at the estimated coefficients to see the size of the effects, but be aware that it contains only a subset of the possible effects; it would contain different values if a different treatment combination were set to be the intercept of the model. Also be aware that often we are mostly interested in the \\(F\\)-test and their hypotheses test, and are less interest in the coefficients and their \\(t\\)-tests (unlike in a regression model where we are often interested in the coefficients and their \\(t\\)-tests).\nFinally, be aware that it needs a bit of work to interpret the coefficients, because they are relative to the intercept. Let’s try to figure out what that means. First, back to the table of the experimental design. This time we will put in the cells an expression for the mean of that treatment combination:\n\n\n\n\n\n\n\nExercise (G)\n\n\n\n\n\n\nDiet (B)\n\n\nLow (1)\n\n\nHigh (2)\n\n\n\n\nMeat heavy (1)\n\n\n\\(B_1G_1\\)\n\n\n\\(B_1G_2\\)\n\n\n\n\nMediterranean (2)\n\n\n\\(B_2G_1\\)\n\n\n\\(B_2G_2\\)\n\n\n\n\nVegetarian (3)\n\n\n\\(B_2G_1\\)\n\n\n\\(B_3G_2\\)\n\n\n\n\nSo, for example, the mean of the treatment combination “Meat heavy, Low” is \\(B_1G_1\\). And the mean of the treatment combination “Mediterranean, High” is \\(B_2G_2\\).\nHowever, the coefficients in the summary table given by R are not like this. They are coefficients relative to an intercept / reference treatment combination. The reference treatment combination chosen by R is the first level of the first factor and the first level of the second factor. In this case, that is “Meat heavy, Low” – \\(B_1G_1\\).\nAll of the other coefficients are about differences from this reference treatment combination.\nSo, for example, the coefficient for “High” in the “Exercise” factor (appearing as exercisehigh in the summary table) is the difference in mean blood pressure between the treatment combination “Meat heavy, High” (\\(B_1G_2\\)) and the treatment combination “Meat heavy, Low”. Put another way, \\(B_1G_2 = B_1G_1 + \\gamma_2\\) where \\(\\gamma_2\\) is the coefficient for “High” in the “Exercise” factor.\nAnd the coefficient for “Mediterranean” in the “Diet” factor (appearing as dietMediterranean in the summary table) is the difference in mean blood pressure between the treatment combination “Mediterranean, Low” and the treatment combination “Meat heavy, Low”. Put another way, \\(B_2G_1 = B_1G_1 + \\beta_2\\) where \\(\\beta_2\\) is the coefficient for “Mediterranean” in the “Diet” factor.\nLet us for a moment assume that the effects of diet and exercise are additive. If this is the case, then the mean for \\(B_2G_2\\) = \\(B_1G_1 + \\beta_2 + \\gamma_2\\). That is, the mean for “Mediterranean, High” is the mean for “Meat heavy, Low” plus the effect of “Mediterranean” plus the effect of “High”.\nHowever, if the effects are not additive, then the mean for \\(B_2G_2\\) is not \\(B_1G_1 + \\beta_2 + \\gamma_2\\). Rather, it is \\(B_2G_2 = B_1G_1 + \\beta_2 + \\gamma_2 + (\\beta\\gamma)_{22}\\). That is, the mean for “Mediterranean, High” is the mean for “Meat heavy, Low” \\(B_1G_1\\) plus the effect of “Mediterranean” \\(\\beta_2\\) plus the effect of “High” \\(\\gamma_2\\) plus the non-additive effect between “Mediterranean” and “High” \\((\\beta\\gamma)_{22}\\).\nNon-additivity implies an interaction, therefore the non-additive effect is the interaction effect. In the summary table these interaction effects are those that contain a colon (:), e.g., dietMediterranean:exercisehigh.\nHere’s a graphical representation of how the coefficients in the summary table relate to the means of the treatment combinations:\n\n\n\nUnderstanding coefficients\n\n\n\n\n\n\n\n\nThink, Pair, Share (#veghigh-estimate)\n\n\n\nFrom the values in the coefficients table, calculate the estimated mean of the treatment combination “vegetarian, High”.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#summing-up",
    "href": "6.1-anova.html#summing-up",
    "title": "ANOVA (L6)",
    "section": "Summing up",
    "text": "Summing up\n\nANOVA is just another linear model.\nIt is used when we have categorical explanatory variables.\nWe use \\(F\\)-tests to test the null hypothesis of no difference among the means of the groups (categories).\nWe can use contrasts and post-hoc tests to test specific hypotheses about the means of the groups.\nTwo-way ANOVA is used when we have two categorical explanatory variables and can be used to test for interactions between them.",
    "crumbs": [
      "ANOVA (L6)"
    ]
  },
  {
    "objectID": "6.1-anova.html#additional-reading",
    "href": "6.1-anova.html#additional-reading",
    "title": "ANOVA (L6)",
    "section": "Additional reading",
    "text": "Additional reading\nPlease feel free to look at the follow resources for a slightly different perspective and some more information on ANOVA:\n\nChapter 12 from Stahel book Statistische Datenenalyse\nGetting Started with R chapters 5.6 and 6.2",
    "crumbs": [
      "ANOVA (L6)"
    ]
  }
]