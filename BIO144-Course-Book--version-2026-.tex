% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
  8pt,
  a4paper]{book}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{anyfontsize}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={BIO144 Course Book (version 2026)},
  pdfauthor={Owen Petchey},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{BIO144 Course Book (version 2026)}
\author{Owen Petchey}
\date{2026-02-02}
\begin{document}
\frontmatter
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{1}
\tableofcontents
}

\mainmatter
\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This book contains the content of the course BIO144 Data Analysis in
Biology at the University of Zurich. It is intended to be used as a
companion to the lectures and practical exercises of the course. All of
the required content of the course (i.e., what could be in the final
exam) is included in this book. Additional content is included for those
who want to learn more.

Beware that Owen sometimes makes updates to the book during the
semester, so if you have downloaded a copy or taken screenshots, your
copy may not exactly match the most current version. However, all of the
required content will be the same, and any changes will be correcting
typos or improving explanations. If content does change in a way that
would change the answer to a question in the final exam, Owen will
announce this in the lectures and on OLAT.

\section*{How to get a copy of this
book}\label{how-to-get-a-copy-of-this-book}
\addcontentsline{toc}{section}{How to get a copy of this book}

\markright{How to get a copy of this book}

If you'd like a copy of this book for yourself, there are a few ways.
But beware: if you take a local copy then it will not be updated when
Owen makes changes to the online version!

\begin{itemize}
\item
  You can download a PDF version of the entire book: ðŸ“„ Download PDF
\item
  You can download a complete local copy of the HTML version of the
  BIO144 course book from here:\\
  \url{https://github.com/opetchey/BIO144_Course_Book/tree/main}. The
  html files for the book are in the \texttt{\_book} folder, and this is
  the only folder you need for your offline html copy of the book. You
  can open the \texttt{index.html} file in your web browser to read the
  book offline.
\item
  You can get all of the source code for the book from the
  \href{https://github.com/opetchey/BIO144_Course_Book}{GitHub
  repository}. However, you may find it a little complicated to do
  anything useful with it!
\end{itemize}

\section*{Datasets are not real}\label{datasets-are-not-real}
\addcontentsline{toc}{section}{Datasets are not real}

\markright{Datasets are not real}

The datasets used in this book are not real datasets. They were created
to illustrate the methods taught in the course. Any resemblance to real
data is purely coincidental. Please do not use these datasets for any
purpose other than learning the methods taught in this course. The
patterns in the data may not reflect real-world patterns, and should not
be used to draw any conclusions about real-world phenomena.

\section*{Getting the datasets}\label{getting-the-datasets}
\addcontentsline{toc}{section}{Getting the datasets}

\markright{Getting the datasets}

The datasets used in this book are available for download as a zip file
here: \url{course_book_datasets.zip}. You can download this file and
unzip it to get all of the datasets used in the book. The datasets are
in CSV format, which can be opened in R or other statistical software.

\section*{Packages used in this book}\label{packages-used-in-this-book}
\addcontentsline{toc}{section}{Packages used in this book}

\markright{Packages used in this book}

This book uses a number of R add-on packages for data analysis and
visualization. You will need to install these packages in order to run
the code in the book. The required packages are listed in the
\texttt{\_common.r} file in the GitHub repository for the book. Here is
a link to that file:
\href{https://raw.githubusercontent.com/opetchey/BIO144_Course_Book/refs/heads/main/_common.R}{\_common.R}.
You can copy and paste the list of packages from that file into your R
console to install them.

\section*{If you think you found a mistake in this
book}\label{if-you-think-you-found-a-mistake-in-this-book}
\addcontentsline{toc}{section}{If you think you found a mistake in this
book}

\markright{If you think you found a mistake in this book}

If you think you have found a mistake in the book, please say. A really
nice way is to submit an issue on the GitHub repository for the book:
\href{https://github.com/opetchey/BIO144_Course_Book/issues}{Issues page
of the GitHub repository}. You will need a GitHub account to do this,
but they are free and easy to set up. Otherwise tell Owen in person
sometime, or in the OLAT Forum, or by email.

When reporting a mistake, please be as specific as possible about where
the mistake is. A screenshot works well. Or give the chapter and section
number, and copy a chunk of text, as well as a description of the issue
problem of course!

\section*{How this book was made}\label{how-this-book-was-made}
\addcontentsline{toc}{section}{How this book was made}

\markright{How this book was made}

The book was written using a type of RMarkdown. It allows a script with
a mix of normal text and R code to produce chapters and a book that has
a mixture of text, R code, and R output. Rmarkdown is very useful for
making reports, books, presentations, and even websites.

This book is a Quarto book. To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\section*{Acknowledgements}\label{acknowledgements}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

The content was based on lectures originally written by
\href{https://www.ntnu.edu/employees/stefanie.muff}{Dr Stefanie Muff}.

The content of the book was written with the assistance of
\href{https://copilot.github.com/}{Github Copilot}, an AI tool that
helps write code and text.

\bookmarksetup{startatroot}

\chapter*{Introduction (L1)}\label{introduction-l1}
\addcontentsline{toc}{chapter}{Introduction (L1)}

\markboth{Introduction (L1)}{Introduction (L1)}

The first lecture of the course introduces it, gives some important
information, and sets the stage for the rest of the course. Some of the
time in the lecture will be used to create a dataset for use during the
course. It also gives an opportunity to review some of the things about
R and statistics that it is very useful to already know.

The lecture includes:

\begin{itemize}
\tightlist
\item
  Goals of the course
\item
  Course organisation
\item
  AI and the course
\item
  Making a course dataset
\item
  Using RStudio
\item
  Reviewing what you should already know
\item
  Learning objectives
\item
  A general workflow for data analysis
\end{itemize}

\section*{Notation and some
definitions}\label{notation-and-some-definitions}
\addcontentsline{toc}{section}{Notation and some definitions}

\markright{Notation and some definitions}

Throughout the course, we will use the following notation:

\begin{itemize}
\tightlist
\item
  \(x\) for a variable. Typically this variable contains a set of
  observations. These observations are said to represent a sample of all
  the possible observations that could be made of a \emph{population}.
\item
  \(x_1, x_2, \ldots\) for the values of a variable
\item
  \(x_i\) for the \(i\)th value of a scalar variable. This is often
  spoken as ``x sub i'' or the ``i-th value of x''.
\item
  \(x^{(1)}\) for variable 1, \(x^{(2)}\) for variable 2, etc.
\item
  The mean of the sample \(x\) is \(\bar{x}\). This is usually spoken as
  ``x-bar''.
\item
  The mean of \(x\) is calculated as
  \(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\).
\item
  \(n\) is the number of observations in a sample.
\item
  The summation symbol \(\sum\) is used to indicate that the values of
  \(x\) are summed over all values of \(i\) from 1 to \(n\).
\item
  The standard deviation of the sample is \(s\). The standard deviation
  of the population is \(\sigma\).
\item
  The variance is \(s^2\). The variance of the population is
  \(\sigma^2\).
\item
  The variance of the sample is calculated as
  \(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\).
\item
  The standard deviation of the sample is calculated as
  \(s = \sqrt{s^2}\).
\item
  \(y\) is usually used to represent a dependent / response variable.
\item
  \(x\) is usually used to represent an independent / predictor /
  explanatory variable.
\item
  \(\beta_0\) is usually used to denote the intercept of a linear model.
\item
  \(\beta_1\), \(\beta_2\), etc. are usually used to denote the
  coefficients of the independent variables in a linear model.
\item
  Estimates are denoted with a hat, so \(\hat{\beta}_0\) is the estimate
  of the intercept of a linear model.
\item
  Hence, the estimated value of \(y_i\) in a linear regression model is
  \(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i^{(1)}\).
\item
  \(e_i\) is the residual for the \(i\)th observation in a linear model.
  The residual is the difference between the observed value of \(y_i\)
  and the predicted value of \(y_i\) (\(\hat{y_i}\)).
\item
  Often we assume errors are normally distributed with mean 0 and
  variance \(\sigma^2\). This is written as \(e_i \sim N(0, \sigma^2)\).
\item
  SST is the total sum of squares. It is the sum of the squared
  differences between the observed values of \(y\) and the mean of
  \(y\). It is calculated as \(\sum_{i=1}^n (y_i - \bar{y})^2\).
\item
  SSM is the model sum of squares. It is the sum of the squared
  differences between the predicted values of \(y\) and the mean of
  \(y\). It is calculated as \(\sum_{i=1}^n (\hat{y_i} - \bar{y})^2\).
\item
  SSE is the error sum of squares. It is the sum of the squared
  differences between the observed values of \(y\) and the predicted
  values of \(y\). It is calculated as
  \(\sum_{i=1}^n (y_i - \hat{y_i})^2\).
\item
  The variance of \(x\) can be written as \(Var(x)\). The covariance
  between \(x\) and \(y\) can be written as \(Cov(x, y)\).
\item
  Covariance is calculated as
  \(Cov(x, y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\).
\item
  \(H_0\) is the null hypothesis.
\item
  \(\alpha\) is the significance level.
\item
  \emph{df} is the degrees of freedom.
\item
  \(p\) is the p-value.
\end{itemize}

\section*{Data analysis workflow}\label{data-analysis-workflow}
\addcontentsline{toc}{section}{Data analysis workflow}

\markright{Data analysis workflow}

A general workflow for data analysis is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Define the question}: What are you trying to find out?
\item
  \textbf{Define the study}: How will you answer the question? What
  subjects, what observations, what measurements? What experimental
  design? What treatments? What graphics and analyses will you use?
\item
  \textbf{Collect the data}: Gather the necessary data to answer the
  question.
\item
  \textbf{Explore the data}: Use summary statistics and graphics to
  understand the data.
\item
  \textbf{Prepare the data}: Clean and format the data for analysis.
\item
  \textbf{Visualise the data}: Create plots to visualise patterns and
  relationships.
\item
  \textbf{Analyse the data}: Use appropriate statistical methods to
  analyse the data, including checking model assumptions.
\item
  \textbf{Interpret the results}: Draw conclusions from the analysis in
  the context of the original question.
\item
  \textbf{Be critical}: Consider limitations, alternative explanations,
  and the robustness of your conclusions.
\item
  \textbf{Communicate the results}: Present the findings in a clear and
  concise manner, using tables, figures, and written summaries.
\end{enumerate}

\section*{Using Generative AI in R and Data Analysis: Guidance and Good
Practice}\label{using-generative-ai-in-r-and-data-analysis-guidance-and-good-practice}
\addcontentsline{toc}{section}{Using Generative AI in R and Data
Analysis: Guidance and Good Practice}

\markright{Using Generative AI in R and Data Analysis: Guidance and Good
Practice}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

For the final examination, you will use your own computer, but the test
will run inside the Safe Exam Browser, which will be configured to block
all access to generative AI tools, browser-based assistants, external
software, online services, and any AI code copilots inside RStudio or
other IDEs. This means \textbf{no form of generative AI will be
available during the exam}. Because of this, please avoid becoming
overly reliant on GenAI---such as ChatGPT, Claude, Gemini, Copilot, or
similar tools for answering quiz questions, explaining results, fixing
errors, guiding your analysis, or writing code. You must be able to
perform all tasks independently. We also strongly recommend that you do
not use RStudio with Copilot integration during the course, as it will
not function in the exam environment and may leave you under prepared.
Throughout the semester, be sure to practice writing your own R code,
interpreting outputs yourself, and applying statistical reasoning
without AI assistance, as your exam performance will depend entirely on
your own knowledge and skills.

\end{tcolorbox}

Generative AI (GenAI) tools can support learning, exploration, and
coding in R. They can be powerful assistants, but they must be used with
care. This section introduces the types of tools available, provides
guidelines for responsible use, highlights red flags for problematic
usage, and gives examples of good and poor practice.

Typical uses:

\begin{itemize}
\tightlist
\item
  asking conceptual questions\\
\item
  summarizing methods\\
\item
  generating example code\\
\item
  explaining error messages
\end{itemize}

Strengths:

\begin{itemize}
\tightlist
\item
  flexible and conversational\\
\item
  good for brainstorming\\
\item
  can generate starter code
\end{itemize}

Limitations:

\begin{itemize}
\tightlist
\item
  often wrong in subtle ways\\
\item
  may hallucinate functions\\
\item
  cannot see your working R session
\end{itemize}

\subsection*{Guidelines for Good Use of Generative
AI}\label{guidelines-for-good-use-of-generative-ai}
\addcontentsline{toc}{subsection}{Guidelines for Good Use of Generative
AI}

\subsubsection*{Use GenAI as a Helper, Not a Source of
Truth}\label{use-genai-as-a-helper-not-a-source-of-truth}
\addcontentsline{toc}{subsubsection}{Use GenAI as a Helper, Not a Source
of Truth}

Best uses:

\begin{itemize}
\tightlist
\item
  drafting\\
\item
  explanation\\
\item
  syntax reminders\\
\item
  scaffolding
\end{itemize}

Not reliable for:

\begin{itemize}
\tightlist
\item
  model choice\\
\item
  statistical inference\\
\item
  interpreting coefficients\\
\item
  designing analysis workflows\\
\item
  checking assumptions
\end{itemize}

\subsubsection*{Always Verify AI-Generated Code and
Explanations}\label{always-verify-ai-generated-code-and-explanations}
\addcontentsline{toc}{subsubsection}{Always Verify AI-Generated Code and
Explanations}

Check:

\begin{itemize}
\tightlist
\item
  does the code run?\\
\item
  do variable names match?\\
\item
  is the model appropriate?\\
\item
  are assumptions addressed?\\
\item
  is the explanation logically correct?
\end{itemize}

\subsubsection*{Keep Human Judgement
Central}\label{keep-human-judgement-central}
\addcontentsline{toc}{subsubsection}{Keep Human Judgement Central}

GenAI cannot:

\begin{itemize}
\tightlist
\item
  understand scientific questions\\
\item
  evaluate model assumptions\\
\item
  know ecological/biological reasoning\\
\item
  determine appropriate models
\end{itemize}

\subsubsection*{Provide Context
Carefully}\label{provide-context-carefully}
\addcontentsline{toc}{subsubsection}{Provide Context Carefully}

When asking GenAI:

\begin{itemize}
\tightlist
\item
  describe variables\\
\item
  provide example data\\
\item
  specify your goal\\
\item
  show your existing code
\end{itemize}

Better context = better answers.

\subsubsection*{Use GenAI to Improve Understanding, Not Bypass
It}\label{use-genai-to-improve-understanding-not-bypass-it}
\addcontentsline{toc}{subsubsection}{Use GenAI to Improve Understanding,
Not Bypass It}

Helpful:

\begin{itemize}
\tightlist
\item
  \emph{``Explain logistic regression.''}\\
\item
  \emph{``Why do residuals fan out?''}
\end{itemize}

Not helpful:

\begin{itemize}
\tightlist
\item
  \emph{``Do my assignment for me.''}
\end{itemize}

\subsection*{Indicators of Problematic
Usage}\label{indicators-of-problematic-usage}
\addcontentsline{toc}{subsection}{Indicators of Problematic Usage}

\subsubsection*{Code That Does Not Reflect
Ability}\label{code-that-does-not-reflect-ability}
\addcontentsline{toc}{subsubsection}{Code That Does Not Reflect Ability}

Signs:

\begin{itemize}
\tightlist
\item
  unfamiliar advanced syntax\\
\item
  unexplained packages\\
\item
  inconsistent style
\end{itemize}

\subsubsection*{Hallucinated Functions or Nonsensical
Code}\label{hallucinated-functions-or-nonsensical-code}
\addcontentsline{toc}{subsubsection}{Hallucinated Functions or
Nonsensical Code}

Examples:

\begin{itemize}
\tightlist
\item
  \texttt{slope(x)} in mixed models\\
\item
  missing arguments\\
\item
  fabricated packages
\end{itemize}

\subsubsection*{Statistical Errors Typical of
AI}\label{statistical-errors-typical-of-ai}
\addcontentsline{toc}{subsubsection}{Statistical Errors Typical of AI}

Common issues:

\begin{itemize}
\tightlist
\item
  wrong model family\\
\item
  wrong inference logic\\
\item
  invented assumptions\\
\item
  incorrect explanation of coefficients
\end{itemize}

\subsubsection*{Lack of Understanding}\label{lack-of-understanding}
\addcontentsline{toc}{subsubsection}{Lack of Understanding}

Indicators:

\begin{itemize}
\tightlist
\item
  cannot explain model\\
\item
  inconsistent interpretations\\
\item
  identical phrasing to AI output
\end{itemize}

\subsubsection*{Over-Reliance on AI}\label{over-reliance-on-ai}
\addcontentsline{toc}{subsubsection}{Over-Reliance on AI}

Signs:

\begin{itemize}
\tightlist
\item
  using AI for every step\\
\item
  no debugging effort\\
\item
  stagnation in skill development
\end{itemize}

\subsection*{Examples of Good and Problematic
Use}\label{examples-of-good-and-problematic-use}
\addcontentsline{toc}{subsection}{Examples of Good and Problematic Use}

\subsubsection*{Good Use Examples}\label{good-use-examples}
\addcontentsline{toc}{subsubsection}{Good Use Examples}

\textbf{A. Syntax help}\\
\emph{``How do I specify a random slope in \texttt{lme4}?''}

\textbf{B. Clarification}\\
\emph{``How does adding an interaction change interpretation?''}

\textbf{C. Debugging}\\
\emph{``What does `object not found' usually mean?''}

\textbf{D. Brainstorming}\\
\emph{``How can I visualise a logistic regression?''}

\subsubsection*{Problematic Use
Examples}\label{problematic-use-examples}
\addcontentsline{toc}{subsubsection}{Problematic Use Examples}

\textbf{A. Blindly copying model code}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{*}\NormalTok{ x3 }\SpecialCharTok{*}\NormalTok{ x1)}
\end{Highlighting}
\end{Shaded}

\textbf{B. Incorrect statistical logic}\\
AI code labelled as a bootstrap but is actually a permutation test.

\textbf{C. Misleading interpretation}\\
Claims that coefficients assume explanatory variable independence.

\textbf{D. Presenting AI-generated plots without understanding}

\textbf{E. Outsourcing entire workflow}\\
\emph{``Write a script that loads data, cleans it, runs models,
interprets, and writes the report.''}

\subsubsection*{Summary}\label{summary}
\addcontentsline{toc}{subsubsection}{Summary}

Generative AI can:

\begin{itemize}
\tightlist
\item
  help learning\\
\item
  support debugging\\
\item
  provide code scaffolds\\
\item
  explain concepts
\end{itemize}

But it can also:

\begin{itemize}
\tightlist
\item
  hallucinate\\
\item
  produce incorrect models\\
\item
  misinterpret statistics
\end{itemize}

\textbf{Use GenAI as a supportive tool---never as an unquestioned
authority.}\\
Good use of GenAI \emph{supports} learning. Problematic use
\emph{replaces} it.

\subsection*{Common GenAI Errors in R and Statistical
Modelling}\label{common-genai-errors-in-r-and-statistical-modelling}
\addcontentsline{toc}{subsection}{Common GenAI Errors in R and
Statistical Modelling}

Generative AI tools can be helpful for writing R code, exploring ideas,
and learning syntax.\\
However, they sometimes produce \emph{plausible but incorrect} code or
explanations.\\
This section provides real examples of typical GenAI mistakes, with
correct solutions and learning points.

\textbf{Why this matters:} GenAI is a \textbf{pattern-matching system},
not a statistical reasoning engine. It does not understand assumptions,
inference, or modelling logic.Therefore, students should \textbf{never
accept code or explanations without checking them}.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{\texorpdfstring{Incorrect formula structure in
\texttt{lm()}}{Incorrect formula structure in lm()}}\label{incorrect-formula-structure-in-lm}
\addcontentsline{toc}{subsubsection}{Incorrect formula structure in
\texttt{lm()}}

\textbf{Prompt:} \emph{Fit a linear model with main effects and a
two-way interaction between \texttt{x2} and \texttt{x3}.}

\textbf{Incorrect GenAI output:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{*}\NormalTok{ x3 }\SpecialCharTok{*}\NormalTok{ x1, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

This includes an unintended \textbf{three-way interaction} and extra
terms.

\textbf{Correct:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{*}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} Always check model formulas carefully. AI often
adds or removes interactions.

\subsubsection*{Confusing bootstrap and permutation
tests}\label{confusing-bootstrap-and-permutation-tests}
\addcontentsline{toc}{subsubsection}{Confusing bootstrap and permutation
tests}

\textbf{Documented case:} GenAI was asked for a \emph{bootstrap t-test}.

\textbf{Incorrect GenAI code (actually a permutation test):}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_stats }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  perm }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{group)}
  \FunctionTok{t.test}\NormalTok{(df}\SpecialCharTok{$}\NormalTok{value }\SpecialCharTok{\textasciitilde{}}\NormalTok{ perm)}\SpecialCharTok{$}\NormalTok{statistic}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\textbf{Correct bootstrap approach:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_stats }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, \{}
\NormalTok{  sample\_df }\OtherTok{\textless{}{-}}\NormalTok{ df[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(df), }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{), ]}
  \FunctionTok{t.test}\NormalTok{(value }\SpecialCharTok{\textasciitilde{}}\NormalTok{ group, }\AttributeTok{data =}\NormalTok{ sample\_df)}\SpecialCharTok{$}\NormalTok{statistic}
\NormalTok{\})}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} The logic of inference matters. Code that runs
is not necessarily correct.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Incorrect explanation of linear-model
coefficients}\label{incorrect-explanation-of-linear-model-coefficients}
\addcontentsline{toc}{subsubsection}{Incorrect explanation of
linear-model coefficients}

\textbf{Incorrect claim:} \emph{``Coefficients assume independence among
explanatory variable.''}

This is false. Linear model coefficients describe \textbf{conditional
effects within the model}, regardless of collinearity.

\textbf{Learning point:} Interpretations come from the model structure,
not from simplistic assumptions GenAI sometimes invents.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Hallucinated functions in mixed
models}\label{hallucinated-functions-in-mixed-models}
\addcontentsline{toc}{subsubsection}{Hallucinated functions in mixed
models}

\textbf{Incorrect GenAI output:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lmer}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ (}\FunctionTok{slope}\NormalTok{(x) }\SpecialCharTok{|}\NormalTok{ group), }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\texttt{slope()} does not exist.

\textbf{Correct random-slope specification:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lmer}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ (x }\SpecialCharTok{|}\NormalTok{ group), }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} Always verify syntax in package documentation.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Wrong variable names}\label{wrong-variable-names}
\addcontentsline{toc}{subsubsection}{Wrong variable names}

The dataset has variables \texttt{height} and \texttt{age}.

\textbf{Incorrect GenAI output:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(Height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Correct:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} GenAI often guesses variable names. Check
against your data.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Wrong model family for binary
data}\label{wrong-model-family-for-binary-data}
\addcontentsline{toc}{subsubsection}{Wrong model family for binary data}

\textbf{Incorrect GenAI output (linear regression):}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Correct logistic regression:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{data =}\NormalTok{ df, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} For binary response variables, specify the
model family explicitly.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Incorrect explanation of random
intercepts}\label{incorrect-explanation-of-random-intercepts}
\addcontentsline{toc}{subsubsection}{Incorrect explanation of random
intercepts}

\textbf{Incorrect claim:}\\
\emph{``Random intercepts eliminate correlation among repeated
measures.''}

Incorrect --- they \textbf{model} correlation, not eliminate it.

\textbf{Learning point:} Random effects structure determines the implied
correlation. AI explanations are often vague or wrong here.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Omitting interaction terms in
ANOVA}\label{omitting-interaction-terms-in-anova}
\addcontentsline{toc}{subsubsection}{Omitting interaction terms in
ANOVA}

\textbf{Prompt:} \emph{Two-way ANOVA with interaction.}

\textbf{Incorrect:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aov}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ factor1 }\SpecialCharTok{+}\NormalTok{ factor2, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Correct:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aov}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ factor1 }\SpecialCharTok{*}\NormalTok{ factor2, }\AttributeTok{data =}\NormalTok{ df)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} Confirm that the model matches the experimental
design.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{\texorpdfstring{Incorrect use of
\texttt{predict()}}{Incorrect use of predict()}}\label{incorrect-use-of-predict}
\addcontentsline{toc}{subsubsection}{Incorrect use of
\texttt{predict()}}

\textbf{Prompt:} \emph{Predict for new x values.}

\textbf{Incorrect GenAI output:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

This gives \textbf{in-sample fitted values}, not predictions for new
data.

\textbf{Correct:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(model, }\AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} Always specify \texttt{newdata} for
predictions.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{Poor explanations of
multicollinearity}\label{poor-explanations-of-multicollinearity}
\addcontentsline{toc}{subsubsection}{Poor explanations of
multicollinearity}

\textbf{Incorrect GenAI claim:}\\
\emph{``Multicollinearity is indicated when the model p-value is low but
the individual explanatory variable p-values are high.''}

This is an unreliable and incomplete diagnostic.

\textbf{Better diagnostics:}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{car}\SpecialCharTok{::}\FunctionTok{vif}\NormalTok{(model)}
\FunctionTok{cor}\NormalTok{(df)}
\FunctionTok{model.matrix}\NormalTok{(model)}
\end{Highlighting}
\end{Shaded}

\textbf{Learning point:} AI often repeats common internet tropes rather
than robust statistical principles.

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsubsection*{GenAI Summary}\label{genai-summary}
\addcontentsline{toc}{subsubsection}{GenAI Summary}

GenAI can:

\begin{itemize}
\tightlist
\item
  write useful scaffolding code,\\
\item
  provide quick reminders,\\
\item
  help with simple tasks.
\end{itemize}

But it can also:

\begin{itemize}
\tightlist
\item
  hallucinate functions,\\
\item
  give subtly incorrect models,\\
\item
  invent statistical logic,\\
\item
  provide plausible but wrong explanations.
\end{itemize}

\textbf{Advice for students:}\\
Use GenAI as a starting point, not an authority.\\
Always check:

\begin{itemize}
\tightlist
\item
  function names,\\
\item
  model formulas,\\
\item
  assumptions,\\
\item
  interpretations,\\
\item
  and logic.
\end{itemize}

In statistics, clarity of reasoning matters more than code that merely
\emph{runs}.

\section*{Further reading}\label{further-reading}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

Students who are curious and would like to explore these topics further
(purely for their own interest) may find the following resources useful.
Material from these resources will not be examined in the final exam,
unless it is also already present in the course book.

If you would like to read more about reaction time differences between
men and women, this is a quite interesting paper:
\href{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026141}{On
the Implications of a Sex Difference in the Reaction Times of Sprinters
at the Beijing Olympics, by Lipps et al (2011)}. The analyses are
relatively simple, and the implications explored are quite interesting.
Unfortunately, the data used in the paper is not publicly available, so
you cannot use it for practice here.

A more detailed data analysis workflow suggestion is here on the
\href{https://insightsfromdata.io/insights-workflow.html\#insights-workflow}{Insights
from data website.}

Here is an article about
\href{https://www.ijcrt.org/papers/IJCRT2309747.pdf}{Exploring The
Ethical Implications Of Ai In Data Analytics: Challenges And Strategies
For Responsible Implementation}. It is rather brief and high-level, but
may be of interest when you seek an ethical perspective on the use of AI
tools in data analysis.

\bookmarksetup{startatroot}

\chapter*{R and RStudio (L2)}\label{r-and-rstudio-l2}
\addcontentsline{toc}{chapter}{R and RStudio (L2)}

\markboth{R and RStudio (L2)}{R and RStudio (L2)}

\section*{Getting R and RStudio}\label{getting-r-and-rstudio}
\addcontentsline{toc}{section}{Getting R and RStudio}

\markright{Getting R and RStudio}

\textbf{\emph{R}} is a programming language and software environment for
statistical computing and graphics. RStudio is an integrated development
environment (IDE) for R. \textbf{\emph{RStudio}} provides a
user-friendly interface for working with R, including a console, a
script editor, and tools for managing packages and projects.

We highly recommend using \textbf{RStudio} to work with \textbf{R}.

There are two ways to use \textbf{RStudio}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{RStudio Desktop}: a standalone application that you can
  install on your computer. If you choose this option, you will need to
  install R first, and then install RStudio. This usually requires
  administrator privileges on your computer. If you have problems
  installing add-on packages, they will have to be fixed by you or with
  our help (rarely we cannot find a solution). Follow the instructions
  on this website about how to install R and RStudio:
  https://posit.co/download/rstudio-desktop/.
\item
  \textbf{Rstudio Cloud}: a web-based version of RStudio that you can
  use in your web browser. You don't need to install anything on your
  computer, and you can access your work from any computer with an
  internet connection. The Faculty of Science has a
  \href{https://rstudio2024.mnf.uzh.ch}{RStudio Cloud here} that you can
  use (and will have to use during the final exam).
\end{enumerate}

What do we recommend? Try the cloud first. If you like it then continue
to use it.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Whether you use the RStudio application on your computer, or use RStudio
on the Cloud, you are responsible for the safety and persistence of your
files (data, code, etc.). Just because you're using RStudio on the Cloud
does not mean your files are automatically saved forever. Make sure to
download and back up your important files regularly!

\end{tcolorbox}

\section*{Getting to know the RStudio
IDE}\label{getting-to-know-the-rstudio-ide}
\addcontentsline{toc}{section}{Getting to know the RStudio IDE}

\markright{Getting to know the RStudio IDE}

When you open RStudio, you will see a window with four main panes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Source pane}: where you can write and edit R scripts, R
  Markdown documents, and other files. This pane can have multiple tabs,
  so you can have several files open at the same time.
\item
  \textbf{Console pane}: where you can type and execute R commands
  directly. This pane has multiple tabs, including: \textbf{Console},
  \textbf{Terminal}, and \textbf{Jobs}. During this course we will
  mostly use the \textbf{Console} tab.
\item
  \textbf{Environment pane}: where you can see the objects (data frames,
  vectors, etc.) that are currently in your R session. This pane has
  multiple tabs, including: \textbf{Environment}, \textbf{History},
  \textbf{Connections}, and \textbf{Tutorial}. During this course we
  will mostly use the \textbf{Environment} tab.
\item
  \textbf{Files/Plots/Packages/Help pane}: where you can manage files,
  view plots, manage packages, and access help documentation. This pane
  has multiple tabs, including: \textbf{Files}, \textbf{Plots},
  \textbf{Packages}, \textbf{Help}, and \textbf{Viewer}. During this
  course we will mostly use the \textbf{Files}, \textbf{Plots},
  \textbf{Packages}, and \textbf{Help} tabs.
\end{enumerate}

Our \textbf{Scripts} are in the Source pane tabs. The code / script we
write in R is usually saved in a file with the extension \texttt{.R}.
This file can be opened and edited in the Source pane. Creating a new R
script: File \textgreater{} New File \textgreater{} R Script.

You can run code from the script by selecting the code and clicking the
``Run'' button, or by using the keyboard shortcut
\texttt{Ctrl\ +\ Enter} (Windows) or \texttt{Cmd\ +\ Enter} (Mac).

There is so much more to learn about the RStudio IDE, but we will cover
that as we go along in the course.

\section*{Getting to know R}\label{getting-to-know-r}
\addcontentsline{toc}{section}{Getting to know R}

\markright{Getting to know R}

In our newly opened script file, type the following code:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This is a comment. Comments are ignored by R.}
\CommentTok{\# They are useful for explaining what your code does.}
\end{Highlighting}
\end{Shaded}

Then type the following code:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \SpecialCharTok{+} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{exp}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.389056
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sqrt}\NormalTok{(}\DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

Select all the code and run it (using the ``Run'' button or
\texttt{Ctrl\ +\ Enter} / \texttt{Cmd\ +\ Enter}). You should see the
results of the calculations in the Console pane.

Now try assigning values to named object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a }\OtherTok{\textless{}{-}} \DecValTok{5}
\NormalTok{b }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{c }\OtherTok{\textless{}{-}}\NormalTok{ a }\SpecialCharTok{+}\NormalTok{ b}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

In fact, just about everything in R is an object! These objects live in
your R session (i.e., in the memory of your computer), and you can see
them in the Environment pane. You can create objects to store data,
functions, and other information. Your data files are something
different. They are just like other files on your computer, such as
documents, images, and music files. They live on your hard drive (or in
the cloud), and you can import them into R when you need to work with
them.

\end{tcolorbox}

And then print the value of \texttt{c}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{c}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15
\end{verbatim}

You should see the value \texttt{15} printed in the Console pane.

We can also create vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{my\_vector}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1 2 3 4 5
\end{verbatim}

And do maths on vectors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector }\SpecialCharTok{*} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  2  4  6  8 10
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector }\SpecialCharTok{+} \DecValTok{10}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 11 12 13 14 15
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_vector }\SpecialCharTok{\^{}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1]  1  4  9 16 25
\end{verbatim}

We can vectors of strings (text):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_strings }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"apple"}\NormalTok{, }\StringTok{"banana"}\NormalTok{, }\StringTok{"cherry"}\NormalTok{)}
\NormalTok{my\_strings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "apple"  "banana" "cherry"
\end{verbatim}

And can perform operations on strings:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{paste}\NormalTok{(}\StringTok{"I like"}\NormalTok{, my\_strings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "I like apple"  "I like banana" "I like cherry"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{toupper}\NormalTok{(my\_strings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "APPLE"  "BANANA" "CHERRY"
\end{verbatim}

And we can create data frames, which are like tables of data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{),}
  \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{),}
  \AttributeTok{Height =} \FunctionTok{c}\NormalTok{(}\DecValTok{165}\NormalTok{, }\DecValTok{180}\NormalTok{, }\DecValTok{175}\NormalTok{)}
\NormalTok{)}
\NormalTok{my\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     Name Age Height
1   Alice  25    165
2     Bob  30    180
3 Charlie  35    175
\end{verbatim}

Above we have numerous examples of functions: \texttt{exp()},
\texttt{sqrt()}, \texttt{c()}, \texttt{paste()}, \texttt{toupper()}, and
\texttt{data.frame()}. Functions are a fundamental part of R
programming. They are used to perform specific tasks, such as
calculations, data manipulation, and data analysis. All functions have a
name and can take arguments (inputs) and return values (outputs). They
are called by writing the function name followed by parentheses, with
any arguments inside the parentheses.

You likely guessed that there is much much more to learn about R, but we
will cover that as we go along in the course.

\section*{Getting help}\label{getting-help}
\addcontentsline{toc}{section}{Getting help}

\markright{Getting help}

R has a built-in help system that you can use to get information about
functions, packages, and other topics. To access the help system, you
can use the \texttt{?} operator followed by the name of the function or
topic you want to learn about. For example, to get help on the
\texttt{mean()} function, you would type:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?mean}
\end{Highlighting}
\end{Shaded}

This will open the help documentation for the \texttt{mean()} function
in the Help pane of RStudio. The documentation includes a description of
the function, its arguments, and examples of how to use it. Some of the
help documentation is very useful and accessible, other is less so. Over
time you will learn which functions and packages have good
documentation, and you will get better and better at understanding R
help files.

Of course you can use any other resources to get help with R, including
online forums, tutorials, and books. Some popular online resources for R
help include:

\begin{itemize}
\tightlist
\item
  \href{https://stackoverflow.com/questions/tagged/r}{Stack Overflow}
\item
  \href{https://community.rstudio.com/}{RStudio Community}
\item
  \href{https://www.r-bloggers.com/}{R-bloggers}
\item
  \href{https://www.r-graph-gallery.com/}{The R Graph Gallery}
\end{itemize}

You can also use search engines like Google to find answers to your R
questions. Just be sure to include ``R'' in your search query to get
relevant results.

AI assistants like ChatGPT can also be useful for getting help with R
programming. You can ask specific questions about R code, functions, and
packages, and get instant responses.

And of course there is always your course instructors and fellow
students to help you out when you get stuck.

\section*{Add-on packages}\label{add-on-packages}
\addcontentsline{toc}{section}{Add-on packages}

\markright{Add-on packages}

R has a vast ecosystem of add-on packages that extend its functionality.
These packages are collections of functions, data, and documentation
that can be installed and loaded into your R session. There are
thousands of packages available on CRAN (the Comprehensive R Archive
Network) and other repositories like Bioconductor and GitHub.

We will be using several packages throughout this course. To install a
package, you can use the \texttt{install.packages()} function. For
example, to install the \texttt{ggplot2} package, you would type:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

You can also use the RStudio interface to install packages. Go to the
``Packages'' tab in the bottom right pane, click on ``Install'', type
the name of the package you want to install, and click ``Install''.

You can see which packages are currently installed by looking in the
``Packages'' tab.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

You only need to install a package once. After it is installed, you can
load it into your R session using the \texttt{library()} function. Do
not install packages every time you want to use them; just load them
with \texttt{library()}.

\end{tcolorbox}

\section*{R Version and add-on package
versions}\label{r-version-and-add-on-package-versions}
\addcontentsline{toc}{section}{R Version and add-on package versions}

\markright{R Version and add-on package versions}

(This section concerns the Desktop version of R and RStudio, and not so
much the Cloud version, because version management is handled for you in
the Cloud.)

R and its add-on packages are constantly being updated and improved.
This can cause problems when trying to install or use packages that
depend on specific versions of R or other packages.

Imagine that the online version of a package has been updated and now
only works with the lastest version of R. If you are using an older
version of R, you may not be able to install or use that package.

Or if a package depends on another package that has been updated, you
may need to update that package as well to use the first package.

This sounds complicated, but there are some simple steps you can take to
reduce the chances of running into version-related problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Keep your R version up to date.} New versions of R are
  released every 6 months or so, and they often include important bug
  fixes and new features. You can check your current R version by typing
  \texttt{R.version.string} in the Console. To update R, you can
  download the latest version from the
  \href{https://cran.r-project.org/}{CRAN website}.
\item
  \textbf{Keep your add-on packages up to date.} You can update all your
  installed packages by using the \texttt{update.packages()} function.
  This will check for updates for all installed packages and install the
  latest versions. You can also use the RStudio interface to update
  packages by going to the ``Packages'' tab, clicking on ``Update'',
  selecting the packages you want to update, and clicking ``Install
  Updates''.
\item
  \textbf{Do this well before critical deadlines or important events
  (e.g., exams).} Updating R and packages can sometimes lead to
  unexpected issues, so it's best to do it well in advance of when you
  need everything to work perfectly.
\end{enumerate}

Nevertheless, even with these precautions, you may still encounter
version-related issues from time to time. When this happens, don't
panic!

A common problem you might see is an error message when trying to
install or load a package, indicating that the package requires a newer
version of R or another package. The error / warning message might look
like:

\texttt{warning:\ package\ \textquotesingle{}xyz\textquotesingle{}\ requires\ R\ version\ \textgreater{}=\ 4.2.0}

\texttt{Warning\ in\ install.packages:\ package\ â€˜XYZâ€™\ is\ not\ available\ (for\ R\ version\ 4.2.0)}

These messages indicate that the package you are trying to install or
load requires a newer version of R than the one you currently have. To
fix this, you will need to update your R installation to the required
version or higher. Then its also a good idea to update your packages as
well.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

RStudio is also regularly updated, with new version released every
several months or so. Your version of RStudio is independent of your
version of R, so you can update RStudio without changing your R version.
Note that usually your version of RStudio is not as important as your
version of R and the packages you are using. So updating RStudio is
usually not a high priority and doesn't often help solve problems
related to add on package versions.

\end{tcolorbox}

\section*{R Projects}\label{r-projects}
\addcontentsline{toc}{section}{R Projects}

\markright{R Projects}

I always work within R Projects. R Projects help you to organise your
work and keep all files related to a project in one place. They also
make importing data a breeze.

But what is an R Project? An R Project is a directory (folder) that
contains all the files related to a specific project. When you open an R
Project, RStudio automatically sets the working directory to the project
directory, so you don't have to worry about setting the working
directory manually.

To see if you're working within an R Project, look at the top right of
the RStudio window. If you see the name of your project there, you're
good to go. If you see ``Project: (None)'', then you're not working
within an R Project.

If you click on the project name, a dropdown menu will appear. From
there, you can create a new project, open an existing project, or switch
between projects.

\textbf{Create a new R Project:} File \textgreater{} New Project
\textgreater{} New Directory or Existing Directory \textgreater{} New
Project \textgreater{} Choose a name and location for your project
\textgreater{} Create Project.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Get organised!} Put all files for a project in one folder. For
example, I made a folder called \texttt{BIO144\_2026} and put all files
related to this course in that folder. Within that folder, I have
subfolders for \texttt{data}, \texttt{scripts}, and \texttt{results}. I
then create an R Project in the \texttt{BIO144\_2026} folder. This way,
all files related to the course are in one place, and I can easily find
them later.

\end{tcolorbox}

Now, always open and ensure you're working within the R Project for your
project. As mentioned, you can see the project name at the top right of
the RStudio window. And if its not the correct project, click on the
name to get the drop-down list of available projects from which you can
switch to the correct one.

\section*{Importing data}\label{importing-data}
\addcontentsline{toc}{section}{Importing data}

\markright{Importing data}

First, get some data sets for us to work with. \textbf{XYZ} You can
download them from the course website or use your own data sets. Save
the data files in a folder called \texttt{data} within your R Project
directory.

We will use the \texttt{readr} package to import data into R. The
\texttt{readr} package provides functions to read data from various file
formats, including CSV (comma-separated values) files, tab-separated
values files, and others.

To read a CSV file, we can use the \texttt{read\_csv()} function from
the \texttt{readr} package. For example, to read a CSV file called
\texttt{my\_data\_file.csv}, we can use the following code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\NormalTok{my\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/my\_data\_file.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This code will read the \texttt{data.csv} file from the \texttt{data}
folder within the current working directory (which should be the R
Project directory) and store it in a data frame called \texttt{data}.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Easily getting the file path} In RStudio, you can easily get the
file path by putting the cursor in the parentheses of the
\texttt{read\_csv()} function, the press the tab key. A drop-down menu
will appear with options to navigate to the file. This way, you don't
have to type the file path manually.

\end{tcolorbox}

\section*{Viewing the data}\label{viewing-the-data}
\addcontentsline{toc}{section}{Viewing the data}

\markright{Viewing the data}

Once you've imported your data, you can view it in several ways:

\begin{itemize}
\tightlist
\item
  Click on the data frame in the Environment tab in RStudio to open it
  in a new tab.
\item
  Use the \texttt{View()} function to open the data frame in a new tab
  in RStudio.
\item
  Use the \texttt{head()} function to view the first few rows of the
  data frame.
\item
  Use the \texttt{str()} function to view the structure of the data
  frame, including the variable names and types.
\item
  Use the \texttt{summary()} function to get a summary of the data
  frame, including basic statistics for each variable.
\end{itemize}

Another useful function is \texttt{glimpse()} from the \texttt{dplyr}
package, which provides a quick overview of the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{glimpse}\NormalTok{(my\_data)}
\end{Highlighting}
\end{Shaded}

There are many checks you can do to ensure your data was imported
correctly. For example checking if there are duplicated values in a
variable when there shouldn't be:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{any}\NormalTok{(}\FunctionTok{duplicated}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{Name))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] FALSE
\end{verbatim}

The function \texttt{any()} will return \texttt{TRUE} if there are any
duplicated values in the \texttt{Name} variable, and \texttt{FALSE}
otherwise. The function \texttt{duplicated()} returns a logical vector
indicating which values are duplicates. We use the dollar sign
\texttt{\$} to access a specific variable (column) in the data frame. A
logical vector is a vector that contains only \texttt{TRUE} or
\texttt{FALSE} values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{duplicated}\NormalTok{(my\_data}\SpecialCharTok{$}\NormalTok{Name)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] FALSE FALSE FALSE
\end{verbatim}

All three logicals are \texttt{FALSE}, meaning none of the three are
duplicates. If there were duplicates, the corresponding positions in the
logical vector would be \texttt{TRUE}. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{example\_vector }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"A"}\NormalTok{, }\StringTok{"B"}\NormalTok{, }\StringTok{"A"}\NormalTok{, }\StringTok{"C"}\NormalTok{, }\StringTok{"B"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

What do you expect the output of \texttt{duplicated(example\_vector)} to
be?

A final check (though not the final one we could do--there are many
others). Let us check for missing values and get a count of how many
there are in each variable. We can do this with the following
\emph{tidyverse} code:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\NormalTok{my\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\FunctionTok{across}\NormalTok{(}\FunctionTok{everything}\NormalTok{(), }\SpecialCharTok{\textasciitilde{}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(.))))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  Name Age Height
1    0   0      0
\end{verbatim}

Looks complicated eh! Well, that's because it is, for sure. But let's
break it down:

\begin{itemize}
\tightlist
\item
  \texttt{summarise()} creates a new data frame with summary statistics.
\item
  \texttt{across(everything(),\ \textasciitilde{}\ sum(is.na(.)))}
  applies the function \texttt{sum(is.na(.))} to every variable in the
  data frame.
\item
  The \texttt{is.na()} function returns a logical vector indicating
  which values are missing (\texttt{NA}), and the \texttt{sum()}
  function counts the number of \texttt{TRUE} values in that vector
  (i.e., the number of missing values).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Let's assume your data was imported incorrectly.} This means you
have to inspect it carefully. Check that the variable names are correct,
that the data types are correct (e.g., numeric, character, factor), that
there are the correct number of rows and columns. If you find any
issues, you need to find out what caused them, fix them, and re-import
the data (see below).

\end{tcolorbox}

Common data import problems:

\begin{itemize}
\tightlist
\item
  Incorrect delimiter: If your data file uses a different delimiter
  (e.g., tab, semicolon), you need to specify it in the
  \texttt{read\_csv()} function using the \texttt{delim} argument (e.g.,
  \texttt{read\_delim("data.csv",\ delim\ =\ "\textbackslash{}t")} for
  tab-delimited files).
\item
  Missing values: If your data file uses a specific value to represent
  missing data (e.g., ``NA'', ``-999''), you need to specify it in the
  \texttt{read\_csv()} function using the \texttt{na} argument (e.g.,
  \texttt{read\_csv("data.csv",\ na\ =\ c("NA",\ "-999"))}).
\item
  Only one column: If your data file has only one column, it may be
  because the delimiter is incorrect. Check the delimiter and re-import
  the data with the correct delimiter.
\item
  You opened the downloaded file in Excel and then saved it: Excel may
  have changed the format of the file when you opened and saved it.
  Always work with the original downloaded file.
\item
  Wrong path or file name: Make sure the file path and name are correct.
  Remember, when you work in an R Project, you can place the cursor in
  the parentheses of the \texttt{read\_csv()} function and press the tab
  key to navigate to the file.
\end{itemize}

\section*{Data wrangling}\label{data-wrangling}
\addcontentsline{toc}{section}{Data wrangling}

\markright{Data wrangling}

Now we have our data imported and checked, and we're ready to start
working with it. This process is called data wrangling, and it involves
cleaning, transforming, and reshaping the data to make it suitable for
visualisation and analysis.

\subsection*{Clean the variable names}\label{clean-the-variable-names}
\addcontentsline{toc}{subsection}{Clean the variable names}

The first thing I like to do is standardise and clean up the variable
names. I like to use the \texttt{janitor} package for this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'janitor'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    chisq.test, fisher.test
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data }\OtherTok{\textless{}{-}}\NormalTok{ my\_data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

The \texttt{clean\_names()} function from the \texttt{janitor} package
will convert variable names to a consistent format (lowercase, spaces
replaced by underscores, no special characters).

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

When we ran the code \texttt{library(janitor)} we got the message:
\textbf{Attaching package: `janitor' The following objects are masked
from `package:stats': chisq.test, fisher.test} This sometimes happens
when two packages have functions with the same name. In this case, the
\texttt{janitor} package has functions called \texttt{chisq.test()} and
\texttt{fisher.test()}, which are also in the base \texttt{stats}
package. When we load the \texttt{janitor} package, it masks (hides) the
functions from the \texttt{stats} package. This is usually not a
problem, but if you want to use the functions from the \texttt{stats}
package, you can specify the package name when calling the function,
like this: \texttt{stats::chisq.test()}.

\end{tcolorbox}

\subsection*{Manipulate the data frame}\label{manipulate-the-data-frame}
\addcontentsline{toc}{subsection}{Manipulate the data frame}

Functions in the \texttt{dplyr} package are used to manipulate data
frames:

\begin{itemize}
\tightlist
\item
  \texttt{select()}: select columns by position, or by name, or by other
  methods
\item
  \texttt{filter()}: select rows that meet a logical condition
\item
  \texttt{slice()}: select rows by position
\item
  \texttt{arrange()}: reorder rows
\item
  \texttt{mutate()}: add new variables
\end{itemize}

The \texttt{dplyr} package also provides functions to group data frames
and to summarize data:

\begin{itemize}
\tightlist
\item
  \texttt{group\_by()}: add to a data frame a grouping structure
\item
  \texttt{summarize()}: summarize data, respecting any grouping
  structure specified by \texttt{group\_by()}
\end{itemize}

The pipe operator \texttt{\textbar{}\textgreater{}} is used to chain
together multiple operations on a data frame.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Note that you will often see another pipe operator
\texttt{\%\textgreater{}\%} used in examples. The pipe operator
\texttt{\textbar{}\textgreater{}} is a newer version of
\texttt{\%\textgreater{}\%} that is more efficient and easier to use.
The pipe operator \texttt{\textbar{}\textgreater{}} is available in R
version 4.1.0 and later.

\end{tcolorbox}

Lets work through some examples with a sample data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data1 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{, }\StringTok{"David"}\NormalTok{, }\StringTok{"Eva"}\NormalTok{),}
  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{25}\NormalTok{, }\DecValTok{30}\NormalTok{, }\DecValTok{35}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{45}\NormalTok{),}
  \AttributeTok{score =} \FunctionTok{c}\NormalTok{(}\DecValTok{90}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{95}\NormalTok{, }\DecValTok{80}\NormalTok{, }\DecValTok{70}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Here is the same dataset with 100 rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{my\_data2 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{name =} \FunctionTok{paste0}\NormalTok{(}\StringTok{"Person\_"}\NormalTok{, }\FunctionTok{sprintf}\NormalTok{(}\StringTok{"\%03d"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)),}
  \AttributeTok{age =} \FunctionTok{sample}\NormalTok{(}\DecValTok{20}\SpecialCharTok{:}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{),}
  \AttributeTok{score =} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{, }\AttributeTok{mean =} \DecValTok{75}\NormalTok{, }\AttributeTok{sd =} \DecValTok{10}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\subsection*{Select columns}\label{select-columns}
\addcontentsline{toc}{subsection}{Select columns}

We can select columns by name

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(name, score)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   name       score
   <chr>      <dbl>
 1 Person_001  91.9
 2 Person_002  87.3
 3 Person_003  77.8
 4 Person_004  64.5
 5 Person_005  69.8
 6 Person_006  91.2
 7 Person_007  64.3
 8 Person_008  91.9
 9 Person_009  72.6
10 Person_010  70.3
# i 90 more rows
\end{verbatim}

We can select columns by position

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   name       score
   <chr>      <dbl>
 1 Person_001  91.9
 2 Person_002  87.3
 3 Person_003  77.8
 4 Person_004  64.5
 5 Person_005  69.8
 6 Person_006  91.2
 7 Person_007  64.3
 8 Person_008  91.9
 9 Person_009  72.6
10 Person_010  70.3
# i 90 more rows
\end{verbatim}

We can select columns by a condition, for example select only the
numeric columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{where}\NormalTok{(is.numeric))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
     age score
   <int> <dbl>
 1    50  91.9
 2    34  87.3
 3    38  77.8
 4    33  64.5
 5    22  69.8
 6    29  91.2
 7    37  64.3
 8    41  91.9
 9    30  72.6
10    24  70.3
# i 90 more rows
\end{verbatim}

We can select a column by pattern matching, using helper functions, for
example select columns that contain the letter ``a'':

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(}\FunctionTok{contains}\NormalTok{(}\StringTok{"a"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   name         age
   <chr>      <int>
 1 Person_001    50
 2 Person_002    34
 3 Person_003    38
 4 Person_004    33
 5 Person_005    22
 6 Person_006    29
 7 Person_007    37
 8 Person_008    41
 9 Person_009    30
10 Person_010    24
# i 90 more rows
\end{verbatim}

Other helpers include \texttt{starts\_with()}, \texttt{ends\_with()},
\texttt{matches()}, and \texttt{everything()}.

\subsection*{Filter: Getting particular rows of data
{[}\#filter-rows{]}}\label{filter-getting-particular-rows-of-data-filter-rows}
\addcontentsline{toc}{subsection}{Filter: Getting particular rows of
data {[}\#filter-rows{]}}

To get particular rows of data, we can use the \texttt{filter()}
function. This function takes a \emph{logical condition} as an argument
and returns only the rows that meet that condition. For example, to get
all rows where the Age is greater than 30:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 66 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_002    34  87.3
 3 Person_003    38  77.8
 4 Person_004    33  64.5
 5 Person_007    37  64.3
 6 Person_008    41  91.9
 7 Person_011    39  67.3
 8 Person_012    33  96.5
 9 Person_013    41  61.7
10 Person_014    44  80.0
# i 56 more rows
\end{verbatim}

Here, the logical condition is \texttt{age\ \textgreater{}\ 30}.

We can combine multiple conditions using the logical operators
\texttt{\&} (and), \texttt{\textbar{}} (or), and \texttt{!} (not). For
example, to get all rows where the Age is greater than 30 and the Score
is less than 90:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(age }\SpecialCharTok{\textgreater{}} \DecValTok{30} \SpecialCharTok{\&}\NormalTok{ score }\SpecialCharTok{\textless{}} \DecValTok{90}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 60 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_002    34  87.3
 2 Person_003    38  77.8
 3 Person_004    33  64.5
 4 Person_007    37  64.3
 5 Person_011    39  67.3
 6 Person_013    41  61.7
 7 Person_014    44  80.0
 8 Person_015    45  87.3
 9 Person_016    46  81.3
10 Person_018    38  82.9
# i 50 more rows
\end{verbatim}

Other logical operators include \texttt{==} (equal to), \texttt{!=} (not
equal to), \texttt{\textless{}=} (less than or equal to), and
\texttt{\textgreater{}=} (greater than or equal to).

\subsection*{Slice: Getting rows by position
{[}\#slice-rows{]}}\label{slice-getting-rows-by-position-slice-rows}
\addcontentsline{toc}{subsection}{Slice: Getting rows by position
{[}\#slice-rows{]}}

The \texttt{slice()} function allows us to get rows by their position in
the data frame. For example, to get the first two rows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 3
  name         age score
  <chr>      <int> <dbl>
1 Person_001    50  91.9
2 Person_002    34  87.3
\end{verbatim}

I very rarely use this function, as I prefer to use \texttt{filter()}
with logical conditions. I can't think of a good use case for this
function right now! Perhaps you can?

\subsection*{Arrange: Reordering rows}\label{arrange-reordering-rows}
\addcontentsline{toc}{subsection}{Arrange: Reordering rows}

The \texttt{arrange()} function allows us to reorder the rows of a data
frame based on the values in one or more columns. For example, to
reorder the rows by Age in ascending order:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_064    20  88.8
 2 Person_056    21  79.5
 3 Person_091    21  67.4
 4 Person_005    22  69.8
 5 Person_025    22  74.9
 6 Person_033    23  67.3
 7 Person_092    23  72.4
 8 Person_010    24  70.3
 9 Person_017    24  79.1
10 Person_058    24  72.7
# i 90 more rows
\end{verbatim}

I f we want to reorder the rows by Age in descending order, we can use
the \texttt{desc()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(age))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_061    50  79.9
 3 Person_075    50  67.3
 4 Person_085    50  50.1
 5 Person_096    50  86.8
 6 Person_031    49  71.8
 7 Person_078    49  72.6
 8 Person_024    48  81.2
 9 Person_057    48  80.3
10 Person_021    47  66.0
# i 90 more rows
\end{verbatim}

It's unusual to need the rows of a dataset to be arranged in a specific
order, but it can be useful when looking at the data directly.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Note that when you view the data in RStudio, it will always be arranged
by the row number. In the viewer you can sort by clicking on the column
headers.

\end{tcolorbox}

\subsection*{Mutate: Adding new variables
{[}\#mutate-variables{]}}\label{mutate-adding-new-variables-mutate-variables}
\addcontentsline{toc}{subsection}{Mutate: Adding new variables
{[}\#mutate-variables{]}}

The \texttt{mutate()} function allows us to add new variables to a data
frame. For example, to add a new variable called
\texttt{Age\_in\_5\_years} that is the Age plus 5:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_in\_5\_years =}\NormalTok{ age }\SpecialCharTok{+} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 4
   name         age score age_in_5_years
   <chr>      <int> <dbl>          <dbl>
 1 Person_001    50  91.9             55
 2 Person_002    34  87.3             39
 3 Person_003    38  77.8             43
 4 Person_004    33  64.5             38
 5 Person_005    22  69.8             27
 6 Person_006    29  91.2             34
 7 Person_007    37  64.3             42
 8 Person_008    41  91.9             46
 9 Person_009    30  72.6             35
10 Person_010    24  70.3             29
# i 90 more rows
\end{verbatim}

We can add multiple new variables at once:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{age\_in\_5\_years =}\NormalTok{ age }\SpecialCharTok{+} \DecValTok{5}\NormalTok{,}
    \AttributeTok{percentage\_score =}\NormalTok{ score }\SpecialCharTok{/} \DecValTok{100}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 5
   name         age score age_in_5_years percentage_score
   <chr>      <int> <dbl>          <dbl>            <dbl>
 1 Person_001    50  91.9             55            0.919
 2 Person_002    34  87.3             39            0.873
 3 Person_003    38  77.8             43            0.778
 4 Person_004    33  64.5             38            0.645
 5 Person_005    22  69.8             27            0.698
 6 Person_006    29  91.2             34            0.912
 7 Person_007    37  64.3             42            0.643
 8 Person_008    41  91.9             46            0.919
 9 Person_009    30  72.6             35            0.726
10 Person_010    24  70.3             29            0.703
# i 90 more rows
\end{verbatim}

\subsection*{Working with categorical
variables}\label{working-with-categorical-variables}
\addcontentsline{toc}{subsection}{Working with categorical variables}

Variables in a data frame in R have a \emph{type}. The most common types
of variables are numeric and categorical. Numeric variables are
variables that take on numerical values, such as age or score.
Categorical variables are variables that take on a limited number of
values, often representing categories or groups. In R, categorical
variables are typically have \emph{type}
\texttt{\textless{}chr\textgreater{}} which is \texttt{character}. Or
they can be of type \texttt{\textless{}fct\textgreater{}} which is
\texttt{factor}.

When we import data categorical variable is usually imported as a
\texttt{character} variable. For example, the variable \texttt{name} in
our example dataset is a categorical variable of type
\texttt{character}. Look at the first few rows of the dataset again, and
see that below the variable name it says
\texttt{\textless{}chr\textgreater{}} for the \texttt{name} variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_002    34  87.3
 3 Person_003    38  77.8
 4 Person_004    33  64.5
 5 Person_005    22  69.8
 6 Person_006    29  91.2
 7 Person_007    37  64.3
 8 Person_008    41  91.9
 9 Person_009    30  72.6
10 Person_010    24  70.3
# i 90 more rows
\end{verbatim}

This is all totally fine. There are, however, use cases where we might
want to convert a \texttt{character} variable to a \texttt{factor}
variable. Factors are useful when we have a categorical variable with a
fixed number of levels, and we want to specify the order of those
levels. For example, if we had a variable called
\texttt{education\_level} with the values ``High School'',
``Bachelor's'', ``Master's'', and ``PhD'', we might want to convert this
variable to a factor and specify the order of the levels.

Let's make a new dataset to illustrate this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data3 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{name =} \FunctionTok{c}\NormalTok{(}\StringTok{"Alice"}\NormalTok{, }\StringTok{"Bob"}\NormalTok{, }\StringTok{"Charlie"}\NormalTok{, }\StringTok{"David"}\NormalTok{, }\StringTok{"Eve"}\NormalTok{),}
  \AttributeTok{education\_level =} \FunctionTok{c}\NormalTok{(}\StringTok{"Bachelor\textquotesingle{}s"}\NormalTok{, }\StringTok{"Master\textquotesingle{}s"}\NormalTok{, }\StringTok{"PhD"}\NormalTok{, }\StringTok{"High School"}\NormalTok{, }\StringTok{"Bachelor\textquotesingle{}s"}\NormalTok{),}
  \AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{19}\NormalTok{, }\DecValTok{23}\NormalTok{, }\DecValTok{25}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{20}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Look at the structure of this new dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 3
  name    education_level   age
  <chr>   <chr>           <dbl>
1 Alice   Bachelor's         19
2 Bob     Master's           23
3 Charlie PhD                25
4 David   High School        16
5 Eve     Bachelor's         20
\end{verbatim}

We can see that the \texttt{education\_level} variable is of type
\texttt{\textless{}chr\textgreater{}}, which is \texttt{character}.

We can convert the \texttt{education\_level} variable to a factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data3 }\OtherTok{\textless{}{-}}\NormalTok{ my\_data3 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{education\_level =} \FunctionTok{factor}\NormalTok{(education\_level))}
\NormalTok{my\_data3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 3
  name    education_level   age
  <chr>   <fct>           <dbl>
1 Alice   Bachelor's         19
2 Bob     Master's           23
3 Charlie PhD                25
4 David   High School        16
5 Eve     Bachelor's         20
\end{verbatim}

Now, the \texttt{education\_level} variable is of type
\texttt{\textless{}fct\textgreater{}}, which is \texttt{factor}.

Here is a graph of age by education level:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data3, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ education\_level, }\AttributeTok{y =}\NormalTok{ age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-41-1.pdf}}

We have a problem here: the education levels are not in a sensible
order. The first level is ``Bachelor's'', followed by ``High School'',
``Master's'', and ``PhD''.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Why do you think the levels are in this order? We didn't tell R to order
them like this! The answer is that R orders factor levels alphabetically
by default. So when we convert a character variable to a factor without
specifying the order of the levels, R will order them alphabetically.

\end{tcolorbox}

It would be much better to have the levels ordered as ``High School'',
``Bachelor's'', ``Master's'', and ``PhD''.

We can fix this by specifying the order of the levels when we convert
the variable to a factor:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data3 }\OtherTok{\textless{}{-}}\NormalTok{ my\_data3 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{education\_level =} \FunctionTok{factor}\NormalTok{(education\_level,}
                                  \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"High School"}\NormalTok{, }\StringTok{"Bachelor\textquotesingle{}s"}\NormalTok{, }\StringTok{"Master\textquotesingle{}s"}\NormalTok{, }\StringTok{"PhD"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Now when we plot the data again, the education levels are in the correct
order:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data3, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ education\_level, }\AttributeTok{y =}\NormalTok{ age)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-43-1.pdf}}

Another use case is when we are making a linear model and want to
specify the reference level for a categorical variable. We will look at
this when we get to linear models. If you want to skip ahead, you can
see how this works in a section at the end of this chapter.

\section*{Visualisation}\label{visualisation}
\addcontentsline{toc}{section}{Visualisation}

\markright{Visualisation}

There are many many many types of data visualisation. We will not
explore them all in this course! In fact, we will use only a few basic
types of visualisation, but we will use them well and critically. The
three types of visualisation we will focus on are scatter plots,
histograms, and box and whisker plots.

\subsection*{Three basic types of visualisation
{[}\#three-basic-visualisations{]}}\label{three-basic-types-of-visualisation-three-basic-visualisations}
\addcontentsline{toc}{subsection}{Three basic types of visualisation
{[}\#three-basic-visualisations{]}}

\emph{Scatterplots} are used to visualise the relationship between two
continuous variables. Here is an example of a scatterplot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-44-1.pdf}}

\emph{Histograms} are used to visualise the distribution of a single
continuous variable. The axiss are different to scatterplots: the x-axis
is the variable being measured, and the y-axis is the count (or
frequency) of observations in each bin. A bin is a range of values. Here
is an esample of a histogram:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-45-1.pdf}}

\emph{Box and whisker plots} are used to visualise the distribution of a
continuous variable across different categories. Here is an example of a
box and whisker plot. First we add a new variable that is age group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data2 }\OtherTok{\textless{}{-}}\NormalTok{ my\_data2 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    age }\SpecialCharTok{\textless{}} \DecValTok{30} \SpecialCharTok{\textasciitilde{}} \StringTok{"20{-}29"}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{\textgreater{}=} \DecValTok{30} \SpecialCharTok{\&}\NormalTok{ age }\SpecialCharTok{\textless{}} \DecValTok{40} \SpecialCharTok{\textasciitilde{}} \StringTok{"30{-}39"}\NormalTok{,}
\NormalTok{    age }\SpecialCharTok{\textgreater{}=} \DecValTok{40} \SpecialCharTok{\textasciitilde{}} \StringTok{"40{-}49"}
\NormalTok{  ))}
\end{Highlighting}
\end{Shaded}

The new variable \texttt{age\_group} is a categorical variable with
three levels: ``20-29'', ``30-39'', and ``40-49''. We make this using
the \texttt{case\_when()} function. This function works by checking each
condition (which are given as the arguments to the function) in turn,
and assigning the corresponding value when the condition is true. Now we
can make the box and whisker plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(age\_group), }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-47-1.pdf}}

\subsection*{Understanding ggplot2 syntax
{[}\#understanding-ggplot2{]}}\label{understanding-ggplot2-syntax-understanding-ggplot2}
\addcontentsline{toc}{subsection}{Understanding ggplot2 syntax
{[}\#understanding-ggplot2{]}}

We have used the \texttt{ggplot2} package to create visualisations. The
\texttt{ggplot2} package is based on the grammar of graphics, which
provides a consistent way to create visualisations. It is amazing, and
when it was created it revolutionised data visualisation in R.

You can see that for each of the three visualisations, we use the
\texttt{ggplot()} function to create the base plot, and then we add
layers to the plot using the \texttt{+} operator.

The first argument to the \texttt{ggplot()} function is the data frame
that we want to visualise. The layers that we add to the plot each have
two main components. The first component is the \emph{aesthetic
mappings}, which specify how the variables in the data frame are mapped
to the visual properties of the plot (e.g., x-axis, y-axis, color,
size). The second component is the \emph{geometric object}, which
defines how the data is represented in the plot (e.g., points, lines,
bars).

The \emph{aesthetic mappings} are specified using the \texttt{aes()}
function, which takes arguments that define the mappings. Inside the
\texttt{aes()} function, we specify the variables from the data frame
that we want to map to the visual properties of the plot. For example,
in the scatterplot, we map the \texttt{age} variable to the x-axis and
the \texttt{score} variable to the y-axis using
\texttt{aes(x\ =\ age,\ y\ =\ score)}.

The \emph{geometric object} is specified using functions that start with
\texttt{geom\_}, such as \texttt{geom\_point()},
\texttt{geom\_histogram()}, and \texttt{geom\_boxplot()}.

You will notice that for the scatterplot and the box and whisker plot,
we specify both an x- and a y-variable, but for the histogram we only
specify an x-variable. This is because histograms only have one
variable, which is the variable being measured. The y-axis is
automatically calculated as the count (or frequency) of observations in
each bin.

We can customise many features of the graph using additional arguments
to the \texttt{ggplot()} function and the \texttt{geom\_} functions. For
example, we can add titles and labels to the axes using the
\texttt{labs()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Scatterplot of Age vs Score"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Score"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-48-1.pdf}}

We can also change the theme of the plot using the \texttt{theme\_}
functions. For example, to use a minimal theme, and add it the
customisations we already made:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Scatterplot of Age vs Score"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Score"}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-49-1.pdf}}

There are a million and one ways to customise visualisations in
\texttt{ggplot2}. We will explore many of them during the course in a
rather ad-hoc way. In this course we do not \emph{assess} your skill and
competence in making clear and beautiful visualisations. We will,
however, be very happy to help you make beautiful and effective
visualisations for your assignments and projects. And please be sure
that making beautiful and effective visualisations is a skill that is
very highly valued in the workplace.

\subsection*{Saving ggplot visualisations
{[}\#saving-ggplot{]}}\label{saving-ggplot-visualisations-saving-ggplot}
\addcontentsline{toc}{subsection}{Saving ggplot visualisations
{[}\#saving-ggplot{]}}

Another feature that is very useful is to save ggplot visualisations to
objects and then save to a file (for example a pdf). First, here is how
we save a ggplot to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Scatterplot of Age vs Score"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Score"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Now we can save the plot to a file using the \texttt{ggsave()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"scatterplot\_age\_vs\_score.pdf"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ plot1, }\AttributeTok{width =} \DecValTok{8}\NormalTok{, }\AttributeTok{height =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Note two things about the \texttt{ggsave()} function. First, the first
argument is the file name (including the file extension). The file
extension determines the file type (e.g., pdf, png, jpeg). Second, we
can specify the width and height of the plot in inches.

Also note that the file is saved to the current working directory. When
you're working in an R project, this is usually the base directory of
the project. If you want to save your plots in a folder named
\texttt{plots} you would first need to create the folder (if it doesn't
already exist) and then specify the path in the file name:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dir.create}\NormalTok{(}\StringTok{"plots"}\NormalTok{)  }\CommentTok{\# Create the folder if it doesn\textquotesingle{}t exist}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in dir.create("plots"): 'plots' already exists
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"plots/scatterplot\_age\_vs\_score.pdf"}\NormalTok{, }\AttributeTok{plot =}\NormalTok{ plot1, }\AttributeTok{width =} \DecValTok{8}\NormalTok{, }\AttributeTok{height =} \DecValTok{6}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\section*{Review}\label{review}
\addcontentsline{toc}{section}{Review}

\markright{Review}

In this chapter we learned about using R and RStudio for data analysis.
We covered the basics of R programming, including data types, variables,
functions, and control structures. We also learned about importing data
into R, cleaning and wrangling data using the \texttt{dplyr} package,
and visualising data using the \texttt{ggplot2} package. A great
starting point for your journey into data analysis with R!

\section*{Further reading}\label{further-reading-1}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

The data wrangling packages we use, such as \texttt{dplyr} and
\texttt{tidyr}, are part of the \emph{tidyverse}, a collection of R
packages designed for data science. These packages were developed by
Hadley Wickham and his team at RStudio. Moreover, Hadley Wickham is also
the creator of \texttt{ggplot2}, the package we use for data
visualisation.

A great place to consolidate your learning so far, and to learn more
about the \emph{tidyverse} and data science with R is the book \emph{R
for Data Science} by Hadley Wickham and Garrett Grolemund
(https://r4ds.had.co.nz/). This book is available for free online and
covers many topics related to data science with R, including:

\begin{itemize}
\tightlist
\item
  \href{https://r4ds.hadley.nz/workflow-scripts.html}{Scripts and
  projects}
\item
  \href{https://r4ds.hadley.nz/data-import.html}{Data import}
\item
  \href{https://r4ds.hadley.nz/data-transform.html}{Data wrangling with
  dplyr and tidyr}
\item
  \href{https://r4ds.hadley.nz/data-visualize.html}{Data visualisation
  with ggplot2}
\item
  And much more\ldots{}
\end{itemize}

\textbf{Base R} is a term used to describe the core functionality of R,
without any additional packages. While the \emph{tidyverse} packages are
very useful and powerful, it can be useful to learn about the base R
functions, especially if someone else is using base R in their code. A
starting point for learning about base R is the chapter ``A field guide
to base R'' in the \emph{R for Data Science} book --
\href{https://r4ds.hadley.nz/base-R.html}{A field guide to base R}.

\subsection*{Cheat sheets}\label{cheat-sheets}
\addcontentsline{toc}{subsection}{Cheat sheets}

There are a series of excellent cheat sheets for R and RStudio, and for
packages in the \emph{tidyverse}. They're pretty dense and packed with
information, but they can be very useful as a quick reference. You can
find them here: \href{https://posit.co/resources/cheatsheets/}{RStudio
Cheat Sheets}.

\section*{Extras}\label{extras}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{Making reports directly using Quarto
{[}\#quarto-reports{]}}\label{making-reports-directly-using-quarto-quarto-reports}
\addcontentsline{toc}{subsection}{Making reports directly using Quarto
{[}\#quarto-reports{]}}

We don't explicitly ask you to make reports using Quarto in this course,
but it is a very useful skill to have, and I highly recommend you
explore it further in your own time. Here are a few basics to get you
started.

One of the great features of R and RStudio is the ability to create
reports that combine text, code, and visualisations. One of the most
popular tools for this is Quarto (https://quarto.org/), which allows you
to create documents in various formats (HTML, PDF, Word, etc.) using a
combination of \emph{Markdown} and R code.

**Why is this so great???* If you want to show someone your analysis and
visualisation, say a team member or supervisor, it is often good to
prepare a report that explains what you did, perhaps shows the code you
used, and presents the results (including visualisations). One way to go
about this is to prepare a powerpoint presentation or a word document,
and then copy and paste code and visualisations into the document. Its
what I used to do. It works. But it is tedious, error prone, and when
you change something in your code or data, you have to remember to go
back and update the powerpoint or word document.

With Quarto, you can create a report that automatically includes the
code and visualisations directly from your R script. This way, if you
change the code or data, you can simply re-render the report and
everything is automatically updated. It takes away a lot of the
tediousness and potential for errors. And it makes updating reports much
easier.

If you'd like to get started with Quarto, check out the Quarto website
(https://quarto.org/) and the RStudio Quarto documentation
(https://quarto.org/docs/get-started/). There are also many tutorials
and resources available online to help you learn how to use Quarto
effectively.

If you have questions about Quarto, feel free to ask me or TAs during
the practicals, though note that any particular TAs may or may not be
experienced with Quarto themselves.

Quarto reports are also covered in the \emph{R for Data Science} book --
\href{https://r4ds.hadley.nz/quarto.html}{Quarto}.

\subsection*{Combining ggplots with patchwork
{[}\#combining-ggplots{]}}\label{combining-ggplots-with-patchwork-combining-ggplots}
\addcontentsline{toc}{subsection}{Combining ggplots with patchwork
{[}\#combining-ggplots{]}}

We often make multiple ggplots in our analyses. Sometimes it is useful
to combine multiple plots into a single figure for easier comparison or
presentation. We can do with ggplots and the lovely add-on package
called \texttt{patchwork}. The \texttt{patchwork} package allows us to
combine multiple ggplots into a single plot layout. Here is an example
of how to use \texttt{patchwork} to combine the three plots we made
earlier (scatterplot, histogram, and boxplot):

First, load the \texttt{patchwork} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(patchwork)}
\end{Highlighting}
\end{Shaded}

Next make the first plot and assign it to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Scatterplot of Age vs Score"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Score"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Now make the second plot and assign it to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Histogram of Scores"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Score"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Count"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Now make the third plot and assign it to an object:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plot3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(my\_data2, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{factor}\NormalTok{(age\_group), }\AttributeTok{y =}\NormalTok{ score)) }\SpecialCharTok{+}
  \FunctionTok{geom\_boxplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Boxplot of Scores by Age Group"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Age Group"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Score"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

Now we can combine the three plots into a single layout using the
\texttt{patchwork} syntax. Here, we arrange \texttt{plot1} on the top
row, and \texttt{plot2} and \texttt{plot3} side by side on the bottom
row:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{combined\_plot }\OtherTok{\textless{}{-}}\NormalTok{ plot1 }\SpecialCharTok{/}\NormalTok{ (plot2 }\SpecialCharTok{|}\NormalTok{ plot3)}
\NormalTok{combined\_plot}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-57-1.pdf}}

Amazing eh! OK, lets leave it there for now. We'll use ggplot2
throughout the course, and explore more features as we go along.

If you'd like to read more about the patchwork package, check out the
package website --
\href{https://patchwork.data-imaginist.com/}{patchwork}.

\subsection*{Setting a reference level in a linear
model}\label{setting-a-reference-level-in-a-linear-model}
\addcontentsline{toc}{subsection}{Setting a reference level in a linear
model}

Sometimes when fitting linear models with categorical explanatory
(independent) variables, it is useful to set a specific reference level
for the categorical variable. This can help in interpreting the model
coefficients. In R, we can set the reference level using the
\texttt{relevel()} function or by using the \texttt{factor()} function
with the \texttt{levels} argument.

First, let's create a simple dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data4 }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{treatment =} \FunctionTok{factor}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"Control"}\NormalTok{, }\StringTok{"Aspirin"}\NormalTok{, }\StringTok{"Ibuprofen"}\NormalTok{, }\StringTok{"Control"}\NormalTok{, }\StringTok{"Aspirin"}\NormalTok{, }\StringTok{"Ibuprofen"}\NormalTok{)),}
  \AttributeTok{response =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{7}\NormalTok{)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By default, R will set the first level of the factor (in alphabetical
order) as the reference level. In this case, ``Aspirin'' would be the
reference level. Therefore when we visualise the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data4, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ treatment, }\AttributeTok{y =}\NormalTok{ response)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Response by Treatment"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Treatment"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Response"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-60-1.pdf}}

It would be nicer to have the ``Control'' group as the first level on
the left of the x-axis.

Likewise, when we make a linear model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment, }\AttributeTok{data =}\NormalTok{ my\_data4)}
\FunctionTok{summary}\NormalTok{(model1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = response ~ treatment, data = my_data4)

Residuals:
   1    2    3    4    5    6 
 0.5 -0.5 -0.5 -0.5  0.5  0.5 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)          7.5000     0.5000  15.000 0.000643 ***
treatmentControl    -3.0000     0.7071  -4.243 0.023981 *  
treatmentIbuprofen  -1.0000     0.7071  -1.414 0.252215    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7071 on 3 degrees of freedom
Multiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 
F-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152
\end{verbatim}

The (Intercept) term corresponds to the ``Aspirin'' group, and the
coefficients for ``Control'' and ``Ibuprofen'' are relative to
``Aspirin''. \emph{R} has done this because in the factor levels,
``Aspirin'' comes first alphabetically and was therefore set as the
reference level when the factor variable was created.

If we want to set ``Control'' as the reference level, we can do so using
\texttt{relevel()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{my\_data4 }\OtherTok{\textless{}{-}}\NormalTok{ my\_data4 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{treatment =} \FunctionTok{relevel}\NormalTok{(treatment, }\AttributeTok{ref =} \StringTok{"Control"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Now when we visualise the data again:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(my\_data4, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ treatment, }\AttributeTok{y =}\NormalTok{ response)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title =} \StringTok{"Response by Treatment"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"Treatment"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"Response"}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-63-1.pdf}}

Magic! The ``Control'' group is now the first level on the left of the
x-axis.

And when we fit the linear model again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{model2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(response }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment, }\AttributeTok{data =}\NormalTok{ my\_data4)}
\FunctionTok{summary}\NormalTok{(model2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = response ~ treatment, data = my_data4)

Residuals:
   1    2    3    4    5    6 
 0.5 -0.5 -0.5 -0.5  0.5  0.5 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)   
(Intercept)          4.5000     0.5000   9.000   0.0029 **
treatmentAspirin     3.0000     0.7071   4.243   0.0240 * 
treatmentIbuprofen   2.0000     0.7071   2.828   0.0663 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7071 on 3 degrees of freedom
Multiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 
F-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152
\end{verbatim}

The (Intercept) term now corresponds to the ``Control'' group, and the
coefficients for ``Aspirin'' and ``Ibuprofen'' are relative to
``Control''. This makes interpretation of the model coefficients more
intuitive.

Also see the chapter about factors in the R for Data Science book --
\href{https://r4ds.hadley.nz/factors.html}{Factors}.

\bookmarksetup{startatroot}

\chapter*{Regression Part 1 (L3)}\label{regression-part-1-l3}
\addcontentsline{toc}{chapter}{Regression Part 1 (L3)}

\markboth{Regression Part 1 (L3)}{Regression Part 1 (L3)}

\section*{Introduction}\label{introduction}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Linear regression is a common statistical method that models the
relationship between a dependent (response) variable and one or more
independent (explanatory) variables. The relationship is modeled with
the equation for a straight line (\(y = a + bx\)).

With linear regression we can answer questions such as:

\begin{itemize}
\tightlist
\item
  How does the dependent (response) variable change with respect to the
  independent (explanatory) variable?
\item
  What amount of variation in the dependent variable can be explained by
  the independent variable?
\item
  Is there a statistically significant relationship between the
  dependent variable and the independent variable?
\item
  Does the linear model fit the data well?
\end{itemize}

In this chapter / lesson we will explore what is linear regression and
how to use it to answer these questions. We'll cover the following
topics:

\begin{itemize}
\tightlist
\item
  Why use linear regression?
\item
  What is the linear regression model?
\item
  Fitting the regression model (= finding the intercept and the slope).
\item
  Is linear regression a good enough model to use?
\item
  What do we do when things go wrong?
\item
  Transformation of variables/the response.
\item
  Identifying and handling odd data points (aka outliers).
\end{itemize}

In this chapter / lesson we will not discuss the statistical
significance of the model. We will cover this topic in the next chapter
/ lesson.

\subsection*{Why use linear
regression?}\label{why-use-linear-regression}
\addcontentsline{toc}{subsection}{Why use linear regression?}

\begin{itemize}
\tightlist
\item
  It's a good starting point because it is a relatively simple model.
\item
  Relationships are sometimes close enough to linear.
\item
  It's easy to interpret.
\item
  It's easy to use.
\item
  It's actually quite flexible (e.g.~can be used for non-linear
  relationships, e.g., a quadratic model is still a linear model!!!).
\end{itemize}

\subsection*{An example - blood pressure and
age}\label{an-example---blood-pressure-and-age}
\addcontentsline{toc}{subsection}{An example - blood pressure and age}

There are lots of situations in which linear regression can be useful.
For example, consider hypertension. Hypertension is a condition in which
the blood pressure in the arteries is persistently elevated.
Hypertension is a major risk factor for heart disease, stroke, and
kidney disease. It is estimated that hypertension affects about 1
billion people worldwide. Hypertension is a complex condition that is
influenced by many factors, including age. In fact, it is well known
that blood pressure increases with age. But how much does blood pressure
increase with age? This is a question that can be answered using linear
regression.

Here is an example of a study that used linear regression to answer this
question:
https://journals.lww.com/jhypertension/fulltext/2021/06000/association\_of\_age\_and\_blood\_pressure\_among\_3\_3.15.aspx

In this study, the authors used linear regression to model the
relationship between age and blood pressure. They found that systolic
blood pressure increased by 0.28--0.85 mmHg/year. This is a small
increase, but it is statistically significant. This means that the
observed relationship between age and blood pressure is unlikely to be
due to chance.

Lets look at some simulated example data:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data}
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/Simulated\_Blood\_Pressure\_and\_Age\_Data.csv"}\NormalTok{)}

\CommentTok{\# How many data points with both age and blood pressure?}
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(bp\_age\_data)}
\CommentTok{\# No rows with missing values}

\CommentTok{\# Visualize the data}
\FunctionTok{ggplot}\NormalTok{(bp\_age\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =}\NormalTok{ Systolic\_BP)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Systolic Blood Pressure vs. Age"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Age"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Systolic Blood Pressure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-2-1.pdf}}

Well, that is pretty conclusive. We hardly need statistics. There is a
clear positive relationship between age and systolic blood pressure. But
how can we quantify this relationship? And in less clear-cut cases what
is the strength of evidence for a relationship? This is where linear
regression comes in. Linear regression models the relationship between
age and systolic blood pressure. With linear regression we can answer
the following questions:

\begin{itemize}
\tightlist
\item
  What is the value of the intercept and slope of the relationship?
\item
  Is the relationship different from what we would expect if there were
  no relationship?
\item
  How well does the mathematical representation match the observed
  values?
\item
  How much uncertainty is there in predictions?
\end{itemize}

Lets try to figure some of these out from the visualisation.

\section*{Calculating the intercept and
slope}\label{calculating-the-intercept-and-slope}
\addcontentsline{toc}{section}{Calculating the intercept and slope}

\markright{Calculating the intercept and slope}

\subsection*{Regression from a mathematical
perspective}\label{regression-from-a-mathematical-perspective}
\addcontentsline{toc}{subsection}{Regression from a mathematical
perspective}

Given an \textbf{independent/explanatory variable} (\(X\)) and a
\textbf{dependent/response variable} (\(Y\)) all points \((x_i,y_i)\),
\(i= 1,\ldots, n\), on a straight line follow the equation

\[y_i = \beta_0 + \beta_1 x_i\ .\]

\begin{itemize}
\tightlist
\item
  \(\beta_0\) is the \textbf{intercept} - the value of \(Y\) when
  \(x_i = 0\)
\item
  \(\beta_1\) the \textbf{slope} of the line, also known as the
  regression coefficient of \(X\).
\item
  If \(\beta_0=0\) the line goes through the origin \((x,y)=(0,0)\).
\item
  \textbf{Interpretation} of linear dependency: proportional increase in
  \(y\) with increase (decrease) in \(x\).
\end{itemize}

\subsection*{Finding the intercept and the
slope}\label{finding-the-intercept-and-the-slope}
\addcontentsline{toc}{subsection}{Finding the intercept and the slope}

In a regression analysis, one task is to estimate the intercept and the
slope. These are known as the \textbf{regression coefficients}
\(\beta_0\), \(\beta_1\).

\begin{itemize}
\item
  \textbf{Problem}: For more than two points \((x_i,y_i)\),
  \(i=1,\ldots, n\), there is generally no perfectly fitting line.
\item
  \textbf{Aim:} We want to estimate the parameters \((\beta_0,\beta_1)\)
  of the \textbf{best fitting} line \(Y = \beta_0 + \beta_1 x\).
\item
  \textbf{Idea:} Find the \textbf{best fitting line} by minimizing the
  deviations between the data points \((x_i,y_i)\) and the regression
  line. I.e., minimising the residuals.
\end{itemize}

But which deviations?

These ones?

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-3-1.pdf}}

Or these?

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-4-1.pdf}}

Or maybe even these?

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-5-1.pdf}}

Well, actually its none of these!!!

\subsection*{Least squares}\label{least-squares}
\addcontentsline{toc}{subsection}{Least squares}

For multiple reasons (theoretical aspects and mathematical convenience),
the intercept and slope are estimated using the \textbf{least squares}
approach. In this, yet something else is minimized:

The parameters \(\beta_0\) and \(\beta_1\) are estimated such that the
\textbf{sum of squared vertical distances} (sum of squared residuals /
errors) is minimised.

\textbf{SSE} means \textbf{S}um of \textbf{S}quared \textbf{E}rrors:

\[SSE = \sum_{i=1}^n e_i^2 \]

where,

\[e_i = y_i - \underbrace{(\beta_0 + \beta_1 x_i)}_{=\hat{y}_i} \]
\textbf{Note:} \(\hat y_i = \beta_0 + \beta_1 x_i\) are the
\emph{predicted values}.

In the graph just below, one of these squares is shown in red.

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Residuals are model-based quantities, not properties of the raw data.

\end{tcolorbox}

\subsection*{Least squares estimates}\label{least-squares-estimates}
\addcontentsline{toc}{subsection}{Least squares estimates}

With a linear model, we can calculate the least squares estimates of the
parameters \(\beta_0\) and \(\beta_1\) directly using the following
formulas.

For a given sample of data \((x_i,y_i), i=1,..,n\), with mean values
\(\overline{x}\) and \(\overline{y}\), the least squares estimates
\(\hat\beta_0\) and \(\hat\beta_1\) are computed as

\[ \hat\beta_1 = \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)}\]

\[\hat\beta_0 = \overline{y} - \hat\beta_1 \overline{x}  \]

Moreover,

\[ \hat\sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i) \]

is an unbiased estimate of the residual variance \(\sigma^2\).

(Derivations of the equations above are in the Stahel script 2.A b.
Hint: differentiate, set to zero, solve.)

\subsection*{\texorpdfstring{Why division by \(n-2\) ensures an unbiased
estimator}{Why division by n-2 ensures an unbiased estimator}}\label{why-division-by-n-2-ensures-an-unbiased-estimator}
\addcontentsline{toc}{subsection}{Why division by \(n-2\) ensures an
unbiased estimator}

When estimating parameters (\(\beta_0\) and \(\beta_1\)), the square of
the residuals is minimised. This fitting process inherently \emph{uses
up} two \emph{degrees of freedom}, as the model forces the residuals to
sum to zero and aligns the slope to best fit the data. I.e., one degree
of freedom is lost due to the estimation of the intercept, and another
due to the estimation of the slope.

The adjustment (division by \(n-2\) instead of \(n\)) compensates for
the loss of variability due to parameter estimation, ensuring the
estimator of the residual variance is unbiased. Mathematically, dividing
by n - 2 adjusts for this loss and gives an accurate estimate of the
population variance when working with sample data.

We'll look at degrees of freedom in more detail later, so don't worry if
this is a bit confusing right now.

\subsection*{Let's do it in R}\label{lets-do-it-in-r}
\addcontentsline{toc}{subsection}{Let's do it in R}

First we read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/Simulated\_Blood\_Pressure\_and\_Age\_Data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The we make a graph of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_age\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =}\NormalTok{ Systolic\_BP)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age (years)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Systolic Blood Pressure (mmHg)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-8-1.pdf}}

Then we make the linear model, using the \texttt{lm()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Systolic\_BP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age, }\AttributeTok{data =}\NormalTok{ bp\_age\_data)}
\end{Highlighting}
\end{Shaded}

Then we can look at the summary of the model. It contains a lot of
information, so can be a bit confusing at first.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(bp\_age\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = Systolic_BP ~ Age, data = bp_age_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.2195  -3.4434  -0.0808   3.1383  12.6025 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 98.96874    1.46102   67.74   <2e-16 ***
Age          0.82407    0.02771   29.74   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.971 on 98 degrees of freedom
Multiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 
F-statistic: 884.4 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

How do our guesses of the intercept and slope compare to the guesses we
made earlier?

Recal that the units of the \emph{Age} coefficient are in mmHg per year.
This means that for each additional year of age, the systolic blood
pressure increases by Â´r round(coef(bp\_age\_model){[}2{]},2)Â´ mmHg.

\section*{Dealing with the error}\label{dealing-with-the-error}
\addcontentsline{toc}{section}{Dealing with the error}

\markright{Dealing with the error}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-11-1.pdf}}

The line is not a perfect fit to the data. There is scatter around the
line.

Some of this scatter could be caused by other factors that influence
blood pressure, such as diet, exercise, and genetics. Also, the there
could be differences due to the measurement instrument (i.e., some
measurement error).

These other factors are not included in the model (only age is in the
model), so they create variation that can only appear in error term.

In the linear regression model the dependent variable \(Y\) is related
to the independent variable \(x\) as

\[Y = \beta_0 + \beta_1 x + \epsilon \ \] where

\begin{itemize}
\tightlist
\item
  \(\epsilon\) is the error term
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the slope
\item
  \(\epsilon\) is the error term.
\end{itemize}

The error term captures the difference between the observed value of the
dependent variable and the value predicted by the model. The error term
includes the effects of other factors that influence the dependent
variable, as well as measurement error.

\[Y \quad= \quad \underbrace{\text{expected value}}_{E(Y) = \beta_0 + \beta_1 x} \quad + \quad \underbrace{\text{random error}}_{\epsilon}  \ .\]

Graphically the error term is the vertical distance between the observed
value of the dependent variable and the value predicted by the model.

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-12-1.pdf}}

The error term is also known as the residual. It is the variation that
\emph{resides} (is left over / is left unexplained) after accounting for
the relationship between the dependent and independent variables.

\subsection*{Example in R}\label{example-in-r}
\addcontentsline{toc}{subsection}{Example in R}

Let's look at observed values, expected (predicted) values, and
residuals (error) in R.

The observed values of the response (dependent) variable are already in
the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_age\_data}\SpecialCharTok{$}\NormalTok{Systolic\_BP)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 150.3277 170.0801 139.7174 135.4089 151.9041 122.0296
\end{verbatim}

To get the expected values, we need to find the intercept and slope of
the linear model. We can do this using the \texttt{lm()} function in R.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Systolic\_BP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age, }\AttributeTok{data =}\NormalTok{ bp\_age\_data)}
\end{Highlighting}
\end{Shaded}

And we can get the intercept and slope using the \texttt{coef()}
function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(lm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept)         Age 
 98.9687381   0.8240678 
\end{verbatim}

We can then use the mutate function from the dplyr package to add the
expected values to the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}}\NormalTok{ bp\_age\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Expected\_BP =} \FunctionTok{coef}\NormalTok{(lm1)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{coef}\NormalTok{(lm1)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ Age)}
\end{Highlighting}
\end{Shaded}

And we can get the residuals by subtracting the expected values from the
observed values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}}\NormalTok{ bp\_age\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Residuals =}\NormalTok{ Systolic\_BP }\SpecialCharTok{{-}}\NormalTok{ Expected\_BP)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

We can also get the expected values and residuals directly from the
\texttt{lm} object using the \texttt{fitted()} (or \texttt{predicted()})
and \texttt{residuals()} functions:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}}\NormalTok{ bp\_age\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Expected\_BP =} \FunctionTok{fitted}\NormalTok{(lm1),}
         \AttributeTok{Residuals =} \FunctionTok{residuals}\NormalTok{(lm1))}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Now we have a model that gives the expected values (on the regression
line) and that gives us a residual. Because the expected value plus the
residual equals the observed value, if we use each of the residuals as
the error for each respective data point, we end up with a perfect fit
to the data. All we are doing is describing the observed data in a
different way. This is known as over-fitting. In fact, we have gained
very little by fitting the model. We have simply memorized / copied the
data!!!

In order to avoid this, we need to assume something about the residuals
-- we need to \emph{model} the residuals. The most common model for the
residuals is a normal distribution with mean 0 and constant variance.

\[\epsilon \sim N(0,\sigma^2)\]

\textbf{This is known as the normality assumption.} The normality
assumption is important because it allows us to make inferences about
the \emph{population parameters} based on the \emph{sample data}.

The linear regression model then becomes:

\[Y = \beta_0 + \beta_1 x + N(0,\sigma^2) \ \]

where \(\sigma^2\) is the variance of the error term. The variance of
the error term is the amount of variation in the dependent variable that
is not explained by the independent variable. The variance of the error
term is also known as the residual variance.

An alternate and equivalent formulation is that \(Y\) is a random
variable that follows a normal distribution with mean
\(\beta_0 + \beta_1 x\) and variance \(\sigma^2\).

\[Y \sim N(\beta_0 + \beta_1 x, \sigma^2)\]

So, the answer to the question ``how do we deal with the error term'' is
that we model the error term as normally distributed with mean 0 and
constant variance. Put another way, the error term is assumed to be
normally distributed with mean 0 and constant variance.

\subsection*{Back to blood pressure and
age}\label{back-to-blood-pressure-and-age}
\addcontentsline{toc}{subsection}{Back to blood pressure and age}

The mathematical model in this case is:

\[SystolicBP = \beta_0 + \beta_1 \times Age + \epsilon\]

where: \emph{SystolicBP} is the dependent (response) variable,
\(\beta_0\) is the intercept, \(\beta_1\) is the coefficient of Age,
\emph{Age} is the independent (explanatory) variable, \(\epsilon\) is
the error term.

Let's ensure we understand this, by thinking about the units of the
variables in this model. This can be very useful because it can help us
to understand the model better and to check that the model makes sense.

\section*{Is the model good enough to
use?}\label{is-the-model-good-enough-to-use}
\addcontentsline{toc}{section}{Is the model good enough to use?}

\markright{Is the model good enough to use?}

\begin{itemize}
\tightlist
\item
  All models are wrong, but is ours good enough to be useful?
\item
  Are the assumption of the model justified?
\item
  It would be very unwise to use the model before we know if it is good
  enough to use.
\item
  \emph{Don't jump out of an aeroplane until you know your parachute is
  good enough!}
\end{itemize}

\subsection*{What assumptions do we
make?}\label{what-assumptions-do-we-make}
\addcontentsline{toc}{subsection}{What assumptions do we make?}

We already heard about one. We assume that the residuals follow a
\(N(0,\sigma^2)\) distribution (that is, a Gaussian / Normal distrution
with mean of zero and variance of \(\sigma^2\)). We make this assumption
because it is often well enough met, and it gives great mathematical
tractability.

This assumption implies that:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  The \(\epsilon_i\) are normally distributed.
\item
  \(\epsilon_i\) has constant variance: \(Var(\epsilon_i)=\sigma^2\).
\item
  The \(\epsilon_i\) are independent of each other.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

These assumptions are about the residuals, not the observed data!!!

\end{tcolorbox}

Furthermore:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  we assumed a linear relationship.
\item
  implies there are no outliers (implied by (a) above)
\end{enumerate}

Lets go through each five assumptions.

\subsection*{(a) Normally distributed
residuals}\label{a-normally-distributed-residuals}
\addcontentsline{toc}{subsection}{(a) Normally distributed residuals}

Recall that we make the assumption that the residuals are normally
distributed with mean 0 and constant variance:

\[\epsilon \sim N(0,\sigma^2)\]

Here we are concerned with the first part of this assumption, that the
residuals are normally distributed.

What does this mean? How can we check it?

A normal distribution is symmetric and bell-shaped\ldots{}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-19-1.pdf}}

Lets look at the frequency distribution of the residuals of the linear
regression of blood pressure and age:

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-20-1.pdf}}

The normal distribution assumption (a) seems ok as well.

\subsection*{(a) Normally distributed residuals: The
QQ-plot}\label{a-normally-distributed-residuals-the-qq-plot}
\addcontentsline{toc}{subsection}{(a) Normally distributed residuals:
The QQ-plot}

Usually, not the histogram of the residuals is plotted, but the
so-called \textbf{quantile-quantile} (QQ) plot. The quantiles of the
observed distribution are plotted against the quantiles of the
respective theoretical (normal) distribution:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(bp\_age\_model))}
\FunctionTok{qqline}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(bp\_age\_model))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-21-1.pdf}}

If the points lie approximately on a straight line, the data is fairly
normally distributed.

This is often ``tested'' by eye, and needs some experience.

\emph{But what on earth is a quantile???}

Imagine we make 21 measures of something, say 21 blood pressures:

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-22-1.pdf}}

The median of these is 127.8. The median is the 50\% or 0.5 quantile,
because half the data points are above it, and half below.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{quantile}\NormalTok{(dd}\SpecialCharTok{$}\NormalTok{Blood\_Pressure, }\FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  50% 
127.8 
\end{verbatim}

The \emph{theoretical quantiles} come from the normal distribution. The
\emph{sample quantiles} come from the distribution of our residuals.

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-24-1.pdf}}

\subsubsection*{How do I know if a QQ-plot looks
``good''?}\label{how-do-i-know-if-a-qq-plot-looks-good}
\addcontentsline{toc}{subsubsection}{How do I know if a QQ-plot looks
``good''?}

There is \textbf{no quantitative rule} to answer this question. Instead
experience is needed. You can gain this experience from simulations. To
this end, we can generate the same number of data points of a normally
distributed variable and compare this simulated qqplot to our observed
one.

Example: Generate 100 points \(\epsilon_i \sim N(0,1)\) each time:

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-25-1.pdf}}

Each of the graphs above has data points that are randomly generated
from a normal distribution. In all cases the data points are close to
the line. This is what we would expect if the data were normally
distributed. The amount of deviation from the line is what we would
expect from random variation, and so seeing this amount of variation in
a QQ-plot of your model should not be cause for concern.

\subsection*{(b) Constant error variance
(homoscedasticity)}\label{b-constant-error-variance-homoscedasticity}
\addcontentsline{toc}{subsection}{(b) Constant error variance
(homoscedasticity)}

Recall that we assume the errors are normally distributed with constant
variance \(\sigma^2\):

\[\epsilon_i \sim N(0, \sigma^2)\]

Here we're concerned with the second part of this assumption, that the
variance is constant.That is, variance of the residuals is a constant:
\(\text{Var}(\epsilon_i) = \sigma^2\). And not, for example
\(\text{Var}(\epsilon_i) = \sigma^2 \cdot x_i\).

Put another way, we're interested if the size of the residuals tends to
show a pattern with the fitted values. By \emph{size} of the residuals
we mean the \emph{absolute value} of the residuals. In fact, we often
look at the square root of the absolute value of the standardized
residuals:

\[R_i = \frac{\epsilon_i}{\hat{\sigma}}\] Where \(\hat{\sigma}\) is the
estimated standard deviation of the residuals:

\[\hat{\sigma} = \sqrt{\frac{1}{n-2} \sum_{i=1}^n \epsilon_i^2}\]

So that the full equation of the square root of the standardised
residuals is:

\[\sqrt{|R_i|} = \sqrt{\left|\frac{\epsilon_i}{\hat{\sigma}}\right|}\]

To look to see if the variance of the residuals is constant, we need to
see if there is any relationship between the size of the residuals and
the fitted values. A commonly used visualistion for this is a plot of
the size of the residuals against the fitted values.

Lets first calculated the \(\sqrt{|R_i|}\) values for our blood pressure
model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}}\NormalTok{ bp\_age\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{fitted =} \FunctionTok{predict}\NormalTok{(bp\_age\_model),}
         \AttributeTok{residuals =} \FunctionTok{residuals}\NormalTok{(bp\_age\_model),}
         \AttributeTok{sigma\_hat =} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{sum}\NormalTok{(residuals}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{n}\NormalTok{()}\SpecialCharTok{{-}}\DecValTok{2}\NormalTok{)),}
         \AttributeTok{R\_i =}\NormalTok{ residuals }\SpecialCharTok{/}\NormalTok{ sigma\_hat,}
         \AttributeTok{sqrt\_abs\_R\_i =} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(R\_i)))}
\end{Highlighting}
\end{Shaded}

And now visualise the relationship between the fitted values and the
size of the residuals:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_age\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ fitted, }\AttributeTok{y =}\NormalTok{ sqrt\_abs\_R\_i)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Fitted values"}\NormalTok{, }\AttributeTok{y =} \FunctionTok{expression}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(R[i])))) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-28-1.pdf}}

This graph is known as the scale-location plot. It is particularly
suited to check the assumption of equal variances
(\textbf{homoscedasticity / HomoskedastizitÃ¤t}). There should be
\textbf{no trend} or pattern.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

We can also use the built-in plot function for linear models to create
this plot. It is the third plot in the set of model checking plots.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(bp\_age\_model, }\AttributeTok{which=}\DecValTok{3}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-29-1.pdf}}

\end{tcolorbox}

\subsubsection*{How it looks with the variance increasing with the
fitted
values}\label{how-it-looks-with-the-variance-increasing-with-the-fitted-values}
\addcontentsline{toc}{subsubsection}{How it looks with the variance
increasing with the fitted values}

Here's a graphical example of how it would look if the variance of the
residuals increases with the fitted values.

First here is a graph of the relationship:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{100} \SpecialCharTok{+} \DecValTok{5}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{*}\FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-30-1.pdf}}

And here the scale-location plot for a linear model of that data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x)}
\FunctionTok{plot}\NormalTok{(m,}\AttributeTok{which=}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-31-1.pdf}}

\subsection*{(c) Independence (residuals are independent of each
other)}\label{c-independence-residuals-are-independent-of-each-other}
\addcontentsline{toc}{subsection}{(c) Independence (residuals are
independent of each other)}

We assume that the residuals (\(\epsilon_i\)) are independent of each
other. This means that the value of one residual is not somehow related
to the value of another.

The dataset about blood pressure we looked at contained 100
observations, each one made from a different person. In such a study
design, we could be safe in the assumption that the people are
independent, and therefore the assumption that the residuals are
independent.

Imagine, however, if we had 100 observations of blood pressure collected
from 50 people, because we measured the blood pressure of each person
twice. In this case, the residuals would not be independent, because two
measures of the blood pressure of the same person are likely to be
similar. A person is likely to have a high blood pressure in both
measurements, or a low blood pressure in both measurements. This would
mean they have a high residual in both measurements, or a low residual
in both measurements.

In this case, we would need to account for the fact that the residuals
are not independent. We would need to use a more complex model, such as
a mixed effects model, to account for the fact that the residuals are
not independent. We will talk about this again in the last week of this
course.

In general, you should always think about the study design when you are
analysing data. You should always think about whether the residuals are
likely to be independent of each other. If they are not, you should
think about how you can account for this in your analysis.

A good way to assess if there could be dependencies in the residuals is
to be critical about what is the unit of observation in the data. In the
blood pressure example, the unit of observation is the person. Count the
number of persons in the study. If there are fewer persons than
observations, then at least some people must have been measured at least
twice. Repeating measures on the same person is a common way to get
dependent residuals.

So, to check the assumption of independence, you should:

\begin{itemize}
\tightlist
\item
  Think carefully about the study design.
\item
  Think carefully about the unit of observation in the data.
\item
  Compare the number of observations to the number of units of
  observation.
\end{itemize}

\subsection*{(d) Linearity assumption}\label{d-linearity-assumption}
\addcontentsline{toc}{subsection}{(d) Linearity assumption}

The linearity assumption states that the relationship between the
independent variable and the dependent variable is linear. This means
that the dependent variable changes by a constant amount for a one-unit
change in the independent variable. And that this slope is does not
change with the value of the independent variable.

The blood pressure data seems to be linear:

\begin{verbatim}
Warning: `fortify(<lm>)` was deprecated in ggplot2 3.6.0.
i Please use `broom::augment(<lm>)` instead.
i The deprecated feature was likely used in the ggplot2 package.
  Please report the issue at <https://github.com/tidyverse/ggplot2/issues>.
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-32-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_age\_model, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y=}\NormalTok{Systolic\_BP)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In contrast, look at this linear regression through data that appears
non-linear:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{,}\DecValTok{0}\NormalTok{,}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*}\NormalTok{x}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y}\SpecialCharTok{\textasciitilde{}}\NormalTok{x)}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-34-1.pdf}}

And with the residuals shown as red lines:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x,}\AttributeTok{y=}\NormalTok{y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_segment}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xend=}\NormalTok{x, }\AttributeTok{yend=}\NormalTok{m}\SpecialCharTok{$}\NormalTok{fitted), }\AttributeTok{color=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-35-1.pdf}}

At low values of \(y\), the residuals are positive, at intermediate
values of \(y\) the residuals are negative, and at high values of \(y\)
the residuals are positive. This pattern in the residuals is a sign that
the relationship between \(x\) and \(y\) is not linear.

We can plot the value of the residuals against the \(y\) value directly,
instead of looking at the pattern in the graph above. This is called a
\textbf{Tukey-Anscombe plot}. It is a graph of the residuals versus the
fitted \(y\) values:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fitted}\NormalTok{(m), }\AttributeTok{y =} \FunctionTok{resid}\NormalTok{(m))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype=}\StringTok{"dashed"}\NormalTok{, }\AttributeTok{color=}\StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-36-1.pdf}}

We can very clearly see pattern in the residuals in this Tukey-Anscombe
plot. The residuals are positive, then negative, then positive, as the
fitted \(y\) value gets larger.

We can also make this Tukey-Anscombe plot using the built-in plot
function for linear models in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(m, }\AttributeTok{which=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-37-1.pdf}}

The red line in the Tukey-Anscombe plot is a loess smooth. It is
automatically added to the plot. It is a way of estimating the pattern
in the residuals. If the red line is not flat, then there is a pattern
in the residuals. However, the loess smooth is not always reliable. It
is a good idea to look at the residuals directly, without this smooth.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(m, }\AttributeTok{which=}\DecValTok{1}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-38-1.pdf}}

The data here is simulated to show a very clear pattern in the
residuals. In real data, the pattern might not be so clear. But if you
suspect you see a pattern in the residuals, it could be a sign that the
relationship between the independent and dependent variable is not
linear.

Here is the Tukey-Anscombe plot for the blood pressure data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(bp\_age\_model, }\AttributeTok{which=}\DecValTok{1}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-39-1.pdf}}

There is very little evidence of any pattern in the residuals. This data
is simulated with a truly linear relationship, so we would not expect to
see any pattern in the residuals.

\subsection*{(e) No outliers}\label{e-no-outliers}
\addcontentsline{toc}{subsection}{(e) No outliers}

An outlier is a data point that is very different from the other data
points. Outliers can have a big effect on the results of a regression
analysis. They can pull the line of best fit towards them, and make the
line of best fit a poor representation of the data.

Lets again look at the blood pressure versus age data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_age\_model, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y=}\NormalTok{Systolic\_BP)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se=}\ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{85}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylim}\NormalTok{(}\DecValTok{100}\NormalTok{, }\DecValTok{180}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-41-1.pdf}}

There are no obvious outliers in this data. The data points are all
close to the line of best fit. This is a good sign that the line of best
fit is a good representation of the data.

Data points that are far from the mean of the independent variable have
a large effect on the value of the slope. These data points have a large
leverage. They are data points that are far from the other data points
in the \(x\) direction.

We can think of this with the analogy of a seesaw. The slope of the line
of best fit is like the pivot point of a seesaw. Data points that are
far from the pivot point have a large effect on the slope. Data points
that are close to the pivot point have a small effect on the slope.

A measure of distance from the pivot point is called the \(leverage\) of
a data point. In simple regression, the leverage of individual \(i\) is
defined as

\(h_{i} = (1/n) + (x_i-\overline{x})^2 / SSX\).

where \(SSX = \sum_{i=1}^n (x_i - \overline{x})^2\). (\textbf{S}um of
\textbf{S}quares of \textbf{\(X\)})

So, the leverage of a data point is inversely related to \(n\) (the
number of data points). The leverage of a data point is also inversely
related to the sum of the squares of the \(x\) values. The leverage of a
data point is directly related to the square of the distance of the
\(x\) value from the mean of the \(x\) values.

More intuitively perhaps, the leverage of a data point will be greater
when the are fewer other data points. It will also be greater when the
distance from the mean value of \(x\) is greater.

Going back to the analogy of a seesaw, with data points as children on
the seesaw, the leverage of a data point is like the distance from the
pivot a child sits. But we also have children of different weights. A
lighter child will have less effect on the tilt of the seesaw. A heavier
one will have a greater effect on the tilt. A heavier child sitting far
from the pivot will have a very large effect.

The size of the residuals are like the weight of the child. Data points
with large residuals have a large effect on the slope of the line of
best fit. Data points with small residuals have a small effect on the
slope of the line of best fit.

So the overall effect of a data point on the slope of the line of best
fit is a combination of the leverage and the residual. This quantity is
called the \(influence\) of a data point.

Let's add a rather extreme data point to the blood pressure versus age
data:

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-42-1.pdf}}

This is a bit ridiculous, but it is a good example of an outlier. The
data point is far from the other data points. It has a large residual.
And it is a long way from the pivot (the middle of the \(x\) data) so
has large leverage.

We can make a histogram of the residuals and see that the outlier has a
large residual:

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-43-1.pdf}}

And we can see that the leverage is large.

There is a graph that we can look at to see the influence of a data
point. This is called a \(Cook's\) \(distance\) plot. The Cook's
distance of a data point is a measure of how much the slope of the line
of best fit changes when that data point is removed. The Cook's distance
of a data point is defined as

\(D_i = \sum_{j=1}^n (\hat{y}_j - \hat{y}_{j(i)})^2 / (p \times MSE)\).

where \(\hat{y}_j\) is the predicted value of the dependent variable for
data point \(j\), \(\hat{y}_{j(i)}\) is the predicted value of the
dependent variable for data point \(j\) when data point \(i\) is
removed, \(p\) is the number of parameters in the model (2 in this
case), \(MSE\) is the mean squared error of the model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(bp\_age\_model\_outlier, }\AttributeTok{which=}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-44-1.pdf}}

But does it have a large influence on the value of the slope? In the
next graph we show the line of best fit with the outlier (blue line) and
without the outlier (red line).

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-45-1.pdf}}

No, the outlier doesn't have much influence on the slope. The outlier
has a large leverage. It is far from the pivot. But it does not have
such a large effect (influence) on the slope. This is in large part
because there are a lot data points (100) that are quite tightly
arranged around the regression line.

\subsubsection*{Graphical illustration of the leverage
effect}\label{graphical-illustration-of-the-leverage-effect}
\addcontentsline{toc}{subsubsection}{Graphical illustration of the
leverage effect}

Data points with \(x_i\) values far from the mean have a stronger
leverage effect than when \(x_i\approx \overline{x}\):

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-46-1.pdf}}

The outlier (red circle) in the middle plot ``pulls'' the regression
line in its direction and has large influence on the slope. THe outlier
(red circle) in the right plot has less influence on the slope because
it is closer to the mean of \(x\).

\subsubsection*{Leverage plot
(Hebelarm-Diagramm)}\label{leverage-plot-hebelarm-diagramm}
\addcontentsline{toc}{subsubsection}{Leverage plot (Hebelarm-Diagramm)}

In the leverage plot, (standardized) residuals \(\tilde{R_i}\) are
plotted against the leverage \(H_{ii}\) :

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(bp\_age\_model, }\AttributeTok{which=}\DecValTok{5}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-47-1.pdf}}

Critical ranges are the top and bottom right corners!!

Here, observations 71, 85, and 87 are labelled as potential outliers.

Some texts will give a rule of thumb that points with Cook's distances
greater than 1 should be considered influential, while others claim a
reasonable rule of thumb is \(4 / ( n - p - 1 )\) where \(n\) is the
sample size, and \(p\) is the number of \(beta\) parameters.

\subsection*{\texorpdfstring{The \texttt{autoplot()} function from the
\texttt{ggfortify}
package}{The autoplot() function from the ggfortify package}}\label{the-autoplot-function-from-the-ggfortify-package}
\addcontentsline{toc}{subsection}{The \texttt{autoplot()} function from
the \texttt{ggfortify} package}

So far we have used the built-in \texttt{plot()} function for linear
models to create the model checking plots. Another option is to use the
\texttt{autoplot()} function from the \texttt{ggfortify} package. This
function creates the same four model checking plots, but doesn't require
the \texttt{par(mfrow=c(2,2))} command to arrange the plots in a 2x2
grid, so is slightly more convenient to use. Of course, you do need to
ensure that the \texttt{ggfortify} package is installed and loaded.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggfortify)}
\FunctionTok{autoplot}\NormalTok{(bp\_age\_model, }\AttributeTok{smooth.colour =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

(At the time of writing, this use of \texttt{autoplot()} causes a
warning message to be printed. This is a known issue with the
\texttt{ggfortify} package. You can safely ignore this warning message
for now.)

\section*{What can go ``wrong'' during the modeling
process?}\label{what-can-go-wrong-during-the-modeling-process}
\addcontentsline{toc}{section}{What can go ``wrong'' during the modeling
process?}

\markright{What can go ``wrong'' during the modeling process?}

Answer: a lot of things!

\begin{itemize}
\tightlist
\item
  Non-linearity. We assumed a linear relationship between the response
  and the explanatory variables. But this is not always the case in
  practice. We might find that the relationship is curved and not well
  represtented by a straight line.
\item
  Non-normal distribution of residuals. The QQ-plot data might deviate
  from the straight line so much that we get worried!
\item
  Heteroscadisticity (non-constant variance). We assumed
  homoscadisticity, but the residuals might show a pattern.
\item
  Data point with high influence. We might have a data point that has a
  large influence on the slope of the line of best fit.
\end{itemize}

\subsection*{What to do when things ``go
wrong''?}\label{what-to-do-when-things-go-wrong}
\addcontentsline{toc}{subsection}{What to do when things ``go wrong''?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Now: Transform the response and/or explanatory variables.
\item
  Now: Take care of outliers.
\item
  Later in the course: Improve the model, e.g., by adding additional
  terms or interactions.
\item
  Later in the course: Use another model family (generalized or
  nonlinear regression model).
\end{enumerate}

\subsection*{Dealing with
non-linearity}\label{dealing-with-non-linearity}
\addcontentsline{toc}{subsection}{Dealing with non-linearity}

Here's another example of \(y\) and \(x\) that are not linearly related:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eg\_data }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{x =} \FunctionTok{runif}\NormalTok{(}\DecValTok{50}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{log\_y =} \FunctionTok{log}\NormalTok{(}\FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5}\SpecialCharTok{*} \FunctionTok{log}\NormalTok{(x) }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{50}\NormalTok{,}\DecValTok{0}\NormalTok{,}\FloatTok{0.1}\NormalTok{),}
         \AttributeTok{y =} \FunctionTok{exp}\NormalTok{(log\_y),}
         \AttributeTok{log\_y =} \FunctionTok{log}\NormalTok{(y),}
         \AttributeTok{log\_x =} \FunctionTok{log}\NormalTok{(x),}
         \AttributeTok{sqrt\_y =} \FunctionTok{sqrt}\NormalTok{(y))}

\NormalTok{p1 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(eg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}

\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, eg\_data)}

\NormalTok{p2 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fitted}\NormalTok{(m1), }\AttributeTok{y =} \FunctionTok{residuals}\NormalTok{(m1))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"loess"}\NormalTok{)}

\NormalTok{p1 }\SpecialCharTok{+}\NormalTok{ p2}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-49-1.pdf}}

One way to deal with this is to transform the response variable \(Y\).
Here we try two different transformations: \(\log_{10}(Y)\) and
\(\sqrt{Y}\).

Square root transform of the response variable \(Y\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p3 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(eg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ sqrt\_y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{m2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(sqrt\_y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, eg\_data)}
\NormalTok{p4 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fitted}\NormalTok{(m2), }\AttributeTok{y =} \FunctionTok{residuals}\NormalTok{(m2))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"loess"}\NormalTok{)}

\NormalTok{(p1 }\SpecialCharTok{+}\NormalTok{ p2) }\SpecialCharTok{/}\NormalTok{ (p3 }\SpecialCharTok{+}\NormalTok{ p4)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-50-1.pdf}}

Not great.

Log transformation of the response variable \(Y\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p5 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(eg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ log\_y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{m3 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, eg\_data)}
\NormalTok{p6 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fitted}\NormalTok{(m3), }\AttributeTok{y =} \FunctionTok{residuals}\NormalTok{(m3))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"loess"}\NormalTok{)}

\NormalTok{(p1 }\SpecialCharTok{+}\NormalTok{ p2) }\SpecialCharTok{/}\NormalTok{ (p3 }\SpecialCharTok{+}\NormalTok{ p4) }\SpecialCharTok{/}\NormalTok{ (p5 }\SpecialCharTok{+}\NormalTok{ p6)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-51-1.pdf}}

Nope. Still some evidence of non-linearity.

What about transforming the explanatory variable \(X\) as well?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p7 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(eg\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ log\_x, }\AttributeTok{y =}\NormalTok{ log\_y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{m4 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log\_y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ log\_x, eg\_data)}
\NormalTok{p8 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{fitted}\NormalTok{(m4), }\AttributeTok{y =} \FunctionTok{residuals}\NormalTok{(m4))) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x, }\AttributeTok{method =} \StringTok{"loess"}\NormalTok{)}

\NormalTok{p7 }\SpecialCharTok{+}\NormalTok{ p8}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-52-1.pdf}}

Let's look at the four model checking plots for the log-log-transformed
data:

\begin{center}
\includegraphics[width=14cm,height=\textheight,keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-53-1.pdf}
\end{center}

All looks pretty good except for the scale-location plot, which shows a
bit of a pattern. But overall, this looks much better than our original
model.

But\ldots{} how to know which transformation to use\ldots? It's a bit of
trial and error. But we can use the model checking plots to help us.

\textbf{Very very important} is that we do this trial and error before
we start using the model. E.g., we don't want to jump from the aeroplane
and then find out that our parachute is not working properly! And then
try to fix the parachute while we are falling\ldots.

Likewise, we must not start using the model and then try to fix it. We
need to make sure our model is in good working order before we start
using it.

One of the traps we could fall into is called ``p-hacking''. This is
when we try different transformation until we find one that gives us the
\textbf{result we want}, for example significant relationship. This is a
big no-no in statistics. We need to decide on the model (including any
transformations) before we start using it.

\subsection*{Common transformations}\label{common-transformations}
\addcontentsline{toc}{subsection}{Common transformations}

Which transformations could be considered? There is no simple answer.
But some guidelines. E.g. if we see non-linearity and increasing
variance with increasing fitted values, then a log transform may improve
matter.

Some common and useful transformations are:

\begin{itemize}
\tightlist
\item
  The log transformation for concentrations and absolute values.
\item
  The square-root transformation for count data.
\item
  The arcsin square-root \(\arcsin(\sqrt{\cdot})\) transformation for
  proportions/percentages.
\end{itemize}

Transformations can also be applied on explanatory variables, as we saw
in the example above.

\subsection*{Outliers}\label{outliers}
\addcontentsline{toc}{subsection}{Outliers}

What do we do when we identify the presence of one or more outliers?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start by checking the ``correctness'' of the data. Is there a typo or
  a decimal point that was shifted by mistake? Check both the response
  and explanatory variables.
\item
  If not, ask whether the model could be improved. Do reasonable
  transformations of the response and/or explanatory variables eliminate
  the outlier? Do the residuals have a distribution with a long tail
  (which makes it more likely that extreme observations occur)?
\item
  Sometimes, an outlier may be the most interesting observation in a
  dataset! Was the outlier created by some interesting but different
  process from the other data points?
\item
  Consider that outliers can also occur just by chance!
\item
  Only if you decide to report the results of both scenario can you
  check if inclusion/exclusion changes the qualitative conclusion, and
  by how much it changes the quantitative conclusion.
\end{enumerate}

\subsection*{Removing outliers}\label{removing-outliers}
\addcontentsline{toc}{subsection}{Removing outliers}

It might seem tempting to remove observations that apparently don't fit
into the picture. However:

\begin{itemize}
\tightlist
\item
  Do this \textbf{only with greatest care} e.g., if an observation has
  extremely implausible values!\\
\item
  Before deleting outliers, check points 1-5 above.
\item
  When removing outliers, \textbf{you must mention this in your report}.
\end{itemize}

During the course we'll see many more examples of things going at least
a bit wrong. And we'll do our best to improve the model, so we can be
confident in it, and start to use it. Which we will start to do in the
next lesson. But before we wrap up, some good news\ldots{}

\section*{Its a kind of magic\ldots{}}\label{its-a-kind-of-magic}
\addcontentsline{toc}{section}{Its a kind of magic\ldots{}}

\markright{Its a kind of magic\ldots{}}

Above, we learned about linear regression, the equation for it, how to
estimate the coefficients, and how to check the assumptions. There was a
lot of information, and it might seem a bit overwhelming.

You might also be aware that there are quite a few other types of
statistical model, such as multiple regression, t-test, ANOVA, two-way
ANOVA, and ANCOVA. It could be worrying to think that you need to learn
so much new information for each of these types of tests.

But this is where the kind-of-magic happens. The good news is that the
linear regression model is a special case of what is called a
\emph{general linear model}, or just \emph{linear model} for short. And
that all the tests mentioned above are also types of \emph{linear
model}. So, once you have learned about linear regression, you have
learned a lot about linear models, and therefore also a lot about all of
these other tests as well.

Moreover, the same function in R `lm' is used to make all those
statistical models Awesome.

\subsection*{So what is a linear
model?}\label{so-what-is-a-linear-model}
\addcontentsline{toc}{subsection}{So what is a linear model?}

A common misconception is that a linear model is a model where the
relationship between the dependent variable and the independent
variables must be linear linear. Actually, we can have non-linear
relationships in a linear model. For example, we can have quadratic
terms in a linear model:

\[y = \beta_0 + \beta_1 x + \beta_2 x^2 + \epsilon\]

If this is still a linear model, what then is the meaning of ``linear''
in ``linear model''? The answer is a little nuanced: a linear model is a
mathematical model in which the expected value of the response variable
is expressed as a \textbf{linear combination of parameters
(coefficients).}

When we say a model has ``a linear combination of parameters'', we mean
that the unknown coefficients (the \(\beta\)'s) are only multiplied by
things and added together\ldots{} they are not exponentiated, for
example.

That is, the dependent variable can be expressed as a linear combination
of the independent variables. An example of a linear model is:

\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon\]

where: \(y\) is the dependent variable, \(\beta_0\) is the intercept,
\(\beta_1, \beta_2, \ldots, \beta_p\) are the coefficients of the
independent variables, \(x_1, x_2, \ldots, x_p\) are the independent
variables, \(\epsilon\) is the error term.

In contrast, a non-linear model is one where a parameter enters in a
non-linear way. An example of a non-linear model is:

\[y = \beta_0 + \beta_1 e^{\beta_2 x} + \epsilon\]

where: y is the dependent variable, \(\beta_0\) is the intercept,
\(\beta_1, \beta_2\) are the coefficients of the independent variables,
\(x\) is the independent variable, \(\epsilon\) is the error term.

In this model, changing the value of \[\beta_2\] does not change the
value of \[y\] linearly. This happens because exponentiation is a
non-linear operation.

\section*{Review}\label{review-1}
\addcontentsline{toc}{section}{Review}

\markright{Review}

In this chapter we learned about first steps of regression analysis.
Specifically, we learned about to:

\begin{itemize}
\tightlist
\item
  Motivation for regression analysis.
\item
  The simple linear regression model.
\item
  Estimation of the coefficients (intercept and slope) using the least
  squares method.
\item
  How to model the errors (residuals) and the assumptions of linear
  regression.
\item
  How to check the assumptions of linear regression using model checking
  plots.
\item
  What can go wrong during the modeling process, and how to deal with
  it.
\end{itemize}

With all that in place, we can be sure that when we use a linear
regression model, we can trust the results it gives us. In the next
chapter, we will start to use linear regression models to answer
questions! This involves things such as hypothesis testing, confidence
intervals, and prediction. Exciting times ahead!

\section*{Further reading}\label{further-reading-2}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

There are many good books and online resources about regression models,
fitting them, and checking their assumptions. Here are some suggestions
for further reading:

\begin{itemize}
\tightlist
\item
  For a more mathematical treatment, including matrix representation,
  see Chapter 2 of Faraway, J. J. (2016). Linear models with R (2nd
  ed.). Chapman and Hall/CRC.
  \href{https://www.routledge.com/Linear-Models-with-R-2nd-Edition/Faraway/p/book/9781482215754}{Link}
\item
  The same book (Faraway) covers Diagnostics and model checking in
  Chapter 5.
\item
  For a complementary perspective on regression, see Chapter 7 of The
  New Statistics with R, by Andy Hector.
\end{itemize}

\section*{Extras}\label{extras-1}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{Tests for normality}\label{tests-for-normality}
\addcontentsline{toc}{subsection}{Tests for normality}

We often want to know if our data or our residuals are not normally
distributed. You've now done a lot with QQ-plots to examine how normally
distributed your data are. Some people also like to have a statistical
test for normality. Here I'll show some tests.

\subsubsection*{Shapiro--Wilk test on normally distributed
data}\label{shapirowilk-test-on-normally-distributed-data}
\addcontentsline{toc}{subsubsection}{Shapiro--Wilk test on normally
distributed data}

The Shapiro--Wilk test is frequently used.

\begin{itemize}
\tightlist
\item
  \textbf{Null hypothesis:} the data are normally distributed\\
\item
  \textbf{Alternative hypothesis:} the data are not normally distributed
\end{itemize}

Let's make some data and look at its distribution and QQ-plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{145}\NormalTok{)}
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-54-1.pdf}}

Yes, this is the distribution of data that come from a normal
distribution.

And the QQ-plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(x)}
\FunctionTok{qqline}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-55-1.pdf}}

Looks pretty normal. Let's see what the Shapiro--Wilk test tells us:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  x
W = 0.99467, p-value = 0.9658
\end{verbatim}

The p-value for the test is large (e.g.~p â‰ˆ 0.99). When we get a large
p-value like this, it means we \textbf{cannot reject the null
hypothesis}. (A small p-value, say p \textless{} 0.05, would cause us to
reject the null hypothesis.) Hence, here, we cannot reject the null
hypothesis that the data are normally distributed. (Note that, as usual,
technically we cannot \emph{accept} the null hypothesis.)

So we can happily continue with any work that assumes normally
distributed residuals.

\subsubsection*{Shapiro--Wilk test on non-normally distributed
data}\label{shapirowilk-test-on-non-normally-distributed-data}
\addcontentsline{toc}{subsubsection}{Shapiro--Wilk test on non-normally
distributed data}

Now let's repeat this exercise using data that we \emph{know} are not
normally distributed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{100}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-57-1.pdf}}

This is clearly not normally distributed data.

And the QQ-plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(x)}
\FunctionTok{qqline}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-58-1.pdf}}

OK, we don't need to do a test to see that this is not normally
distributed data. But let's do it anyway:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  x
W = 0.92167, p-value = 1.728e-05
\end{verbatim}

Now the p-value is small, so we \textbf{reject the null hypothesis} that
the data are normally distributed. Now we can't continue with methods,
such as linear models, that assume normally distributed residuals.

\subsubsection*{What if we have fewer data
points?}\label{what-if-we-have-fewer-data-points}
\addcontentsline{toc}{subsubsection}{What if we have fewer data points?}

What about if we have fewer data points, say 30? And let's do the test
lots of times and count how often we reject the null hypothesis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\FunctionTok{shapiro.test}\NormalTok{(}\FunctionTok{runif}\NormalTok{(}\DecValTok{30}\NormalTok{))}\SpecialCharTok{$}\NormalTok{p.value)}
\FunctionTok{sum}\NormalTok{(x }\SpecialCharTok{\textless{}} \FloatTok{0.05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 371
\end{verbatim}

So we reject the null hypothesis in about \textbf{38\% of the trials}.
That is, we \emph{don't} reject the hypothesis that the data are
normally distributed in nearly \textbf{40\% of cases}, even though the
data came from a uniform distribution!

\subsubsection*{Shapiro--Wilk test on real
data}\label{shapirowilk-test-on-real-data}
\addcontentsline{toc}{subsubsection}{Shapiro--Wilk test on real data}

How useful this (or any) test of normality is depends on our data.

\begin{itemize}
\tightlist
\item
  If we have a \textbf{large sample}, the test has very \textbf{high
  power}, and we can easily reject the null hypothesis due to tiny
  deviations from normality.
\item
  If we have a \textbf{small sample}, we could easily fail to reject the
  null hypothesis even when the generating process does not create
  normally distributed data. The test has \textbf{low power} for small
  samples.
\end{itemize}

Here's an example of the latter point. We make data that has 5000
numbers sampled from a normal distribution, and then add a bit of
non-normality:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\DecValTok{5000}\NormalTok{) }\CommentTok{\#+ c(1, 0, 2, 0, 1)}
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x=}\NormalTok{x), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{x)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-61-1.pdf}}

Looks pretty normal.

And the QQ-plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(x)}
\FunctionTok{qqline}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-62-1.pdf}}

And the test.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{shapiro.test}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Shapiro-Wilk normality test

data:  x
W = 0.99966, p-value = 0.5779
\end{verbatim}

Uh oh! The test rejects the null hypothesis.

\subsubsection*{So what do we do?}\label{so-what-do-we-do}
\addcontentsline{toc}{subsubsection}{So what do we do?}

Actually, we might be focusing on the wrong question. Instead of asking
\emph{how close to normally distributed} our data are, we should
consider:

\begin{itemize}
\tightlist
\item
  how \textbf{robust} our analysis is to deviations from normality,
  and\\
\item
  which types of deviations from normality actually matter.
\end{itemize}

Generally speaking, \textbf{linear models are robust} to deviations from
normality.

This means:

\begin{itemize}
\tightlist
\item
  we can be relatively happy that the Shapiro--Wilk test lacks power
  with small samples, and\\
\item
  we should be cautious about rejecting normality when sample sizes are
  large.
\end{itemize}

\subsection*{More insight from
QQ-plots}\label{more-insight-from-qq-plots}
\addcontentsline{toc}{subsection}{More insight from QQ-plots}

It is quite subjective to decide if a QQ-plot looks ok, or not. And if
the tests are not so useful, what can we do?

One option is to make use of simulation to see what kind of variation we
can expect in QQ-plots, even when the data really do come from a normal
distribution.

Here is an example of such a plot (the code to make it is below):

\pandocbounded{\includegraphics[keepaspectratio]{3.1-regression-part1_files/figure-pdf/unnamed-chunk-64-1.pdf}}

The red dots are is the qqplot of the observed data. The grey lines are
qqplots of normally distributed random data, with characteristics equal
to those assumed by the model. Each grey line is a separate draw of
random numbers. So the grey lines give an idea of what kind of spread
one can expect even when the data really do come from a normal
distribution. If the red points lie within the grey region, we're good
to go!

Some of the upper red dots are at the edge of grey region, suggesting
there might be minor deviations from normality. Hence, even with some
help given by showing what we can expect to see, we still have to make
an arbitrary / rather subjective decision about when deviations are bad
enough to warrant doing something, and when they are not so bad, and we
can continue with the assumption of normality. Much of data analysis is
more subjective that we might expect given that it is quantitative!

Here is the code to make the graph, using some example data:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# function adapted from}
\DocumentationTok{\#\# http://www.nate{-}miller.org/1/post/2013/03/how{-}normal{-}is{-}normal{-}a{-}q{-}q{-}plot{-}approach.html}
\NormalTok{qqfunc }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(model, num.reps) \{}
  
\NormalTok{  N }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(}\FunctionTok{resid}\NormalTok{(model))}
\NormalTok{  sigma }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(model)}\SpecialCharTok{$}\NormalTok{sigma}
  
\NormalTok{  x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{0}\NormalTok{, sigma)}
\NormalTok{  xx }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(x, }\AttributeTok{plot.it=}\NormalTok{F)}
\NormalTok{  xx}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ xx}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{order}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x)]}
\NormalTok{  xx}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ xx}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{order}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x)]}
  \FunctionTok{plot}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{scale}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{y), }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#00000011"}\NormalTok{, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{,}
       \AttributeTok{xlab=}\StringTok{"Theoretical quantiles"}\NormalTok{,}
       \AttributeTok{ylab=}\StringTok{"Standardised residuals"}\NormalTok{)}
  \DocumentationTok{\#\#qqline(x)}
  
  \ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \DecValTok{2}\SpecialCharTok{:}\NormalTok{num.reps) \{}
    
\NormalTok{    x }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\DecValTok{0}\NormalTok{, sigma)}
\NormalTok{    xx }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(x, }\AttributeTok{plot.it=}\NormalTok{F)}
\NormalTok{    xx}\SpecialCharTok{$}\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ xx}\SpecialCharTok{$}\NormalTok{y[}\FunctionTok{order}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x)]}
\NormalTok{    xx}\SpecialCharTok{$}\NormalTok{x }\OtherTok{\textless{}{-}}\NormalTok{ xx}\SpecialCharTok{$}\NormalTok{x[}\FunctionTok{order}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x)]}
    \FunctionTok{points}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{scale}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{y), }\AttributeTok{pch=}\DecValTok{19}\NormalTok{, }\AttributeTok{col=}\StringTok{"\#00000011"}\NormalTok{, }\AttributeTok{type=}\StringTok{"l"}\NormalTok{)}
    
\NormalTok{  \}}
  
\NormalTok{  xx }\OtherTok{\textless{}{-}} \FunctionTok{qqnorm}\NormalTok{(m1}\SpecialCharTok{$}\NormalTok{residuals, }\AttributeTok{plot.it=}\NormalTok{F)}
  \FunctionTok{points}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{x, }\FunctionTok{scale}\NormalTok{(xx}\SpecialCharTok{$}\NormalTok{y), }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{pch=}\DecValTok{19}\NormalTok{)}
  
\NormalTok{\}}

\DocumentationTok{\#\# load some useful packages}
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(ggfortify)}

\DocumentationTok{\#\# load the data}
\DocumentationTok{\#\# Here I\textquotesingle{}m loading it direct from online where the dataset are stored}
\NormalTok{fhc }\OtherTok{\textless{}{-}} \FunctionTok{suppressMessages}\NormalTok{(readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/opetchey/BIO144\_Practicals\_WAs/refs/heads/main/assets/datasets/financing\_healthcare.csv"}\NormalTok{,}\AttributeTok{show\_col\_types =} \ConstantTok{FALSE}\NormalTok{))}

\DocumentationTok{\#\# tell me which countries are in the dataset}
\CommentTok{\#unique(fhc$country)}

\DocumentationTok{\#\# tell me which years are in the dataset}
\CommentTok{\#unique(fhc$year)}

\DocumentationTok{\#\# give me the rows in which both child mortality and health care expenditure are not NA}
\CommentTok{\#filter(fhc, !is.na(child\_mort) \& !is.na(health\_exp\_total))}

\DocumentationTok{\#\# Wrange the data}
\NormalTok{fhc1 }\OtherTok{\textless{}{-}}\NormalTok{ fhc }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(year}\SpecialCharTok{==}\DecValTok{2013}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\# only data fro 2013 please}
  \FunctionTok{select}\NormalTok{(year}\SpecialCharTok{:}\NormalTok{continent, health\_exp\_total, child\_mort, life\_expectancy) }\SpecialCharTok{\%\textgreater{}\%} \DocumentationTok{\#\# only these columns please}
  \FunctionTok{drop\_na}\NormalTok{() }\DocumentationTok{\#\# drop rows with any NAs}

\DocumentationTok{\#\# From last weeks work we know we need to log transform the data}
\NormalTok{fhc1 }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(fhc1,}
               \AttributeTok{log10\_health\_exp\_total=}\FunctionTok{log10}\NormalTok{(health\_exp\_total),}
               \AttributeTok{log10\_child\_mort=}\FunctionTok{log10}\NormalTok{(child\_mort))}

\DocumentationTok{\#\# plot the relationship between health care expenditure and child mortality}
\CommentTok{\#ggplot(data=fhc1, aes(x=health\_exp\_total, y=child\_mort)) + geom\_point()}

\DocumentationTok{\#\# fit the linear model of the log transformed data and assign it to object named m1}
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(log10\_child\_mort }\SpecialCharTok{\textasciitilde{}}\NormalTok{ log10\_health\_exp\_total, }\AttributeTok{data=}\NormalTok{fhc1)}

\DocumentationTok{\#\# Here we run the function that makes the qqplot with some random samples from a normal distribution}
\FunctionTok{qqfunc}\NormalTok{(m1, }\DecValTok{100}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\bookmarksetup{startatroot}

\chapter*{Regression Part 2 (L4)}\label{regression-part-2-l4}
\addcontentsline{toc}{chapter}{Regression Part 2 (L4)}

\markboth{Regression Part 2 (L4)}{Regression Part 2 (L4)}

This chapter builds on the previous chapter on simple linear regression.
There we learned how to fit a regression model to data, and how to check
if the assumptions of the regression model are met. In this chapter we
will learn how to interpret the results of a regression model, and how
to use the model to make inferences about the relationship between the
dependent and independent variables.

\section*{Introduction}\label{introduction-1}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Now that we have a satisfactory model, we can start to use it. In the
following material, you will learn:

\begin{itemize}
\tightlist
\item
  How to measure how good is the regression (correlation and \(R^2\)).
\item
  How to test if the parameter estimates are compatible with some
  specific value (\(t\)-test).
\item
  How to find the range of parameters values are compatible with the
  data (confidence intervals).
\item
  How to find the regression lines compatible with the data (confidence
  band).
\item
  How to calculate plausible values of newly collected data (prediction
  band).
\end{itemize}

\section*{How good is the regression
model?}\label{how-good-is-the-regression-model}
\addcontentsline{toc}{section}{How good is the regression model?}

\markright{How good is the regression model?}

What would a good regression model look like? What would a bad one look
like? One could say that a good regression model is one that explains
the dependent variable well. But what could we mean by ``explains the
data well''?

Take these two examples.

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-2-1.pdf}}

The first model seems to fit the data well, while the second one does
not. But how can we quantify this?

Let's say that we will measure the goodness of the model by the amount
of variability of the dependent variable that is explained by the
independent variable. To do this we need to do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Measure the total variability of the dependent variable (total sum of
  squares, \(SST\)).
\item
  Measure the amount of variability of the dependent variable that is
  explained by the independent variable (model sum of squares, \(SSM\)).
\item
  Measure the variability of the dependent variable that is not
  explained by the independent variable (error sum of squares, \(SSE\)).
\item
  Calculate the proportion of variability of the dependent variable that
  is explained by the independent variable (\(R^2\), pronounced as
  ``r-squared'') (also known as the coefficient of determination)
  (\(R^2\) = \(SSM/SST\)).
\end{enumerate}

\textbf{Importantly, note that we will calculate \(SSM\) and \(SSE\) so
that they sum up to \(SST\). I.e., \(SST = SSM + SSE\). That is, the
total variability is the sum of what is explained by the model and what
remains unexplained.}

Let's take each in turn:

\subsection*{\texorpdfstring{\(SST\)}{SST}}\label{sst}
\addcontentsline{toc}{subsection}{\(SST\)}

\textbf{1. The total variability of the dependent variable is the sum of
the squared differences between the dependent variable and its mean.
This is called the total sum of squares (\(SST\)).}

\[SST = \sum_{i=1}^{n} (y_i - \bar{y})^2\]

where: \(y_i\) is the dependent variable, \(\bar{y}\) is the mean of the
dependent variable, \(n\) is the number of observations.

\textbf{Note that sometimes \(SST\) is referred to as \(SSY\) (sum of
squares of \(y\)).}

Graphically, this is the sum of the square of the blue residuals as
shown in the following graph, where the horizontal dashed line is at the
value of the mean of the dependent variable.

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-3-1.pdf}}

We can calculate this in R as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1 }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y1))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\subsection*{SSM and SSE}\label{ssm-and-sse}
\addcontentsline{toc}{subsection}{SSM and SSE}

Now the next two steps, that is getting the model sum of squares (SSM)
and the error sum of squares (SSE) are a bit more complicated. To do
this we need to fit a regression model to the data. Let's see this
graphically, and divide the data into the explained and unexplained
parts.

Make a graph with vertical lines connecting the data to the mean of the
data, but with each line two parts, one from the mean to the data, and
one from the data to the predicted value.

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-5-1.pdf}}

In this graph, the square of the length of the green lines is the model
sum of squares (\(SST\)). The square of the length of the red lines is
the error sum of squares (\(SSE\)).

In a better model the length of the green lines will be \textbf{longer}
(the square of these gives the \(SMM\), the variability explained by the
model). And the length of the red lines will be \textbf{shorter} (the
square of these gives the \(SSE\), the variability not explained by the
model).

\subsection*{\texorpdfstring{\(SSM\)}{SSM}}\label{ssm}
\addcontentsline{toc}{subsection}{\(SSM\)}

Next we will do the second step, that is calculate the model sum of
squares (\(SSM\)).

\textbf{2. The amount of variability of the dependent variable that is
explained by the independent variable is called the model sum of squares
(\(SSM\)).}

This is the difference between the predicted value of the dependent
variable and the mean of the dependent variable, squared and summed:

\[SSE = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2\]

where: \(\hat{y}_i\) is the predicted value of the dependent variable,

In R, we calculate this as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{y1\_predicted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1)}
\NormalTok{SSM }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1\_predicted }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y1))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{SSM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 266.476
\end{verbatim}

\subsection*{\texorpdfstring{\(SSE\)}{SSE}}\label{sse}
\addcontentsline{toc}{subsection}{\(SSE\)}

Third, we calculate the error sum of squares (\(SSE\)) with either of
two methods. We could calculate it as the sum of the squared residuals,
or as the difference between the total sum of squares and the model sum
of squares:

\[SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = SST - SSM\] Let's calculate
this in R uses both approaches:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1 }\SpecialCharTok{{-}}\NormalTok{ y1\_predicted)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.567317
\end{verbatim}

Or\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE }\OtherTok{\textless{}{-}}\NormalTok{ SST }\SpecialCharTok{{-}}\NormalTok{ SSM}
\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.567317
\end{verbatim}

\subsection*{\texorpdfstring{\(R^2\)}{R\^{}2}}\label{r2}
\addcontentsline{toc}{subsection}{\(R^2\)}

Finally, we calculate the proportion of variability of the dependent
variable that is explained by the independent variable (\(R^2\)):

\[R^2 = \frac{SSM}{SST}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{R.squared }\OtherTok{\textless{}{-}}\NormalTok{ SSM}\SpecialCharTok{/}\NormalTok{SST}
\NormalTok{R.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9723864
\end{verbatim}

\subsection*{Is my R squared good?}\label{is-my-r-squared-good}
\addcontentsline{toc}{subsection}{Is my R squared good?}

What value of \(R^2\) is considered good? In ecological research,
\(R^2\) values are often low (less than 0.3), because ecological systems
are complex and many factors influence the dependent variable. However,
in other fields, such as physiology, \(R^2\) values are often higher.
Therefore, the answer of what values of \(R^2\) are good depends on the
field of research.

Here are the four examples and their r-squared.

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-10-1.pdf}}

\section*{How unlikey is the observed data given the null
hypothesis?}\label{how-unlikey-is-the-observed-data-given-the-null-hypothesis}
\addcontentsline{toc}{section}{How unlikey is the observed data given
the null hypothesis?}

\markright{How unlikey is the observed data given the null hypothesis?}

We often hear this expressed as ``is the relationship significant?'' And
maybe we heard that the relationship is significant if the p-value is
less than 0.05. But what does all this actually mean? In this section
we'll figure all this out. The first step to is to formulate a null
hypothesis.

What is a meaningful null hypothesis for a regression model?

As mentioned, often we're interested in whether there is a relationship
between the dependent (response) and independent (explanatory) variable.
Therefore, the null hypothesis is that there is no relationship between
the dependent and independent variable. This means that the null
hypothesis is that the slope of the regression line is zero.

Recall the regression model: \[y = \beta_0 + \beta_1 x + \epsilon\]

The null hypothesis is that the slope of the regression line is zero:
\[H_0: \beta_1 = 0\]

What is the alternative hypothesis?

\[H_1: \beta_1 \neq 0\]

So, how do we test the null hypothesis? More precisely, we are going to
calculate the probability of observing the data we have, given that the
null hypothesis is true. If this probability is very low, then we can
reject the null hypothesis.

Does that make sense? Does it seem a bit convoluted? It is a bit!!!

But this is how hypothesis testing works. We never prove the null
hypothesis is true. Instead, we calculate the probability of observing
our data given that the null hypothesis is true. If this probability is
very low, we reject the null hypothesis.

To make the calculation we can use the fact that the slope of the
regression line is an estimate of the true slope. This estimate has
uncertainty associated with it. We can use this uncertainty to calculate
the probability of observing the data we have, given the null hypothesis
is true.

We can see that the slope estimate (the \(x\) row) has uncertainty by
looking at the regression output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Estimate Std. Error
(Intercept) -0.7638353  0.6652233
x            2.1161160  0.1072104
\end{verbatim}

The estimate is the mean of the distribution of the parameter (slope)
and the standard error is a measure of the uncertainty of the estimate.

The standard error is calculated as:

\[\sigma^{(\beta_1)} = \sqrt{ \frac{\hat\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}}\]

Where \(\hat\sigma^2\) is the expected residual variance of the model.
This is calculated as:

\[\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n-2}\]

Where \(\hat y_i\) is the predicted value of \(y_i\) from the regression
model.

OK, let's take a look at this intuitively. We have the estimate of the
slope and the standard error of the estimate.

Here is a graph of the value of the slope estimate versus the standard
error of the estimate:

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-12-1.pdf}}

When the slope estimate is larger, it is less likely to have been
observed by chance. And when the standard error is larger, it is more
likely to have been observed by chance. How can we put these together
into a single measure?

If we divide the slope estimate by the standard error, we get a measure
of how many standard errors the slope estimate is from the null
hypothesis slope of zero. This is the \(t\)-statistic:

\[t = \frac{\hat\beta_1 - \beta_{1,H_0}}{\sigma^{(\beta_1)}}\]

Where \(\beta_{1,H_0}\) is the null hypothesis value of the slope,
usually zero, so that

\[t = \frac{\hat\beta_1}{\sigma^{(\beta_1)}}\]

\textbf{The \(t\)-statistic is a measure of how many standard errors the
slope estimate is from the null hypothesis value of the slope. The
larger the \(t\)-statistic, the less likely the slope estimate was
observed by chance.}

How can we transform the value of a \(t\)-statistic into a p-value? We
can use the \textbf{\(t\)-distribution}, which quantifies the
probability of observing a value of the \(t\)-statistic under the null
hypothesis.

But what is the \(t\)-distribution? It is a distribution of the
\(t\)-statistic under the null hypothesis. It is a bell-shaped
distribution that is centered on zero. The shape of the distribution is
determined by the degrees of freedom, which is \(n-2\) for a simple
linear regression model.

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-13-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

By the way, it is named the \(t\)-distribution by it's developer,
William Sealy Gosset, who worked for the Guinness brewery in Dublin,
Ireland. In his 1908 paper, Gosset introduced the \(t\)-distribution but
he didn't explicitly explain his choice of the letter \(t\). The choice
of the letter \(t\) could be to indicate ``Test'', as the
\(t\)-distribution was developed specifically for hypothesis testing.

\end{tcolorbox}

Now, recall that the p-value is the probability of observing the value
of the test statistic (so here the \(t\)-statistic) at least as extreme
as the one we have, given the null hypothesis is true. We can calculate
this probability by integrating the \(t\)-distribution from the observed
\(t\)-statistic to the tails of the distribution.

Here is a graph of the \(t\)-distribution with 100 degrees of freedom
with the tails of the distribution shaded so that the area of the shaded
region is 0.05 (i.e., 5\% of the total area).

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-14-1.pdf}}

And here's a graph of the \(t\)-distribution with 1000 degrees of
freedom (blue line) and the normal distribution (green dashed line):

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-15-1.pdf}}

So, with a large number of observations, the \(t\)-distribution
approaches the normal distribution. For the normal distribution, the
95\% area is between -1.96 and 1.96.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# x value for 95\% area of normal distribution}
\NormalTok{x\_value }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}
\NormalTok{x\_value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.959964
\end{verbatim}

\texttt{qnorm} is a function that calculates the \(x\) value for a given
quantile (probability) of the normal distribution. In simpler terms, it
finds the value \(x\) at which the area under the normal curve (up to
\(x\)) equals the given probability \(p\) (0.975 in the example
immediately above here).

Let's go back to the age - blood pressure data and calculate the p-value
for the slope estimate.

Read in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(here}\SpecialCharTok{::}\FunctionTok{here}\NormalTok{(}\StringTok{"datasets/Simulated\_Blood\_Pressure\_and\_Age\_Data.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 100 Columns: 2
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (2): Age, Systolic_BP

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Make a graph:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Age, }\AttributeTok{y =}\NormalTok{ Systolic\_BP)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-19-1.pdf}}

Here's the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Systolic\_BP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age, }\AttributeTok{data =}\NormalTok{ bp\_data)}
\end{Highlighting}
\end{Shaded}

Here we calculate the \(t\)-statistic for the slope estimate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{t\_stat }\OtherTok{\textless{}{-}}\NormalTok{ mod1}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{/} \FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

And here we calculate the one-tailed and two-tailed \(p\)-values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_tailed\_p\_value }\OtherTok{\textless{}{-}} \FunctionTok{pt}\NormalTok{(}\SpecialCharTok{{-}}\FunctionTok{abs}\NormalTok{(t\_stat), }\AttributeTok{df =} \FunctionTok{nrow}\NormalTok{(bp\_data) }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{)}
\NormalTok{two\_tailed\_p\_value }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ one\_tailed\_p\_value}
\NormalTok{one\_tailed\_p\_value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Age 
3.746958e-51 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{two\_tailed\_p\_value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Age 
7.493917e-51 
\end{verbatim}

We can get the \(p\)-value directly from the \texttt{summary} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coefficients[}\DecValTok{2}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    Estimate   Std. Error      t value     Pr(>|t|) 
8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 
\end{verbatim}

Conclusion: there is \textbf{very strong evidence} that the blood
pressure is associated with age, because the \(p\)-value is extremely
small (thus it is very unlikely that the observed slope value or a large
one would be seen if there was really no association). Thus, we can
reject the null hypothesis that the slope is zero.

This basically answers question 1: ``Are the parameters compatible with
some specific value?''

\subsection*{\texorpdfstring{Recap: Formal definition of the
\(p\)-value}{Recap: Formal definition of the p-value}}\label{recap-formal-definition-of-the-p-value}
\addcontentsline{toc}{subsection}{Recap: Formal definition of the
\(p\)-value}

\textbf{The formal definition of \(p\)-value is the probability to
observe a data summary (e.g., an average or a slope) that is at least as
extreme as the one observed, given that the null hypothesis is correct.}

Example (normal distribution): Assume that we calculated that
\(t\)-value = -1.96

\(\Rightarrow\) \(Pr(|t|\geq 1.96)=0.05\) (two-tailed) and
\(Pr(t\leq-1.96)=0.025\) (one-tailed).

And here is a graph showing this:

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-24-1.pdf}}

\subsection*{\texorpdfstring{A cautionary note on the use of
\(p\)-values}{A cautionary note on the use of p-values}}\label{a-cautionary-note-on-the-use-of-p-values}
\addcontentsline{toc}{subsection}{A cautionary note on the use of
\(p\)-values}

Maybe you have seen that in statistical testing, often the criterion
\(p\leq 0.05\) is used to test whether \(H_0\) should be rejected. This
is often done in a black-or-white manner. However, we will put a lot of
attention to a more reasonable and cautionary interpretation of
\(p\)-values in this course!

\section*{How strong is the
relationship?}\label{how-strong-is-the-relationship}
\addcontentsline{toc}{section}{How strong is the relationship?}

\markright{How strong is the relationship?}

The actual value of the slope has practical meaning. The slope of the
regression line tells us how much the dependent variable changes when
the independent variable changes by one unit. The slope is one measure
of the strength of the relationship between the two variables.

We can ask what values of a parameter estimate are compatible with the
data (confidence intervals)? To answer this question, we can determine
the confidence intervals of the regression parameters.

The confidence interval of a parameter estimate is defined as the
interval that contains the true parameter value with a certain
probability. So the 95\% confidence interval of the slope is the
interval that contains the true slope with a probability of 95\%.

We can then imagine two cases. The 95\% confidence interval of the slope
includes 0:

\begin{verbatim}
Warning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.
i Please use the `orientation` argument of `geom_errorbar()` instead.
\end{verbatim}

\begin{verbatim}
`height` was translated to `width`.
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-25-1.pdf}}

Or where the confidence interval does not include zero:

\begin{verbatim}
`height` was translated to `width`.
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-26-1.pdf}}

How do we calculate the lower and upper limits of the 95\% confidence
interval of the slope?

Recall that the \(t\)-value for a null hypothesis of slope of zero is
defined as:

\[t = \frac{\hat\beta_1}{\hat\sigma^{(\beta_1)}}\]

The first step is to calculate the \(t\)-value that corresponds to a
p-value of 0.05. This is the \(t\)-value that corresponds to the 97.5\%
quantile of the \(t\)-distribution with \(n-2\) degrees of freedom.

\(t_{0.975} = t_{0.025} = 1.96\), for large \(n\).

The 95\% confidence interval of the slope is then given by:

\[\hat\beta_1 \pm t_{0.975} \cdot \hat\sigma^{(\beta_1)}\]

In our blood pressure example the estimated slope is 0.8240678 and the
standard error of the slope is 0.0277096. We can calculate the 95\%
confidence interval of the slope in \emph{R} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{t\_0975 }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\AttributeTok{df =}\NormalTok{ n }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{)}
\NormalTok{half\_interval }\OtherTok{\textless{}{-}}\NormalTok{ t\_0975 }\SpecialCharTok{*} \FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{lower\_limit }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ half\_interval}
\NormalTok{upper\_limit }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ half\_interval}
\NormalTok{ci\_slope }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(lower\_limit, upper\_limit)}
\NormalTok{slope }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{]}
\NormalTok{slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Age 
0.8240678 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ci\_slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Age       Age 
0.7690791 0.8790565 
\end{verbatim}

Or, using the \texttt{confint} function:

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# 95\% confidence interval of the slope of mod1}
\NormalTok{ci\_slope\_2 }\OtherTok{\textless{}{-}} \FunctionTok{confint}\NormalTok{(mod1, }\AttributeTok{level=}\FunctionTok{c}\NormalTok{(}\FloatTok{0.95}\NormalTok{))[}\DecValTok{2}\NormalTok{,]}
\NormalTok{ci\_slope\_2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
    2.5 %    97.5 % 
0.7690791 0.8790565 
\end{verbatim}

Or we can do it using values from the coefficients table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coef}
\NormalTok{beta }\OtherTok{\textless{}{-}}\NormalTok{ coefs[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{sdbeta }\OtherTok{\textless{}{-}}\NormalTok{ coefs[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }
\NormalTok{beta }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{,}\DecValTok{241}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ sdbeta }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7694840 0.8786516
\end{verbatim}

\emph{Interpretation}: for an increase in the age by one year, roughly
0.82 mmHg increase in blood pressure is expected, and all true values
for \(\beta_1\) between 0.77 and 0.88 are compatible with the observed
data.

\section*{Confidence and Prediction
Bands}\label{confidence-and-prediction-bands}
\addcontentsline{toc}{section}{Confidence and Prediction Bands}

\markright{Confidence and Prediction Bands}

\begin{itemize}
\item
  Remember: If another sample from the same population was taken, the
  regression line would look slightly different.
\item
  There are two questions to be asked:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which other regression lines are compatible with the observed data?
  This leads to the \emph{confidence band}.
\item
  Where do future observations (\(y\)) with a given \(x\) coordinate
  lie? This leads to the \emph{prediction band}.
\end{enumerate}

Note: The prediction band is much broader than the confidence band.

\section*{Calculation of the confidence
band}\label{calculation-of-the-confidence-band}
\addcontentsline{toc}{section}{Calculation of the confidence band}

\markright{Calculation of the confidence band}

Given a fixed value of \(x\), say \(x_0\). The question is:

Where does \(\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0\) lie with a
certain confidence (i.e., 95\%)?

This question is not trivial, because both \(\hat\beta_0\) and
\(\hat\beta_1\) are estimates from the data and contain uncertainty.

The details of the calculation are given in Stahel 2.4b.

Plotting the confidence interval around all \(\hat y_0\) values one
obtains the \emph{confidence band} or \emph{confidence band for the
expected values} of \(y\).

Note: For the confidence band, only the uncertainty in the estimates
\(\hat\beta_0\) and \(\hat\beta_1\) matters.

Here is the confidence band for the blood pressure data:

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-30-1.pdf}}

Very narrow confidence bands indicate that the estimates are very
precise. In this case the estimated intercept and slope are precise
because the sample size is large and the data points are close to the
regression line.

\section*{Calculations of the prediction
band}\label{calculations-of-the-prediction-band}
\addcontentsline{toc}{section}{Calculations of the prediction band}

\markright{Calculations of the prediction band}

We can easily predicted an expected value of \(y\) for a given \(x\)
value. But we can also ask w where does a \emph{future observation} lie
with a certain confidence (i.e., 95\%)?

To answer this question, we have to \emph{consider not only the
uncertainty in the predicted value caused by uncertainty in the
parameter estimates} \(\hat y_0 =  \hat\beta_0 + \hat\beta_1 x_0\), but
also the \emph{error term} \(\epsilon_i \sim N(0,\sigma^2)\)\}.

This is the reason why the \textbf{prediction band} is wider than the
confidence band.

Here's a graph showing the prediction band for the blood pressure data:

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-32-1.pdf}}

Another way to think of the 95\% confidence band is that it is where we
would expect 95\% of the regression lines to lie if we were to collect
many samples from the same population. The 95\% prediction band is where
we would expect 95\% of the future observations to lie.

\section*{Review}\label{review-2}
\addcontentsline{toc}{section}{Review}

\markright{Review}

That is regression done (at least for our current purposes). Here is a
summary of what we have covered:

Previous chapter:

\begin{itemize}
\tightlist
\item
  Why use (linear) regression?
\item
  Fitting the line (= parameter estimation)
\item
  Is linear regression good enough model to use?
\item
  What to do when things go wrong?
\item
  Transformation of variables/the response.
\item
  Handling of outliers.
\end{itemize}

This chapter:

\begin{itemize}
\tightlist
\item
  Sums of squares: \(SST\), \(SSM\), \(SSE\)
\item
  \(R^2\) as a measure of goodness of fit
\item
  Hypothesis testing in regression
\item
  Null and alternative hypotheses
\item
  t-statistic and p-values
\item
  Tests and confidence intervals
\item
  Confidence and prediction bands
\end{itemize}

\section*{Further reading}\label{further-reading-3}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

Again, there are many good books and online resources about regression
models. The same ones as last week are recommended, also for
interpreting results of regression models.

\begin{itemize}
\tightlist
\item
  Faraway, J. J. (2016). Linear models with R (2nd ed.). Chapman and
  Hall/CRC.
  \href{https://www.routledge.com/Linear-Models-with-R-2nd-Edition/Faraway/p/book/9781482215754}{Link}.
\item
  Chapter 7 of The New Statistics with R, by Andy Hector.
\end{itemize}

\section*{Extras}\label{extras-2}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{Randomisation test for the slope of a regression
line}\label{randomisation-test-for-the-slope-of-a-regression-line}
\addcontentsline{toc}{subsection}{Randomisation test for the slope of a
regression line}

Let's use randomisation as another method to understand how likely we
are to observe the data we have, given the null hypothesis is true.

If the null hypothesis is true, we expect no relationship between \(x\)
and \(y\). Therefore, we can shuffle the \(y\) values and fit a
regression model to the shuffled data. We can repeat this many times and
calculate the slope of the regression line each time. This will give us
a distribution of slopes we would expect to observe if the null
hypothesis is true.

First, we'll make some data and get the slope of the regression line.
Here is the observed slope and relationship:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n}
\NormalTok{y }\OtherTok{\textless{}{-}} \FloatTok{0.1}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\NormalTok{m }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\FunctionTok{coef}\NormalTok{(m)[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        x 
0.1251108 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ x, }\AttributeTok{y =}\NormalTok{ y)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-34-1.pdf}}

Now we'll use randomisation to test the null hypothesis. We can create
lots of examples where the relationship is expected to have a slope of
zero by shuffling randomly the \(y\) values. Here are 20:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{5}\NormalTok{, }\DecValTok{4}\NormalTok{))}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) \{}
\NormalTok{  y\_rand }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(y)}
\NormalTok{  m\_rand }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_rand }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
  \FunctionTok{plot}\NormalTok{(x, y\_rand, }\AttributeTok{main =} \FunctionTok{paste}\NormalTok{(}\StringTok{"Slope = "}\NormalTok{, }\FunctionTok{round}\NormalTok{(}\FunctionTok{coef}\NormalTok{(m\_rand)[}\DecValTok{2}\NormalTok{], }\DecValTok{2}\NormalTok{)))}
  \FunctionTok{abline}\NormalTok{(m\_rand)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-35-1.pdf}}

Now let's create 19 and put the real one in there somewhere random.
Here's a case where the real data has a quite strong relationship:

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-36-1.pdf}}

We can confidently find the real data amount the shuffled data. But what
if the relationship is weaker?

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-37-1.pdf}}

Now its less clear which is the real data. We can use this idea to test
the null hypothesis.

We do the same procedure of but instead of just looking at the graphs,
we calculate the slope of the regression line each time. This gives us a
distribution of slopes we would expect to observe if the null hypothesis
is true. We can then see where the observed slope lies in this
distribution of null hypothesis slopes.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# repeat 10000 time a randomisation test}
\NormalTok{y }\OtherTok{\textless{}{-}} \FloatTok{0.15}\SpecialCharTok{*}\NormalTok{x }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{rand\_slopes }\OtherTok{\textless{}{-}} \FunctionTok{replicate}\NormalTok{(}\DecValTok{10000}\NormalTok{, \{}
\NormalTok{  y\_rand }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(y)}
\NormalTok{  m\_rand }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y\_rand }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
  \FunctionTok{coef}\NormalTok{(m\_rand)[}\DecValTok{2}\NormalTok{]}
\NormalTok{\})}

\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(}\AttributeTok{slopes =}\NormalTok{ rand\_slopes), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ slopes)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{bins =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FunctionTok{coef}\NormalTok{(m)[}\DecValTok{2}\NormalTok{], }\AttributeTok{color =} \StringTok{"red"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-38-1.pdf}}

We can now calculate the probability of observing the data we have,
given the null hypothesis is true.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_value }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(rand\_slopes) }\SpecialCharTok{\textgreater{}=} \FunctionTok{abs}\NormalTok{(}\FunctionTok{coef}\NormalTok{(m)[}\DecValTok{2}\NormalTok{]))}
\NormalTok{p\_value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.0172
\end{verbatim}

\subsection*{Visualising p-values for regression
slopes}\label{visualising-p-values-for-regression-slopes}
\addcontentsline{toc}{subsection}{Visualising p-values for regression
slopes}

\pandocbounded{\includegraphics[keepaspectratio]{4.1-regression-part2_files/figure-pdf/unnamed-chunk-40-1.pdf}}

\bookmarksetup{startatroot}

\chapter*{Analysis of variance (L5)}\label{analysis-of-variance-l5}
\addcontentsline{toc}{chapter}{Analysis of variance (L5)}

\markboth{Analysis of variance (L5)}{Analysis of variance (L5)}

The previous two chapters were about linear regression. \emph{Linear
regression} is a type of \emph{linear model} -- recall that in \emph{R}
we used the function \texttt{lm()} to make the regression model. In this
chapter we will look at a different type of linear model: analysis of
variance (ANOVA).

\section*{Introduction}\label{introduction-2}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Recall that linear regression is a linear model with one continuous
explanatory (independent) variable. A continuous explanatory variable is
a variable in which values can take any value within a range (e.g.,
height, weight, temperature).

In contrast, analysis of variance (ANOVA) is a linear model with one or
more categorical explanatory variables. We will first look at a one-way
ANOVA, which has one categorical explanatory variable. Later (in a
following chapter) we will look at two-way ANOVA, which has two
categorical explanatory variables.

What is a categorical variable? A categorical explanatory variable is a
variable that contains values that fall into distinct groups or
categories. For example, habitat type (e.g., forest, grassland,
wetland), treatment group (e.g., control, low dose, high dose), or diet
type (e.g., vegetarian, vegan, omnivore).

This means that each observation belongs to one of a limited number of
categories or groups. For example, in a study of how blood pressure
varies with diet type, diet type is a categorical variable with several
levels (e.g., vegetarian, vegan, omnivore). A person can only belong to
one diet type category.

Here are the first several rows of a dataset that contains blood
pressure measurements for individuals following different diet types:

Reading in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/bp\_data\_diet.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50 Columns: 3
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (2): diet, person_ID
dbl (1): bp

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(bp\_data\_diet, bp, diet, person\_ID)}
\FunctionTok{head}\NormalTok{(bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
     bp diet          person_ID
  <dbl> <chr>         <chr>    
1   120 meat heavy    person_1 
2    89 vegan         person_2 
3    86 vegetarian    person_3 
4   116 meat heavy    person_4 
5   115 Mediterranean person_5 
6   134 meat heavy    person_6 
\end{verbatim}

There are three variables: - \texttt{bp}: blood pressure (continuous
response variable) - \texttt{diet}: diet type (categorical explanatory
variable) - \texttt{person\_ID}: unique identifier for each individual
(not used in the analysis)

Note that the \texttt{diet} variable is of type
\texttt{\textless{}chr\textgreater{}} which is short for
\texttt{character}. In \emph{R}, categorical variables are often
represented as factors.

As usual, its a really good idea to visualise the data in as close to
``raw'' form as possible before doing any analysis. We'll make a
scatterplot of blood pressure versus diet type.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure by Diet Type"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet Type"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood Pressure (mm Hg)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-tip-color!10!white, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, colframe=quarto-callout-tip-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

We just used \texttt{geom\_jitter()} instead of \texttt{geom\_point()}
to make a scatterplot. This is because \texttt{geom\_jitter()} adds a
small amount of random noise to the points, which helps to prevent
overplotting when multiple points have the same value (which is common
when the x-axis is categorical).

When we use \texttt{geom\_jitter()}, we can specify the amount of noise
to add in the x and y directions using the \texttt{width} and
\texttt{height} arguments, respectively. We must be very careful to not
add noise to the y direction if we care about the actual y values (e.g.,
blood pressure). In this case, we only added noise in the x direction by
setting \texttt{height\ =\ 0} to separate the points just enough, but
not so much that we could get confused about which of the diets they
belong to.

\end{tcolorbox}

Looking at this graph it certainly looks like diet type has an effect on
blood pressure. But is this effect statistically significant? In other
words, are the differences in mean blood pressure between diet types
larger than we would expect due to random variation alone?

Analysis of variance (ANOVA) is a statistical method that can help us
answer this question, and also others.

\section*{How does it look like in R?}\label{how-does-it-look-like-in-r}
\addcontentsline{toc}{section}{How does it look like in R?}

\markright{How does it look like in R?}

We can fit a one-way ANOVA model in \emph{R} using the same
\texttt{lm()} function that we used for linear regression. The only
difference is that the explanatory variable is categorical.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{anova\_model }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

Then instead of using \texttt{summary()} to look at the results, we use
the \texttt{anova()} function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(anova\_model)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
          Df Sum Sq Mean Sq F value    Pr(>F)    
diet       3 5274.2 1758.08  20.728 1.214e-08 ***
Residuals 46 3901.5   84.82                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This is an ANOVA table. It shows us the sources of variation in the
data, along with their associated degrees of freedom (Df), sum of
squares (Sum Sq), mean square (Mean Sq), F value, and p-value
(Pr(\textgreater F)) associate with a getting a F value the same as or
greater than the observed F value if the null hypothesis were true.

The challenge now is to understand what all of these values mean! Let's
take it step by step.

\section*{What is ANOVA?}\label{what-is-anova}
\addcontentsline{toc}{section}{What is ANOVA?}

\markright{What is ANOVA?}

Analysis of variance is a method to compare whether the observations
(e.g., of blood pressure) differ according to some grouping (e.g., diet)
that the subjects (e.g., people) belong to.

We already know a lot about analysing variance: we compared the total
sum of squares (SST), model sum of squares (SSM) and the residual sum of
squares (SSE) in the context of linear regression. We used these to
calculated the \(R^2\) value. The \(R^2\) value tells us how much of the
total variance in the response variable (e.g., blood pressure) is
explained by the explanatory variable (e.g., diet).

The same applies to analysis of variance (ANOVA) (as well as regression)
because ANOVA is a special case of a linear model, just like regression
is also a special case of a linear model.

The defining characteristic of ANOVA is that we are comparing the means
of groups by analysing variances. Put another way, we will have a single
categorical explanatory variable with two or more levels. We will test
whether the means of the response variable are the same across all
levels of the explanatory variable, and we test this by analysing the
variances.

When we have only one categorical explanatory variable, we use a
\emph{one-way} ANOVA. When we have two categorical explanatory
variables, we will use a \emph{two-way} ANOVA (we'll look at this in a
subsequent chapter).

\section*{ANOVA as a linear model}\label{anova-as-a-linear-model}
\addcontentsline{toc}{section}{ANOVA as a linear model}

\markright{ANOVA as a linear model}

Just like linear regression, ANOVA can be expressed as a linear model.
The key difference is that in ANOVA, the explanatory variable is
categorical rather than continuous.We formulate the linear model as
follows:

\[y_{ij} = \mu_j + \epsilon_{i}\]

where:

\begin{itemize}
\tightlist
\item
  \(y_{ij}\) = Blood pressure of individual \(i\) with diet \(j\)
\item
  \(\mu_i\) = Mean blood pressure of an individual with diet \(j\)
\item
  \(\epsilon_{i}\sim N(0,\sigma^2)\) is an independent error term.
\end{itemize}

Graphically, with the blood pressure and diet data, this looks like:

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

There is lots of hidden code used to create the data used in the graph
above, and to make the graph itself. You can see the code by going to
the \href{https://github.com/opetchey/BIO144_Course_Book}{Github
repository for this book}.

\end{tcolorbox}

\subsection*{Rewrite the model}\label{rewrite-the-model}
\addcontentsline{toc}{subsection}{Rewrite the model}

We usually use a different formulation of the linear model for ANOVA.
This is because we usually prefer to express the estimated parameters in
terms of \emph{differences between means} (rather than the means
themselves). The reason for this is that then the null hypothesis can be
that the differences are zero.

To proceed with this formulation, we define one of the groups as the
reference group, and make the mean of that equal to the intercept of the
model. For example, if we choose the ``meat heavy'' diet as the
reference group, we can write:

\[\mu_{meat} = \beta_0\]

And then to express the other group means as deviations from the
reference group mean:

\[\mu_{Med} = \beta_0 + \beta_1\] \[\mu_{vegan} = \beta_0 + \beta_2\]
\[\mu_{veggi} = \beta_0 + \beta_3\]

When we write out the entire model, we get:

\[y_i = \beta_0 + \beta_1 x_i^{1} + \beta_2 x_i^{2} + \beta_3 x_i^{3} + \epsilon_i\]
where: \(y_i\) is the blood pressure of individual \(i\). \(x_i^{1}\) is
a binary variable indicating whether individual \(i\) is on the
Mediterranean diet. \(x_i^{2}\) is a binary variable indicating whether
individual \(i\) is on the vegan diet. \(x_i^{3}\) is a binary variable
indicating whether individual \(i\) is on the vegetarian diet.

Graphically, the model now looks like this:

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-12-1.pdf}}

Here is something to warp you mind\ldots{} we described one-way ANOVA as
a linear model with one categorical explanatory variable. But as you can
see above, we can also describe it as a linear model with multiple
binary explanatory variables (one for each group except the reference
group). And when we make a linear model in R it really does create
multiple binary explanatory variables behind the scenes. So one-way
ANOVA and multiple linear regression with multiple binary explanatory
variables are really the same thing! And, even more mind-warping,
one-way ANOVA and multiple regression (regression with multiple
continuous explanatory variables) are also the same thing! So when we
look at multiple regression later in the course, you can think of it as
just an extension of one-way ANOVA.

\subsection*{\texorpdfstring{The ANOVA test: The
\(F\)-test}{The ANOVA test: The F-test}}\label{the-anova-test-the-f-test}
\addcontentsline{toc}{subsection}{The ANOVA test: The \(F\)-test}

\textbf{Aim of ANOVA}: to test \emph{globally} if the groups differ.
That is we want to test the null hypothesis that all of the group means
are equal:

\[H_0: \mu_1=\mu_2=\ldots = \mu_g\] This is equivalent to testing if all
\(\beta\)s that belong to a categorical variable are = 0.

\[H_0: \beta_1 = \ldots = \beta_{g-1} = 0\] The alternate hypothesis is
that \({H_1}\): The group means are not all the same.

A key point is that we are testing a null hypothesis that concerns all
the groups. We are not testing if one group is different from another
group (which we could do with a \(t\)-test on one of the non-intercept
\(\beta\)s).

Because we are testing a null hypothesis that concerns all the groups,
we need to use an \(F\)-test. It asks if the model with the group means
is better than a model with just the overall mean.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

The \(F\)-test is called the ``\(F\)-test'' because it is based on the
\(F\)-distribution, which was named after the statistician
\href{https://en.wikipedia.org/wiki/Ronald_Fisher}{Sir Ronald A.
Fisher}. Fisher developed this statistical method as part of his
pioneering work in analysis of variance (ANOVA) and other fields of
experimental design and statistical inference.

\end{tcolorbox}

Actually, the \(F\)-test does not directly test the null hypothesis that
all the group means are equal. Instead, it tests whether the model that
includes the \emph{group means} explains significantly more variance in
the data than a model that only includes the overall mean (i.e., without
considering group differences).

The \(F\)-test does this by comparing two variance estimates: the
variance explained by the group means (between-group variance) and the
variance that remains unexplained within each group (within-group
variance).

\subsection*{\texorpdfstring{Interpretation of the \(F\)
statistic}{Interpretation of the F statistic}}\label{interpretation-of-the-f-statistic}
\addcontentsline{toc}{subsection}{Interpretation of the \(F\) statistic}

The \(F\)-test involves calculating from the observed data the value of
the \(F\) statistic, and then computing if that value is large enough to
reject the null hypothesis.

The \(F\) statistic is a ratio of two variances: the variance
\textbf{between} groups, and the variance \textbf{within} groups.

Here is an example with very low within group variability, and high
between group variability:

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-13-1.pdf}}

And here's an example with very high within group variability, and low
between group variability:

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-14-1.pdf}}

So, when the ratio of between group variance to within group variance is
large, the group means are very different compared to the variability
within groups. This suggests that the groups are different.

When the ratio is small, the group means are similar compared to the
variability within groups. This suggests that the groups are not
different.

\begin{itemize}
\tightlist
\item
  \textbf{\(F\) increases}

  \begin{itemize}
  \tightlist
  \item
    when the group means become more different, or
  \item
    when the variability within groups decreases.
  \end{itemize}
\item
  \textbf{\(F\) decreases}

  \begin{itemize}
  \tightlist
  \item
    when the group means become more similar, or
  \item
    when the variability within groups increases.
  \end{itemize}
\end{itemize}

\(\rightarrow\) The larger \(F\), the less likely are the data seen
under \(H_0\).

\subsection*{\texorpdfstring{Calculating the \(F\)
statistic}{Calculating the F statistic}}\label{calculating-the-f-statistic}
\addcontentsline{toc}{subsection}{Calculating the \(F\) statistic}

Recall that the \(F\) statistic is a ratio of two variances.
Specifically, it is the ratio of two mean squares (MS):

\begin{itemize}
\tightlist
\item
  \(MS_{model}\): the variability \textbf{between} groups.
\item
  \(MS_{residual}\): the variability \textbf{within} groups.
\end{itemize}

\(MS\) stands for Mean Square, and is a variance estimate.

The \(F\) statistic is calculated as:

\[F = \frac{MS_{model}}{MS_{residual}}\]

To find the mean squares, we need to calculate the within and the
between group sums of squares, and the corresponding degrees of freedom.
Let's go though this step by step.

\subsection*{Calculating the sums of
squares}\label{calculating-the-sums-of-squares}
\addcontentsline{toc}{subsection}{Calculating the sums of squares}

First we get the total sum of squares (SST), which quantifies the total
variability in the data. This is then split into the explained
variability (SSM), and the residual variability (SSE).

\textbf{Total variability:} SST =
\(\sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij}-\overline{y})^2\)

where:

\begin{itemize}
\tightlist
\item
  \(y_{ij}\) is the blood pressure of individual \(j\) in group \(i\)
\item
  \(\overline{y}\) is the overall mean blood pressure
\item
  \(n_i\) is the number of individuals in group \(i\)
\item
  \(k\) is the number of groups
\end{itemize}

\textbf{Explained variability (between group variability)}: = SSM =
\(\sum_{i=1}^k n_i (\overline{y}_{i} - \overline{y})^2\)

where:

\begin{itemize}
\tightlist
\item
  \(\overline{y}_{i}\) is the mean blood pressure of group \(i\)
\end{itemize}

\textbf{Residual variability (within group variability)}: = SSE =
\(\sum_{i=1}^k \sum_{j=1}^{n_i}  (y_{ij} - \overline{y}_{i} )^2\)

\subsection*{Calculating the degrees of
freedom}\label{calculating-the-degrees-of-freedom}
\addcontentsline{toc}{subsection}{Calculating the degrees of freedom}

And now we need the degrees of freedom for each sum of squares:

\textbf{SST degrees of freedom}: \(n - 1\) (total degrees of freedom is
number of observations \(n\) minus 1)

\textbf{SSM degrees of freedom}: \(k - 1\) (model degrees of freedom is
number of groups \(k\) minus 1)

\textbf{SSE degrees of freedom}: \(n - k\) (residual degrees of freedom
is total degrees of freedom \(n - 1\) minus model degrees of freedom
\(k - 1\))

\subsubsection*{Total degrees of
freedom}\label{total-degrees-of-freedom}
\addcontentsline{toc}{subsubsection}{Total degrees of freedom}

The total degrees of freedom are the degrees of freedom associated with
the total sum of squares (\(SST\)).

In order to calculate the \(SST\), we need to calculate the mean of the
response variable. This implies that we estimate one parameter (the mean
of the response variable). As a consequence, we lose one degree of
freedom and so there remain \(n-1\) degrees of freedom associated with
the total sum of squares (where \(n\) is the number of observations).

What do we mean by ``lose one degree of freedom''? Imagine we have ten
observations. We can calculate the mean of these ten observations. But
if we know the mean and nine of the observations, we can calculate the
tenth observation. So, in a sense, once we calculate the mean, the value
of one of the ten observations is fixed. This is what we mean by
``losing one degree of freedom''. When we calculate and use the mean,
one of the observations ``loses its freedom''.

For example, take the numbers 1, 3, 5, 7, 9. The mean is 5. The sum of
the squared differences between the observations and the mean is
\((1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2 = 20\). This is the
total sum of squares. The degrees of freedom are \(5-1 = 4\).

The total degrees of freedom are the total number of observations minus
one. That is, the total sum of squares is associated with \(n-1\)
degrees of freedom.

Another perspective in which to think about the total sum of squares and
total degrees of freedom is to consider the intercept only model. The
intercept only model is a model that only includes the intercept term.
The equation of this model would be:

\[y_i = \beta_0 + \epsilon_i\] The sum of the square of the residuals
for this model is minimised when the predicted value of the response
variable is the mean of the response variable. That is, the least
squares estimate of \(\beta_0\) is the mean of the response variable:

\[\hat{\beta}_0 = \bar{y}\]

Hence, the predicted value of the response variable is the mean of the
response variable. The equation is:

\[\hat{y}_i = \bar{y} + \epsilon_i\]

The error term is therefore:

\[\epsilon_i = y_i - \bar{y}\] And the total sum of squares is:

\[SST = \sum_{i=1}^n (y_i - \bar{y})^2\]

where \(\hat{y}_i\) is the predicted value of the response variable for
the \(i\)th observation, \(\bar{y}\) is the mean of the response
variable, and \(\epsilon_i\) is the residual for the \(i\)th
observation.

The intercept only model involves estimating only one parameter, so the
total degrees of freedom are the total number of observations minus one
\(n - 1\).

Therefore, the total degrees of freedom are the total number of
observations minus one.

Bottom line: \(SST\) is the residual sum of squares when we fit the
intercept only model. The total degrees of freedom are the total number
of observations minus one.

\subsubsection*{Model degrees of
freedom}\label{model-degrees-of-freedom}
\addcontentsline{toc}{subsubsection}{Model degrees of freedom}

The model degrees of freedom are the degrees of freedom associated with
the model sum of squares (\(SSM\)).

In the case of the intercept only model, we estimated one parameter, the
mean of the response variable.

In the case of a categorical variable with \(k\) groups, we need \(k-1\)
parameters (non intercept \(\beta\) parameters), so we lose \(k-1\)
degrees of freedom. Put another way, when we fit a model with a
categorical explanatory variable with \(k\) groups, we estimate \(k-1\)
parameters in addition to the intercept. That is, we estimate the
difference between each group and the reference group.

Each time we estimate a new parameter, we lose a degree of freedom.

\subsubsection*{Residual degrees of
freedom}\label{residual-degrees-of-freedom}
\addcontentsline{toc}{subsubsection}{Residual degrees of freedom}

The residual degrees of freedom are the total degrees of freedom
(\(n-1\)) minus the model degrees of freedom (\(k-1\)).

Therefore, the residual degrees of freedom are the degrees of freedom
remaining after we estimate the intercept and the other \(\beta\)
parameters. There is one intercept and \(k-1\) other \(\beta\)
parameters, so the residual degrees of freedom are
\(n-1- ( k-1) = n - k\).

\subsection*{\texorpdfstring{Calculating the mean square and \(F\)
statistic}{Calculating the mean square and F statistic}}\label{calculating-the-mean-square-and-f-statistic}
\addcontentsline{toc}{subsection}{Calculating the mean square and \(F\)
statistic}

From these sums of squares and degrees of freedom we can calculate the
mean squares and \(F\)-statistic:

\[MS_{model} = \frac{SS_{\text{between}}}{k-1} = \frac{SSM}{k-1}\]

\[MS_{residual} = \frac{SS_{\text{within}}}{n-k} = \frac{SSE}{n-k}\]

\[F = \frac{MS_{model}}{MS_{residual}}\]

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Why divide by the degrees of freedom?} The more observations we
have, the greater will be the total sum of squares. The more
observations we have, the greater will be the residual sum of squares.
So it is not very informative to compare totals. Rather, we need to
compare the mean of the sums of squares. Except we don't calculate the
mean by dividing by the number of observations. Rather we divide by the
degrees of freedom. The total mean square is an estimate of the variance
of the response variable. And the residual mean square is an estimate of
the variance of the residuals.

\end{tcolorbox}

\subsection*{\texorpdfstring{\(SST\), \(SSM\), \(SSE\), and degrees of
freedom}{SST, SSM, SSE, and degrees of freedom}}\label{sst-ssm-sse-and-degrees-of-freedom}
\addcontentsline{toc}{subsection}{\(SST\), \(SSM\), \(SSE\), and degrees
of freedom}

Just a reminder and a summary of some of the material above:

\begin{itemize}
\tightlist
\item
  \(SST\): degrees of freedom = \(n-1\)
\item
  \(SSM\): degrees of freedom = \(k-1\)
\item
  \(SSE\): degrees of freedom = \(n-k\)
\end{itemize}

The sum of squares add up:

\[SST = SSM + SSE\]

and the degrees of freedom add up

\[(n-1) = (k-1) + (n - k)\]

\subsection*{Source of variance table}\label{source-of-variance-table}
\addcontentsline{toc}{subsection}{Source of variance table}

Now we have nearly everything we need. We often express all of this (and
a few more quantities) in a convenient table called the \textbf{sources
of variance table} (or ANOVA table).

The \textbf{sources of variance table} is a table that conveniently and
clearly gives all of the quantities mentioned above. It breaks down the
total sum of squares into the sum of squares explained by the model and
the sum of squares due to error. The source of variance table is used to
calculate the \(F\)-statistic.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.0702}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1404}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.1754}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3070}}
  >{\raggedright\arraybackslash}p{(\linewidth - 8\tabcolsep) * \real{0.3070}}@{}}
\caption{Sources of variance table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F-statistic
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F-statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Model & \(SSM\) & \(k-1\) & \(MSE_{model} = SSM / k-1\) &
\(\frac{MSE_{model}}{MSE_{error}}\) \\
Error & \(SSE\) & \(n - 1 - (k-1)\) &
\(MSE_{error} = SSE / (n - 1 - (k-1))\) & \\
Total & \(SST\) & \(n - 1\) & & \\
\end{longtable}

\subsection*{\texorpdfstring{Is my \(F\)-statistic large or
small?}{Is my F-statistic large or small?}}\label{is-my-f-statistic-large-or-small}
\addcontentsline{toc}{subsection}{Is my \(F\)-statistic large or small?}

OK, so we have calculated the \(F\) statistic. But how do we use it to
test our hypothesis?

We can use the \(F\) statistic to calculate a \(p\)-value, which tells
us how likely our data is under the null hypothesis.

Some key points:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(F\)-Distribution: The test statistic of the \(F\)-test (that is, the
  \(F\)-statistic) follows the \(F\)-distribution under the null
  hypothesis. This distribution arises when comparing the ratio of two
  independent sample variances (or mean squares).
\item
  Ronald Fisher's Contribution: Fisher introduced the \(F\)-distribution
  in the early 20th century as a way to test hypotheses about the
  equality of variances and to analyze variance in regression and
  experimental designs. The ``\(F\)'' in \(F\)-distribution honours him.
\item
  Variance Ratio: The test statistic for the \(F\)-test is the ratio of
  two variances (termed mean squares in this case), making the
  \(F\)-distribution the natural choice for modeling this ratio when the
  null hypothesis is true.
\end{enumerate}

The \(F\)-test is widely used, including when comparing variances,
assessing the significance of multiple regression models (see later
chapter), conducting ANOVA to test for differences among group means,
and for comparing different models.

Recall that ``The \(F\)-statistic is calculated as the ratio of the mean
square error of the model to the mean square error of the residuals.''
And that a large \(F\)-statistic is evidence against the null hypothesis
that the slopes of the explanatory variables are zero. And that a small
\(F\)-statistic is evidence to not reject the null hypothesis that the
slopes of the explanatory variables are zero.

But how big does the F-statistic need to be in order to confidently
reject the null hypothesis?

The null hypothesis that the explained variance of the model is no
greater than would be expected by chance. Here, ``by chance'' means that
the slopes of the explanatory variables are zero.

\[H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0\]

The alternative hypothesis is that the explained variance of the model
is greater than would be expected by chance. This would occur if the
slopes of some or all of the explanatory variables are not zero.

\[H_1: \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{ or } \ldots \text{ or } \beta_p \neq 0\]

To test this hypothesis we are going to, as usual, calculate a
\(p\)-value. The \(p\)-value is the probability of observing a test
statistic as or more extreme as the one we observed, assuming the null
hypothesis is true. To do this, we need to know the distribution of the
test statistic under the null hypothesis. The distribution of the test
statistic under the null hypothesis is known as the \(F\)-distribution.

The \(F\)-distribution has two degrees of freedom values associated with
it: the degrees of freedom of the model and the degrees of freedom of
the residuals. The degrees of freedom of the model are the number of
parameters estimated by the model corresponding to the null hypothesis.
The degrees of freedom of the residuals are the total degrees of freedom
minus the degrees of freedom of the model.

Here is the \(F\)-distribution with 2 and 99 degrees of freedom:

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-15-1.pdf}}

The F-distribution is skewed to the right and has a long tail. The area
to the right of 3.89 is shaded in red. This area represents the
probability of observing an F-statistic as or more extreme as 3.89,
assuming the null hypothesis is true. This probability is the
\(p\)-value of the hypothesis test.

The \(F\)-statistic and \(F\)-test is briefly recaptured in 3.1.f) of
the Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact
that

\[\frac{MSE_{model}}{MSE_{residual}} =  \frac{SSM/p}{SSE/(n-1-p)} \sim F_{p,n-1-p}\]

follows an \(F\)-distribution with \(p\) and \((n-1-p)\) degrees of
freedom, where \(p\) are the number of continuous variables, \(n\) the
number of data points.

\begin{itemize}
\tightlist
\item
  \(SSE=\sum_{i=1} ^n(y_i-\hat{y}_i)^2\) is the residual sum of squares
\item
  \(SSM = SST - SSE\) is the sum of squares of the model
\item
  \(SST=\sum_{i=1}^n(y_i-\overline{y})^2\) is the total sum of squares
\item
  \(n\) is the number of data points
\item
  \(p\) is the number of explanatory variables in the regression model
\end{itemize}

Well, that is ANOVA conceptually. But how does it actually look like in
R?

\section*{Doing ANOVA in R}\label{doing-anova-in-r}
\addcontentsline{toc}{section}{Doing ANOVA in R}

\markright{Doing ANOVA in R}

Let's go back again the question of how diet effects blood pressure.
Here is the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(bp\_data\_diet, bp, diet, person\_ID)}
\FunctionTok{head}\NormalTok{(bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
     bp diet          person_ID
  <dbl> <chr>         <chr>    
1   120 meat heavy    person_1 
2    89 vegan         person_2 
3    86 vegetarian    person_3 
4   116 meat heavy    person_4 
5   115 Mediterranean person_5 
6   134 meat heavy    person_6 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood pressure by diet"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood pressure"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-17-1.pdf}}

And here is how we fit a linear model to this data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

\textbf{IMPORTANT}: Since ANOVA is a linear model, it is important to
check the assumptions of linear models before interpreting the results.
These are some of the same assumptions we checked for simple linear
regression, including: independence of errors, normality of residuals,
and homoscedasticity (constant variance of residuals).

As with linear regression, we check the assumptions are not too badly
broken by looking at model checking plots:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(fit, }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-19-1.pdf}}

Nothing looks too bad.

Now we can look at the ANOVA table:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
          Df Sum Sq Mean Sq F value    Pr(>F)    
diet       3 5274.2 1758.08  20.728 1.214e-08 ***
Residuals 46 3901.5   84.82                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The ANOVA table shows the sum of squares, degrees of freedom, mean
square, F value, and p-value for the model and residuals. As we know,
the \(F\) value (\(F\) statistics) is calculated as the mean square of
the model divided by the mean square of the residuals. The p-value is
calculated based on the F-distribution with the appropriate degrees of
freedom.

A suitable sentence to report our findings would be: ``Diet has a
significant effect on blood pressure
(\(F(2, 27) = 20.7, p < 0.0001\))''. This means that the probability of
observing such a large \(F\) value under the null hypothesis is less
than 0.01\%.

\section*{Difference between pairs of
groups}\label{difference-between-pairs-of-groups}
\addcontentsline{toc}{section}{Difference between pairs of groups}

\markright{Difference between pairs of groups}

``\emph{ANOVA does not tell you which groups differ.''}

Recall that the \(F\) test is a global test. It tests the null
hypothesis that all group means are equal. It does not tell us which
groups are different from each other. It just tells us that at least one
group mean is different. Sometimes researchers are interested in more
specific questions such as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  finding the actual group(s) that deviate(s) from the others.
\item
  in estimates of the pairwise differences.
\end{enumerate}

The summary table in R provides some of these comparison, specifically
it contains the estimates for \(\beta_1\), \(\beta_2\), \(\beta_3\)
(while the reference was set to \(\beta_0 = 0\)).

For example, here is the summary table for our diet data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(fit)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = bp ~ diet, data = bp_data_diet)

Residuals:
     Min       1Q   Median       3Q      Max 
-17.9375  -5.9174  -0.4286   5.2969  22.3750 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        122.625      2.302  53.260  < 2e-16 ***
dietMediterranean  -12.688      3.256  -3.897 0.000314 ***
dietvegan          -26.768      4.173  -6.414 6.92e-08 ***
dietvegetarian     -23.625      3.607  -6.549 4.33e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.21 on 46 degrees of freedom
Multiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 
F-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08
\end{verbatim}

In this table we have the intercept (\(\beta_0\)) and the three
\(\beta\) values for the diet groups: ``dietMeat'', ``dietVegetarian'',
and ``dietVegan''. The Estimate column shows the estimated coefficients
for each group. The intercept (\(\beta_0\)) represents the mean blood
pressure for the reference group (in this case, the ``meat'' diet
group). The other three coefficients represent \emph{the difference} in
mean blood pressure between each diet group and the reference group.

All well and good up to a point. But there are two issues with using the
results from this table:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The greater the number of individual tests, the more likely one will
  be significant just by chance. This is called the problem of multiple
  comparisons. Many test can result in a type-I error: rejecting the
  null hypothesis when it is actually true. The more tests one does, the
  more likely one is to make a type-I error.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The summary table does not provide all the possible pairwise
  comparisons. It does not, for example, provide the comparison between
  the ``vegan'' and the ``vegetarian'' group.
\end{enumerate}

Several methods to circumvent the problem of too many ``significant''
test results (type-I error) have been proposed. The most prominent ones
are:

\begin{itemize}
\tightlist
\item
  Bonferroni correction
\item
  Tukey \textbf{H}onest \textbf{S}ignificant \textbf{D}ifferences (HSD)
  approach
\item
  Fisher \textbf{L}east \textbf{S}ignificant \textbf{D}ifferences (LSD)
  approach
\end{itemize}

The second two when implemented in R also provide all possible pairwise
comparisons.

\subsection*{Bonferroni correction}\label{bonferroni-correction}
\addcontentsline{toc}{subsection}{Bonferroni correction}

\textbf{Idea:} If a total of \(m\) tests are carried out, simply divide
the type-I error level \(\alpha_0\) (often 5\%) such that

\[\alpha = \alpha_0 / m \ .\]

But this still leaves the problem of how to efficiently get all of the
possible pairwise comparisons. We can do this using the
\texttt{pairwise.t.test} function in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairwise.t.test}\NormalTok{(bp\_data\_diet}\SpecialCharTok{$}\NormalTok{bp,}
\NormalTok{                bp\_data\_diet}\SpecialCharTok{$}\NormalTok{diet,}
                \AttributeTok{p.adjust.method =} \StringTok{"bonferroni"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pairwise comparisons using t tests with pooled SD 

data:  bp_data_diet$bp and bp_data_diet$diet 

              meat heavy Mediterranean vegan 
Mediterranean 0.0019     -             -     
vegan         4.2e-07    0.0091        -     
vegetarian    2.6e-07    0.0239        1.0000

P value adjustment method: bonferroni 
\end{verbatim}

Here we can see that all pairwise comparisons have a p-value less than
0.05, except for the comparison of vegan versus vegetarian, which has a
p-value that rounds to 1.0000.

We also see in the output the note that ``P value adjustment method:
bonferroni'', indicating that the Bonferroni correction has been applied
to the p-values.

\subsection*{Tukey HSD approach}\label{tukey-hsd-approach}
\addcontentsline{toc}{subsection}{Tukey HSD approach}

\textbf{Idea:} Take into account the distribution of \emph{ranges}
(max-min) and design a new test.

In R we can use the \texttt{multcomp} package to do Tukey HSD tests:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}}\NormalTok{ bp\_data\_diet }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diet =} \FunctionTok{as.factor}\NormalTok{(diet))}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\FunctionTok{suppressMessages}\NormalTok{(}\FunctionTok{library}\NormalTok{(multcomp))}
\NormalTok{tukey\_test }\OtherTok{\textless{}{-}} \FunctionTok{glht}\NormalTok{(fit, }\AttributeTok{linfct =} \FunctionTok{mcp}\NormalTok{(}\AttributeTok{diet =} \StringTok{"Tukey"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(tukey\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lm(formula = bp ~ diet, data = bp_data_diet)

Linear Hypotheses:
                                Estimate Std. Error t value Pr(>|t|)    
Mediterranean - meat heavy == 0  -12.688      3.256  -3.897  0.00168 ** 
vegan - meat heavy == 0          -26.768      4.173  -6.414  < 0.001 ***
vegetarian - meat heavy == 0     -23.625      3.607  -6.549  < 0.001 ***
vegan - Mediterranean == 0       -14.080      4.173  -3.374  0.00759 ** 
vegetarian - Mediterranean == 0  -10.938      3.607  -3.032  0.01951 *  
vegetarian - vegan == 0            3.143      4.453   0.706  0.89305    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- single-step method)
\end{verbatim}

We get all the pairwise comparisons, along with their estimates,
standard errors, t-values, and p-values. We also get a note
\texttt{Adjusted\ p\ values\ reported\ -\/-\ single-step\ method},
indicating that the Tukey HSD adjustment has been applied to the
p-values.

Again, all pairwise comparisons have a p-value less than 0.05, except
for the comparison of vegan versus vegetarian, which has a p-value of
0.89305.

\subsection*{Fisher's LSD approach}\label{fishers-lsd-approach}
\addcontentsline{toc}{subsection}{Fisher's LSD approach}

\textbf{Idea:} Adjust the idea of a two-sample test, but use a larger
variance (namely the pooled variance of all groups).

\subsection*{Other contrasts}\label{other-contrasts}
\addcontentsline{toc}{subsection}{Other contrasts}

A contrast is a specific comparison between groups. So far we have only
considered pairwise contrasts (i.e., comparing two groups at a time).
But we can also design more complex contrasts. For example: are diets
that contain meat different from diets that do not contain meat?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(bp\_data\_diet,}
                       \AttributeTok{meat\_or\_no\_meat =} \FunctionTok{ifelse}\NormalTok{(diet }\SpecialCharTok{==} \StringTok{"meat"} \SpecialCharTok{|}
\NormalTok{                                                diet }\SpecialCharTok{==} \StringTok{"Mediterranean"}\NormalTok{,}
                                                \StringTok{"meat"}\NormalTok{, }\StringTok{"no meat"}\NormalTok{))}
\FunctionTok{head}\NormalTok{(bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
     bp diet          person_ID meat_or_no_meat
  <dbl> <fct>         <chr>     <chr>          
1   120 meat heavy    person_1  no meat        
2    89 vegan         person_2  no meat        
3    86 vegetarian    person_3  no meat        
4   116 meat heavy    person_4  no meat        
5   115 Mediterranean person_5  meat           
6   134 meat heavy    person_6  no meat        
\end{verbatim}

Here we defined a new explanatory variable that groups the meat heavy
and Mediterranean diet together into a single ``meat'' group and
vegetarian and vegan into a single ``no meat'' group. We then fit a
model with this explanatory variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_mnm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ meat\_or\_no\_meat, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

(We should not look at model checking plots here, before using the
model. But let us continue as if the assumptions are sufficiently met.)

We now do something a bit more complicated: we compare the variance
explained by the model with four diets to the model with two diets. This
is done by comparing the two models using an \(F\)-test. We are testing
the null hypothesis that the two models are equally good at explaining
the data, in which case the two diet model will explain as much variance
as the four diet model.

Let's look at the ANOVA table of the model comparison:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(fit, fit\_mnm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: bp ~ diet
Model 2: bp ~ meat_or_no_meat
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     46 3901.5                                  
2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see the residual sum of squares of the model with meat or no meat is
over 9'000, while that of the four diet model is less than 4'000. That
is, the four diet model explains much more variance in the data than the
two diet model. The \(F\)-test is highly significant, so we reject the
null hypothesis that the two models are equally good at explaining the
data. And we conclude that its not just whether people eat meat or not,
but rather what kind of diet they eat that affects their blood pressure.

Ideally we do not make a lot of contrasts after we have collected and
looked at our data. Rather, we would specify the contrasts we are
interested in before we collect the data. This is called a priori
contrasts. But sometimes we do exploratory data analysis and then we can
make post hoc contrasts. In this case we should be careful to adjust for
multiple comparisons.

\subsection*{Choosing the reference
category}\label{choosing-the-reference-category}
\addcontentsline{toc}{subsection}{Choosing the reference category}

\textbf{Question}: Why was the ``heavy meat'' diet chosen as the
reference (intercept) category?

\textbf{Answer}: Because R orders the categories alphabetically and
takes the first level alphabetically as reference category.

Sometimes we may want to override this, for example if we have a
treatment that is experimentally the control, then it will usually be
useful to set this as the reference / intercept level.

In R we can set the reference level using the \texttt{relevel} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet}\SpecialCharTok{$}\NormalTok{diet }\OtherTok{\textless{}{-}} \FunctionTok{relevel}\NormalTok{(}\FunctionTok{factor}\NormalTok{(bp\_data\_diet}\SpecialCharTok{$}\NormalTok{diet), }\AttributeTok{ref =} \StringTok{"vegan"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And now make the model and look at the estimated coefficients:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_vegan\_ref }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\FunctionTok{summary}\NormalTok{(fit\_vegan\_ref)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = bp ~ diet, data = bp_data_diet)

Residuals:
     Min       1Q   Median       3Q      Max 
-17.9375  -5.9174  -0.4286   5.2969  22.3750 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)         95.857      3.481  27.538  < 2e-16 ***
dietmeat heavy      26.768      4.173   6.414 6.92e-08 ***
dietMediterranean   14.080      4.173   3.374  0.00151 ** 
dietvegetarian       3.143      4.453   0.706  0.48386    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.21 on 46 degrees of freedom
Multiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 
F-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08
\end{verbatim}

Now we see the estimated coefficients for all diets except the vegan
diet. The intercept is the mean individuals with vegan diet.

\section*{Communicating the results of
ANOVA}\label{communicating-the-results-of-anova}
\addcontentsline{toc}{section}{Communicating the results of ANOVA}

\markright{Communicating the results of ANOVA}

When communicating the results of an ANOVA, we usually report the
\(F\)-statistic, the degrees of freedom of the numerator and
denominator, and the p-value. For example, we could say:

\begin{quote}
Blood pressure differed significantly between groups, with the mean of a
meat heavy diet being 123 mmHg, while the mean blood pressure of the
vegan group was 27 mmHg lower (One-way ANOVA, \(F(3, 46) = 20.7\),
\(p < 0.0001\).
\end{quote}

And we would make a nice graph, in this case showing each individual
observation since there are not too many to cause overplotting. We can
also add the estimated means of each group if we like:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure by Diet"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood Pressure (mmHg)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-29-1.pdf}}

Some people like to see error bars as well, for example showing the 95\%
confidence intervals of the means:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun.data =}\NormalTok{ mean\_cl\_normal, }\AttributeTok{geom =} \StringTok{"errorbar"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure by Diet}\SpecialCharTok{\textbackslash{}n}\StringTok{Black points and error bars show mean Â± 95\% CI"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood Pressure (mmHg)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{5.1-anova_files/figure-pdf/unnamed-chunk-30-1.pdf}}

There are many many plotting styles and preferences. The important thing
is to clearly communicate the results, and to not mislead the reader. I
find that plotting the individual data points is often a good idea,
especially when the sample size is not too large.

\section*{Review}\label{review-3}
\addcontentsline{toc}{section}{Review}

\markright{Review}

Here are the key points from this chapter:

\begin{itemize}
\tightlist
\item
  ANOVA is just another linear model.
\item
  It is used when we have categorical explanatory variables.
\item
  We use \(F\)-tests to test the null hypothesis of no difference among
  the means of the groups (categories).
\item
  We can use contrasts and post-hoc tests to test specific hypotheses
  about the means of the groups.
\end{itemize}

\section*{Further reading}\label{further-reading-4}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

If you'd like some further reading on ANOVA and ANOVA in R, here are
some good resources. There is lots of overlap among them, however, and
with the material in this chapter. I suggest to look at them mostly for
different perspectives and examples. And only if you'd like to solidify
your understanding.

\begin{itemize}
\tightlist
\item
  Chapter 5, Section 5.6, of \emph{Getting Started with R} by Beckerman
  et al.~This section is about 10 pages.
\item
  Chapter 11 of \emph{The New Statistics with R} by Hector, not
  including the section on two-way ANOVA. This chapter is about 10
  pages.
\item
  Chapter 14 of \emph{Linear Models with R} by Faraway. This chapter is
  eight pages.
\item
  Chapter 9 of \textbf{Statistics. An Introduction using R} by Crawley,
  up to the section \emph{Factorial experiments}. This section is about
  16 pages.
\end{itemize}

\section*{Extras}\label{extras-3}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{ANOVA and experimental
design}\label{anova-and-experimental-design}
\addcontentsline{toc}{subsection}{ANOVA and experimental design}

This is not a course about experimental design (this is much too large a
subject to cover). You may have noticed, however, that all of the
example datasets and questions in your reading about ANOVA have been
experiments, where researchers had various treatment groups containing
the experimental subjects. That is, the researchers manipulated
something to be different among the groups, and looked for any evidence
of resulting differences.

This might lead you to wonder if we analyse experiments with ANOVA, and
analyse observations (involving no manipulations) with regression. The
answer is no, we can use either for either. It just happens that
manipulations in experiments are often of the categorical nature: type
of food, parasite species, light level (high or low), and so on. Of
course some experiments contain manipulations that are continuous
(e.g.~a range of temperatures or drug concentrations) and these are
analysed with regression. And some studies include a manipulation (e.g.,
drug concentration) and observed variables like gender.

So there isn't necessarily and association between type of explanatory
variable (continuous or categorical) and whether an study is
experimental or observational. Important, no essential, is for the
researcher to clearly be aware of which variables are manipulation and
which are observations. Because only the manipulations can be used to
infer causation.

\bookmarksetup{startatroot}

\chapter*{Multiple regression (L6)}\label{multiple-regression-l6}
\addcontentsline{toc}{chapter}{Multiple regression (L6)}

\markboth{Multiple regression (L6)}{Multiple regression (L6)}

\section*{Introduction}\label{introduction-3}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In the previous chapters we covered simple linear regression and one-way
analysis of variance. In both we had one response variable and one
explanatory variable. In both cases we made a linear model to relate the
response variable to the explanatory variable. The two cases differed in
the type of explanatory variable. In the case of simple linear
regression, the explanatory variable was continuous. In the case of
one-way ANOVA, the explanatory variable was categorical.

We will now extend the linear model and analyses to cases with more than
one (i.e., multiple) explanatory variables. The explanatory variables
can be continuous or categorical, and can be a mixture of the two.

Some combinations of explanatory variables have special names:

\begin{itemize}
\tightlist
\item
  Multiple (more than one) continuous explanatory variables
  -\textgreater{} Multiple linear regression.
\item
  Two categorical explanatory variables -\textgreater{} Two-way ANOVA.
\item
  One continuous and one categorical explanatory variable
  -\textgreater{} Analysis of covariance (ANCOVA).
\end{itemize}

We will look at each of these, and start with multiple linear regression
which is usually shortened to just multiple regression.

\section*{Multiple regression}\label{multiple-regression}
\addcontentsline{toc}{section}{Multiple regression}

\markright{Multiple regression}

We previously looked at whether blood pressure is associated with age.
This is an important question, because blood pressure has many health
implications. However, blood pressure is not only associated with age,
but also with other factors, such as weight, height, and lifestyle. In
this chapter, we will look at how to investigate the association between
blood pressure and multiple explanatory variables.

When we have multiple explanatory variables, we are often interested in
questions such as:

\begin{itemize}
\tightlist
\item
  Question 1: As an ensemble (i.e., all together), are the explanatory
  variables ``useful''?
\item
  Question 2: Are each of the explanatory variables associated with the
  response?
\item
  Question 3: What proportion of variability is explained?
\item
  Question 4: Are some explanatory variables more important than others?
\end{itemize}

\subsection*{An example dataset}\label{an-example-dataset}
\addcontentsline{toc}{subsection}{An example dataset}

Blood pressure is again the response variable, with age and lifestyle as
two explanatory variables. Lifestyle is a continuous variable that is
the number of minutes of exercise per week.

Reading in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_multreg }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/bp\_data\_multreg.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 100 Columns: 3
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (3): age, mins_exercise, bp

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Here is a look at the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_data\_multreg)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
    age mins_exercise    bp
  <dbl>         <dbl> <dbl>
1    74            17   140
2    40           201    90
3    44           203    97
4    72           110   104
5    36           231    98
6    53            93    90
\end{verbatim}

Since there are three variables, we can make three different scatter
plots to visualise the relationships:

\textbf{1. Age vs blood pressure.} This is the graph of the response
variable (blood pressure) against one of the explanatory variables
(age). It looks like there is evidence of a positive relationship.

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-6-1.pdf}}

Here we see a positive relationship between age and blood pressure.
Blood pressure tends to increase with age.

\textbf{2. Minutes of exercise vs blood pressure.} This is a graph of
the response variable (blood pressure) against the other explanatory
variable (minutes of exercise). It looks like there is evidence of a
negative relationship.

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-7-1.pdf}}

Here we see a negative relationship between minutes of exercise and
blood pressure. Blood pressure tends to decrease with more minutes of
exercise.

\textbf{3. Age vs minutes of exercise.} This is a graph of the two
explanatory variables against each other. It looks like there is no
relationship.

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-8-1.pdf}}

And here we see no relationship between age and minutes of exercise. The
two explanatory variables appear to be independent.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

The lack of correlation between the two explanatory variables is very
important. If the two explanatory variables were correlated, we would
have a situation known as multicollinearity. Multicollinearity can
greatly complicate the interpretation of the results of a multiple
regression analysis. We will discuss multicollinearity later.

\end{tcolorbox}

\subsection*{The multiple linear regression
model}\label{the-multiple-linear-regression-model}
\addcontentsline{toc}{subsection}{The multiple linear regression model}

The multiple linear regression model is an extension of the simple
linear regression model. Recall the simple linear regression model is:

\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(y_i\) is the response variable
\item
  \(x_i\) is the explanatory variable
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the slope
\item
  \(\epsilon_i\) is the error term.
\end{itemize}

The multiple linear regression model with two explanatory variables is:

\[y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(x_i^{(1)}\) and \(x_i^{(2)}\) are the two explanatory variables
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the slope for the first explanatory variable
\item
  \(\beta_2\) is the slope for the second explanatory variable
\end{itemize}

Note that the intercept \(\beta_0\) is the value of the response
variable when all explanatory variables are zero. In this example, it
would be the blood pressure for someone that is 0 years old and does 0
minutes of exercise per week. This is not a particularly useful
scenario, but it is a necessary mathematical construct that helps us to
build the model.

We can extend the multiple regression model to have an arbitrary number
of explanatory variables:

\[y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} + \epsilon_i\]

Where:

\(x_i^{(1)}, x_i^{(2)}, \ldots, x_i^{(p)}\) are the \(p\) explanatory
variables and all else is as before.

or with summation notation:

\[y_i = \beta_0 + \sum_{j=1}^p \beta_j x_i^{(j)} + \epsilon_i\]

Just like in simple linear regression, we can estimate the parameters
\(\beta_0, \beta_1, \ldots, \beta_p\) using the method of least squares.
The least squares method minimizes the sum of the squared residuals:

\[\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2\]

where \(\hat{y}_i\) is the predicted value of the response variable for
the \(i\)th observation:

\[\hat{y}_i = \hat{\beta}_0 + \sum_{j=1}^p \hat{\beta}_j x_i^{(j)}\]
where:

\(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p\) are the
estimated parameters.

Here is a graph of the geometric representation of a multiple linear
regression model with two explanatory variables (please note that this
plot is best viewed in the HTML version of the book; in the PDF version,
it will appear as a static image):

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-9-1.pdf}}

Let's write the equation for the blood pressure data:

\[bp_i = \beta_0 + \beta_1 \cdot age_i + \beta_2 \cdot mins\_exercise_i + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(bp_i\) is the blood pressure for the \(i\)th observation
\item
  \(age_i\) is the age for the \(i\)th observation
\item
  \(mins\_exercise_i\) is the minutes of exercise for the \(i\)th
  observation
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the slope for age
\item
  \(\beta_2\) is the slope for minutes of exercise
\item
  \(\epsilon_i\) is the error term
\end{itemize}

and the error term is assumed to be normally distributed with mean 0 and
constant variance, just as was the case for simple linear regression:

\[\epsilon_i \sim N(0, \sigma^2)\]

\textbf{Seventh}, we know how to make predictions using the model, and
to make a prediction band.

\textbf{What we don't know} is how to answer the four questions already
mentioned above:

\begin{itemize}
\tightlist
\item
  Question 1: As an ensemble (i.e., all together), are the explanatory
  variables ``useful''?
\item
  Question 2: Are each of the explanatory variables associated with the
  response?
\item
  Question 3: What proportion of variability is explained?
\item
  Question 4: Are some explanatory variables more important than others?
\end{itemize}

Let's answer these questions using the blood pressure example.

\subsection*{Fitting the model}\label{fitting-the-model}
\addcontentsline{toc}{subsection}{Fitting the model}

We know how to estimate the parameters
\(\beta_0, \beta_1, \ldots, \beta_p\) using the method of least squares.

In R, we can fit a multiple linear regression model using the
\texttt{lm()} function in a very similar way to the simple linear
regression model. Here is the code for the blood pressure example. To
fit two explanatory variables, we simply add the second variable to the
formula using the \texttt{+} sign:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ mins\_exercise, }\AttributeTok{data =}\NormalTok{ bp\_data\_multreg)}
\end{Highlighting}
\end{Shaded}

\subsection*{Checking the assumptions}\label{checking-the-assumptions}
\addcontentsline{toc}{subsection}{Checking the assumptions}

Great news --\textgreater{} the five assumptions of the multiple linear
regression model are the same as for the simple linear regression model:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Normality of residuals.
\item
  Homoscedasticity = constant variance of residuals.
\item
  Independence of residuals.
\item
  Linearity.
\item
  No outliers.
\end{enumerate}

We can check the assumptions of the multiple linear regression model
using the same methods as for the simple linear regression model. Here
is the code for the blood pressure example:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Check the assumptions}
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(m1, }\AttributeTok{which =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{,}\DecValTok{2}\NormalTok{,}\DecValTok{3}\NormalTok{,}\DecValTok{5}\NormalTok{), }\AttributeTok{add.smooth =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-11-1.pdf}}

We see that the assumptions are met for the blood pressure example:

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Normality of residuals: The QQ-plot shows that the residuals are
  normally distributed.
\item
  Homoscedasticity: The scale-location plot shows that the residuals
  have constant variance.
\item
  Independence of residuals: No evidence of pattern or clustering. But
  also need to know about study design to properly assess independence.
\item
  Linearity: The residuals vs.~fitted values plot shows no clear pattern
  in the residuals.
\item
  No outliers: No points with high leverage or high residuals.
\end{enumerate}

\section*{Question 1: As an ensemble, are the explanatory variables
useful?}\label{question-1-as-an-ensemble-are-the-explanatory-variables-useful}
\addcontentsline{toc}{section}{Question 1: As an ensemble, are the
explanatory variables useful?}

\markright{Question 1: As an ensemble, are the explanatory variables
useful?}

Recall that when we learned about ANOVA we saw that a single categorical
explanatory variable with multiple levels can be represented as multiple
binary (0/1) explanatory variables. In that case, we used the \(F\)-test
to test the null hypothesis of no effect / relationship for all binary
variables together.

Likewise, when we have multiple continuous explanatory variables, we use
the \(F\)-test to test the null hypothesis that \textbf{together} the
explanatory variables have no association with the response variable.
That is, we use the \(F\)-test to test the null hypothesis that the
ensemble of explanatory variables is not associated with the response
variable.

This corresponds to the same null hypothesis as we used in one-way
ANOVA: The null hypothesis that the explained variance of the model is
no greater than would be expected by chance. Here, ``by chance'' means
that the slopes of the explanatory variables are zero:

\[H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0\]

And the alternative hypothesis (just as in one-way ANOVA) is that the
explained variance of the model is greater than would be expected by
chance. This would occur if the slopes of some or all of the explanatory
variables are not zero:

\[H_1: \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{ or } \ldots \text{ or } \beta_p \neq 0\]

Recall that the \(F\)-test compares the variance explained by the model
to the variance not explained by the model (i.e., the variance of the
residuals). If the variance explained by the model is significantly
greater than the variance not explained by the model, then we can
conclude that the explanatory variables are associated with the response
variable.

If we reject the null hypothesis, we can conclude that some combination
of the explanatory variables is associated with the response variable.
However, we cannot conclude which specific explanatory variables are
associated with the response variable. To determine which specific
explanatory variables are associated with the response variable, we need
to perform individual \(t\)-tests for each explanatory variable. We will
do this in the next section.

OK, back to the \(F\)-test.

We know a lot already from the ANOVA chapter. Let's review how we
calculate the \(F\)-statistic.

The \(F\)-statistic is calculated as the ratio of two mean squares:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The mean square of the model (\(MSE_{model}\)).
\item
  The mean square of the residuals (\(MSE_{residual}\)).
\end{enumerate}

Recall that a mean square is a sum of squares divided by the associated
degrees of freedom. The formulas for these are the same as for ANOVA.

So, to calculate these two mean squares, we need to calculate three sums
of squares:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The total sum of squares (\(SST\)).
\item
  The sum of squares of the model (\(SSM\)).
\item
  The sum of squares of the residuals (\(SSE\)).
\end{enumerate}

We also need to calculate the degrees of freedom associated with each
sum of squares.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The total degrees of freedom is \(n-1\), where \(n\) is the number of
  observations.
\item
  The model degrees of freedom is \(p\), where \(p\) is the number of
  explanatory variables. This is because for each explanatory variable
  we estimate one parameter (the slope), and each estimated parameter
  uses up one degree of freedom.
\item
  The residual degrees of freedom is \(n-1-p\).
\end{enumerate}

\subsection*{The F-statistic in R}\label{the-f-statistic-in-r}
\addcontentsline{toc}{subsection}{The F-statistic in R}

We could do all these calculations ourselves (and you might be asked to
in the exam), but also we can just ask R! For question 1 we need to know
the \(F\)-statistic for the multiple linear regression model. We can
easily get this from R using the \texttt{summary()} function, and by
looking in the right place in the output:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = bp ~ age + mins_exercise, data = bp_data_multreg)

Residuals:
    Min      1Q  Median      3Q     Max 
-32.096  -7.976  -0.891   6.877  37.785 

Coefficients:
              Estimate Std. Error t value Pr(>|t|)    
(Intercept)   87.18298    4.48538  19.437  < 2e-16 ***
age            0.54284    0.07195   7.545 2.46e-11 ***
mins_exercise -0.10296    0.01476  -6.978 3.71e-10 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 12.23 on 97 degrees of freedom
Multiple R-squared:  0.5148,    Adjusted R-squared:  0.5048 
F-statistic: 51.45 on 2 and 97 DF,  p-value: 5.863e-16
\end{verbatim}

In the final line of output we see ``F-statistic: 51.45 on 2 and 97 DF,
p-value: \ensuremath{5.86\times 10^{-16}}''.

The model degrees of freedom is 2 (because we have two explanatory
variables), and the residual degrees of freedom is 97 (because we have
100 observations and 2 explanatory variables, so \(100 - 1 - 2 = 97\)).

So the \texttt{summary()} function gives us everything we need to answer
question 1. It even gives us the \(p\)-value for the \(F\)-test.

\textbf{How to report the result}. We could write somthing like this:
``The combination of age and minutes of exercise is significantly
associated with blood pressure (F(2, 97) = 51.45, p =
\ensuremath{5.86\times 10^{-16}}).'' Note that this is rather an
undesirable statement, because it focuses too much on the statistics and
not enough on the science. Indeed, perhaps we care more about the
association of each explanatory variable with blood pressure, which we
will look at next.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

If you want to review how to calculate a p-value from an F-statistic,
see the corresponding section of the one-way ANOVA chapter.

\end{tcolorbox}

\section*{Question 2: Which variables are associated with the
response?}\label{question-2-which-variables-are-associated-with-the-response}
\addcontentsline{toc}{section}{Question 2: Which variables are
associated with the response?}

\markright{Question 2: Which variables are associated with the
response?}

As we did for simple linear regression, we can perform a \(t\)-test for
one explanatory variable to determine if it is associated with the
response. And we can do this for each of the explanatory variables. As
before, the null hypothesis for each \(t\)-test is that the slope of the
explanatory variable is zero. The alternative hypothesis is that the
slope of the explanatory variable is not zero.

Here is the coefficients table, which includes the results of the
\(t\)-tests for each explanatory variable:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{coef}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                Estimate Std. Error   t value     Pr(>|t|)
(Intercept)   87.1829818 4.48537574 19.437164 3.187511e-35
age            0.5428378 0.07194857  7.544803 2.456520e-11
mins_exercise -0.1029588 0.01475564 -6.977587 3.712390e-10
\end{verbatim}

And we can get the 95\% CI for each slope estimate \(\hat\beta_j\) as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                   2.5 %      97.5 %
(Intercept)   78.2807525 96.08521099
age            0.4000398  0.68563580
mins_exercise -0.1322446 -0.07367291
\end{verbatim}

Reminder: The 95\% confidence interval is
\([\hat\beta - c \cdot \sigma^{(\beta)} ; \hat\beta + c \cdot \sigma^{(\beta)}]\),
where \(c\) is the 97.5\% quantile of the \(t\)-distribution with
\(n-p\) degrees of freedom).

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{However} Please insert a note into your brain that we are
dealing here with an ideal case of \textbf{uncorrelated explanatory
variables}. You'll learn later in the course about what happens when
explanatory variables are correlated. Hint: interpretation is difficult
and unstable!

\end{tcolorbox}

\section*{Question 3: What proportion of variability is
explained?}\label{question-3-what-proportion-of-variability-is-explained}
\addcontentsline{toc}{section}{Question 3: What proportion of
variability is explained?}

\markright{Question 3: What proportion of variability is explained?}

\subsection*{\texorpdfstring{Multiple
\(R^2\)}{Multiple R\^{}2}}\label{multiple-r2}
\addcontentsline{toc}{subsection}{Multiple \(R^2\)}

We can calculate the \(R^2\) value for the multiple linear regression
model just like we already did for a simple linear regression model. The
\(R^2\) value is the proportion of variability in the response variable
that is explained by the model. As before, the \(R^2\) value ranges from
0 to 1, where 0 indicates that the model does not explain any
variability in the response variable, and 1 indicates that the model
explains all the variability in the response variable.

For multiple linear regression, we often use the term ``multiple
\(R^2\)'' to distinguish it from the \(R^2\) value for simple linear
regression. The multiple \(R^2\) is the proportion of variability in the
response variable that is explained by the model, taking into account
all the explanatory variables in the model.

As before, for simple linear regression, the multiple \(R^2\) value is
calculated as the sum of squares explained by the model divided by the
total sum of squares:

\[R^2 = \frac{SSM}{SST}\]

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sss }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(m1)}
\NormalTok{SSM }\OtherTok{\textless{}{-}}\NormalTok{ sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{[}\DecValTok{2}\NormalTok{]}
\NormalTok{SST }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{)}
\NormalTok{R\_squared }\OtherTok{\textless{}{-}}\NormalTok{ SSM }\SpecialCharTok{/}\NormalTok{ SST}
\CommentTok{\#R\_squared}
\end{Highlighting}
\end{Shaded}

where \(SSM\) is the sum of squares explained by the model and \(SST\)
is the total sum of squares, and \(SSM = SST - SSE\).

For the blood pressure data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5147784
\end{verbatim}

\(R^2\) for multiple linear regression can also be calculated as the
squared correlation between \((y_1,\ldots,y_n)\) and
\((\hat{y}_1,\ldots,\hat{y}_n)\), where the \(\hat y\) are the fitted
values from the model. The fitted values are calculated as:

\[\hat{y}_i = \hat\beta_0 + \hat\beta_1 x^{(1)} + \ldots + \hat\beta_m x^{(m)}\]

In R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{r\_squared }\OtherTok{\textless{}{-}} \FunctionTok{cor}\NormalTok{(m1}\SpecialCharTok{$}\NormalTok{fitted.values, bp\_data\_multreg}\SpecialCharTok{$}\NormalTok{bp)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{r\_squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5147784
\end{verbatim}

Or:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sss }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(m1)}
\NormalTok{SSM }\OtherTok{\textless{}{-}}\NormalTok{ sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{[}\DecValTok{2}\NormalTok{]}
\NormalTok{SST }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(sss}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Sum Sq}\StringTok{\textasciigrave{}}\NormalTok{)}
\NormalTok{R\_squared }\OtherTok{\textless{}{-}}\NormalTok{ SSM }\SpecialCharTok{/}\NormalTok{ SST}
\NormalTok{R\_squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5147784
\end{verbatim}

\subsection*{\texorpdfstring{Adjusted
\(R^2\)}{Adjusted R\^{}2}}\label{adjusted-r2}
\addcontentsline{toc}{subsection}{Adjusted \(R^2\)}

However, we have a little problem to address. The \(R^2\) value
increases as we add more explanatory variables to the model, even if the
additional variables are not associated with the response. This is
because the \(R^2\) value is calculated as the proportion of variability
in the response variable that is explained by the model. As we add more
explanatory variables to the model, the model will always explain more
variability in the response variable, even if the additional variables
are not associated with the response. Some of the variance will be
explained by chance.

Here is an example of this problem. First, here's the explanatory power
of the model with only age and minutes of exercise as the explanatory
variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ mins\_exercise, }\AttributeTok{data =}\NormalTok{ bp\_data\_multreg)}
\FunctionTok{summary}\NormalTok{(m1)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5147784
\end{verbatim}

Now, we can add a new explanatory variable to the blood pressure model
that is not associated with the response:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_multreg}\SpecialCharTok{$}\NormalTok{random\_variable }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(bp\_data\_multreg))}
\NormalTok{m2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age }\SpecialCharTok{+}\NormalTok{ mins\_exercise }\SpecialCharTok{+}\NormalTok{ random\_variable, }\AttributeTok{data =}\NormalTok{ bp\_data\_multreg)}
\FunctionTok{summary}\NormalTok{(m2)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5149697
\end{verbatim}

The \(R^2\) value for the model with the random variable is higher than
the \(R^2\) value for the model without the random variable. This is
because the model with the random variable explains more variability in
the response variable, even though the random variable is not associated
with the response.

To address this problem, we can use the adjusted \(R^2\) value. The
adjusted \(R^2\) value is calculated as:

\[R^2_{\text{adj}} = 1 - \frac{SSE / (n - p - 1)}{SST / (n - 1)}\]

where * \(SSE\) is the sum of squared errors * \(SST\) is the total sum
of squares * \(n\) is the number of observations * \(p\) is the number
of explanatory variables in the model.

Or put another way:

\[R^2_{adj} = 1-(1-R^2 )\frac{n-1}{n-p-1}\] In this form, we can see
that as \(p\) increases (as we add explanatory variables) the term
\((n-1)/(n-p-1)\) increases, and the adjusted \(R^2\) value will
decrease if the additional variables are not associated with the
response.

Take home: when we want to compare the explanatory power of models that
differ in the number of explanatory variables, we should use the
adjusted \(R^2\) value.

\section*{Question 4: Are some explanatory variables more important than
others?}\label{question-4-are-some-explanatory-variables-more-important-than-others}
\addcontentsline{toc}{section}{Question 4: Are some explanatory
variables more important than others?}

\markright{Question 4: Are some explanatory variables more important
than others?}

We know how to test the significance of the parameters using the
\(t\)-test. We also know how to calculate the confidence intervals for
the parameters, and to make a confidence band.

How important are the explanatory variables and how important are they
relative to each other?

The importance of an explanatory variable can be assessed by looking at
the size of the coefficient for that variable. The larger the
coefficient, the more important the variable is in explaining the
response variable.

It is, however, important to remember that the size of the coefficient
depends on the scale of the explanatory variable. If the explanatory
variables are on different scales, then the coefficients will be on
different scales and cannot be directly compared.

In our example, the age variable is measured in years, so the
coefficient is in units mmHg (pressure) per year. The mins\_exercise
variable is measured in minutes, so the coefficient is in units mmHg per
minute. The coefficients are on different scales and cannot be directly
compared. Furthermore, the value of the coefficients would change if we
measured age in months or minutes of exercise in hours.

There are other perspectives we can take when we're assessing
importance. For example, we cannot change our age, but we can change the
number of minutes of exercise. So, the practical importance of the two
variables is quite different in that sense also.

To compare the importance of the explanatory variables that are measured
on different scales, we can standardize the variables before fitting the
model. This means that we subtract the mean of the variable and divide
by the standard deviation. This puts all the variables on the same
scale, so the coefficients can be directly compared. The coefficients
are then in units of the response variable per standard deviation of the
explanatory variable.

However, the coefficients are then not in the original units of the
explanatory variables, so it is not always easy to interpret the
coefficients. So while we can compare the coefficients, they have lost a
bit of their original meaning and are not so easy to interpret.

One way to relate the coefficients in this case is to realise that to
compensate for the blood pressure increase associated with one year of
age, one would need to exercise for a certain number of minutes more.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{extra\_mins\_exercise }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(m1)[}\StringTok{"age"}\NormalTok{] }\SpecialCharTok{/} \SpecialCharTok{{-}}\FunctionTok{coef}\NormalTok{(m1)[}\StringTok{"mins\_exercise"}\NormalTok{]}
\CommentTok{\#extra\_mins\_exercise}
\end{Highlighting}
\end{Shaded}

\section*{Question 5: How do we make
predictions?}\label{question-5-how-do-we-make-predictions}
\addcontentsline{toc}{section}{Question 5: How do we make predictions?}

\markright{Question 5: How do we make predictions?}

We already made predictions from a simple linear regression model?

Recall that the equation for multiple linear regression is:

\[y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} + \epsilon_i\]

Therefore to get a predicted value of \(y_i\), we can use the estimated
parameters (\(\hat\beta_0, \hat\beta_1, \ldots, \hat\beta_p\)) and the
values of the explanatory variables
(\(x_i^{(1)}, x_i^{(2)}, \ldots, x_i^{(p)}\)):

\[\hat{y}_i = \hat\beta_0 + \hat\beta_1 x_i^{(1)} + \hat\beta_2 x_i^{(2)} + \ldots + \hat\beta_p x_i^{(p)}\]

In R, we can use the \texttt{predict()} function to make predictions
from a multiple linear regression model. Here is an example of how to
make predictions for the values of the explanatory variables in the
original dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

Where \texttt{m1} is the multiple linear regression model fitted
earlier. The \texttt{predict} function automatically uses the original
data because we did not provide any new data.

We can also make predictions for new values of the explanatory variables
by providing a new data frame to the \texttt{predict()} function. Here
is an example of how to make predictions for new values of age and
minutes of exercise:

First we need to make some new values of age and minutes of exercise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{c}\NormalTok{(}\DecValTok{30}\NormalTok{, }\DecValTok{40}\NormalTok{, }\DecValTok{50}\NormalTok{),}
                       \AttributeTok{mins\_exercise =} \FunctionTok{c}\NormalTok{(}\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{150}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Then we can use the \texttt{predict()} function to make predictions for
these new values:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1, }\AttributeTok{newdata =}\NormalTok{ new\_data)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

The new data frame must have the all the explanatory variables used in
the model, and the variable names must match exactly those used in the
model.

\end{tcolorbox}

We can take this to the next level and make what is called a
``conditional effects plot'' or ``effect plot''. This is a plot that
shows the predicted values of the response variable for different values
of one explanatory variable, while holding the other explanatory
variables constant. Here is an example of how to make a conditional
effects plot for age, while holding minutes of exercise constant at 100:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_data\_effects }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =} \FunctionTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\AttributeTok{by =} \DecValTok{1}\NormalTok{),}
                                \AttributeTok{mins\_exercise =} \DecValTok{100}\NormalTok{)}
\NormalTok{new\_data\_effects }\OtherTok{\textless{}{-}}\NormalTok{ new\_data\_effects }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{new\_predictions\_effects =} \FunctionTok{predict}\NormalTok{(m1, }\AttributeTok{newdata =}\NormalTok{ new\_data\_effects))}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ new\_data\_effects, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ new\_predictions\_effects)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Predicted blood pressure"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Conditional effects plot for}\SpecialCharTok{\textbackslash{}n}\StringTok{age (mins\_exercise = 100)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-26-1.pdf}}

And lets take this to the next level again and make a conditional
effects plot for three different levels of \texttt{mins\_exercise}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_data\_effects\_3 }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{age =} \FunctionTok{seq}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{, }\AttributeTok{by =} \DecValTok{1}\NormalTok{),}
                                   \AttributeTok{mins\_exercise =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{150}\NormalTok{, }\DecValTok{300}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

A new function! And it's one of Owen's favourites:
\texttt{expand.grid()}. This function creates a data frame from all
combinations of the supplied vectors or factors. Here, we are creating a
data frame with all combinations of age (from 20 to 80) and minutes of
exercise (0, 150, and 300).

We then give the new data frame to the \texttt{predict()} function to
get the predicted values for each combination of age and minutes of
exercise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_data\_effects\_3 }\OtherTok{\textless{}{-}}\NormalTok{ new\_data\_effects\_3 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{prediction =} \FunctionTok{predict}\NormalTok{(m1, }\AttributeTok{newdata =}\NormalTok{ new\_data\_effects\_3),}
         \AttributeTok{mins\_exercise\_fac =} \FunctionTok{as.factor}\NormalTok{(mins\_exercise)) }
\end{Highlighting}
\end{Shaded}

(Note that we changed)

And make a graph:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ new\_data\_effects\_3, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ prediction,}
                                             \AttributeTok{col =}\NormalTok{ mins\_exercise\_fac)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Age"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Predicted blood pressure"}\NormalTok{, }\AttributeTok{color =} \StringTok{"Minutes}
\StringTok{ of exercise"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Conditional effects plot for age}\SpecialCharTok{\textbackslash{}n}\StringTok{at three levels of minutes of exercise"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-29-1.pdf}}

\section*{Collinearity}\label{collinearity}
\addcontentsline{toc}{section}{Collinearity}

\markright{Collinearity}

In the blood pressure example data used previously in this chapter there
is no evidence of correlation between the two explanatory variables.
However, in practice, it is common for explanatory variables to be
correlated with each other. This is known as collinearity. It can be
quite problematic for us!

Collinearity, specifically \emph{harmful} collinearity, is extremely
common in real dataset that result from observational studies. This is
because in observational studies there are often numerous explanatory
variables, and they are often correlated with each other. This is a
situation that is ripe for collinearity problems. (Collinearity can also
happen in data resulting from designed manipulative experiments, but is
hopefully relatively rare there because a well-designed experiment will
try to avoid collinearity by ensuring that the explanatory variables are
independent.)

So what is collinearity, and why is it a problem?

\textbf{\emph{Put simply, collinearity is when one explanatory variable
is predictable from a linear combination of others.}}

This can happen due to strong correlation among pairs of explanatory
variables, or due to more complex relationships involving three or more
explanatory variables.

For example, if we have three explanatory variables, \(x_1, x_2,\) and
\(x_3\), and if \(x_3\) can be predicted from a linear combination of
\(x_1\) and \(x_2\), then we have collinearity. For example, if:

\[x_3 = 2 \cdot x_1 + 3 \cdot x_2 + \text{small random noise}\]

In this case, variable \(x_3\) is a linear combination of \(x_1\) and
\(x_2\), plus some small random noise. This means that if we know the
values of \(x_1\) and \(x_2\), we can predict the value of \(x_3\) quite
accurately. Also, in this case we might not have strong correlation
between any pair of the explanatory variables, but there is still
collinearity because \(x_3\) is predictable from \(x_1\) and \(x_2\). So
lack of correlation between pairs of explanatory variables does not
guarantee that there is no collinearity.

\textbf{\emph{It is a problem because it makes the slope estimates
unstable and therefore difficult to interpret.}}

Let us see this instability in practice. First let's look at the really
extreme example of perfect collinearity. Here's a new version of the
blood pressure data in which the minutes of exercise variable is
perfectly predicted from age:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{age }\OtherTok{\textless{}{-}} \FunctionTok{ceiling}\NormalTok{(}\FunctionTok{runif}\NormalTok{(n, }\DecValTok{20}\NormalTok{, }\DecValTok{80}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Here is the line of code that makes the perfect correlation between age
and minutes of exercise:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mins\_exercise }\OtherTok{\textless{}{-}} \DecValTok{100} \SpecialCharTok{{-}}\NormalTok{ age}
\end{Highlighting}
\end{Shaded}

Now we generate the blood pressure variable as before:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp }\OtherTok{\textless{}{-}} \DecValTok{100} \SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ age }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{15}\NormalTok{)}
\NormalTok{bp\_data\_perfect }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{age =}\NormalTok{ age,}
                              \AttributeTok{mins\_exercise =}\NormalTok{ mins\_exercise,}
                              \AttributeTok{bp =}\NormalTok{ bp)}
\end{Highlighting}
\end{Shaded}

Read in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_perfect }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/bp\_data\_perfect.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 100 Columns: 3
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (3): age, mins_exercise, bp

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Now we fit a multiple linear regression model with only age:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1\_age }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age, }\AttributeTok{data =}\NormalTok{ bp\_data\_perfect)}
\FunctionTok{summary}\NormalTok{(m1\_age)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = bp ~ age, data = bp_data_perfect)

Residuals:
    Min      1Q  Median      3Q     Max 
-33.546  -9.147  -0.327   8.938  33.213 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 100.43331    4.54751  22.085  < 2e-16 ***
age           0.47541    0.08549   5.561 2.33e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 14.54 on 98 degrees of freedom
Multiple R-squared:  0.2399,    Adjusted R-squared:  0.2321 
F-statistic: 30.92 on 1 and 98 DF,  p-value: 2.325e-07
\end{verbatim}

The estimated slope (coefficient) is 0.48, which is close to the true
value of 0.5. The 95\% confidence interval is 0.31 to 0.65, which
includes the true value of 0.5. All good then.

Now we fit the multiple linear regression model with both age and
minutes of exercise included:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1\_both }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mins\_exercise }\SpecialCharTok{+}\NormalTok{ age, }\AttributeTok{data =}\NormalTok{ bp\_data\_perfect)}
\FunctionTok{summary}\NormalTok{(m1\_both)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = bp ~ mins_exercise + age, data = bp_data_perfect)

Residuals:
    Min      1Q  Median      3Q     Max 
-33.546  -9.147  -0.327   8.938  33.213 

Coefficients: (1 not defined because of singularities)
               Estimate Std. Error t value Pr(>|t|)    
(Intercept)   147.97398    4.48276  33.010  < 2e-16 ***
mins_exercise  -0.47541    0.08549  -5.561 2.33e-07 ***
age                  NA         NA      NA       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 14.54 on 98 degrees of freedom
Multiple R-squared:  0.2399,    Adjusted R-squared:  0.2321 
F-statistic: 30.92 on 1 and 98 DF,  p-value: 2.325e-07
\end{verbatim}

This output is a bit strange. The estimate for age is now \texttt{NA}.
This is because the model cannot distinguish between the effects of age
and minutes of exercise, since they are perfectly correlated. The model
is unable to estimate the coefficient for age of exercise, so it returns
\texttt{NA}.

This is an example of instability of coefficients due to collinearity.
The coefficient for age is completely unstable, as it changes from a
number to \texttt{NA} depending on whether minutes of exercise is
included in the model or not.

Actually, you can see that the estimate for \texttt{mins\_exercise} are
the same (except the sign of the coefficient) as the estimate for age in
the previous model. This is because minutes of exercise is perfectly
correlated with age, so the model is essentially using minutes of
exercise as a proxy for age. With perfect collinearity, the model cannot
distinguish between the effects of the two variables\ldots{} they are
effectively identical.

You can also see that the \(R^2\) value doesn't change when the second
variable is added. That is, the model with only age included is
identical to the model with both age and minutes of exercise included.
This is because minutes of exercise is perfectly correlated age, so
including minutes of exercise in the model does not add any new
information.

That is a pretty extreme example of perfect collinearity. In practice,
collinearity is often not perfect, but still strong enough to cause
problems.

Let's look at the less extreme example of collinearity we had earlier,
with three explanatory variables, \(x_1, x_2,\) and \(x_3\), and where
\(x_3\) can be predicted from a linear combination of \(x_1\) and
\(x_2\) plus some random noise:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x3 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{5}\NormalTok{)}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \FloatTok{1.5} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x3 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{data\_collinear }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x1 =}\NormalTok{ x1, }\AttributeTok{x2 =}\NormalTok{ x2, }\AttributeTok{x3 =}\NormalTok{ x3, }\AttributeTok{y =}\NormalTok{ y)}
\FunctionTok{cor}\NormalTok{(data\_collinear)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            x1          x2        x3          y
x1  1.00000000 -0.04953215 0.1877534  0.6079515
x2 -0.04953215  1.00000000 0.5194100 -0.1488812
x3  0.18775342  0.51940999 1.0000000  0.6434430
y   0.60795153 -0.14888117 0.6434430  1.0000000
\end{verbatim}

We see that there is not strong correlation between any pair of the
explanatory variables, but there is still collinearity because \(x_3\)
is predictable from \(x_1\) and \(x_2\).

Let's look for evidence of instability of the coefficients. First, we
fit the multiple linear regression model with all three explanatory
variables included:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_collinear\_123 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ data\_collinear)}
\FunctionTok{summary}\NormalTok{(m\_collinear\_123)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Estimate Std. Error   t value     Pr(>|t|)
(Intercept)  4.9806739 0.10734366  46.39933 1.490805e-67
x1           1.4675042 0.11972274  12.25752 2.384482e-21
x2          -1.9193496 0.12990373 -14.77517 1.832330e-26
x3           0.4885226 0.02244616  21.76419 6.723265e-39
\end{verbatim}

Do these change much if we fit the model with only \(x_1\) and \(x_2\)
included?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_collinear\_12 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2, }\AttributeTok{data =}\NormalTok{ data\_collinear)}
\FunctionTok{summary}\NormalTok{(m\_collinear\_12)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              Estimate Std. Error   t value     Pr(>|t|)
(Intercept)  5.3105865  0.2575325 20.621034 3.113931e-37
x1           2.1192629  0.2809162  7.544111 2.464756e-11
x2          -0.3956201  0.2651792 -1.491897 1.389714e-01
\end{verbatim}

Yes, they do change quite a bit. The coefficients for \(x_1\) and
\(x_2\) are quite different when \(x_3\) is included in the model
compared to when it is not included. This is because \(x_3\) is
predictable from \(x_1\) and \(x_2\), so including \(x_3\) in the model
changes the interpretation of the coefficients for \(x_1\) and \(x_2\).

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

In a not so extreme case such as this one, the coefficients will not be
completely unstable (i.e., they will not change from a number to NA) but
they can still change quite a bit depending on which other collinear
variables are included in the model.

\end{tcolorbox}

The bottom line is that collinearity between explanatory variables
complicates the interpretation of the model coefficients. If there is
collinearity/correlation between the explanatory variables, then the
model coefficients can be unstable and difficult to interpret.

Let's have one more look at this instability that is caused by
collinearity. And make this demonstration a bit more general. We'll
simulate data with two explanatory variables that are correlated with
each other to varying degrees. We'll then fit multiple linear regression
models with both explanatory variables included, and see how the
stability of the coefficients changes as we change the correlation
between the explanatory variables.

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-40-1.pdf}}

In the left panel we see the slope estimates for \(x_1\) from the
multiple linear regression model with both \(x_1\) and \(x_2\) included,
for different levels of correlation between \(x_1\) and \(x_2\). When
there is no correlation between \(x_1\) and \(x_2\) (i.e., no
collinearity), the slope estimates are quite stable and close to the
true value of 1.

As the correlation between \(x_1\) and \(x_2\) increases, the slope
estimates become more variable (= less stable). At correlation of 0.9,
the slope estimates are quite variable and can be very far from the true
value of 1.

The right panel shows the variance of the slope estimates as a function
of the correlation between \(x_1\) and \(x_2\). Variance here is a
measure of the amount of vertical spread in the left panel.

We see that the variance of the slope estimates increases as the
correlation between \(x_1\) and \(x_2\) increases. This shows that
greater collinearity between explanatory variables leads to greater
instability (variance) of the slope estimates.

This increase in variance caused by collinearity is known as the
\textbf{variance inflation effect}. The variance inflation effect makes
it difficult to interpret the coefficients of the model, because the
coefficients can be quite unstable and can change a lot depending on
which other collinear variables are included in the model.

\subsection*{\texorpdfstring{Collinearity and interpretation of
\(R^2\)}{Collinearity and interpretation of R\^{}2}}\label{collinearity-and-interpretation-of-r2}
\addcontentsline{toc}{subsection}{Collinearity and interpretation of
\(R^2\)}

Collinearity also affects the interpretation of the \(R^2\) values.
Collinearity will cause the collinear explanatory variables to share
some of the explained variance. The \(R^2\) value of the multiple
regression will then be less than the sum of the \(R^2\) values of the
individual regressions of the response variable on each of the
explanatory variables separately.

\(R^2\) of the age only model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m2\_age)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2712336
\end{verbatim}

\(R^2\) of the mins\_exercise only model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m2\_mins\_exercise)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2300281
\end{verbatim}

\(R^2\) of the model with both age and mins\_exercise:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m2\_both)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5147784
\end{verbatim}

In this case the two explanatory variables are strongly correlated and
so share a lot of the explained variance. The \(R^2\) value of the model
with both explanatory variables is much less than the sum of the \(R^2\)
values of the models with each explanatory variable separately. In fact,
either of the models with only one explanatory variable is nearly as
good as the model with both explanatory variables. We don't gain much
from including another explanatory variable in the model when we already
include one explanatory variable that is strongly correlated with the
other.

\subsection*{Do I have a problem (with
collinearity)?}\label{do-i-have-a-problem-with-collinearity}
\addcontentsline{toc}{subsection}{Do I have a problem (with
collinearity)?}

There are several ways to measuring collinearity between explanatory
variables and to assess if it is a problematic. One way is to look at
the correlation matrix of the explanatory variables. If there are strong
correlations between any pair of explanatory variables, then there is
likely to be collinearity.

But recall that collinearity can also occur without strong pairwise
correlations. So another way to detect collinearity is to calculate the
Variance Inflation Factor (VIF). This is so named because it measures
how much the variance of the estimated regression coefficients is
increased due to collinearity. (Recall that we saw the variance
inflation effect in the simulation above.)

Recall the definition of collinearity: it is when an explanatory
variable is predictable from a linear combination of others. To
calculate the VIF for a specific explanatory variable, we fit a linear
regression model with that explanatory variable as the response
variable, and all the other explanatory variables as the explanatory
variables. We then calculate the \(R^2\) value for this model. The VIF
is then calculated as:

\[VIF_j = \frac{1}{1 - R^2_j}\]

where \(R^2_j\) is the \(R^2\) value from the model with explanatory
variable \(j\) as the response variable, and all other explanatory
variables as the explanatory variables.

In the case where the explanatory variable \(j\) is not predictable from
the other explanatory variables at all, then \(R^2_j = 0\), and the VIF
is 1. This indicates that there is no collinearity.

In the case where the explanatory variable \(j\) is perfectly
predictable from the other explanatory variables, then \(R^2_j = 1\),
and the VIF is infinite. This indicates that there is perfect
collinearity.

To get the VIF for a multiple linear regression model with multiple
explanatory variables, we calculate the VIF for each explanatory
variable separately.

The VIF measures how much the variance of the estimated regression
coefficients is increased due to collinearity.

In R, we can calculate the VIF using the \texttt{vif()} function from
the \texttt{car} package. Here is the code to calculate the VIF for the
blood pressure model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vif}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
          age mins_exercise 
     1.000694      1.000694 
\end{verbatim}

We see that the VIF values for both explanatory variables are close to
1, indicating that there is no collinearity between the explanatory
variables.

For the previous example with collinearity among three explanatory
variables, we can calculate the VIF as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{vif}\NormalTok{(m\_collinear\_123)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      x1       x2       x3 
1.069365 1.412832 1.460863 
\end{verbatim}

We see that the VIF values for all three explanatory variables are
greater than 1, indicating that there is collinearity between the
explanatory variables. The VIF for \(x_3\) is highest, indicating that
\(x_3\) is predictable from \(x_1\) and \(x_2\). But none of the VIF
values are extremely high, indicating that the collinearity is not
severe.

A VIF value greater than 5 or 10 is often used as a rule of thumb to
indicate that there is collinearity between the explanatory variables.

Let's make an example of three explanatory variables with more severe
collinearity:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(n)}
\NormalTok{x3 }\OtherTok{\textless{}{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{+} \DecValTok{3} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)  }\CommentTok{\# less noise, more collinearity}
\NormalTok{y }\OtherTok{\textless{}{-}} \DecValTok{5} \SpecialCharTok{+} \FloatTok{1.5} \SpecialCharTok{*}\NormalTok{ x1 }\SpecialCharTok{{-}} \DecValTok{2} \SpecialCharTok{*}\NormalTok{ x2 }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{*}\NormalTok{ x3 }\SpecialCharTok{+} \FunctionTok{rnorm}\NormalTok{(n, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{)}
\NormalTok{data\_collinear\_severe }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{x1 =}\NormalTok{ x1, }\AttributeTok{x2 =}\NormalTok{ x2, }\AttributeTok{x3 =}\NormalTok{ x3, }\AttributeTok{y =}\NormalTok{ y)}
\NormalTok{m\_collinear\_severe }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, data}
 \OtherTok{=}\NormalTok{ data\_collinear\_severe)}
\FunctionTok{vif}\NormalTok{(m\_collinear\_severe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       x1        x2        x3 
 4.277416 10.644760 13.360572 
\end{verbatim}

We see that the VIF values of \(x_2\) and \(x_3\) are greater than 10,
indicating that there is strong collinearity between the explanatory
variables. The coefficients for \(x_2\) and \(x_3\) will be quite
unstable and difficult to interpret.

\subsection*{What to do about
collinearity?}\label{what-to-do-about-collinearity}
\addcontentsline{toc}{subsection}{What to do about collinearity?}

Imagine we see high VIFs, we can conclud that there is collinearity
between explanatory variables, and that the coefficients for those
variables are likely to be unstable and difficult to interpret. What can
we do???

The answer is we should \emph{think}!

\begin{itemize}
\tightlist
\item
  Are explanatory variables measuring the same concept?
\item
  Is this collinearity expected from the study design?
\item
  Do I care about interpretation, or am I only interested in prediction?
\end{itemize}

And then consider some possible remedies:

\begin{itemize}
\tightlist
\item
  Combine explanatory variables (index, PCA, biologically meaningful
  composite) (covered later in the course).
\item
  Look at the explanatory power of a variable once all other variables
  are accounted for (see below).
\item
  Remove one of a set of redundant explanatory variables (not covered in
  this course).
\item
  Use regularization if prediction is the goal (not covered in this
  course).
\end{itemize}

\subsection*{Assessing the importance of an explanatory variables in the
presence of
collinearity}\label{assessing-the-importance-of-an-explanatory-variables-in-the-presence-of-collinearity}
\addcontentsline{toc}{subsection}{Assessing the importance of an
explanatory variables in the presence of collinearity}

Imagine that we want to know the importance of an explanatory variable
in the presence of collinearity. For example, we might want to know how
important age is in explaining blood pressure. But we also know that we
have collinearity with other explanatory variables, such as minutes of
exercise. How can we assess the importance of a particular explanatory
variable in the presence of collinearity? Let us call the explanatory
variable of interest the ``focal'' explanatory variable.

One way to assess the importance of the focal explanatory variable is to
compare two models. Both should have all other explanatory variables
included, but one model should have the focal explanatory variable
included, and the other model should have the focal explanatory variable
removed. We can then make an \(F\)-test to compare the two models. The
null hypothesis is that the focal explanatory variable does not explain
any additional variance in the response variable, once all other
explanatory variables are accounted for. The alternative hypothesis is
that the focal explanatory variable does explain additional variance in
the response variable, once all other explanatory variables are
accounted for.

The question is essentially: does including the focal explanatory
variable improve the model fit, once all other explanatory variables are
already accounted for?

Here is an example of how to do this in R. Lets use the data with three
explanatory variables, \(x_1, x_2,\) and \(x_3\), where \(x_3\) is
predictable from a linear combination of \(x_1\) and \(x_2\) plus some
random noise. We will assess the importance of \(x_1\) in explaining the
response variable \(y\), once \(x_2\) and \(x_3\) are accounted for.
First, we fit the full model with all three explanatory variables
included:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_full }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x1 }\SpecialCharTok{+}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ data\_collinear)}
\end{Highlighting}
\end{Shaded}

Then, we fit the reduced model with \(x_1\) removed:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_reduced }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x2 }\SpecialCharTok{+}\NormalTok{ x3, }\AttributeTok{data =}\NormalTok{ data\_collinear)}
\end{Highlighting}
\end{Shaded}

Now, we can use the \texttt{anova()} function to compare the two models:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(m\_reduced, m\_full)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: y ~ x2 + x3
Model 2: y ~ x1 + x2 + x3
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     97 272.27                                  
2     96 106.14  1    166.12 150.25 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the R output we see information about two models.

\begin{itemize}
\tightlist
\item
  \textbf{Model 1} is the reduced model with \(x_1\) removed.
\item
  \textbf{Model 2} is the second model is the full model with all three
  explanatory variables included.
\end{itemize}

Then we have a table that looks somewhat like an ANOVA table. The first
column has no title and contain 1 and 2. This indicates the two models
being compared.

The second column is \textbf{Res.Df}, which is the residual degrees of
freedom for each model. We can see that the residual degrees of freedom
for Model 2 is one less than that for Model 1, because Model 2 has one
more explanatory variable (i.e., \(x_1\)) than Model 1.

The third column is ``RSS'', which is the Residual Sum of Squares for
each model. The residual sum of squares for Model 2 is less than that
for Model 1, because Model 2 has one more explanatory variable (i.e.,
\(x_1\)) than Model 1, and because that explanatory variable helps to
explain some of the variance in the response variable.

The fourth column is ``Df'', which is the difference in degrees of
freedom between the two models. This is one in this case, because Model
2 has one more explanatory variable than Model 1.

The fifth column is ``Sum of Sq'', which is the difference in residual
sum of squares between the two models. This the amount of variance in
the response variable that is explained by the focal explanatory
variable (i.e., \(x_1\)) once all other explanatory variables are
already accounted for.

The sixth column is ``F value'', which is the F-statistic for the
comparison of the two models. This is calculated as:

\[F = \frac{(RSS_1 - RSS_2) / (df_1-df_2)}{RSS_2 / df_2}\]

where \(RSS_1\) and \(RSS_2\) are the residual sum of squares for Model
1 and Model 2, respectively, and \(df_1\) and \(df_2\) are the residual
degrees of freedom for Model 1 and Model 2, respectively.

Put another way, the numerator is the mean square difference between the
two models, and the denominator is the mean square error for the full
model.

In our example data, the numbers are: 272.3 - 106.1 = 166.1 divided by -
96 = 1 for the numerator.

And for the denominator: 106.1 divided by 96.

This gives an F value of 150.25. This is a very large F value,
indicating that the focal explanatory variable (i.e., \(x_1\)) explains
a significant amount of variance in the response variable, even once all
other explanatory variables are already accounted for. This means that
even though there is collinearity between the explanatory variables, we
can still find that \(x_1\) is important in explaining the response
variable.

We can also find the partial \(R^2\) value for the focal explanatory
variable (i.e., \(x_1\)) from this analysis. The partial \(R^2\) value
is the proportion of variance in the response variable that is explained
by the focal explanatory variable, as a fraction of the variance that is
not explained by the other explanatory variables.

The partial \(R^2\) is calculated as:

\[R^2_{partial} = \frac{RSS_1 - RSS_2}{RSS_1}\]

We can find this in R with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rss1 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(m\_reduced, m\_full)}\SpecialCharTok{$}\StringTok{"RSS"}\NormalTok{[}\DecValTok{1}\NormalTok{]}
\NormalTok{rss2 }\OtherTok{\textless{}{-}} \FunctionTok{anova}\NormalTok{(m\_reduced, m\_full)}\SpecialCharTok{$}\StringTok{"RSS"}\NormalTok{[}\DecValTok{2}\NormalTok{]}
\NormalTok{partial\_r2 }\OtherTok{\textless{}{-}}\NormalTok{ (rss1 }\SpecialCharTok{{-}}\NormalTok{ rss2) }\SpecialCharTok{/}\NormalTok{ rss1}
\NormalTok{partial\_r2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6101473
\end{verbatim}

Don't confuse this partial \(R^2\) with what is sometimes known as the
semi-partial \(R^2\). The semi-partial \(R^2\) is the proportion of
variance in the response variable that is explained by the focal
explanatory variable, as a fraction of the total variance in the
response variable. The semi-partial \(R^2\) is calculated as:

\[R^2_{semi-partial} = \frac{RSS_1 - RSS_2}{TSS}\]

where \(TSS\) is the total sum of squares of the response variable.

In R in our example this is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tss }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((data\_collinear}\SpecialCharTok{$}\NormalTok{y }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(data\_collinear}\SpecialCharTok{$}\NormalTok{y))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{semi\_partial\_r2 }\OtherTok{\textless{}{-}}\NormalTok{ (rss1 }\SpecialCharTok{{-}}\NormalTok{ rss2) }\SpecialCharTok{/}\NormalTok{ tss}
\NormalTok{semi\_partial\_r2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1625303
\end{verbatim}

This means that about 16.25\% of the total variance in the response
variable is explained by the focal explanatory variable (i.e., \(x_1\)),
even once all other explanatory variables are already accounted for.
This is explanatory power that is unique to the focal explanatory
variable.

\section*{Review}\label{review-4}
\addcontentsline{toc}{section}{Review}

\markright{Review}

Simple regression:

\begin{itemize}
\tightlist
\item
  How well does the model describe the data: Correlation and \(R^2\)
\item
  Are the parameter estimates compatible with some specific value
  (\(t\)-test)?
\item
  What range of parameters values are compatible with the data
  (confidence intervals)?
\item
  What regression lines are compatible with the data (confidence band)?
\item
  What are plausible values of other data (prediction band)?
\end{itemize}

Multiple regression:

\begin{itemize}
\tightlist
\item
  Multiple linear regression \(x_1\), \(x_2\), \ldots, \(x_m\)
\item
  Checking assumptions.
\item
  Question 1: As an ensemble, do the explanatory variables explain
  variation in the response variable? This is done with an overall
  \(F\)-test.
\item
  Question 2: Which of the explanatory variables are important in
  explaining variation in the response variable? This is done by looking
  at the coefficients, t-tests, and confidence intervals for each
  explanatory variable.
\item
  Question 3: How well does the model describe the data? This is done
  with \(R^2\) and adjusted \(R^2\).
\item
  Question 4: Are some explanatory variables more important than others
  in explaining variation in the response variable? This is done by
  looking at the sizes of the coefficients, and comparing those for
  different explanatory variables. But caution is required.
\item
  Question 5: How do we make predictions from the model? This is done
  with the \texttt{predict()} function, and often requires fixing the
  values of some explanatory variables to specific values.
\item
  Collinearity between explanatory variables can make the coefficients
  unstable and difficult to interpret. This can be assessed with the
  Variance Inflation Factor (VIF). Remedies include combining
  explanatory variables, removing redundant predictors, or using
  regularization if prediction is the goal. The importance of an
  explanatory variable in the presence of collinearity can be assessed
  by comparing models with and without that variable, using an F-test.
\end{itemize}

\section*{Further reading}\label{further-reading-5}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

If you'd like to solidify your understanding of multiple linear
regression, you might like to look at Chapter 11 of \textbf{Statistics.
An Introduction using R} by Crawley. This section is about 14 pages. You
will also see that the chapter includes material not included in this
course, such as interactions, polynomial regression, model selection,
and regression trees. Fun stuff (of course it will not be in the
examination for this course!).

\section*{Extras}\label{extras-4}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{3D plot of multiple linear
regression}\label{d-plot-of-multiple-linear-regression}
\addcontentsline{toc}{subsection}{3D plot of multiple linear regression}

3D plots can help us to visualise multiple linear regression models with
two explanatory variables. They are also kind of cool. They can also be
difficult to interpret. So use with caution!

Here is the code to make a 3D plot of the blood pressure data. The
y-axis is blood pressure, the x-axis is age, and the z-axis is minutes
of exercise. Here is a 3d plot that we can interactive with and rotate
(please note that this plot is best viewed in the HTML version of the
book; in the PDF version, it will appear as a static image):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plotly}\SpecialCharTok{::}\FunctionTok{plot\_ly}\NormalTok{(bp\_data\_multreg, }\AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{age, }\AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{mins\_exercise, }\AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{bp,}
                \AttributeTok{type =} \StringTok{"scatter3d"}\NormalTok{, }\AttributeTok{mode =} \StringTok{"markers"}\NormalTok{, }\AttributeTok{marker =} \FunctionTok{list}\NormalTok{(}\AttributeTok{size =} \DecValTok{5}\NormalTok{),}
                \AttributeTok{width=}\DecValTok{500}\NormalTok{, }\AttributeTok{height=}\DecValTok{500}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\NormalTok{  plotly}\SpecialCharTok{::}\FunctionTok{layout}\NormalTok{(}\AttributeTok{scene =} \FunctionTok{list}\NormalTok{(}\AttributeTok{xaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Age"}\NormalTok{),}
                             \AttributeTok{yaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Minutes of exercise"}\NormalTok{),}
                             \AttributeTok{zaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood pressure"}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-53-1.pdf}}

\subsection*{Publication ready table of
results}\label{publication-ready-table-of-results}
\addcontentsline{toc}{subsection}{Publication ready table of results}

Sometimes we need to put a table of coefficients and confidence
intervals into a report or publication. One option is to make a table in
Word, and to manually enter the coefficients and confidence intervals.
This is tedious and error prone. A much better option is to use R to
make the table for us. One way to do this is to use the \texttt{broom}
package to tidy up the model output into a data frame, and then use the
\texttt{knitr} package to make a table from the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_m1 }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(m1, }\AttributeTok{conf.int =} \ConstantTok{TRUE}\NormalTok{)}
\FunctionTok{kable}\NormalTok{(tidy\_m1, }\AttributeTok{digits =} \DecValTok{3}\NormalTok{, }\AttributeTok{caption =} \StringTok{"Multiple linear regression results}
\StringTok{for blood pressure data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1286}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1143}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1286}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 12\tabcolsep) * \real{0.1429}}@{}}
\caption{Multiple linear regression results for blood pressure
data}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
std.error
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
conf.low
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
conf.high
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
std.error
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p.value
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
conf.low
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
conf.high
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(Intercept) & 87.183 & 4.485 & 19.437 & 0 & 78.281 & 96.085 \\
age & 0.543 & 0.072 & 7.545 & 0 & 0.400 & 0.686 \\
mins\_exercise & -0.103 & 0.015 & -6.978 & 0 & -0.132 & -0.074 \\
\end{longtable}

Another approach is to use \texttt{tbl\_regression} function within the
\texttt{gtsummary} package to get a publication ready table of the
coefficients and confidence intervals:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tbl\_regression}\NormalTok{(m1)}
\end{Highlighting}
\end{Shaded}

\begin{table}
\fontsize{12.0pt}{14.4pt}\selectfont
\begin{tabular*}{\linewidth}{@{\extracolsep{\fill}}lccc}
\toprule
\textbf{Characteristic} & \textbf{Beta} & \textbf{95\% CI}\textsuperscript{\textit{1}} & \textbf{p-value} \\ 
\midrule\addlinespace[2.5pt]
age & 0.54 & 0.40, 0.69 & <0.001 \\ 
mins\_exercise & -0.10 & -0.13, -0.07 & <0.001 \\ 
\bottomrule
\end{tabular*}
\begin{minipage}{\linewidth}
\textsuperscript{\textit{1}}CI = Confidence Interval\\
\end{minipage}
\end{table}

To use either approach most efficiently you will, however, need to write
your report using R Markdown or Quarto so that the table is
automatically created in your report document.

\subsection*{A figure showing the coefficients and confidence
intervals}\label{a-figure-showing-the-coefficients-and-confidence-intervals}
\addcontentsline{toc}{subsection}{A figure showing the coefficients and
confidence intervals}

As well as or rather than a table of coefficients, we could make a
figure showing the coefficients and confidence intervals for each
explanatory variable. This can be a nice way to visualise the results of
the multiple linear regression model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tidy\_m1 }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(m1, }\AttributeTok{conf.int =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{tidy\_m1 }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(term }\SpecialCharTok{!=} \StringTok{"(Intercept)"}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ term, }\AttributeTok{y =}\NormalTok{ estimate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ conf.low, }\AttributeTok{ymax =}\NormalTok{ conf.high), }\AttributeTok{width =}
                \FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Explanatory variable"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Coefficient estimate"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Multiple linear regression coefficients}\SpecialCharTok{\textbackslash{}n}\StringTok{and confidence intervals"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{6.1-multiple-regression_files/figure-pdf/unnamed-chunk-56-1.pdf}}

\bookmarksetup{startatroot}

\chapter*{Interactions (L7)}\label{interactions-l7}
\addcontentsline{toc}{chapter}{Interactions (L7)}

\markboth{Interactions (L7)}{Interactions (L7)}

\section*{Thought for the week:}\label{thought-for-the-week}
\addcontentsline{toc}{section}{Thought for the week:}

\markright{Thought for the week:}

``\emph{You should arrive at answers on your own, and not rely upon what
you get from someone else. Answers from others are nothing more than
stopgap measures, they're of no value.}''

\begin{itemize}
\tightlist
\item
  Ichiro Kishimi \& Fumitake Koga, The Courage to Be Disliked
\end{itemize}

They instead encourage answers gained from curiosity, observation, and
dialogue. Answers you develop for yourself are more likely to stick with
you, and to be meaningful.

When we mentor, ask questions that encourage others to think for
themselves, rather than simply giving answers. This is hard, but
worthwhile.

\section*{Introduction}\label{introduction-4}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In the previous chapters we have looked at models with a special type of
simplicity: the effect of each explanatory variable on the response
variable is independent of the other explanatory variables. This is
special because it means that we can understand the effect of each
explanatory variable on the response variable separately, without
considering the other explanatory variables. Nevertheless, this is often
not the case in real life. The effect of one explanatory variable on the
response variable may depend on the value of another explanatory
variable. This is called an interaction effect, or simply an
interaction.

Interactions are some of the most interesting phenomena in science,
including biology. We are not talking about interactions between
species, like predation, though these are also very interesting. We are
talking about effects of one thing, like diet, depending on another
thing, like exercise. Let's break that down a bit\ldots{}

Imagine we make a study of the effect of exercise (minutes per week) on
blood pressure for people with a meat heavy diet.

Read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_meatheavy }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/bp\_meatheavy.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here are the first few rows of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_meatheavy)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         bp mins_per_week       diet
1  94.12854      53.10173 meat heavy
2  90.99957      74.42478 meat heavy
3  73.83541     114.57067 meat heavy
4  77.05434     181.64156 meat heavy
5 100.14578      40.33639 meat heavy
6  95.61900     179.67794 meat heavy
\end{verbatim}

And here is a graph of the relationship between blood pressure and
minutes of exercise for people with a meat heavy diet:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-6-1.pdf}}

We see that exercise seems to lower blood pressure. But what if we look
at the effect of exercise on blood pressure for people with a vegetarian
diet? The relationship might look something like this:

\begin{verbatim}
         bp mins_per_week       diet
1  77.56704     122.92899 vegetarian
2  84.30684     111.43191 vegetarian
3  75.23323      65.75546 vegetarian
4  85.67402      90.62629 vegetarian
5  77.45327     100.08819 vegetarian
6 102.31114      36.17327 vegetarian
\end{verbatim}

Read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_vegetarian }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/bp\_vegetarian.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here is a graph of the relationship between blood pressure and minutes
of exercise for people with a vegetarian diet:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-10-1.pdf}}

We see that exercise seems to lower blood pressure for vegetarians too,
but the effect seems to be weaker.

To summarise this finding, we can say that the effect of exercise on
blood pressure is stronger for people with a meat heavy diet than for
people with a vegetarian diet. This means that the effect of exercise on
blood pressure depends on diet.

This is very clear when we look at the both diets in the same graph:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-11-1.pdf}}

(The dataset with both combined is in the file
\texttt{bp\_1cont1cat.csv}.)

I think it is clear that the interaction was easier to see when we
plotted all the data in one graph\ldots{} it is much easier to visually
compare the slopes of the two regression lines when they are on the same
graph.

As we will see later in this chapter, the same holds true for
statistical tests of interactions: it is much easier to make a
statistical test of the interaction when we make a single model with
\emph{an interaction term}.

It is harder and is not recommended to make a separate regression for
each level of the second variable (diet) and then compare the slopes of
the regression lines (it is possible, just not at all efficient).

\section*{Parallel and non-parallel
effects}\label{parallel-and-non-parallel-effects}
\addcontentsline{toc}{section}{Parallel and non-parallel effects}

\markright{Parallel and non-parallel effects}

In the example above, the effect of exercise on blood pressure was
stronger for people with a meat heavy diet than for people with a
vegetarian diet. That is, the slope of the regression line was steeper
for the meat heavy diet than for the vegetarian diet. Put another way,
the regression lines are not parallel.

\textbf{Parallel = no interaction}: Parallel regression lines are
evidence of no interaction. This means that the effect of one variable
(exercise) is the same for all levels of another variable (diet).

\textbf{Non-parallel = interaction}: When the regression lines are not
parallel, there is evidence of an interaction. This means that the
effect of one variable (exercise) depends on the level of another
variable (diet).

That was an example of an interaction between a continuous explanatory
variable (exercise) and a categorical explanatory variable (diet).
Interactions can also occur between two categorical explanatory
variables, or between two continuous explanatory variables. Let us look
at some more examples.

\section*{Two cats}\label{two-cats}
\addcontentsline{toc}{section}{Two cats}

\markright{Two cats}

Two categorical explanatory variables: diet (meat heavy or vegetarian)
and exercise (low or high), and one continuous response variable (blood
pressure).

Read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_2cat }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/bp\_2cat.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here are the first few rows of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_2cat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        diet exercise reps        bp
1 meat heavy     high    1  83.73546
2 meat heavy      low    1 115.11781
3 vegetarian     high    1  74.18977
4 vegetarian      low    1 108.58680
5 meat heavy     high    2  91.83643
6 meat heavy      low    2 103.89843
\end{verbatim}

And here is the data in a graph, where we show the individual data
points as well as the group means connected by lines. The lines
connecting the means are only to help visualize whether there is an
interaction or not. If we see non-parallel lines connecting the means,
we have evidence of an interaction.

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-17-1.pdf}}

Indeed, the lines connecting the means are not parallel, which is
evidence of an interaction between diet and exercise on blood pressure.

\section*{Two continuous}\label{two-continuous}
\addcontentsline{toc}{section}{Two continuous}

\markright{Two continuous}

Two continuous explanatory variables (age and exercise minutes) and one
continuous response variable (blood pressure).

Read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_2cont }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/bp\_2cont.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here is the first few rows of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_2cont)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         bp      age mins_per_week
1  61.58319 35.93052     130.94479
2  83.86716 42.32743      70.63945
3  93.01438 54.37120      54.05203
4  82.14413 74.49247     198.53681
5  60.13102 32.10092     126.69865
6 102.00306 73.90338      42.64163
\end{verbatim}

Now we have a little challenge, namely that we have two continuous
explanatory variables. This means that we need to use three dimensions
to visualize the data. Here is a standard 2D scatter plot of blood
pressure against minutes of exercise, though with age represented by
color grading from young (dark blue) to old (yellow):

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-22-1.pdf}}

And we can make the complementary plot of blood pressure against age,
with minutes of exercise represented by color grading from low (green)
to high (orange):

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-23-1.pdf}}

What do we see? If we look at a set of points of the similar colour
(i.e., similar number of minutes of exercised), we can see that the
slope of the relationship between blood pressure and age depends on
exercise. The slope is steeper for people that exercise more.

Yet another way to visualise this interaction is to create categorical
versions of age and minutes of exercise, and then plot the data with
these categorical variables:

Here is the first few rows of the modified data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_2cont)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         bp      age mins_per_week age_class mins_per_week_class
1  61.58319 35.93052     130.94479     30-40             120-140
2  83.86716 42.32743      70.63945     40-50               60-80
3  93.01438 54.37120      54.05203     50-60               40-60
4  82.14413 74.49247     198.53681     70-80             180-200
5  60.13102 32.10092     126.69865     30-40             120-140
6 102.00306 73.90338      42.64163     70-80               40-60
\end{verbatim}

And here is the plot of blood pressure against minutes of exercise, with
age class represented by colour. Regression lines have been added, to
help visualize any interaction:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-26-1.pdf}}

It is as we saw in the previous version of the graph. The slope of the
relationship between blood pressure and minutes of exercise depends on
age. The slope is shallower for older people.

In the complementary graph of blood pressure against age, with minutes
of exercise represented by colour, we see the same interaction:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-27-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{There is only one interaction} We made two graphs of the same
data, one with age as a categorical variable and one with minutes of
exercise as a categorical variable. In both graphs we see an
interaction. This is not us seeing two separate interactions, however.
There is only one interaction here, namely that the effect of age on
blood pressure depends on minutes of exercise, and equivalently, that
the effect of minutes of exercise on blood pressure depends on age. The
two graphs just look at the same interaction from different
perspectives.

\end{tcolorbox}

\section*{Other perspectives}\label{other-perspectives}
\addcontentsline{toc}{section}{Other perspectives}

\markright{Other perspectives}

\subsection*{Interactions and additivity of
effects}\label{interactions-and-additivity-of-effects}
\addcontentsline{toc}{subsection}{Interactions and additivity of
effects}

Another way of thinking about interactions is from the perspective of
additivity or non-additivity of effects. Imagine we made two separate
studies, one of the effect of diet on blood pressure, and one of the
effect of exercise on blood pressure. In the first study we only varied
diet, and in the second study we only varied exercise. In the first
study we found an effect size of diet on blood pressure of say 10 mmHg
(e.g., difference between meat heavy and vegetarian). And in the second
study we found an effect size of exercise on blood pressure of 15 mmHg
(e.g., difference between low and high exercise).

If the effects are non-additive, we would expect the effect size to be
different from additive. For example, if we found the combined effect of
diet and exercise on blood pressure to be 40, we would say that the
effects are non-additive. Their combined effect is more than the sum of
their individual effects. This example is of a synergistic interaction
because the combined effect (40) is greater than the sum of the
individual effects (25).

\subsection*{Drug interactions}\label{drug-interactions}
\addcontentsline{toc}{subsection}{Drug interactions}

When a doctor is considering giving us a particular medication, we are
asked if we are taking any other medications. This is because the
effects of drugs can interact. For example, if we take two drugs that
both lower blood pressure and they interfere with each other, the
combined effect might be less than the sum of their individual effects.
This is an antagonistic interaction. It could be worse than that though,
the interaction might actually be harmful, which is why doctors are so
careful about drug interactions.

\section*{The maths bit}\label{the-maths-bit}
\addcontentsline{toc}{section}{The maths bit}

\markright{The maths bit}

Let us return to the example of the effects of number of minutes of
exercise and diet on blood pressure:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-28-1.pdf}}

We have one continuous explanatory variable (minutes of exercise) and
one binary explanatory variable (diet) and one continuous response
variable (blood pressure).

\[y_i = \beta_0 + \beta_1 x_{i}^{(1)} + \beta_2 x_{i}^{(2)} + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(y_i\) is the blood pressure of the \(i\)th participant
\item
  \$x\_i\^{}\{(1)\} is the number of minutes of exercise of the \(i\)th
  participant
\item
  \(x_i^{(2)}\) is the diet of the \(i\)th participant
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the effect of exercise on blood pressure
\item
  \(\beta_2\) is the effect of diet on blood pressure
\item
  \(\epsilon_i\) is the error term for the \(i\)th participant.
\end{itemize}

This model is a multiple regression model in which one of the
explanatory variables is binary.

\[y_i = \beta_0 + \beta_1 x_{i}^{(1)} + \beta_2 x_{i}^{(2)} + \beta_3 (x_{i}^{(1)} x_{i}^{(2)}) + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(x_{i}^{(1)}x_{i}^{(2)}\) is the product of the number of minutes of
  exercise and the diet of the \(i\)th participant.
\item
  \(\beta_3\) is the coefficient of the interaction term between diet
  and exercise.
\end{itemize}

We could also write this model as:

\[y_i = \beta_0 + \beta_1 x_{i}^{(1)} + \beta_2 x_{i}^{(2)} + \beta_3 x_{i}^{(3)} + \epsilon_i\]

where:

\begin{itemize}
\tightlist
\item
  \(x_{i}^{(3)} = x_{i}^{(1)} x_{i}^{(2)}\)
\end{itemize}

This is again a multiple regression model, but now with three
explanatory variables.

\section*{Hypothesis testing}\label{hypothesis-testing}
\addcontentsline{toc}{section}{Hypothesis testing}

\markright{Hypothesis testing}

If we want to test whether the effect of minutes of exercise on blood
pressure is different for people with different diets, we need a null
hypothesis to test.

The null hypothesis is that the effect of minutes of exercise on blood
pressure is the same for people with different diets. This is a null
hypothesis of no interaction between diet and exercise. In terms of the
coefficients of the model, the null hypothesis is that \(\beta_3 = 0\).

If we reject the null hypothesis, we conclude that the effect of minutes
of exercise on blood pressure is different for people with different
diets. This is a non-additive effect.

\section*{Doing it in R}\label{doing-it-in-r}
\addcontentsline{toc}{section}{Doing it in R}

\markright{Doing it in R}

Let us fit the model with the interaction term in R. There are two
methods to do this and they are equivalent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mins\_per\_week }\SpecialCharTok{+}\NormalTok{ diet }\SpecialCharTok{+}\NormalTok{ mins\_per\_week}\SpecialCharTok{:}\NormalTok{diet, }\AttributeTok{data=}\NormalTok{bp\_1cont1cat)}
\NormalTok{mod2 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mins\_per\_week }\SpecialCharTok{*}\NormalTok{ diet, }\AttributeTok{data=}\NormalTok{bp\_1cont1cat)}
\end{Highlighting}
\end{Shaded}

The second is a shorthand for the first. The \texttt{*} operator
includes the main effects (main effects are terms in the model that
don't include interactions) and the interaction term. The \texttt{:}
operator includes only the interaction term.

Of course, we check the model assumptions before we interpret the
results:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod2, }\AttributeTok{add.smooth=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-30-1.pdf}}

All of the plots look good. Now we can do hypothesis testing of the
interaction term using an F-test:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(mod2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
                   Df Sum Sq Mean Sq F value    Pr(>F)    
mins_per_week       1 1133.6 1133.55 13.4800 0.0003964 ***
diet                1 1560.8 1560.75 18.5601 3.979e-05 ***
mins_per_week:diet  1  340.4  340.36  4.0475 0.0470408 *  
Residuals          96 8072.8   84.09                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the ANOVA table we see four rows. The first row is for the main
effect of minutes of exercise, the second row is for the main effect of
diet, the third row is for the interaction effect between diet and
minutes of exercise, and the fourth row is for the residuals. As always,
an interaction term in the R output is shown with a colon \texttt{:}
between the two variables (here it looks like
\texttt{mins\_per\_week:diet}).

In this example, the F-statistic for the interaction term is quite large
(4.05), and the p-value is very small (0.047). This means that we reject
the null hypothesis that there is no interaction between diet and
minutes of exercise on blood pressure. We conclude that the effect of
minutes of exercise on blood pressure is different for people with
different diets.

If we like (and we don't have to), we can look at the coefficients of
the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod2)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                                 Estimate Std. Error   t value     Pr(>|t|)
(Intercept)                  100.62716134 2.87228930 35.033783 1.714199e-56
mins_per_week                 -0.09661054 0.02406014 -4.015376 1.177969e-04
dietvegetarian               -15.27591304 4.09882726 -3.726898 3.275630e-04
mins_per_week:dietvegetarian   0.06907583 0.03433487  2.011828 4.704076e-02
\end{verbatim}

As expected, there are four coefficients.

The first is \texttt{(Intercept)}, which is the expected blood pressure
for a person who does 0 minutes of exercise and is on diet ``meat
heavy''.

The second is \texttt{mins\_per\_week}, which is the effect (slope) of
minutes of exercise on blood pressure for a person on diet ``meat
heavy''.

The third is \texttt{dietvegetarian}, which is the effect of being on a
vegetarian diet on blood pressure for a person who does 0 minutes of
exercise. This can be thought of as the change in the intercept for a
person on a vegetarian diet compared to a person on a ``meat heavy''
diet.

The fourth is the interaction term
\texttt{mins\_per\_week:dietvegetarian}, which is the difference in the
effect (slope) of minutes of exercise on blood pressure for a person on
a vegetarian diet compared to a person on a ``meat heavy'' diet.

\section*{Reporting our findings}\label{reporting-our-findings}
\addcontentsline{toc}{section}{Reporting our findings}

\markright{Reporting our findings}

Of course a nice graph is always helpful. We already have quite a nice
one:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-33-1.pdf}}

We also might want some tables summarizing the model results. Here is a
table of the coefficients:

\begin{table}
\centering
\begin{tabular}{lrrrr}
\toprule
  & Estimate & Std. Error & t value & Pr(>|t|)\\
\midrule
(Intercept) & 100.6271613 & 2.8722893 & 35.033783 & 0.0000000\\
mins\_per\_week & -0.0966105 & 0.0240601 & -4.015377 & 0.0001178\\
dietvegetarian & -15.2759130 & 4.0988273 & -3.726898 & 0.0003276\\
mins\_per\_week:dietvegetarian & 0.0690758 & 0.0343349 & 2.011828 & 0.0470408\\
\bottomrule
\end{tabular}
\end{table}

We could also report the \(R^2\) of the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod2)}\SpecialCharTok{$}\NormalTok{r.squared}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2732093
\end{verbatim}

And also a table of the variances of the terms in the model:

\begin{table}
\centering
\begin{tabular}{lrrrrr}
\toprule
  & Df & Sum Sq & Mean Sq & F value & Pr(>F)\\
\midrule
mins\_per\_week & 1 & 1133.554 & 1133.55446 & 13.47998 & 0.0003964\\
diet & 1 & 1560.753 & 1560.75255 & 18.56012 & 0.0000398\\
mins\_per\_week:diet & 1 & 340.357 & 340.35702 & 4.04745 & 0.0470408\\
Residuals & 96 & 8072.804 & 84.09171 & NA & NA\\
\bottomrule
\end{tabular}
\end{table}

We also might use a sentence like this to report the results: ``The
effect of minutes of exercise is generally negative, but the effect is
stronger for people on a meat heavy diet than for people on a vegetarian
diet (\(F\)-statistics of interaction term = 4.05, degrees of freedom =
1, degrees of freedom residuals = 96, \(p\)-value = 0.047).''

\section*{Multiple regression vs.~many single
regressions}\label{multiple-regression-vs.-many-single-regressions}
\addcontentsline{toc}{section}{Multiple regression vs.~many single
regressions}

\markright{Multiple regression vs.~many single regressions}

Question: Why not just fit a separate simple regression model and then
test whether the slopes are the same (i.e., if they are parallel)? That
is, why not fit the two models:

\[y_i = \beta_{0,veg} + \beta_{1,veg} x_i^{(1)} + \epsilon_i\]

\[y_i = \beta_{0,meat} + \beta_{1,meat} x_i^{(2)} + \epsilon_i\]

and compare the estimate of \(\beta_{1,veg}\) to the estimate of
\(\beta_{1,meat}\)?

Well, you could do that, and could probably find a way to test for
whether the difference in the slopes is different from 0. This would be
a test of the null hypothesis that the effect of minutes of exercise on
blood pressure is the same for people with different diets. But, this
would be a more complicated way to do it, and would not be as general as
the model with the interaction term. The model with the interaction term
is more general, more flexible, and more elegant.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

It is usually better model the whole dataset to test a single
hypothesis, rather dividing up the dataset into smaller parts, fitting a
model to each part, and then comparing the results of the models. The
latter approach is less efficient and less elegant. \textbf{One
hypothesis = one model.}

\end{tcolorbox}

\section*{ANCOVA}\label{ancova}
\addcontentsline{toc}{section}{ANCOVA}

\markright{ANCOVA}

The example we just worked through was with one continuous explanatory
variable (minutes of exercise) and one categorical explanatory variable
(diet). This is an example of an analysis of covariance (ANCOVA). ANCOVA
is a type of linear model in which there are both continuous and
categorical explanatory variables.

ANCOVA is often used in two main ways.

First, it can be used to account for covariates (continuous variables).
In this case, the main interest is in comparing groups (as in ANOVA),
while one or more continuous variables are included to explain
additional variation in the response. These covariates are not the main
focus of interpretation; instead, they help adjust group means and
improve the precision of group comparisons.

Second, ANCOVA can be used to test whether covariate effects differ
between groups. Here, the continuous variable is of real interest, and
the question is whether the relationship between the covariate and the
response is the same across groups. This is done by including a group Ã—
covariate interaction, which allows the slope of the relationship to
differ between groups.

An important distinction is that the first use assumes the covariate has
the same effect in all groups (parallel slopes), while the second
explicitly tests whether this assumption is valid.

\section*{Two-way ANOVA}\label{two-way-anova}
\addcontentsline{toc}{section}{Two-way ANOVA}

\markright{Two-way ANOVA}

Above we had an example with two categorical explanatory variables (diet
{[}levels: meat heavy or vegetarian{]} and exercise {[}levels: low or
high{]}) and one continuous response variable (blood pressure). This is
an example of a two-way ANOVA. Two-way ANOVA is used to test for effects
of two categorical explanatory variables, as well as their interaction
effect on a continuous response variable.

Here are the first few rows of the data again:

\begin{verbatim}
        diet exercise reps        bp
1 meat heavy     high    1  83.73546
2 meat heavy      low    1 115.11781
3 vegetarian     high    1  74.18977
4 vegetarian      low    1 108.58680
5 meat heavy     high    2  91.83643
6 meat heavy      low    2 103.89843
\end{verbatim}

Here is the graph of the data again:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-38-1.pdf}}

We can fit a two-way ANOVA model in R as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_2cat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{*}\NormalTok{ exercise, }\AttributeTok{data=}\NormalTok{bp\_2cat)}
\end{Highlighting}
\end{Shaded}

Recall that this fits a model with main effects of diet and exercise, as
well as their interaction effect. Recall that we could specify the model
equivalently as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_2cat }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{+}\NormalTok{ exercise }\SpecialCharTok{+}\NormalTok{ diet}\SpecialCharTok{:}\NormalTok{exercise, }\AttributeTok{data=}\NormalTok{bp\_2cat)}
\end{Highlighting}
\end{Shaded}

We can check the model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod\_2cat, }\AttributeTok{add.smooth=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-41-1.pdf}}

These look ok.

\emph{Hypothesis testing} the interaction is just as before. We use an
F-test on the interaction term to test the null hypothesis that there is
no interaction between diet and exercise on blood pressure.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(mod\_2cat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
              Df Sum Sq Mean Sq F value    Pr(>F)    
diet           1 2879.8  2879.8  34.695 9.741e-07 ***
exercise       1 4776.5  4776.5  57.546 5.680e-09 ***
diet:exercise  1 1142.5  1142.5  13.765  0.000696 ***
Residuals     36 2988.1    83.0                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the ANOVA table we see four rows. The first row is for the main
effect of diet, the second row is for the main effect of exercise, the
third row is for the interaction effect between diet and exercise, and
the fourth row is for the residuals. As always, an interaction term in
the R output is shown with a colon \texttt{:} between the two variables
(here it looks like \texttt{diet:exercise}).

In this example, the F-statistic for the interaction term is quite large
(13.76 and the corresponding p-value is quite small
(\ensuremath{7\times 10^{-4}}. Hence, we conclude that there is a strong
evidence of an interaction between diet and exercise on blood pressure.
The effect of exercise on blood pressure depends on diet.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Reporting two-way ANOVA results} When reporting the results of a
two-way ANOVA, it is important to focus on the biological interpretation
of the results, rather than the statistical details. Include statistics
in parentheses, in support of your statements. For example, you might
say something like: ``The effect of exercise was different for people on
different diets, with a stronger effect for those on a vegetarian diet
compared to those on a meat heavy diet (F(1, 36) = 13.77, p \textless{}
0.001).''

\end{tcolorbox}

\subsection*{More than two levels}\label{more-than-two-levels}
\addcontentsline{toc}{subsection}{More than two levels}

What if we had a categorical explanatory variable with more than two
levels? For example, what if diet had three levels: meat heavy,
vegetarian, and vegan?

Here is an example dataset.

Read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_2cat\_3levels }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/bp\_3cat.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here are the first few rows of the data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(bp\_2cat\_3levels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
        diet exercise reps        bp
1 meat heavy     high    1  83.73546
2 meat heavy      low    1 115.11781
3      vegan     high    1  59.18977
4      vegan      low    1  98.58680
5 vegetarian     high    1  68.35476
6 vegetarian      low    1  98.98106
\end{verbatim}

Here is the graph of the data:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-47-1.pdf}}

Or plotted differently:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-48-1.pdf}}

The hypothesis testing is the same as before. We fit the model with
interaction term:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_2cat\_3levels }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{*}\NormalTok{ exercise, }\AttributeTok{data=}\NormalTok{bp\_2cat\_3levels)}
\end{Highlighting}
\end{Shaded}

We check the model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod\_2cat\_3levels, }\AttributeTok{add.smooth=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-50-1.pdf}}

These look ok.

Now we do the ANOVA:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(mod\_2cat\_3levels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
              Df Sum Sq Mean Sq F value    Pr(>F)    
diet           2 8724.1  4362.1  55.636 7.644e-14 ***
exercise       1 9078.3  9078.3 115.789 4.791e-15 ***
diet:exercise  2 1741.3   870.6  11.104 9.130e-05 ***
Residuals     54 4233.8    78.4                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\emph{Important}: Although we have a categorical variable with three
rather than two levels, we still have only four rows in the ANOVA table.
One for each main effect (diet and exercise), one for the interaction
effect (diet:exercise), and one for the residuals. This is because the
ANOVA table tests each effect as a whole, rather than testing each level
of the categorical variable separately.

In this case, we see that there is strong evidence of main effects of
diet and exercise on blood pressure, as well as strong evidence of an
interaction effect between diet and exercise on blood pressure.

Reporting the patterns and statistics is similar to before, but now we
have more levels to consider so the reporting is a bit more complex, and
we have to be careful to not over-interpret the results. For example,
when the hypothesis test is on the interaction term via an F-test, we
can only say that there is evidence of an interaction between diet and
exercise on blood pressure. We cannot say which diets have different
effects of exercise on blood pressure. To do that, we would need to do
post-hoc tests, such as pairwise comparisons between the levels of diet
within each level of exercise.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Degrees of freedom} Look at the ANOVA table and the degrees of
freedom column. For diet, the degrees of freedom is 2, because there are
three levels of diet (meat heavy, vegetarian, vegan), and the degrees of
freedom is number of levels minus 1. For exercise, the degrees of
freedom is 1, because there are two levels of exercise (low, high). For
the interaction term diet:exercise, the degrees of freedom is 2, which
is the product of the degrees of freedom for diet (2) and exercise (1).

Another way to think about this is how many parameters are estimated?
The answer is as follows: six parameters are estimated in total, one for
each of the combinations of diet and exercise (3 diets x 2 exercises = 6
combinations). Hence the residual degrees of freedom is total number of
observations (60) minus 6.

Don't worry if this is a bit confusing at first. It will become clearer
with practice, and you can ask for it to be explained again and in
different ways.

\end{tcolorbox}

\section*{Multiple regression with interaction
term}\label{multiple-regression-with-interaction-term}
\addcontentsline{toc}{section}{Multiple regression with interaction
term}

\markright{Multiple regression with interaction term}

Above we had the example of two continuous explanatory variables (age
and minutes of exercise) and one continuous response variable (blood
pressure). We saw evidence of an interaction between age and minutes of
exercise on blood pressure.

Here are the first few rows of the data again:

\begin{verbatim}
         bp      age mins_per_week age_class mins_per_week_class
1  61.58319 35.93052     130.94479     30-40             120-140
2  83.86716 42.32743      70.63945     40-50               60-80
3  93.01438 54.37120      54.05203     50-60               40-60
4  82.14413 74.49247     198.53681     70-80             180-200
5  60.13102 32.10092     126.69865     30-40             120-140
6 102.00306 73.90338      42.64163     70-80               40-60
\end{verbatim}

Here is the data in a graph, with age represented by a colour gradient:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-53-1.pdf}}

We can fit a multiple regression model with an interaction term in R as
follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_2cont }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ mins\_per\_week }\SpecialCharTok{*}\NormalTok{ age, }\AttributeTok{data=}\NormalTok{bp\_2cont)}
\end{Highlighting}
\end{Shaded}

We check the model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow=}\FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod\_2cont, }\AttributeTok{add.smooth=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-55-1.pdf}}

These look ok.

Now we do the F-test for the interaction term:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(mod\_2cont)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: bp
                  Df  Sum Sq Mean Sq F value    Pr(>F)    
mins_per_week      1 14307.2 14307.2 1534142 < 2.2e-16 ***
age                1 10220.0 10220.0 1095879 < 2.2e-16 ***
mins_per_week:age  1  1764.8  1764.8  189242 < 2.2e-16 ***
Residuals         96     0.9     0.0                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\textbf{It looks the same as in the two-way ANOVA case!} This is because
we still have two variables and their interaction, so the ANOVA table
has one row for each main effect (mins\_per\_week and age), one row for
the interaction effect (mins\_per\_week:age), and one row for the
residuals.

The F-statistic for the interaction term is very large
(\ensuremath{1.8924178\times 10^{5}}) and the corresponding p-value is
very small (\ensuremath{5.6\times 10^{-160}}). Hence, we conclude that
there is very strong evidence of an interaction between minutes of
exercise and age on blood pressure. The effect of minutes of exercise on
blood pressure depends on age, with a stronger effect for younger people
compared to older people.

\section*{Intepreting main effects and interaction
effects}\label{intepreting-main-effects-and-interaction-effects}
\addcontentsline{toc}{section}{Intepreting main effects and interaction
effects}

\markright{Intepreting main effects and interaction effects}

The term \emph{main effect} refers to the individual effect of each
categorical explanatory variable on the response variable, ignoring the
other variable. For example, the main effect of diet would be the
difference in blood pressure between meat heavy and vegetarian diets,
averaged over both levels of exercise.

However, if there is an interaction between the two categorical
explanatory variables, the main effects may not fully capture the
relationship. The interaction effect indicates that the effect of one
categorical variable on the response variable depends on the level of
the other categorical variable. For example, the effect of diet on blood
pressure may differ between low and high exercise groups.

We can see this in the example:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-57-1.pdf}}

There is a main effect of diet on blood pressure, as well as a main
effect of exercise on blood pressure. However, there is also an
interaction effect between diet and exercise on blood pressure, as the
effect of diet on blood pressure depends on the level of exercise.

It could look otherwise. For example, if we found a (albeit rather
unlikely pattern) of higher blood pressure for vegetarians how exercise
little, and lower blood pressure for vegetarians who exercise a lot, we
would have a pattern as follows:

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-58-1.pdf}}

Here there is no main effect of diet on blood pressure, as the average
blood pressure for meat heavy and vegetarian diets is the same when
averaged over both levels of exercise. There is still a main effect of
exercise on blood pressure, as blood pressure is lower for high exercise
compared to low exercise. And there is still an interaction effect
between diet and exercise on blood pressure, as the effect of diet on
blood pressure depends on the level of exercise.

\textbf{Take home}: Interactions are very interesting, but also will
require careful and nuanced interpretation.

\section*{More than two explanatory
variables}\label{more-than-two-explanatory-variables}
\addcontentsline{toc}{section}{More than two explanatory variables}

\markright{More than two explanatory variables}

All the examples above had two explanatory variables. What if we have
more than two explanatory variables? The principles are the same, but
the models and interpretations become more complex. For example, if we
have three categorical explanatory variables (A, B, C), we can fit a
model with main effects of A, B, and C, as well as all possible
interaction effects (A:B, A:C, B:C, A:B:C). The ANOVA table will have
one row for each main effect, one row for each two-way interaction
effect, one row for the three-way interaction effect, and one row for
the residuals.

Interpretation of three-way interactions can be quite complex, as it
involves understanding how the effect of one variable on the response
variable depends on the levels of the other two categorical variables.
Very careful consideration and visualization of the data are often
necessary to fully understand and communicate the results.

\section*{Review}\label{review-5}
\addcontentsline{toc}{section}{Review}

\markright{Review}

\begin{itemize}
\tightlist
\item
  Interactions are some of the most interesting effects in biology and
  medicine. They occur when the effect of one explanatory variable on
  the response variable depends on the level of another explanatory
  variable.
\item
  Interactions can occur between continuous and categorical explanatory
  variables, or between two categorical explanatory variables, or
  between two continuous explanatory variables.
\item
  Visualization is key to understanding interactions. Use graphs to
  explore and communicate interactions.
\item
  Hypothesis testing for interactions is done using \(F\)-tests on the
  interaction terms in linear models.
\item
  The degrees of freedom for interaction terms depend on the levels of
  the categorical variables involved. Each categorical variable takes
  degrees of freedom equal to the number of levels minus one. Each
  continuous variable takes one degree of freedom. The degrees of
  freedom for the interaction term is the product of the degrees of
  freedom of the individual variables.
\item
  Report interactions carefully, focusing on the biological
  interpretation and including relevant statistics (i.e., the
  \(F\)-statistic, degrees of freedom for the interaction term, degrees
  of freedom for error, and p-value).
\end{itemize}

\section*{Further reading}\label{further-reading-6}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

\begin{itemize}
\tightlist
\item
  Wikipedia is always a good place to start for understanding concepts:
  \href{https://en.wikipedia.org/wiki/Interaction_(statistics)}{Interaction
  term (statistics)}
\item
  This is a nice paper for clinicians on understanding interactions:
  \href{https://publications.aap.org/hospitalpediatrics/article/13/10/e319/194006/How-to-Interact-With-Interactions-What-Clinicians?autologincheck=redirected}{How
  to interact with interactions: what clinicians should know about
  statistical interactions}
\item
  If you want a challenge, and to see how nuanced interactions and
  interpretation can be, have a look at this paper
  \href{https://besjournals.onlinelibrary.wiley.com/doi/10.1111/2041-210X.13714}{Interactions
  in statistical models: Three things to know}
\end{itemize}

\section*{Extras}\label{extras-5}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{Degrees of freedom}\label{degrees-of-freedom}
\addcontentsline{toc}{subsection}{Degrees of freedom}

It can be useful to think about degrees of freedom in terms of how many
parameters / coefficients have to be estimated by the model.

In one-way ANOVA, this is the number of groups one has. So if you have a
categorical variable with five categories (in another language one would
say a factor variable with five levels), there will be five means
estimated, and so the model uses five degrees of freedom. So error
degrees of freedom will be the total number of observation minus five.

In two-way ANOVA (which includes the interaction of the two categorical
variables) we have to estimate a mean for each of the combinations of
the categories. So if we have one categorical variable with three levels
and another with four, there are 3*4 combinations, so 12 means to
estimate. So the model takes 12 degrees of freedom, and the error
degrees of freedom will be the number of observations minus 12.

If we made a two-way ANOVA without an interaction (i.e.~we included only
the main effects) between the two explanatory variables, and we could
have (i.e.~we had performed a fully factorial experiment), well, this is
just weird. Why would we not do the analysis we planned when we designed
the experiment?

(I raised that in case anyone wants to know how to calculate degrees of
freedom with only main effects. In that case, the two explanatory
variables share the same reference level (intercept) so in the case
above, only 6 things would need to be estimated: A shared intercept, and
the other five means. Its a bit like when we previously made a
regression model with two regression lines, both with the same slope.
They shared the same slope -- so only one needed to be estimated. So
often when we use only main effects, getting the degrees of freedom is a
bit tricky.)

Bottom line: more explanatory variables, more interactions, more levels
in categorical explanatory variables = more things have to be estimated
= fewer degrees of freedom for error. And few degrees of freedom for
error is not good, generally speaking.

\subsection*{3d scatter plot}\label{d-scatter-plot}
\addcontentsline{toc}{subsection}{3d scatter plot}

Here is a 3D scatter plot of the data with two continuous explanatory
variables (age and minutes of exercise) and one continuous response
variable (blood pressure). This plot helps to visualise the interaction
between age and minutes of exercise on blood pressure.

(Please note that this plot is best viewed in the HTML version of the
book; in the PDF version, it will appear as a static image.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plotly}\SpecialCharTok{::}\FunctionTok{plot\_ly}\NormalTok{(bp\_2cont, }\AttributeTok{x =} \SpecialCharTok{\textasciitilde{}}\NormalTok{mins\_per\_week, }\AttributeTok{y =} \SpecialCharTok{\textasciitilde{}}\NormalTok{age, }\AttributeTok{z =} \SpecialCharTok{\textasciitilde{}}\NormalTok{bp, }\AttributeTok{type =} \StringTok{"scatter3d"}\NormalTok{, }\AttributeTok{mode =} \StringTok{"markers"}\NormalTok{, }\AttributeTok{color =}\NormalTok{ age, }\AttributeTok{width=}\DecValTok{500}\NormalTok{, }\AttributeTok{height=}\DecValTok{500}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  plotly}\SpecialCharTok{::}\FunctionTok{layout}\NormalTok{(}\AttributeTok{title =} \StringTok{"3D Scatter plot of Blood Pressure vs Exercise and Age"}\NormalTok{,}
         \AttributeTok{scene =} \FunctionTok{list}\NormalTok{(}\AttributeTok{xaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Minutes per week of exercise"}\NormalTok{),}
                      \AttributeTok{yaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Age"}\NormalTok{),}
                      \AttributeTok{zaxis =} \FunctionTok{list}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure"}\NormalTok{))}
\NormalTok{         )}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-59-1.pdf}}

\subsection*{Types of sums of squares}\label{types-of-sums-of-squares}
\addcontentsline{toc}{subsection}{Types of sums of squares}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{A note on anova() and ``Type I'' (sequential) sums of squares}

In this course we often use \texttt{anova()} to test terms in a linear
model. In base R, \texttt{anova()} for an \texttt{lm} uses Type I
(sequential) sums of squares. That means the test for each term is done
in the order the terms appear in the model formula: each term is tested
after the terms before it have already been included.

In balanced designs (equal sample sizes in each group/combination), the
order usually does not matter much. But in unbalanced designs (unequal
sample sizes), the p-value for a ``\,``main effect'' or an interaction
can change if you change the order of terms in the model formula.

This is related to collinearity between explanatory variables. When
explanatory variables are correlated, the variance they explain in the
response variable overlaps. In that case, the order of terms matters
because earlier terms get to ``claim'' more of the shared variance.

So: when you see an ANOVA table from \texttt{anova()}, remember that it
is testing terms sequentially, not ``all at once''.

\end{tcolorbox}

\subsection*{Box and whisker plot for two
cats}\label{box-and-whisker-plot-for-two-cats}
\addcontentsline{toc}{subsection}{Box and whisker plot for two cats}

Often you will see box and whisker plots used to visualise data with two
categorical explanatory variables. Here is how to make such a plot in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_2cat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{diet, }\AttributeTok{y=}\NormalTok{bp, }\AttributeTok{fill=}\NormalTok{exercise)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{position=}\FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.8}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Blood pressure vs diet and exercise"}\NormalTok{,}
       \AttributeTok{x=}\StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Blood pressure"}\NormalTok{,}
       \AttributeTok{fill=}\StringTok{"Exercise"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-60-1.pdf}}

I (Owen) prefer to show the individual data points whenever possible, so
here is a box and whisker plot with the individual data points overlaid:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_2cat, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{diet, }\AttributeTok{y=}\NormalTok{bp, }\AttributeTok{fill=}\NormalTok{exercise)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{position=}\FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.8}\NormalTok{), }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{col=}\NormalTok{exercise), }\AttributeTok{position=}\FunctionTok{position\_jitterdodge}\NormalTok{(}\AttributeTok{jitter.width=}\FloatTok{0.2}\NormalTok{, }\AttributeTok{dodge.width=}\FloatTok{0.8}\NormalTok{), }\AttributeTok{size=}\DecValTok{2}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.7}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Blood pressure vs diet and exercise"}\NormalTok{,}
       \AttributeTok{x=}\StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Blood pressure"}\NormalTok{,}
       \AttributeTok{fill=}\StringTok{"Exercise"}\NormalTok{,}
       \AttributeTok{col=}\StringTok{"Exercise"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-61-1.pdf}}

I, and others, prefer to not use bar plots with error bars to visualise
data with categorical explanatory variables. Bar plots hide the
individual data points and can be misleading. Box and whisker plots are
better, but still hide some of the data. Scatter plots with jittered
points are often the best way to visualise such data.

In case you must use bar plots with error bars, here is how to make one
in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{grouped\_data }\OtherTok{\textless{}{-}} \FunctionTok{group\_by}\NormalTok{(bp\_2cat, diet, exercise) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean\_bp =} \FunctionTok{mean}\NormalTok{(bp), }\AttributeTok{sd\_bp =} \FunctionTok{sd}\NormalTok{(bp), }\AttributeTok{n =} \FunctionTok{n}\NormalTok{(), }\AttributeTok{se\_bp =}\NormalTok{ sd\_bp }\SpecialCharTok{/} \FunctionTok{sqrt}\NormalTok{(n), }\AttributeTok{.groups =} \StringTok{"drop"}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(grouped\_data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{diet, }\AttributeTok{y=}\NormalTok{mean\_bp, }\AttributeTok{fill=}\NormalTok{exercise)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat=}\StringTok{"identity"}\NormalTok{, }\AttributeTok{position=}\FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.8}\NormalTok{), }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{mean\_bp }\SpecialCharTok{{-}}\NormalTok{ se\_bp, }\AttributeTok{ymax=}\NormalTok{mean\_bp }\SpecialCharTok{+}\NormalTok{ se\_bp), }\AttributeTok{position=}\FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width=}\FloatTok{0.8}\NormalTok{), }\AttributeTok{width=}\FloatTok{0.2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title=}\StringTok{"Blood pressure vs diet and exercise"}\NormalTok{,}
       \AttributeTok{x=}\StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y=}\StringTok{"Blood pressure"}\NormalTok{,}
       \AttributeTok{fill=}\StringTok{"Exercise"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{7.1-interactions_files/figure-pdf/unnamed-chunk-62-1.pdf}}

\bookmarksetup{startatroot}

\chapter*{Count data (L8)}\label{count-data-l8}
\addcontentsline{toc}{chapter}{Count data (L8)}

\markboth{Count data (L8)}{Count data (L8)}

In all the previous chapters, we have focused on \textbf{linear models}.
These are suitable for continuous response variables that can,
mathematically, take negative values. However, response variables can be
otherwise. For example, we might have a response variable that is the
number of offspring produced by a mother. This is a \emph{count} type of
response variable. In this chapter, you'll see why the models we used so
far are usually inappropriate for count data. Then you'll learn about a
more appropriate type of model: \textbf{Generalized Linear Models
(GLMs)}.

\section*{Introduction}\label{introduction-5}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

So far in BIO144, we have focused on \textbf{linear models} fitted using
\texttt{lm()}. These models assume:

\begin{itemize}
\tightlist
\item
  A \textbf{continuous response variable} that can take any real value.
\item
  Normally distributed residuals.
\item
  Constant variance (homoscedasticity).
\end{itemize}

Linear models are powerful, but these assumptions can be violated in
biological data. In this chapter we move beyond linear models to handle
an important new type of response variable: \textbf{counts}. We
introduce \textbf{Generalized Linear Models (GLMs)}, which extend linear
models by allowing the response variable to follow distributions other
than the normal distribution.

By the end of this chapter, you should be able to:

\begin{itemize}
\tightlist
\item
  Recognise when linear regression is inappropriate
\item
  Understand the core components of a GLM
\item
  Fit and interpret Poisson regression models
\item
  Diagnose common problems such as overdispersion and zero inflation
\end{itemize}

\section*{Example: Soay sheep}\label{example-soay-sheep}
\addcontentsline{toc}{section}{Example: Soay sheep}

\markright{Example: Soay sheep}

A feral population of Soay sheep on the island of Hirta (Scotland) has
been studied extensively. Ecologists were interested in whether the
\textbf{body mass of female sheep} influences their fitness, measured as
\textbf{lifetime reproductive success} (number of offspring produced
over a lifetime).

\textbf{Question:} Are heavier females fitter than lighter females?

Read in an example dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{soay }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/soay\_sheep.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here are the first few rows of the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(soay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  body.size fitness
1  53.99529      12
2  32.69467       0
3  33.55580       2
4  43.20078       2
5  43.34102       4
6  59.52711       7
\end{verbatim}

As always, we start by exploring the data visually:

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\subsection*{The wrong analysis}\label{the-wrong-analysis}
\addcontentsline{toc}{subsection}{The wrong analysis}

Let's analyse the data with linear regression, treating counts as if
they were continuous.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_soay\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(fitness }\SpecialCharTok{\textasciitilde{}}\NormalTok{ body.size, }\AttributeTok{data =}\NormalTok{ soay)}
\end{Highlighting}
\end{Shaded}

The model checking plots show clear violations of linear regression
assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod\_soay\_lm, }\AttributeTok{which =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-7-1.pdf}}

The qq-plot looks ok-ish, though the larger residuals show a tendency to
be larger than expected. The scale-location plot shows that variance
increases with fitted values, violating homoscedasticity. Also, there is
a clear non-linear pattern in the residuals vs fitted plot, suggesting
that the linear model is not capturing the relationship well.

When we plot the fitted line, we see the linear relationship:

\begin{verbatim}
`geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-8-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Why this matters}\\
Violating model assumptions can lead to biased estimates, incorrect
standard errors, and misleading p-values.

\end{tcolorbox}

Another issue remains, however: linear regression can predict negative
counts, which are impossible. We can see this in a plot of the data and
the fitted regression line.

\subsection*{Why linear regression fails for count
data}\label{why-linear-regression-fails-for-count-data}
\addcontentsline{toc}{subsection}{Why linear regression fails for count
data}

\begin{itemize}
\tightlist
\item
  The normal distribution is for continuous variables.
\item
  It allows negative values.
\item
  It assumes constant variance.
\end{itemize}

However, many biological response variables are:

\begin{itemize}
\tightlist
\item
  \textbf{Counts} (e.g.~offspring, parasites, species)
\item
  \textbf{Binary responses} (e.g.~alive/dead)
\item
  \textbf{Proportions}
\end{itemize}

In these cases, forcing the data into a linear model often leads to
invalid predictions and misleading inference. \textbf{Generalized Linear
Models (GLMs)} solve this problem.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Key idea} GLMs are not a replacement for linear models --- they
are a \emph{generalisation} of them. Linear regression is a special case
of a GLM.

\end{tcolorbox}

\section*{From LM to GLM}\label{from-lm-to-glm}
\addcontentsline{toc}{section}{From LM to GLM}

\markright{From LM to GLM}

\textbf{A linear model is actually a special case of a generalized
linear model.}

What you already know: in a linear model we write:

\[y_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)} + \epsilon_i, \quad \epsilon_i \sim N(0, \sigma^2)\]

In words, this means an observed response variable \(y_i\) is modelled
as a linear combination of explanatory variables plus normally
distributed error.

This implies that

\[y_i \sim N(\mu_i, \sigma^2)\]

where \(\mu_i\) is the mean of the normal distribution for observation
\(i\), and

\[\mu_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\]

Moving to things you don't know:

A generalised linear model has three components: a random component
(\emph{family}), a systematic component (\emph{linear predictor}), and a
link function. In a linear model (like the ones you already saw), these
are as follows:

\begin{itemize}
\tightlist
\item
  \textbf{Random component (family)}: Normal distribution
  \(N(\mu_i, \sigma^2)\)
\item
  \textbf{Systematic component (linear predictor)}:
  \(\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)
\item
  \textbf{Link function}: Identity link, \(\mu_i = \eta_i\).
\end{itemize}

Putting this all together in a description of a generalised linear model
we have:

\textbf{Family}: \(y_i \sim N(\mu_i, \sigma^2)\)

\textbf{Linear predictor}:
\(\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)

\textbf{Link function}: \(\mu_i = \eta_i\)

In R, we use the function \texttt{glm()} to fit generalised linear
models. For a normal linear model of the Soay sheep data, we would
write:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_soay\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(fitness }\SpecialCharTok{\textasciitilde{}}\NormalTok{ body.size, }\AttributeTok{data =}\NormalTok{ soay, }\AttributeTok{family =} \FunctionTok{gaussian}\NormalTok{(}\AttributeTok{link =} \StringTok{"identity"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Here we have specified the family as Gaussian (i.e., Normal) with an
identity link. This model produces the same results as the linear model
we fitted earlier with \texttt{lm()}. And we can get the following model
summary:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod\_soay\_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = fitness ~ body.size, family = gaussian(link = "identity"), 
    data = soay)

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -5.16768    0.68895  -7.501  2.9e-11 ***
body.size    0.21163    0.01501  14.097  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 4.805055)

    Null deviance: 1425.8  on 99  degrees of freedom
Residual deviance:  470.9  on 98  degrees of freedom
AIC: 444.73

Number of Fisher Scoring iterations: 2
\end{verbatim}

Compare that to the linear model summary:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod\_soay\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = fitness ~ body.size, data = soay)

Residuals:
    Min      1Q  Median      3Q     Max 
-4.9760 -1.5468 -0.2568  0.9733  6.3887 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) -5.16768    0.68895  -7.501  2.9e-11 ***
body.size    0.21163    0.01501  14.097  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.192 on 98 degrees of freedom
Multiple R-squared:  0.6697,    Adjusted R-squared:  0.6664 
F-statistic: 198.7 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

The coefficients table is exactly the same. Other parts of the output
differ slightly because \texttt{glm()} uses maximum likelihood
estimation (MLE) while \texttt{lm()} uses least squares estimation. Put
that aside for now; the key point is that both R functions fit the same
model when using a normal family with an identity link.

So, you now know what is a generalised linear model.

In one sentence, a GLM is a model where the response variable follows a
specified distribution (family), the mean of that distribution is linked
to a linear combination of explanatory variables (linear predictor) via
a link function. We will now look at a specific type of GLM for count
data: the Poisson GLM.

\section*{Poisson GLM}\label{poisson-glm}
\addcontentsline{toc}{section}{Poisson GLM}

\markright{Poisson GLM}

We need three components for a GLM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A probability distribution (family) for the response variable.
\item
  A linear predictor.
\item
  A link function relating the linear predictor to the mean of the
  distribution.
\end{enumerate}

Let's start with a discussion of the probability distribution (family)
for count data. And what we're aiming for is a probability distribution
that we expect to be a reasonably good starting point for modelling
count data. A common probability model for counts is the \textbf{Poisson
distribution}. The Poisson distribution is often the default starting
point for modelling counts because it is the simplest distribution that
respects characteristics of count data: discreteness, non-negativity,
and increasing variance.

Unlike the normal distribution, which has two parameters (mean and
spread), the Poisson distribution has only one parameter: the mean
\(\lambda\). In the Poisson distribution, the mean and variance are
equal.

In a Poisson distribution, the probability of observing a count \(y\) is
given by:

\(P(Y = y) = \frac{\lambda^y e^{-\lambda}}{y!}\)

where:

\begin{itemize}
\tightlist
\item
  \(Y\) is a random variable representing the count.
\item
  \(y = 0, 1, 2, \ldots\)
\item
  \(\lambda > 0\) is the mean (and variance) of the distribution.
\end{itemize}

In R, we can get the probability of observing a count of 3, when the
expected count is 5, using the \texttt{dpois()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dpois}\NormalTok{(}\DecValTok{3}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1403739
\end{verbatim}

And we expect the probability of observing a count of 5 when the
expected count is 5 to be higher:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dpois}\NormalTok{(}\DecValTok{5}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1754674
\end{verbatim}

What happens when we ask for the probability of observing a count of 3.5
when the expected count is 5? Or a count of -1?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dpois}\NormalTok{(}\FloatTok{3.5}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in dpois(3.5, lambda = 5): non-integer x = 3.500000
\end{verbatim}

\begin{verbatim}
[1] 0
\end{verbatim}

The warning message tells us that the Poisson distribution is only
defined for non-negative integers. This is a key property of the Poisson
distribution: it is only defined for integers (0, 1, 2, \ldots).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dpois}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{lambda =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0
\end{verbatim}

And we see that the probability of observing a negative count is zero.

Here are a few of Poisson distributions with different means:

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-16-1.pdf}}

The Poisson distribution has two important properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  It is defined only for \textbf{non-negative integers} (0, 1, 2,
  \ldots)
\item
  The \textbf{mean and variance are equal}.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Mean--variance relationship}\\
In a Poisson distribution, the mean and variance are equal. This
captures an important feature of count data, but it will later lead to
the concept of \emph{overdispersion}.

\end{tcolorbox}

\textbf{So, in a Poisson GLM we assume a Poisson distribution (family)
for the response variable.}

\[y_i \sim \text{Poisson}(\mu_i)\]

where \(\mu_i = \lambda_i\) is the expected count for observation \(i\).

Next we need to define the linear predictor (and then we finish with the
link function).

The linear predictor is the same as in linear regression:

\(\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)

It is just the linear combination of explanatory variables and
coefficients. Nothing new here.

Finally, we need to define the link function that relates the linear
predictor to the expected count \(\mu_i\).

We cannot use the identity link here, because that would allow negative
expected counts. Instead, we use the \textbf{log link}:

\[\eta_i = \log(\mu_i)\]

which implies:

\(\mu_i = \exp(\eta_i)\)

The log link ensures that the expected count is always positive,
regardless of the values of the \(\beta\) parameters and explanatory
variables. Whatever the value of the linear predictor \(\eta_i\), the
exponential of it \(\exp(\eta_i)\) is always positive.

Putting this all together, a Poisson GLM can be summarised as:

\textbf{Family}: \(y_i \sim \text{Poisson}(\mu_i)\)

\textbf{Linear predictor}:
\(\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)

\textbf{Link function}: \(\log(\mu_i) = \eta_i\)

We could also write this as:

\[y_i \sim \mathrm{Poisson}\!\left(\exp\!\left(\beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\right)\right)\]

\subsection*{Parameter estimation}\label{parameter-estimation}
\addcontentsline{toc}{subsection}{Parameter estimation}

We observe the counts \(y_i\), and the model predicts the expected
counts \(\mu_i\). The GLM estimates the parameters
\(\beta_0, \beta_1, \ldots, \beta_p\) that make the observed data most
probable under the assumed Poisson model. This is done using maximum
likelihood estimation (MLE). The likelihood of the data given the model
is maximised.

Recall that we saw that the summary information of the linear model and
the GLM with normal family and identity link were different. This is
because of differences in estimation methods (least squares vs MLE). In
GLMs, we always use MLE for parameter estimation. In this course we will
not go into the mathematical details of MLE, but the key idea is that we
find parameter values that make the observed data most probable under
the assumed model. This is analogous to least squares estimation in
linear models, which is a special case of MLE for Normally distributed
errors.

\section*{R - Poisson GLM}\label{r---poisson-glm}
\addcontentsline{toc}{section}{R - Poisson GLM}

\markright{R - Poisson GLM}

When fitting a Poisson GLM in R, we use the \texttt{glm()} function,
specifying the family as \texttt{poisson}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{soay\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(fitness }\SpecialCharTok{\textasciitilde{}}\NormalTok{ body.size, }\AttributeTok{data =}\NormalTok{ soay, }\AttributeTok{family =} \FunctionTok{poisson}\NormalTok{(}\AttributeTok{link =} \StringTok{"log"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

Specifying \texttt{family\ =\ poisson} tells R to use the Poisson
distribution with a log link function by default. We also specify the
link explicitly as \texttt{link\ =\ "log"}. You will, however, often
only see \texttt{family\ =\ poisson} and not the log link, because the
log link is the default for the Poisson family.

\subsection*{Checking model
assumptions}\label{checking-model-assumptions}
\addcontentsline{toc}{subsection}{Checking model assumptions}

As always, we should check model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(soay\_glm, }\AttributeTok{which =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-18-1.pdf}}

The QQ-plot is rather concerning. However, QQ-plots in GLMs are not
testing normality of residuals in the same way as for linear models, so
their interpretation differs. The \emph{deviance residuals} should
approximately follow a normal distribution if the model fits well. Here,
there are some deviations from normality, which could be somewhat
concerning, but for now we will focus on the other plots. (If you're
interested in what are deviance residuals, please see the \emph{Extras}
section at the end of this chapter.)

The other plots look much better than for the linear model. The
residuals vs fitted plot shows no obvious pattern, and the
scale-location plot shows more constant variance.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

You must specify the \textbf{family} in \texttt{glm()}. If you omit the
link function, R uses the default link for that family.

\end{tcolorbox}

\subsection*{Interpreting coefficients}\label{interpreting-coefficients}
\addcontentsline{toc}{subsection}{Interpreting coefficients}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(soay\_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = fitness ~ body.size, family = poisson(link = "log"), 
    data = soay)

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.253092   0.210186  -5.962 2.49e-09 ***
body.size    0.053781   0.003735  14.400  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 333.105  on 99  degrees of freedom
Residual deviance:  97.783  on 98  degrees of freedom
AIC: 378.16

Number of Fisher Scoring iterations: 5
\end{verbatim}

So we have an intercept of \(\beta_0 = -1.253\). Because of the log
link, this means that when body size is 0 kg (which is outside the range
of our data), the expected count of fitness is \(\exp(-1.253) = 0.285\)
offspring.

But what is the biological meaning of the slope for body size:
\(\beta_1 = 0.054\)? It must be describing how something changes as a
result of 1 unit change in body size (1 kg). But what?

Lets's find out by calculating the expected fitness for two body sizes
that differ by 1 kg.

For a sheep of body size 40 kg, the linear predictor is:

\(\eta = \beta_0 + \beta_1 \times \text{body.size} = -1.253 + 0.054 \times 40 = 0.907\)

But this doesn't mean we predict a fitness of 0.907 offspring. Instead,
we need to back-transform this using the link function. Since we used a
log link, we exponentiate the linear predictor to get the expected
count:

\(E(y_{40}) = \exp(\eta) = \exp(0.907) = 2.48\)

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Some of the calculations in this text are done with rounded values for
clarity. In practice, you should use the full precision values from R to
avoid rounding errors. You will see below that the predicted value
calculated here (2.48) differs from that calculated in R (2.46) due to
rounding. Always do calculations with full precision values from R, and
only round the final result if needed.

\end{tcolorbox}

Let us now calculate the expected fitness for a sheep of body size 41
kg. First calculate the value of the linear predictor:

\(\eta = -1.253 + 0.054 \times 40 + 0.054 = 0.907 + 0.054 = 0.961\)

The expected count is then \(E(y) = \exp(0.961) = 2.61\)

Now let's compare the value of the predicted fitness for the 41 and 40
kg sheep:

\(\frac{E(y_{41})}{E(y_{40})} = \frac{\exp(0.907 + 0.054)}{\exp(0.907)}\)

And because \(\frac{\exp(a)} {\exp(b)} = \exp(a - b)\), we have:

\(\frac{E(y_{41})}{E(y_{40})} = \exp(0.054) = 1.055\)

This means that each increase in body size of 1 kg multiplies the
expected count of fitness by a factor of \(\exp(0.054) = 1.055\) (i.e.,
a 5.5\% increase).

This shows that the exponentiated coefficient \(\exp(\beta_1)\) gives
the multiplicative change in expected count for a one-unit change in the
explanatory variable.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

A one-unit increase in an explanatory variable multiplies the expected
count by \(\exp(\beta)\).

\end{tcolorbox}

This means that for each additional kilogram of body size, the expected
count of fitness increases by a factor of approximately 1.06 (i.e., a
6\% increase). We use the number multiplicatively because of the log
link function.

Pleaes note that of course all these calculations can be done in R. For
example, for a body size of 40 kg:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{body\_size\_example }\OtherTok{\textless{}{-}} \DecValTok{40}
\NormalTok{linear\_predictor }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(soay\_glm)[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{+} \FunctionTok{coef}\NormalTok{(soay\_glm)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ body\_size\_example}
\NormalTok{expected\_count }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(linear\_predictor)}
\NormalTok{expected\_count}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
(Intercept) 
   2.455011 
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Although counts can only be integers, the expected value from a Poisson
model can be any positive real number. This is because the expected
value is a mean over many possible counts.

\end{tcolorbox}

\subsection*{Analysis of deviance}\label{analysis-of-deviance}
\addcontentsline{toc}{subsection}{Analysis of deviance}

There is something technically different that we have glossed over until
now: how model fit is assessed. In linear regression we estimate
parameters by minimizing the sum of squared residuals. In GLMs, we use
\textbf{maximum likelihood estimation (MLE)}. In this course we will not
go into the mathematical details of MLE, but the key idea is that we
find the parameter values that make the observed data most probable
under the assumed model (just like in least squares). Instead of
minimising sums of squares, we maximise the \textbf{likelihood} of the
data given the model. Maximising the likelihood is equivalent to
minimising the \textbf{deviance}, which is a measure of model fit based
on likelihoods. Hence, when we fit a GLM in R, we get output including
the \textbf{deviance} (and not sums of squares):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(soay\_glm, }\AttributeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Deviance Table

Model: poisson, link: log

Response: fitness

Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                         99     333.11              
body.size  1   235.32        98      97.78 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the output we see the deviance for the null model and the fitted
model, as well as the change in deviance when adding explanatory
variables. We can use a chi-squared test to assess whether adding
explanatory variables significantly improves model fit.

We use a chi-squared test here because, under certain regularity
conditions, the change in deviance between nested models follows a
chi-squared distribution with degrees of freedom equal to the difference
in the number of parameters. If you are interested in what this means,
you can read more about it in more advanced statistics textbooks.

You may be wondering if we can calculate an r-squared value for GLMs.
While there is no direct equivalent of r-squared for GLMs, several
pseudo-r-squared measures have been proposed. However, these are not as
widely used or interpreted as r-squared in linear regression. If you're
interested, check out the \emph{Extras} section at the end of this
chapter for more information.

\subsection*{Reporting}\label{reporting}
\addcontentsline{toc}{subsection}{Reporting}

When reporting results from a Poisson GLM, we can make a graph and a
sentence describing the pattern and related statistics.

When we wish to make a graph of the fitted relationship, we can either
make it with the y-axis on the log scale (linear predictor scale) or
back-transform to the original count scale. I usually prefer the
original scale, as it is easier to interpret.

The first step is to create a new data frame with a sequence of body
sizes for prediction, and errors for the 95\% confidence intervals:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{body.size =} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(soay}\SpecialCharTok{$}\NormalTok{body.size), }\FunctionTok{max}\NormalTok{(soay}\SpecialCharTok{$}\NormalTok{body.size), }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{))}
\NormalTok{predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(soay\_glm, }\AttributeTok{newdata =}\NormalTok{ new\_data, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{new\_data}\SpecialCharTok{$}\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(predictions}\SpecialCharTok{$}\NormalTok{fit)}
\NormalTok{new\_data}\SpecialCharTok{$}\NormalTok{lower }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(predictions}\SpecialCharTok{$}\NormalTok{fit }\SpecialCharTok{{-}} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{se.fit)}
\NormalTok{new\_data}\SpecialCharTok{$}\NormalTok{upper }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(predictions}\SpecialCharTok{$}\NormalTok{fit }\SpecialCharTok{+} \FloatTok{1.96} \SpecialCharTok{*}\NormalTok{ predictions}\SpecialCharTok{$}\NormalTok{se.fit)}
\end{Highlighting}
\end{Shaded}

Note that we back-transform the predictions and confidence intervals
using the exponential function, since the link function is the log. And
note that we must calculate the confidence intervals on the log scale
first, and then back-transform them.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

If we only wanted the fitted value (and not confidence intervals) we
could have used \texttt{type\ =\ "response"} in the \texttt{predict()}
function to get predictions on the original count scale
(back-transformed from the log scale). When we don't specified this, we
would get predictions on the log scale--this is the default behavior.

\end{tcolorbox}

Now we can plot the data and the fitted relationship with confidence
intervals:

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-23-1.pdf}}

Excellent! We can now write a nice sentence for our results:

Reproductive fitness (in terms of lifetime number of offspring)
increased significantly with body mass, with a unit increase in body
mass associated with a multiplicative increase in expected fitness of
1.06 (95\% CI: 1.05, 1.06; \(\chi^2 =\) 235.32, \(df = 1\), \(p <\)
\ensuremath{4.1\times 10^{-53}}).

\section*{Well done!}\label{well-done}
\addcontentsline{toc}{section}{Well done!}

\markright{Well done!}

We have made our first steps into the world of GLMs and Poisson
regression for count data. You should now be able to:

\begin{itemize}
\tightlist
\item
  Recognise why linear regression is often inappropriate for count data.
\item
  Understand the components of a GLM: linear predictor, link function,
  and family.
\item
  Fit a Poisson GLM in R using \texttt{glm()}.
\item
  Interpret coefficients from a Poisson GLM.
\end{itemize}

Next, we will look at common issues that arise when fitting Poisson
models, such as overdispersion and zero inflation.

\section*{Overdispersion}\label{overdispersion}
\addcontentsline{toc}{section}{Overdispersion}

\markright{Overdispersion}

In a Poisson model, mean and variance are assumed equal. In practice,
variance often exceeds the mean, a phenomenon called
\textbf{overdispersion}.

Common causes include:

\begin{itemize}
\tightlist
\item
  Unmeasured explanatory variables. This means important explanatory
  variables are missing from the model, leading to extra variability.
\item
  Individual heterogeneity. This can arise when individuals differ in
  ways not captured by measured explanatory variables, leading to extra
  variability in counts.
\item
  Correlated observations. This can occur when observations are not
  independent, such as repeated measures on the same individual.
\end{itemize}

A simple check for overdispersion is to compare the residual deviance to
the residual degrees of freedom:

\(\text{Dispersion} = \frac{\text{Residual Deviance}}{\text{Residual DF}}\)

If this ratio is substantially greater than 1 (e.g., \textgreater{} 1.5
or 2), it indicates overdispersion.

Check this for our Soay sheep model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dispersion\_soay }\OtherTok{\textless{}{-}} \FunctionTok{deviance}\NormalTok{(soay\_glm) }\SpecialCharTok{/} \FunctionTok{df.residual}\NormalTok{(soay\_glm)}
\NormalTok{dispersion\_soay}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9977877
\end{verbatim}

Here, the dispersion is quite close to 1 (it is 1), indicating little
overdispersion. However, in many real datasets, overdispersion is
common.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Ignoring overdispersion leads to \textbf{anti-conservative p-values}
(too small). This would be a typical example of \textbf{Type I error
inflation}. Type I errors occur when we incorrectly reject a true null
hypothesis, leading to false positives. In the context of statistical
modeling, ignoring overdispersion can result in underestimating standard
errors, which in turn leads to smaller p-values. Consequently, we may
conclude that an effect is statistically significant when it is not,
thereby increasing the likelihood of Type I errors.

\end{tcolorbox}

\subsection*{Quasi-Poisson and negative binomial
models}\label{quasi-poisson-and-negative-binomial-models}
\addcontentsline{toc}{subsection}{Quasi-Poisson and negative binomial
models}

One solution is the \textbf{quasi-Poisson} model, which estimates an
additional dispersion parameter:

For this, we can use \texttt{family\ =\ quasipoisson} in \texttt{glm()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{soay\_quasi }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(fitness }\SpecialCharTok{\textasciitilde{}}\NormalTok{ body.size, }\AttributeTok{data =}\NormalTok{ soay, }\AttributeTok{family =}\NormalTok{ quasipoisson)}
\FunctionTok{summary}\NormalTok{(soay\_quasi)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
glm(formula = fitness ~ body.size, family = quasipoisson, data = soay)

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) -1.253092   0.209446  -5.983 3.59e-08 ***
body.size    0.053781   0.003722  14.450  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for quasipoisson family taken to be 0.992977)

    Null deviance: 333.105  on 99  degrees of freedom
Residual deviance:  97.783  on 98  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
\end{verbatim}

\section*{Zero inflation}\label{zero-inflation}
\addcontentsline{toc}{section}{Zero inflation}

\markright{Zero inflation}

A special case of overdispersion arises when there are \textbf{more
zeros than expected} under a Poisson model. This is called \textbf{zero
inflation}.

Examples include:

\begin{itemize}
\tightlist
\item
  Number of cigarettes smoked (many people do not smoke, more than can
  be modelled by a Poisson distribution)
\item
  Parasite counts (many host individual have no parasites, more than can
  be modelled by a Poission distribution)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Zero inflation often reflects \textbf{two processes}: whether an
observation can be non-zero at all, and how large it is if it is. E.g.,
whether an individual smokes at all, and then how many cigarettes they
smoke if they do.

\end{tcolorbox}

In this course we will not describe in detail or practice fitting
zero-inflated models, but they are an important tool for count data with
many zeros. Examples of models include zero-inflated Poisson (ZIP) and
zero-inflated negative binomial (ZINB) models. These models combine a
count model (e.g., Poisson or negative binomial) with a separate model
for the probability of being a structural zero. There is also a model
called the \textbf{hurdle model}, which is similar but has a different
interpretation.

\section*{Multiple explanatory
variables}\label{multiple-explanatory-variables}
\addcontentsline{toc}{section}{Multiple explanatory variables}

\markright{Multiple explanatory variables}

GLMs can include multiple explanatory variables, just like linear
models. And they can include only categorical variables, only continuous
variables, or a mix of both.

For example, we could include parasite load as an additional predictor
of fitness in the Soay sheep data:

Read in the updated dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{soay }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"datasets/soay\_sheep\_with\_parasites.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Here is the first few rows of the updated dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(soay)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  body.size fitness parasite.load
1  53.99529       1             2
2  32.69467       1             6
3  33.55580       0             5
4  43.20078       1             6
5  43.34102       0             7
6  59.52711       3             6
\end{verbatim}

We can visualise the relationship between parasite load and fitness:

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-29-1.pdf}}

It looks like higher parasite loads are associated with lower fitness.

We can fit a Poisson GLM with both body size and parasite load as
explanatory variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{soay\_glm2 }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(fitness }\SpecialCharTok{\textasciitilde{}}\NormalTok{ body.size }\SpecialCharTok{+}\NormalTok{ parasite.load, }\AttributeTok{data =}\NormalTok{ soay, }\AttributeTok{family =}\NormalTok{ poisson)}
\end{Highlighting}
\end{Shaded}

As always we check model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(soay\_glm2, }\AttributeTok{which =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{4}\NormalTok{, }\AttributeTok{add.smooth =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{8.1-GLM1-count-data_files/figure-pdf/unnamed-chunk-31-1.pdf}}

The model checking plots look good. We can summarise the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(soay\_glm2, }\AttributeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Deviance Table

Model: poisson, link: log

Response: fitness

Terms added sequentially (first to last)

              Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                             99     226.57              
body.size      1   95.723        98     130.85 < 2.2e-16 ***
parasite.load  1   11.063        97     119.78 0.0008809 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see that both body size and parasite load significantly affect
fitness. Body size has a positive effect, while parasite load has a
negative effect. These effects are interpreted conditional on the other
explanatory variable being held constant, just as in multiple linear
regression.

\section*{Review}\label{review-6}
\addcontentsline{toc}{section}{Review}

\markright{Review}

In this chapter we have introduced Generalized Linear Models (GLMs) for
count data, focusing on Poisson regression. Key points include:

\begin{itemize}
\tightlist
\item
  Linear regression is often inappropriate for count data due to
  violations of assumptions.
\item
  GLMs extend linear models by allowing different distributions and link
  functions.
\item
  Poisson GLMs use the Poisson distribution with a log link to model
  counts.
\item
  Coefficients are interpreted on the log scale, with exponentiation
  giving multiplicative effects.
\item
  Overdispersion and zero inflation are common issues that need to be
  addressed.
\item
  GLMs can include multiple explanatory variables, just like linear
  models.
\end{itemize}

With this foundation, you are now equipped to analyse count data using
GLMs in R. In the next chapter, we will explore further extensions and
applications of GLMs.

\section*{Further reading}\label{further-reading-7}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

\begin{itemize}
\tightlist
\item
  Chapter 14 of \emph{The R Book} by Crawley (2012) provides a
  comprehensive introduction to GLMs in R, including Poisson regression.
  The R is a bit old fashioned but the concepts are well explained.
\item
  Generalized Linear Models With Examples in R (2018) by Peter K. Dunn
  and Gordon K. Smyth.
\end{itemize}

\section*{Extras}\label{extras-6}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\subsection*{Deviance residuals}\label{deviance-residuals}
\addcontentsline{toc}{subsection}{Deviance residuals}

In linear models, residuals are simply the difference between observed
and predicted values. In GLMs, residuals are more complex due to the
non-normal distribution of the response variable.

Recall from above that a Poisson GLM is mathematically summarised as:

\(y_i \sim \text{Poisson}(\lambda_i)\)

where
\(\log(\lambda_i) = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)

The \textbf{deviance residuals} are a type of residual used in GLMs to
assess model fit. They are derived from the concept of deviance, which
measures the difference between the fitted model and a saturated model
(a model that perfectly fits the data).

The deviance residual for each observation is calculated as:
\(r_i = \text{sign}(y_i - \hat{y}_i) \sqrt{2 \left( y_i \log\left(\frac{y_i}{\hat{y}_i}\right) - (y_i - \hat{y}_i) \right)}\)

where: - \(y_i\) is the observed count - \(\hat{y}_i\) is the predicted
count from the model

Deviance residuals have the following properties:

\begin{itemize}
\tightlist
\item
  They are approximately normally distributed if the model fits well.
  \textbf{Therefore they can be used in QQ-plots to assess model fit.}
\item
  They can be positive or negative, indicating whether the observed
  count is above or below the predicted
\item
  They are used in diagnostic plots to assess model fit, similar to
  residuals in linear models.
\end{itemize}

\subsection*{Pseudo-R-squared for GLMs}\label{pseudo-r-squared-for-glms}
\addcontentsline{toc}{subsection}{Pseudo-R-squared for GLMs}

While there is no direct equivalent of r-squared for GLMs, several
pseudo-r-squared measures have been proposed. These measures aim to
provide an indication of model fit similar to r-squared in linear
regression, but they do not have the same interpretation.

To understand how pseudo-r-squared works, we first need to understand
the concept of deviance and log-likelihoods in GLMs. Deviance is a
measure of model fit based on likelihoods. It compares the fitted model
to a null model (a model with only an intercept) and a saturated model
(a model that perfectly fits the data). Let's break that down a bit
further:

\textbf{Residual deviance} is calculated as:

\(D = -2 \left( \log L(\text{fitted model}) - \log L(\text{saturated model}) \right)\)

where - \(\log L(\text{fitted model})\) is the log-likelihood of the
fitted model and - \(\log L(\text{saturated model})\) is the
log-likelihood of the saturated model.

The log-likelihood measures how well the model explains the observed
data; higher values indicate better fit. The fitted model is the model
we have estimated, while the saturated model is a hypothetical model
that perfectly fits the data.

\textbf{Null deviance} is calculated similarly, but for the null model:

\(D_{null} = -2 \left( \log L(\text{null model}) - \log L(\text{fitted model}) \right)\)

where - \(\log L(\text{null model})\) is the log-likelihood of the null
model.

(By the way, the -2 factor is included to make the deviance comparable
to the chi-squared distribution, with degrees of freedom equal to the
difference in the number of parameters between models. This is useful
for hypothesis testing.)

So:

\begin{itemize}
\tightlist
\item
  \textbf{Null deviance}: This is the deviance of the null model, which
  includes only an intercept. It represents how well a model with no
  predictors fits the data.
\item
  \textbf{Residual deviance}: This is the deviance of the fitted model.
  It represents how well the model with predictors fits the data
  relative to the null model.
\end{itemize}

One common pseudo-r-squared measure is McFadden's R-squared, defined as:

\(R^2_{McFadden} = 1 - \frac{D_{fitted}}{D_{null}}\) where
\(D_{fitted}\) is the residual deviance of the fitted model and
\(D_{null}\) is the null deviance.

This measure ranges from 0 to 1, with higher values indicating better
model fit. However, it is important to note that pseudo-r-squared values
for GLMs are generally lower than r-squared values in linear regression,
and they should be interpreted with caution.

There is much more to learn about the methods surrounding GLMs and their
diagnostics, but this introduction should give you a solid foundation to
start working with count data in R using Poisson regression.

\bookmarksetup{startatroot}

\chapter*{Binomial data (L9)}\label{binomial-data-l9}
\addcontentsline{toc}{chapter}{Binomial data (L9)}

\markboth{Binomial data (L9)}{Binomial data (L9)}

\section*{Introduction}\label{introduction-6}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In the previous chapter we moved from linear models and normal errors to
generalized linear models (GLMs) to handle count responses with a
Poisson GLM. In this chapter we move to another common case:
\textbf{binary} and \textbf{binomial responses}. Because binary data is
a special case of binomial data, we will often refer to both as
``binomial data''.

At the heart of binomial data is an event that happens or does not
happen. Examples include:

\begin{itemize}
\tightlist
\item
  Disease status: infected (1) or not infected (0)
\item
  Survival: dead (1) or alive (0)
\item
  Presence/absence of a species at a site: present (1) or absent (0)
\item
  Success/failure of a treatment: success (1) or failure (0)
\end{itemize}

The central question is therefore:

\textbf{Which explanatory variables influence the probability of the
event happening.}

\section*{Overview}\label{overview}
\addcontentsline{toc}{section}{Overview}

\markright{Overview}

In this chapter we will cover:

\begin{itemize}
\tightlist
\item
  A review of Bernoulli and binomial distributions (the family).
\item
  Probabilities, odds, log-odds, odds ratios (the link function).
\item
  Two examples of binomial GLM.
\item
  Interpreting coefficients (odds ratios and probabilities).
\item
  Model assumptions, deviances, and (some) model checking.
\item
  Overdispersion in \emph{aggregated} binomial data and the
  quasibinomial fix.
\item
  Practical complications for individual-level (non-aggregated) 0/1
  data.
\end{itemize}

\section*{Binary and binomial data}\label{binary-and-binomial-data}
\addcontentsline{toc}{section}{Binary and binomial data}

\markright{Binary and binomial data}

When we model binary/binomial data, we are interested in the
\textbf{probability} of an event happening. For example, the probability
of a heart attack, or the probability of a species being present at a
site. And we are interested in whether such probabilities differ between
groups, or vary along a continuous explanatory variable.

Let us label this probability \(\pi\). So \(\pi\) is the probability of
``yes'' (e.g., pass a test), and \(1-\pi\) is the probability of ``no''
(e.g., fail a test).

So, if the probability \(\pi\) is 0.8, then there is an 80\% chance of
``yes'' and a 20\% chance of ``no''. In R we can simulate the pass or
fail of each of a group of people with an expected pass rate
(probability) of 0.8 like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{number\_persons }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{y\_binary }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(number\_persons, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This code says please draw 10 random numbers from a binomial
distribution where each number is the result of 1 trial (size = 1) with
a probability of success of 0.8 (prob = 0.8). Because size = 1, each
number will be either 0 or 1.

Hence, the object \texttt{y} contains 10 simulated persons, where each
person has an 80\% chance of being 1:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_binary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 1 1 1 0 0 1 1 0 1 1
\end{verbatim}

What we have done is to simulate 10 \emph{Bernoulli trials} with
probability 0.8. A Bernoulli trial is one random experiment with exactly
two possible outcomes, ``success'' (1) and ``failure'' (0), where the
probability of success is \(\pi\).)

\textbf{You can see that the observations in \texttt{y} are either 0 or
1: they are \texttt{binary} data.}

An alternate method to simulate and express the same kind of data is
binomial data, where we count the number of successes (1s) in a number
of trials. In R, we simulate this like so:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{number\_persons }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{y\_binomial }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{size =}\NormalTok{ number\_persons, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This code says please draw 1 random number from a binomial distribution
where the number of trials is 10 (size = 10) with a probability of
success of 0.8 (prob = 0.8). The result is the number of successes (1s)
in those 10 trials, in this particular simulation it is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_binomial}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9
\end{verbatim}

We know to expect that 8 of the 10 persons will be 1 (success), because
the probability of success \(\pi = 0.8\). But the actual number of
successes in any one simulation can vary due to chance. In the binary
data example above we by chance got

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(y\_binary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7
\end{verbatim}

successes (1s) out of 10 persons. In the binomial data example above we
got:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_binomial}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9
\end{verbatim}

So in neither case did we get exactly 8 successes! What is the chance of
that!

Well, we can calculate the probability of getting exactly \(k\)
successes in \(n\) trials with success probability \(\pi\) using the
binomial probability formula:

\(P(Y=k) = \binom{n}{k} \pi^k (1-\pi)^{n-k}\)

where \(\binom{n}{k} = \frac{n!}{k!(n-k)!}\) is the binomial
coefficient, which counts the number of ways to choose \(k\) successes
from \(n\) trials.

In R, we can calculate the probability of getting exactly 8 successes in
10 trials with success probability 0.8 as:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{8}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.3019899
\end{verbatim}

And we can make a graph of the full probability distribution for getting
\(k\) successes in 10 trials with success probability 0.8:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_k\_successes }\OtherTok{\textless{}{-}} \FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{size =} \DecValTok{10}\NormalTok{, }\AttributeTok{prob =} \FloatTok{0.8}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \DecValTok{0}\SpecialCharTok{:}\DecValTok{10}\NormalTok{, }\AttributeTok{y =}\NormalTok{ p\_k\_successes)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{col=}\StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \FunctionTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{10}\NormalTok{, }\AttributeTok{by =} \DecValTok{1}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Number of successes"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-9-1.pdf}}

So the probability of getting exactly 8 successes is actually quite low,
at about 0.3. But it is still the most likely outcome.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{So what???} You should now understand two important things:

\begin{itemize}
\tightlist
\item
  The relationship between binary data (0/1) and binomial data (counts
  of successes out of trials).
\item
  That the binomial distribution is appropriate for modelling both
  binary and binomial data. Hint: in a GLM, we will therefore use the
  \texttt{binomial} family.
\end{itemize}

\end{tcolorbox}

\section*{Probabilities, odds, log-odds, and odds
ratios}\label{probabilities-odds-log-odds-and-odds-ratios}
\addcontentsline{toc}{section}{Probabilities, odds, log-odds, and odds
ratios}

\markright{Probabilities, odds, log-odds, and odds ratios}

\subsection*{Odds}\label{odds}
\addcontentsline{toc}{subsection}{Odds}

OK, so we know that when we analyse binary/binomial data, we are
interested in the \textbf{probability} of an event happening, denoted by
\(\pi\).

A related quantity is the \textbf{odds} of the event happening. The odds
is defined as the ratio of the probability of the event happening to the
probability of it not happening:

\begin{itemize}
\tightlist
\item
  \textbf{Odds} is \(\pi/(1-\pi)\)
\end{itemize}

We sometimes say ``the odds of an event happening are x to y''. For
example, a horse has 4 to 1 odds of winning a race. This means that in 5
races we expect the horse to win 4 times and lose once. The odds are
\(4/1 = 4\). The probability of winning is then \(4/(4+1) = 0.8\).

If the horse was really bad and had 1 to 4 odds of winning, then the
odds of winning would be 0.2/0.8 = 0.25, and the probability of winning
would be 0.2.

So, odds and probabilities are related mathematically, but they are not
the same thing. Lets look at a graph of probability on the \(x\)-axis
and odds on the \(y\)-axis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prob\_seq }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\FloatTok{0.99}\NormalTok{, }\AttributeTok{by =} \FloatTok{0.01}\NormalTok{)}
\NormalTok{odds\_seq }\OtherTok{\textless{}{-}}\NormalTok{ prob\_seq }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ prob\_seq)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prob\_seq, }\AttributeTok{y =}\NormalTok{ odds\_seq)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Probability"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Odds"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-10-1.pdf}}

So, when we transform probabilities to odds, we go from a bounded scale
(0 to 1) to an unbounded scale (0 to infinity).

\subsection*{Log-odds (logit)}\label{log-odds-logit}
\addcontentsline{toc}{subsection}{Log-odds (logit)}

Another useful transformation is the \textbf{log-odds}, also known as
the \textbf{logit}:

\begin{itemize}
\tightlist
\item
  \textbf{Log-odds (logit)} is \(\log\left(\frac{\pi}{1-\pi}\right)\)
\end{itemize}

Here is the relationship between probability and log-odds:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{log\_odds\_seq }\OtherTok{\textless{}{-}} \FunctionTok{log}\NormalTok{(odds\_seq)}
\FunctionTok{ggplot}\NormalTok{(}\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ prob\_seq, }\AttributeTok{y =}\NormalTok{ log\_odds\_seq)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Probability"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Log{-}odds (logit)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-11-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{So what???}

The reason why the logit is useful is that it maps probabilities (0 to
1) to the whole real line (-infinity to infinity). This is important
because in a GLM, the linear predictor can take any value from -infinity
to infinity, so we need a link function that can map between the two
scales. The logit does this perfectly.

\end{tcolorbox}

\subsection*{Odds ratio}\label{odds-ratio}
\addcontentsline{toc}{subsection}{Odds ratio}

Finally, we often want to compare the odds of an event happening in two
different groups. Say we have two groups, group 1 and group 2, with
probabilities \(\pi_1\) and \(\pi_2\). The odds in each group are:

\begin{itemize}
\tightlist
\item
  Odds in group 1: \(\frac{\pi_1}{1-\pi_1}\)
\item
  Odds in group 2: \(\frac{\pi_2}{1-\pi_2}\)
\end{itemize}

The \textbf{odds ratio (OR)} compares the odds in two groups:

Odds Ratio = OR =
\(\frac{\text{odds in group 1}}{\text{odds in group 2}}\).

Let's go through a couple of examples. First, suppose the probability of
success is 0.2 in both groups. Then the odds in both groups are:

\begin{itemize}
\tightlist
\item
  Odds in group 1 and 2: \(\frac{0.2}{1-0.2} = \frac{0.2}{0.8} = 0.25\)
\end{itemize}

So the odds ratio is:

\(OR = \frac{0.25}{0.25} = 1\)

Now take an example where the probability of success in group 1 is 0.4,
and in group 2 it is 0.2. Then the odds in each group are:

\begin{itemize}
\tightlist
\item
  Odds in group 1: \(\frac{0.4}{1-0.4} = \frac{0.4}{0.6} = 0.667\)
\item
  Odds in group 2: \(\frac{0.2}{1-0.2} = \frac{0.2}{0.8} = 0.250\)
\end{itemize}

So the odds ratio is:

\(OR = \frac{0.667}{0.250} = 2.67\)

This means that the odds of success in group 1 are about 2.67 times the
odds of success in group 2.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{So what???}

You will see below that the coefficients in a binomial GLM with logit
link correspond to log-odds ratios. Exponentiating those coefficients
gives odds ratios. This is a key part of interpreting the results of a
binomial GLM.

For example, when we see a coefficient (e.g., slope) close to zero, that
means the log-odds ratio is close to zero, which means the odds ratio is
close to 1, which means there is little difference in odds between the
two groups.

\end{tcolorbox}

\section*{A simple binomial GLM}\label{a-simple-binomial-glm}
\addcontentsline{toc}{section}{A simple binomial GLM}

\markright{A simple binomial GLM}

We now have the pieces to define a binomial GLM:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Family}: \(y_i \sim \text{Binomial}(n_i,\pi_i)\)
\item
  \textbf{Linear predictor}:
  \(\eta_i = \beta_0 + \beta_1 x_i^{(1)} + \cdots + \beta_p x_i^{(p)}\)
\item
  \textbf{Link}: logit link:
  \(\eta_i = \log\left(\frac{\pi_i}{1-\pi_i}\right)\)
\end{enumerate}

Let us make a simple example in which there we are interested in how the
probability of passing a test differs between children and adults. We
will simulate the data, and then fit a binomial GLM.

To simulate that data we will define the number of children and adults,
and the probability of passing the test in each group:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{8761}\NormalTok{)}
\NormalTok{n\_children }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{n\_adults }\OtherTok{\textless{}{-}} \DecValTok{50}
\NormalTok{p\_pass\_children }\OtherTok{\textless{}{-}} \FloatTok{0.8}
\NormalTok{p\_pass\_adults }\OtherTok{\textless{}{-}} \FloatTok{0.4}
\end{Highlighting}
\end{Shaded}

Now let us simulate whether each person passes or fails, using the
\texttt{rbinom()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y\_children }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n\_children, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p\_pass\_children)}
\NormalTok{y\_adults }\OtherTok{\textless{}{-}} \FunctionTok{rbinom}\NormalTok{(n\_adults, }\AttributeTok{size =} \DecValTok{1}\NormalTok{, }\AttributeTok{prob =}\NormalTok{ p\_pass\_adults)}
\end{Highlighting}
\end{Shaded}

Now we can combine the data into a data frame (tibble), and look at the
data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_binom }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}
  \AttributeTok{age\_group =} \FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"child"}\NormalTok{, }\StringTok{"adult"}\NormalTok{), }\AttributeTok{each =} \DecValTok{50}\NormalTok{),}
  \AttributeTok{pass =} \FunctionTok{c}\NormalTok{(y\_children, y\_adults)}
\NormalTok{)}
\NormalTok{data\_binom}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 100 x 2
   age_group  pass
   <chr>     <int>
 1 child         1
 2 child         0
 3 child         1
 4 child         1
 5 child         1
 6 child         1
 7 child         0
 8 child         1
 9 child         1
10 child         1
# i 90 more rows
\end{verbatim}

We can summarize the data to see how many passed in each group:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data\_summary }\OtherTok{\textless{}{-}}\NormalTok{ data\_binom }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(age\_group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{n\_tested =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{n\_passed =} \FunctionTok{sum}\NormalTok{(pass),}
    \AttributeTok{pass\_rate =} \FunctionTok{mean}\NormalTok{(pass)}
\NormalTok{  )}
\NormalTok{data\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  age_group n_tested n_passed pass_rate
  <chr>        <int>    <int>     <dbl>
1 adult           50       30      0.6 
2 child           50       39      0.78
\end{verbatim}

In the simulated experiment, 78\% of the children passed, when the true
probability of passing was 0.8. The adults did really well! -- 60\% of
the adults passed, when the true probability was 0.4.

If we had collected that data for real, we might be interested in if the
difference in pass rates between children and adults is statistically
significant. We can use a binomial GLM to test that. Here is how we
specify and fit the model in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{binom\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(pass }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_group, }\AttributeTok{data =}\NormalTok{ data\_binom, }\AttributeTok{family =} \FunctionTok{binomial}\NormalTok{(}\AttributeTok{link =} \StringTok{"logit"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

The critical part here is the
\texttt{family\ =\ binomial(link\ =\ "logit")} argument, which tells R
to fit a binomial GLM with a logit link function. Note that if we do not
specify the link function, R will use the logit link by default for the
binomial family. This means that the following model specification is
equivalent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{binom\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(pass }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_group, }\AttributeTok{data =}\NormalTok{ data\_binom, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

We can look at the estimated coefficients to see the result:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{coef}\NormalTok{(}\FunctionTok{summary}\NormalTok{(binom\_glm))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                Estimate Std. Error  z value   Pr(>|z|)
(Intercept)    0.4054651  0.2886751 1.404572 0.16014849
age_groupchild 0.8602013  0.4470831 1.924030 0.05435084
\end{verbatim}

\textbf{Important}: The estimates are on the scale of the linear
predictor, which is the logit (log-odds) scale. The intercept
corresponds to the log-odds of passing for the reference group (adults),
and the coefficient for \texttt{age\_groupchild} corresponds to the
difference in log-odds between adults and children.

So the logit (log-odds) for adults to pass is 0.405. To convert that to
a probability, we can use the inverse logit function:

\(\pi = \frac{\exp(\eta)}{1+\exp(\eta)}\)

Calculating that in R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta\_adults }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(binom\_glm)[[}\StringTok{"(Intercept)"}\NormalTok{]]}
\NormalTok{p\_adults }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(eta\_adults) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(eta\_adults))}
\NormalTok{p\_adults}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6
\end{verbatim}

Perfect. It is as we saw when we summarised the data -- this is the
observed pass rate for adults.

To find the probability of passing for children, we calculate the logit
value for children by adding the coefficient for
\texttt{age\_groupchild} to the intercept:

Logit for children = 0.405 + 0.860 = 1.265

Then we convert that to a probability:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eta\_children }\OtherTok{\textless{}{-}}\NormalTok{ eta\_adults }\SpecialCharTok{+} \FunctionTok{coef}\NormalTok{(binom\_glm)[[}\StringTok{"age\_groupchild"}\NormalTok{]]}
\NormalTok{p\_children }\OtherTok{\textless{}{-}} \FunctionTok{exp}\NormalTok{(eta\_children) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(eta\_children))}
\NormalTok{p\_children}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.78
\end{verbatim}

Perfect again.

\subsection*{The odds ratio}\label{the-odds-ratio}
\addcontentsline{toc}{subsection}{The odds ratio}

It is useful to know about and use something called the \textbf{odds
ratio}. The odds ratio compares the odds of an event happening in two
situations. In our example, we can calculate the odds ratio for passing
the test for children compared to adults.

Let's use the logit values we calculated above, which we have to
exponentiate to transform back to odds:

\begin{itemize}
\tightlist
\item
  Odds for adults: exp(0.405)
\item
  Odds for children: exp(1.265)
\end{itemize}

The odds ratio (OR) for children compared to adults is:

\[\exp(1.265) / \exp(0.405) = \exp(1.265 - 0.405) = \exp(0.860) = 2.363\]

In the line above, notice the \(\exp(0.860)\) term. The number 0.860 is
exactly the coefficient for \texttt{age\_groupchild} in the model
output. So we can get the odds ratio directly from the model
coefficients by exponentiating the coefficient for
\texttt{age\_groupchild}. Put another way, the coefficient for
\texttt{age\_groupchild} is the log-odds ratio for children compared to
adults.

This isn't magic. Its just a straightforward consequence of how the
logit link function works. In the following example of binomial
regression, you will see how with a continuous explanatory variable, the
coefficient (slope) corresponds to the change in log-odds for a one-unit
increase in the explanatory variable, and exponentiating that
coefficient gives the odds ratio for a one-unit increase.

\section*{Naming models (logistic
regression)}\label{naming-models-logistic-regression}
\addcontentsline{toc}{section}{Naming models (logistic regression)}

\markright{Naming models (logistic regression)}

\textbf{Logistic regression} is a very important term. It is a binomial
GLM with logit link. I.e., it is what we have just described above!

The reason it (a binomial GLM with logit link) is called
\textbf{logistic regression} is that the inverse of the logit function
is called the logistic function. So logistic regression is just a
special case of binomial GLM. The logistic function looks like a
``squashed'' S-shaped curve that maps the whole real line to the
interval (0, 1). This is perfect for modelling probabilities.

\section*{Beetle mortality and insecticide dose (aggregated
data)}\label{beetle-mortality-and-insecticide-dose-aggregated-data}
\addcontentsline{toc}{section}{Beetle mortality and insecticide dose
(aggregated data)}

\markright{Beetle mortality and insecticide dose (aggregated data)}

Let's work though an example of binomial regression, with data in what
we call \emph{aggregated} format. This simply means that we have counts
of successes and failures for groups of individuals, rather than
individual-level 0/1 data.

Eight groups of beetles were exposed to an insecticide dose for 5 hours.
For each dose level, we know how many beetles were tested and how many
died. This is \textbf{binomial (aggregated)} data.

Read in the beetle dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beetle }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/beetle\_mortality.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 8 Columns: 4
-- Column specification --------------------------------------------------------
Delimiter: ","
dbl (4): Dose, Number_tested, Number_killed, Mortality_rate

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Here are the first few rows of the beetle dataset:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(beetle)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
   Dose Number_tested Number_killed Mortality_rate
  <dbl>         <dbl>         <dbl>          <dbl>
1  49.1            49             6          0.122
2  53.0            60            13          0.217
3  56.9            62            18          0.290
4  60.8            56            28          0.5  
5  64.8            63            52          0.825
6  68.7            59            53          0.898
\end{verbatim}

It contains one explanatory variable (Dose) and two pieces of
information about the binary response variable (Number\_killed and
Number\_tested). The mortality rate is also included, and is the
proportion killed at each dose (Number\_killed / Number\_tested).

As always, start with a graph.

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-24-1.pdf}}

\section*{Two tempting but wrong
analyses}\label{two-tempting-but-wrong-analyses}
\addcontentsline{toc}{section}{Two tempting but wrong analyses}

\markright{Two tempting but wrong analyses}

\subsection*{Wrong analysis 1: linear regression on mortality
rate}\label{wrong-analysis-1-linear-regression-on-mortality-rate}
\addcontentsline{toc}{subsection}{Wrong analysis 1: linear regression on
mortality rate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_beetle\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Mortality\_rate }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Dose, }\AttributeTok{data =}\NormalTok{ beetle)}
\end{Highlighting}
\end{Shaded}

This can predict probabilities below 0 or above 1 and does not respect
the mean--variance structure of binomial data.

Check out the model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(mod\_beetle\_lm)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-26-1.pdf}}

We don't have so many data points here, but the residuals vs fitted
indicating non-linearity and the Q-Q plot indicating non-normality are
both concerning. Also the scale-location plot suggests non-constant
variance.

\subsection*{Wrong analysis 2: Poisson regression on counts
killed}\label{wrong-analysis-2-poisson-regression-on-counts-killed}
\addcontentsline{toc}{subsection}{Wrong analysis 2: Poisson regression
on counts killed}

A Poisson model ignores the fact that deaths are bounded by the number
tested at each dose.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod\_beetle\_pois }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(Number\_killed }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Dose, }\AttributeTok{data =}\NormalTok{ beetle, }\AttributeTok{family =}\NormalTok{ poisson)}
\FunctionTok{summary}\NormalTok{(mod\_beetle\_pois)}\SpecialCharTok{$}\NormalTok{coefficients}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               Estimate  Std. Error   z value     Pr(>|z|)
(Intercept) -0.77418056 0.494639994 -1.565139 1.175502e-01
Dose         0.06678281 0.007240113  9.224002 2.862145e-20
\end{verbatim}

One issue that can now happen is that we can get ``impossible''
predictions: predicted counts killed that exceed the number tested.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Example of the "impossible" issue:}
\NormalTok{dose\_example }\OtherTok{\textless{}{-}} \FloatTok{76.54}
\NormalTok{pred\_killed }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(mod\_beetle\_pois,}
                       \AttributeTok{newdata =} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{Dose =}\NormalTok{ dose\_example),}
                       \AttributeTok{type =} \StringTok{"response"}\NormalTok{)}
\NormalTok{pred\_killed}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       1 
76.50652 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_tested\_example }\OtherTok{\textless{}{-}}\NormalTok{ beetle}\SpecialCharTok{$}\NormalTok{Number\_tested[beetle}\SpecialCharTok{$}\NormalTok{Dose }\SpecialCharTok{==}\NormalTok{ dose\_example]}
\NormalTok{num\_tested\_example}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 60
\end{verbatim}

At the highest dose (76.54), the model predicts about 76 deaths, but
only 60 beetles were tested! Clearly this is an impossible prediction.
We can't go on with the Poisson model either.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Poisson vs binomial, when to use each}\\
Use a Poisson model when counts have \emph{no known upper limit}.\\
Use a binomial model when counts are ``\(k\) successes out of \(n\)
trials''. Use a binomial/binary model when each observation is a 0/1
response.

\end{tcolorbox}

\subsection*{Doing it right!}\label{doing-it-right}
\addcontentsline{toc}{subsection}{Doing it right!}

For aggregated binomial data, we provide \textbf{successes and
failures}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beetle }\OtherTok{\textless{}{-}}\NormalTok{ beetle }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Number\_survived =}\NormalTok{ Number\_tested }\SpecialCharTok{{-}}\NormalTok{ Number\_killed)}
\FunctionTok{head}\NormalTok{(beetle)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 5
   Dose Number_tested Number_killed Mortality_rate Number_survived
  <dbl>         <dbl>         <dbl>          <dbl>           <dbl>
1  49.1            49             6          0.122              43
2  53.0            60            13          0.217              47
3  56.9            62            18          0.290              44
4  60.8            56            28          0.5                28
5  64.8            63            52          0.825              11
6  68.7            59            53          0.898               6
\end{verbatim}

Now we can fit the logistic regression model, being careful to specify
\texttt{family\ =\ binomial} and to use
\texttt{cbind(successes,\ failures)} for the response. Because we
specify the response this way, R knows we are working with aggregated
binomial data. And because we specify \texttt{family\ =\ binomial}, R
knows to use the logit link (the default).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beetle\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Number\_killed, Number\_survived) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Dose,}
                  \AttributeTok{data =}\NormalTok{ beetle, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

And as usual we can check the model assumptions:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(beetle\_glm)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-31-1.pdf}}

Not great, but again we have only 8 dose levels. Let us continue in any
case and look at the ANOVA table and the coefficients.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{The response in aggregated binomial data}\\
Use \texttt{cbind(successes,\ failures)} in
\texttt{glm(...,\ family\ =\ binomial)}.

\begin{itemize}
\tightlist
\item
  successes: number of 1s\\
\item
  failures: number of 0s
\end{itemize}

\end{tcolorbox}

\section*{Analysis of deviance (likelihood-based
ANOVA)}\label{analysis-of-deviance-likelihood-based-anova}
\addcontentsline{toc}{section}{Analysis of deviance (likelihood-based
ANOVA)}

\markright{Analysis of deviance (likelihood-based ANOVA)}

As for count-data GLMs, the estimation of the coefficients is by maximum
likelihood estimation (MLE). The measure of goodness-of-fit is the
deviance. We can use an analysis of deviance table to see how much
deviance is explained by dose.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(beetle\_glm, }\AttributeTok{test =} \StringTok{"Chisq"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(Number_killed, Number_survived)

Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    
NULL                     7    267.662              
Dose  1   259.23         6      8.438 < 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see a table quite similar to that for the Poisson GLM. In this case
the Dose row shows a huge amount of the deviance (259.23 units) is
explained by dose, and so the \(p\)-value is very small.

Recall that we already used a chi-squared test for the Poisson GLM. We
do the same here for the binomial GLM, because we are working with
models estimated by maximum likelihood. So just as with the Poisson GLM,
we use a chi-squared test here because, under certain regularity
conditions, the change in deviance between nested models follows a
chi-squared distribution with degrees of freedom equal to the difference
in the number of parameters. If you are interested in what this means,
you can read more about it in more advanced statistics textbooks
mentioned in the \emph{Further reading} section at the end of this
chapter.

\section*{Plotting the fitted
relationship}\label{plotting-the-fitted-relationship}
\addcontentsline{toc}{section}{Plotting the fitted relationship}

\markright{Plotting the fitted relationship}

We will plot \(P(Y=1)\) (mortality probability) against dose using model
predictions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dose\_grid }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Dose =} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(beetle}\SpecialCharTok{$}\NormalTok{Dose)}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\FunctionTok{max}\NormalTok{(beetle}\SpecialCharTok{$}\NormalTok{Dose)}\SpecialCharTok{+}\DecValTok{20}\NormalTok{, }\AttributeTok{length.out =} \DecValTok{400}\NormalTok{))}
\NormalTok{preds }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(beetle\_glm, }\AttributeTok{newdata =}\NormalTok{ dose\_grid, }\AttributeTok{se.fit =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
         \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p\_hat =} \FunctionTok{exp}\NormalTok{(fit) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(fit)),}
         \AttributeTok{p\_hat\_upper\_2se =} \FunctionTok{exp}\NormalTok{(fit }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{se.fit) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(fit }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{se.fit)),}
         \AttributeTok{p\_hat\_lower\_2se =} \FunctionTok{exp}\NormalTok{(fit }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{se.fit) }\SpecialCharTok{/}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+} \FunctionTok{exp}\NormalTok{(fit }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{se.fit))}
\NormalTok{       )}
\NormalTok{dose\_grid }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(dose\_grid, preds)}
\end{Highlighting}
\end{Shaded}

And now the plot:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ beetle, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dose, }\AttributeTok{y =}\NormalTok{ Mortality\_rate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dose\_grid, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dose, }\AttributeTok{y =}\NormalTok{ p\_hat), }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_ribbon}\NormalTok{(}\AttributeTok{data =}\NormalTok{ dose\_grid,}
              \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Dose, }\AttributeTok{ymin =}\NormalTok{ p\_hat\_lower\_2se, }\AttributeTok{ymax =}\NormalTok{ p\_hat\_upper\_2se),}
              \AttributeTok{alpha =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{fill =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Dose"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Predicted mortality probability"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-34-1.pdf}}

The plot is intentionally made to show what happens at very low and very
high dose. At very low dose, the predicted mortality probability
approaches 0, and at very high dose it approaches 1. This is a key
feature of binomial regression: the predicted probabilities are always
between 0 and 1. No matter the value of the linear predictor, the
inverse logit transformation always maps it to a probability between 0
and 1.

We also see that the standard errors are larger at intermediate doses,
where the slope of the curve is steepest. And the standard errors are
smaller at very low and very high doses, where the curve flattens out.

\subsection*{Reporting}\label{reporting-1}
\addcontentsline{toc}{subsection}{Reporting}

When reporting results from a logistic regression, you might say
something like:

``A logistic regression was used to model the probability of beetle
mortality as a function of insecticide dose. The odds of mortality
increased by a factor of 1.28 (95\% CI: 1.23, 1.34 per unit increase in
dose. This indicates that higher doses are associated with higher odds
of mortality. The model predicts that at a dose of 50, the probability
of mortality is approximately 0.09 (95\% CI: -2.34, -2.26 transformed to
probability scale). The model explained a significant amount of deviance
(Deviance = 8.4, df = 6, \(\chi^2\) p \textless{} 0.001).''

\section*{Overdispersion in aggregated binomial
data}\label{overdispersion-in-aggregated-binomial-data}
\addcontentsline{toc}{section}{Overdispersion in aggregated binomial
data}

\markright{Overdispersion in aggregated binomial data}

Overdispersion means the variability is larger than the binomial model
assumes. A quick (rough) check for aggregated binomial data is:

\(\text{Residual deviance} \approx \text{df}\)

Values much larger than 1 for the ratio
\(\frac{\text{Residual deviance}}{\text{df}}\) suggest overdispersion.
In practice we look for values above about 1.5 or 2.

In the beetle mortality example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{deviance}\NormalTok{(beetle\_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 8.437898
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df.residual}\NormalTok{(beetle\_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{deviance}\NormalTok{(beetle\_glm) }\SpecialCharTok{/} \FunctionTok{df.residual}\NormalTok{(beetle\_glm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.406316
\end{verbatim}

The ratio is about 1.4, which is suggests some overdispersion, but is on
the limit of what is usually considered acceptable.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Type I error inflation (binomial overdispersion)}\\
If there is overdispersion and we ignore it, standard errors are usually
too small and \(p\)-values can be too small (anti-conservative). That
increases false positives (Type I errors).

\end{tcolorbox}

\subsection*{Quasibinomial as a pragmatic
fix}\label{quasibinomial-as-a-pragmatic-fix}
\addcontentsline{toc}{subsection}{Quasibinomial as a pragmatic fix}

If we had found overdispersion, a simple fix is to use the
\texttt{quasibinomial} family. This estimates an extra dispersion
parameter from the data. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beetle\_glm\_q }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(Number\_killed, Number\_survived) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Dose,}
                    \AttributeTok{data =}\NormalTok{ beetle, }\AttributeTok{family =}\NormalTok{ quasibinomial)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\texttt{quasibinomial} estimates an extra dispersion parameter from the
data. It is often a good ``first fix'' when aggregated binomial data
look overdispersed.

\end{tcolorbox}

\section*{Individual-level binary data
(non-aggregated)}\label{individual-level-binary-data-non-aggregated}
\addcontentsline{toc}{section}{Individual-level binary data
(non-aggregated)}

\markright{Individual-level binary data (non-aggregated)}

In some datasets we record a single 0/1 observation per individual,
rather than aggregated counts for many individuals. The same logistic
regression model is used, but:

\begin{itemize}
\tightlist
\item
  simple plots of \(y\) against explanatory variables look uninformative
  (two bands)
\item
  some assumption and dispersion checks need extra care
\end{itemize}

\subsection*{Example: blood screening
(ESR)}\label{example-blood-screening-esr}
\addcontentsline{toc}{subsection}{Example: blood screening (ESR)}

Individuals with low ESR (ESR is erythrocyte sedimentation rate and is a
measure of general inflammation and infection) are generally considered
healthy; ESR \textgreater{} 20 mm/hr indicates possible disease. We will
model the probability of high ESR using fibrinogen and globulin
concentration.

Read in the plasma dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plasma }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/plasma\_esr\_original\_HSAUR3.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 32 Columns: 4
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): ESR
dbl (3): fibrinogen, globulin, y

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Here are the first few rows of the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plasma}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 32 x 4
   fibrinogen globulin ESR          y
        <dbl>    <dbl> <chr>    <dbl>
 1       2.52       38 ESR < 20     0
 2       2.56       31 ESR < 20     0
 3       2.19       33 ESR < 20     0
 4       2.18       31 ESR < 20     0
 5       3.41       37 ESR < 20     0
 6       2.46       36 ESR < 20     0
 7       3.22       38 ESR < 20     0
 8       2.21       37 ESR < 20     0
 9       3.15       39 ESR < 20     0
10       2.6        41 ESR < 20     0
# i 22 more rows
\end{verbatim}

The ESR response variable is in two variables. \texttt{ESR} is a factor
with levels ``low'' and ``high''. It is also present as a numeric
variable \texttt{y}, coded 0 (low) and 1 (high). The explanatory
variables are \texttt{fibrinogen} and \texttt{globulin}, both
continuous.

\subsection*{Complication 1: graphical
description}\label{complication-1-graphical-description}
\addcontentsline{toc}{subsection}{Complication 1: graphical description}

Simple scatter plots of 0/1 data are not so informative:

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-40-1.pdf}}

A more informative plot is a conditional density plot. This shows the
proportion of 0s and 1s at each value of the explanatory variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{cdplot}\NormalTok{(}\FunctionTok{factor}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ fibrinogen, }\AttributeTok{data =}\NormalTok{ plasma, }\AttributeTok{xlab =} \StringTok{"Fibrinogen"}\NormalTok{)}
\FunctionTok{cdplot}\NormalTok{(}\FunctionTok{factor}\NormalTok{(y) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ globulin, }\AttributeTok{data =}\NormalTok{ plasma, }\AttributeTok{xlab =} \StringTok{"Globulin"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-41-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

It looks like higher fibrinogen and higher globulin are associated with
higher probability of high ESR. The pattern appears stronger for
fibrinogen.

\subsection*{Fit the binomial regression
model}\label{fit-the-binomial-regression-model}
\addcontentsline{toc}{subsection}{Fit the binomial regression model}

In the case of non-aggregated individual-level 0/1 data, we simply use
\texttt{y} as the response variable. We have to remember to specify
\texttt{family\ =\ binomial} in \texttt{glm()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{plasma\_glm }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ fibrinogen }\SpecialCharTok{+}\NormalTok{ globulin, }\AttributeTok{data =}\NormalTok{ plasma, }\AttributeTok{family =}\NormalTok{ binomial)}
\end{Highlighting}
\end{Shaded}

\subsection*{Complication 2: model checking and
dispersion}\label{complication-2-model-checking-and-dispersion}
\addcontentsline{toc}{subsection}{Complication 2: model checking and
dispersion}

Residual plots exist, but can be hard to interpret for 0/1 data:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{mfrow =} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{))}
\FunctionTok{plot}\NormalTok{(plasma\_glm)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{9.1-GLM2-binary-data_files/figure-pdf/unnamed-chunk-43-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Dispersion checks need caution for individual 0/1 data}\\
The simple ``residual deviance \(\approx\) residual df'' rule-of-thumb
is most useful for \emph{aggregated} binomial data. With
individual-level 0/1 data, detecting overdispersion is more subtle and
often requires additional structure (e.g.~grouping, random effects).

\end{tcolorbox}

\section*{Two common practical
issues}\label{two-common-practical-issues}
\addcontentsline{toc}{section}{Two common practical issues}

\markright{Two common practical issues}

\subsection*{1. Separation}\label{separation}
\addcontentsline{toc}{subsection}{1. Separation}

Sometimes a explanatory variable (or combination of explanatory
variables) perfectly predicts the response variable (all 0s on one side,
all 1s on the other). This is called \textbf{(complete) separation} and
can cause extremely large coefficient estimates and warnings.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Separation warning}\\
If your logistic regression produces enormous standard errors or
warnings about fitted probabilities being 0 or 1, check for separation.
Remedies include more data, simpler models, or penalized methods
(advanced topic).

\end{tcolorbox}

\subsection*{2. Probability vs odds ratio in
reporting}\label{probability-vs-odds-ratio-in-reporting}
\addcontentsline{toc}{subsection}{2. Probability vs odds ratio in
reporting}

Log odds ratios are the default ``native'' scale of the coefficients
logistic regression, but probabilities are often easier to interpret. In
practice, you may report odds ratios \textbf{and} translate to
probabilities at meaningful explanatory variable values.

\section*{Multiple explanatory
variables}\label{multiple-explanatory-variables-1}
\addcontentsline{toc}{section}{Multiple explanatory variables}

\markright{Multiple explanatory variables}

Just as with linear models, and with Poisson GLMs, we can include
multiple explanatory variables in a binomial GLM. The interpretation of
coefficients is similar to that in linear models, but on the log-odds
scale. We don't always have well recognised names, like we do with
linear models (e.g., ANCOVA, multiple regression). But the principles
are the same.

Because we don't have well recognised names for different types of
binomial GLM with multiple explanatory variables, we just call them
``binomial GLMs'' or ``logistic regression models'', and then describe
the explanatory variables included. For example:

\begin{itemize}
\tightlist
\item
  A logistic regression model with one continuous and one categorical
  explanatory variable.
\item
  A logistic regression model with two continuous explanatory variables
  and their interaction.
\item
  A logistic regression model with three categorical explanatory
  variables and all possible interactions (a full factorial model,
  analogous to a three-way ANOVA).
\end{itemize}

\section*{Review}\label{review-7}
\addcontentsline{toc}{section}{Review}

\markright{Review}

There is a lot more to logistic regression and binary data. We can of
course make models to analyse more complex data and questions, including
multiple explanatory variables, different types of explanatory variable
(continuous and categorical), interactions, and so on. But the key ideas
are all above. To summarize the main points:

\begin{itemize}
\tightlist
\item
  Data can be either aggregated binomial (successes out of trials) or
  individual-level 0/1.
\item
  The key questions are about how explanatory variables influence the
  probability of ``yes''.
\item
  The distribution family is binomial (Bernoulli for individual-level
  data).
\item
  The link function is the logit (log-odds).
\item
  If we find overdispersion in aggregated binomial data,
  \texttt{quasibinomial} is a pragmatic fix.
\item
  With non-aggregated individual-level 0/1 data, plotting and checking
  model assumptions require more care.
\end{itemize}

\section*{Further reading}\label{further-reading-8}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

\begin{itemize}
\tightlist
\item
  Chapter 17 of \emph{The R Book} by Crawley (2012) provides a
  comprehensive introduction to GLMs in R, including binomial
  regression. The R is a bit old fashioned but the concepts are well
  explained.
\item
  Generalized Linear Models With Examples in R (2018) by Peter K. Dunn
  and Gordon K. Smyth.
\end{itemize}

\bookmarksetup{startatroot}

\chapter*{Ordination (L10)}\label{ordination-l10}
\addcontentsline{toc}{chapter}{Ordination (L10)}

\markboth{Ordination (L10)}{Ordination (L10)}

\section*{Introduction}\label{introduction-7}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

In earlier chapters, we have analysed variability in \textbf{one
response variable at a time}. But many biological questions are
intrinsically \textbf{multivariate}:

\begin{itemize}
\tightlist
\item
  morphology: multiple measurements describe \emph{shape} (not just
  size)
\item
  communities: abundance of many species describes \emph{composition}
\item
  physiology / omics: many traits or genes change together
\end{itemize}

When we have \textbf{many response variables}, two practical problems
appear:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How do we \textbf{visualize} patterns across many variables?
\item
  How do we \textbf{summarize} the dominant patterns without losing the
  biology?
\end{enumerate}

\textbf{Ordination} is a family of methods that helps with both.

It can also help in cases where we have many explanatory variables
(e.g.~environmental gradients). This can be especially useful when
predictors are collinear.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Core idea}\\
Ordination finds a low-dimensional representation (often 2D) of
\emph{multivariate} data, so that points that are ``similar'' in the
original multivariate space end up close together in the ordination
plot.

\end{tcolorbox}

\subsection*{Working example: skull shape through
time}\label{working-example-skull-shape-through-time}
\addcontentsline{toc}{subsection}{Working example: skull shape through
time}

We will use a classic dataset of \textbf{human skull dimensions}
measured in millimeters. The biological question is:

\begin{quote}
Has skull \textbf{shape} changed through time?
\end{quote}

Four measurements of skull were made:

\includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{assets/skull_figure.png}

We will treat the four skull measurements as multivariate responses, and
time as an explanatory variable.

First read the dataset from the csv file:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{here}\NormalTok{(}\StringTok{"datasets"}\NormalTok{, }\StringTok{"skull\_shape\_time.csv"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 150 Columns: 6
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): id
dbl (5): max.breadth, basi.height, basi.length, nasal.height, thousand.years

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\subsection*{Wide vs long format (and why you should
care)}\label{wide-vs-long-format-and-why-you-should-care}
\addcontentsline{toc}{subsection}{Wide vs long format (and why you
should care)}

Multivariate data often come in \textbf{wide format}:

\begin{itemize}
\tightlist
\item
  one row per object (skull)
\item
  one column per variable (measurement)
\end{itemize}

That's great for many ordination functions.

But long format is often easier for: - grouped summaries - plotting
multiple variables with a single ggplot call

We can easily make a long format version with \texttt{pivot\_longer()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull\_long }\OtherTok{\textless{}{-}}\NormalTok{ skull }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{pivot\_longer}\NormalTok{(}
    \AttributeTok{cols =} \FunctionTok{c}\NormalTok{(max.breadth, basi.height, basi.length, nasal.height),}
    \AttributeTok{names\_to =} \StringTok{"dimension"}\NormalTok{,}
    \AttributeTok{values\_to =} \StringTok{"value"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

This will make some of the following exploration easier.

\subsection*{Explore first: univariate views of a multivariate
problem}\label{explore-first-univariate-views-of-a-multivariate-problem}
\addcontentsline{toc}{subsection}{Explore first: univariate views of a
multivariate problem}

A good habit:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  visualize each variable,\\
\item
  look for patterns and surprises,\\
\item
  then use ordination to \emph{summarize}.
\end{enumerate}

\subsection*{Four scatterplots (measurement vs
time)}\label{four-scatterplots-measurement-vs-time}
\addcontentsline{toc}{subsection}{Four scatterplots (measurement vs
time)}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p\_vars }\OtherTok{\textless{}{-}}\NormalTok{ skull\_long }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ thousand.years, }\AttributeTok{y =}\NormalTok{ value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ dimension, }\AttributeTok{scales =} \StringTok{"free\_y"}\NormalTok{, }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Thousand years ago"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Measurement (mm)"}\NormalTok{)}
\NormalTok{p\_vars}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-4-1.pdf}}

\textbf{Interpretation (qualitative):}\\
If all dimensions change similarly through time, that's mostly a
\textbf{size} change. If some change differently, that suggests
\textbf{shape} change.

\subsection*{Why not four separate
regressions?}\label{why-not-four-separate-regressions}
\addcontentsline{toc}{subsection}{Why not four separate regressions?}

Fitting separate regressions for each measurement has two problems: 1.
\textbf{Multiple testing}: testing four times increases the chance of
false positives. 2. \textbf{Ignoring covariance}: measurements may be
correlated, so analyzing them separately misses joint patterns

\section*{Ordination I: PCA (Principal Components
Analysis)}\label{ordination-i-pca-principal-components-analysis}
\addcontentsline{toc}{section}{Ordination I: PCA (Principal Components
Analysis)}

\markright{Ordination I: PCA (Principal Components Analysis)}

PCA is the most common entry point.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{What PCA does}\\
PCA finds new axes (PC1, PC2, \ldots) that are: - linear combinations of
the original variables - orthogonal (uncorrelated) - ordered so that PC1
captures the most variance, PC2 the next most, etc.

\end{tcolorbox}

\subsection*{Intuitive picture of PCA}\label{intuitive-picture-of-pca}
\addcontentsline{toc}{subsection}{Intuitive picture of PCA}

Imagine that each skull is a point in a space with one axis per
measurement (four dimensions here). Together, the skulls form a cloud of
points.

PCA asks from which direction does this cloud show the greatest spread?

To answer this, PCA rotates the coordinate system so that:

\begin{itemize}
\tightlist
\item
  PC1 points in the direction of greatest variation,
\item
  PC2 points in the next greatest direction, orthogonal to PC1,
\item
  and so on.
\end{itemize}

You can think of PCA as turning the data cloud until you find the view
where the points are most spread out. That view becomes PC1. Then PCA
finds the best second view at right angles to it (PC2).

The key idea is that PCA does not invent new information: it simply
re-expresses the same data using new axes that make dominant patterns
easier to see.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Under the hood, PCA is based on the covariance (or correlation) matrix
of the variables. The principal components are eigenvectors of this
matrix, and the amount of variance they capture is given by the
corresponding eigenvalues. You do not need to compute these by hand, but
this explains why PCA is fundamentally about variance and correlation.

\end{tcolorbox}

\subsection*{Centering and scaling}\label{centering-and-scaling}
\addcontentsline{toc}{subsection}{Centering and scaling}

Before PCA, we usually \textbf{center} and \textbf{scale} the data, or
at least think carefully if we should.

\begin{itemize}
\tightlist
\item
  \textbf{Centering} subtracts the mean of each variable (so mean = 0).
\item
  \textbf{Scaling} divides by the SD of each variable (so SD = 1).
\end{itemize}

Scaling matters if variables are on different scales or if you care
about \emph{relative} variation.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Note that above it is written that PCA does not change the relative
positions of the points. However, if we scale the data before performing
PCA, this can affect the relative positions of the points in the PCA
space. Scaling ensures that all variables contribute equally to the
analysis, which can be important when variables are measured on
different scales. If we do not scale the data, variables with larger
variances can dominate the PCA results, potentially distorting the
relative positions of the points in the PCA space.

\end{tcolorbox}

We can make these transformed variables ourself, or use the built-in
options in \texttt{prcomp()}.

So, let us do our first PCA. We must be sure to only do the PCA on the
four skull measurement variables. To do this we will select only those
columns from the original data frame, and pipe these into the
\texttt{prcomp()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull\_pca }\OtherTok{\textless{}{-}}\NormalTok{ skull }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(max.breadth, basi.height, basi.length, nasal.height) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{prcomp}\NormalTok{(}\AttributeTok{center =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{scale. =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

What is this new dataset? Part of it is the new coordinates of each
skull in the PC space:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(skull\_pca}\SpecialCharTok{$}\NormalTok{x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
           PC1         PC2        PC3        PC4
[1,] -3.621758  0.87923353  0.2413718 -0.2421541
[2,] -3.201426 -0.38205353  0.3833647 -0.3500371
[3,] -3.004179 -1.00242513  1.5640064 -0.2348435
[4,] -2.683398  0.08615036  1.6069216 -1.0534579
[5,] -3.001132 -0.46314398 -0.2188543 -0.7276811
[6,] -2.774762  0.09654899  0.2109694 -0.8941411
\end{verbatim}

\subsection*{Variance represented}\label{variance-represented}
\addcontentsline{toc}{subsection}{Variance represented}

Each of the new PC axes captures some of the variance in the original
data. We can summarize this with:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(skull\_pca)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Importance of components:
                          PC1    PC2    PC3     PC4
Standard deviation     1.4916 0.8981 0.7643 0.62007
Proportion of Variance 0.5562 0.2016 0.1460 0.09612
Cumulative Proportion  0.5562 0.7578 0.9039 1.00000
\end{verbatim}

This shows the standard deviation of each PC axis, the proportion of
variance explained by each PC, and the cumulative proportion of variance
explained.

The first PC axis captures 55.6\% of the variance in the original data.
The first two PC axes together capture 75.8\% of the variance.

This is quite a lot, so we can visualize the data well with two PC axes.
When we do so, we ignore the remaining axes, which capture less variance
(24.2\%).

\subsection*{PCA scores plot (PC1 vs
PC2)}\label{pca-scores-plot-pc1-vs-pc2}
\addcontentsline{toc}{subsection}{PCA scores plot (PC1 vs PC2)}

The \textbf{scores} are the coordinates of each skull in the new PC
space. We do a bit of data wrangling to combine these with the original
data (id and time):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scores }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(skull\_pca}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{id =}\NormalTok{ skull}\SpecialCharTok{$}\NormalTok{id, }\AttributeTok{thousand.years =}\NormalTok{ skull}\SpecialCharTok{$}\NormalTok{thousand.years)}
\NormalTok{scores }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(id, PC1, PC2, thousand.years) }\SpecialCharTok{|\textgreater{}} \FunctionTok{slice\_head}\NormalTok{(}\AttributeTok{n =} \DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 5 x 4
  id      PC1     PC2 thousand.years
  <chr> <dbl>   <dbl>          <dbl>
1 S1    -3.62  0.879             888
2 S2    -3.20 -0.382             112
3 S3    -3.00 -1.00              854
4 S4    -2.68  0.0862            171
5 S5    -3.00 -0.463             544
\end{verbatim}

And then make the graph:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(scores, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ PC1, }\AttributeTok{y =}\NormalTok{ PC2, }\AttributeTok{color =}\NormalTok{ thousand.years)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Thousand}\SpecialCharTok{\textbackslash{}n}\StringTok{years ago"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-9-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Interpretation tip}\\
If points separate along PC1 as time increases, that suggests the
dominant multivariate trend is associated with time.

\end{tcolorbox}

\subsection*{What do the axes mean?}\label{what-do-the-axes-mean}
\addcontentsline{toc}{subsection}{What do the axes mean?}

The \textbf{loadings} tell us how each original variable contributes to
each PC. In fact, each PC is a linear combination of the original
variables, weighted by the loadings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loadings }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(skull\_pca}\SpecialCharTok{$}\NormalTok{rotation, }\AttributeTok{rownames =} \StringTok{"variable"}\NormalTok{)}
\NormalTok{loadings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  variable       PC1     PC2    PC3    PC4
  <chr>        <dbl>   <dbl>  <dbl>  <dbl>
1 max.breadth  0.436  0.689  -0.568  0.107
2 basi.height  0.536  0.108   0.643  0.537
3 basi.length  0.440 -0.712  -0.474  0.272
4 nasal.height 0.573 -0.0783  0.196 -0.792
\end{verbatim}

These can help us interpret the PCs. For example, we see a strong
positive loading of all four skull measurements on PC1. This suggests
that PC1 represents overall size, as all measurements increase together.
On the second PC axis, we see a mix of positive and negative loadings,
indicating that PC2 captures shape differences where some measurements
increase while others decrease. The positive PC2 loadings for head
breadth and nasal height, combined with negative loadings for basi
height and basi length, suggest that PC2 reflects a shape change where
skulls become wider and taller in the nasal region while becoming
shorter in the base dimensions.

Another way to interpret PCs is to look at the correlations between the
original variables and the PCs. This can provide insights into how each
original variable relates to the new PC axes. We can calculate these
correlations as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X }\OtherTok{\textless{}{-}}\NormalTok{ skull }\SpecialCharTok{|\textgreater{}}
\NormalTok{  dplyr}\SpecialCharTok{::}\FunctionTok{select}\NormalTok{(max.breadth, basi.height, basi.length, nasal.height)}
\FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(X, skull\_pca}\SpecialCharTok{$}\NormalTok{x), }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
              PC1   PC2   PC3   PC4
max.breadth  0.65  0.62 -0.43  0.07
basi.height  0.80  0.10  0.49  0.33
basi.length  0.66 -0.64 -0.36  0.17
nasal.height 0.86 -0.07  0.15 -0.49
\end{verbatim}

\subsection*{A simple biplot (scores +
loadings)}\label{a-simple-biplot-scores-loadings}
\addcontentsline{toc}{subsection}{A simple biplot (scores + loadings)}

Below is a lightweight biplot-style plot. (There are fancy versions in
packages like \textbf{factoextra}, but we keep it minimal.)

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-12-1.pdf}}

In this graph we see all arrow point in the same horizontal direction,
indicating that PC1 represents overall size. An increase in any of the
four measurements will increase PC1.

In contrast, the arrows for PC2 point in different directions,
indicating that PC2 represents shape differences where some measurements
increase while others decrease. The two longest arrows are for
basi.length and max.breadth, suggesting that these measurements
contribute most strongly to shape variation captured by PC2.

\section*{When ordination helps}\label{when-ordination-helps}
\addcontentsline{toc}{section}{When ordination helps}

\markright{When ordination helps}

We might think that ordination is more useful when we have a greater
number of variables, because then the reducing down to two or three
dimensions is more helpful. But the key factor is actually the
\textbf{correlation structure} among the variables. If many variables
are correlated, then ordination can capture most of the variance in a
few dimensions. If variables are uncorrelated, then ordination may not
help much since most of the variance is spread evenly across many
dimensions. Let's have a look a this with some simulated data. We will
simulate three datasets with three variables each, but with different
correlation structures: high correlation, moderate correlation, and low
correlation.

Now a 3D plot of the simulated data with different correlation
structures (please note that these plots are best viewed in the HTML
version of the book; in the PDF version, they will appear as static
images):

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-18-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-19-1.pdf}}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-20-1.pdf}}

The first plot shows data with high correlation among variables, where
the points are clustered along a diagonal line, indicating that the
variables are strongly related. The second plot shows moderate
correlation, where the points are more spread out but still show some
clustering. The third plot shows low correlation, where the points are
scattered randomly in space, indicating that the variables are largely
independent.

We expect that in the high correlation case, the first principal
component will capture a large proportion of the variance, while in the
low correlation case, the variance will be more evenly distributed
across all principal components. Let's check this by performing PCA on
each dataset and examining the variance explained by each principal
component.

\begin{verbatim}
[1] 0.925377783 0.065681354 0.008940863
\end{verbatim}

\begin{verbatim}
[1] 0.6571611 0.1978438 0.1449951
\end{verbatim}

\begin{verbatim}
[1] 0.3874361 0.3586825 0.2538815
\end{verbatim}

It turns out that in the high correlation case, the first principal
component captures a large proportion of the variance (92.5\%). This is
a large amount of variance explained by a single component. It is caused
because we made a dataset where the variables are strongly correlated.
There is only one axis along which the data vary strongly.

In contrast, in the zero correlation case, the variance is more evenly
distributed across all principal components, with each component
capturing around 38.7\%, 35.9\%, and 25.4\% of the variance
respectively. This indicates that there is no single dominant direction
of variance in the data, as the variables are largely independent.

In the intermediate correlation case, the variance explained by each
principal component is more balanced, with the first component capturing
around 65.7\% of the variance, and the remaining components capturing
significant portions as well. This reflects the moderate correlation
structure in the data.

Ordination is most useful when it can reduce the dimensionality of the
data while retaining most of the variance. This means that ordination
methods like PCA will be more effective in summarizing the data in the
high correlation case, as most of the information can be captured in
just one or two dimensions. In contrast, in the low correlation case,
ordination may not provide much dimensionality reduction, as each
variable contributes independently to the overall variance.

What determines if we have variables with high correlation in real
datasets? Often, it is biological or physical relationships among the
variables. For example, in morphological datasets, measurements of
different body parts may be correlated due to overall size or shape
factors. In ecological datasets, species abundances may be correlated
due to shared environmental preferences or interactions. Understanding
the underlying biology can help us anticipate when ordination methods
will be most useful.

\section*{Ordination II: NMDS (Non-metric Multidimensional
Scaling)}\label{ordination-ii-nmds-non-metric-multidimensional-scaling}
\addcontentsline{toc}{section}{Ordination II: NMDS (Non-metric
Multidimensional Scaling)}

\markright{Ordination II: NMDS (Non-metric Multidimensional Scaling)}

PCA is powerful, but it is a \textbf{linear} method and it relies on
Euclidean geometry in the original variable space.

NMDS is often used when:

\begin{itemize}
\tightlist
\item
  you want ordination based on \textbf{distances/dissimilarities}
\item
  the relationships are not well represented by a linear method
\item
  you want flexibility in the choice of distance (e.g.~Bray--Curtis,
  Gower, \ldots)
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{What NMDS does (conceptually)}\\
1) compute pairwise distances among objects\\
2) place points in a low-dimensional space (usually 2D)\\
3) try to preserve the \textbf{rank order} of distances (non-metric)\\
4) report \textbf{stress}: lower is better (roughly: mismatch between
original dissimilarities and ones in the lower-dimensional space)

\end{tcolorbox}

\subsection*{Step 1: Choose a distance
measure}\label{step-1-choose-a-distance-measure}
\addcontentsline{toc}{subsection}{Step 1: Choose a distance measure}

Here our variables are numeric, so Euclidean distance is a reasonable
default.

We will scale first (so variables contribute comparably), then compute
distances:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{X\_scaled }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(X)}
\NormalTok{D }\OtherTok{\textless{}{-}} \FunctionTok{dist}\NormalTok{(X\_scaled, }\AttributeTok{method =} \StringTok{"euclidean"}\NormalTok{)}
\CommentTok{\#D}
\end{Highlighting}
\end{Shaded}

\subsection*{Step 2: Fit NMDS with multiple random
starts}\label{step-2-fit-nmds-with-multiple-random-starts}
\addcontentsline{toc}{subsection}{Step 2: Fit NMDS with multiple random
starts}

In NMDS, we need to choose some random starting configuration of points.
And we want to make sure that we choose a good solution, not just a
local optimum. Hence, we try multiple random starts and pick the best
one.

In practice, use \texttt{metaMDS()} (from \textbf{vegan}) rather than
calling the low-level optimizer directly. It tries multiple starting
configurations and does useful housekeeping.

When calling \texttt{metaMDS()}, we specify the following:

\begin{itemize}
\tightlist
\item
  \texttt{k\ =\ 2} for a 2D solution.
\item
  \texttt{trymax\ =\ 50} to try up to 50 random starts.
\item
  \texttt{autotransform\ =\ FALSE} because we already scaled the data
  ourselves.
\item
  \texttt{trace\ =\ FALSE} to suppress output during fitting.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{nmds }\OtherTok{\textless{}{-}} \FunctionTok{metaMDS}\NormalTok{(D, }\AttributeTok{k =} \DecValTok{2}\NormalTok{, }\AttributeTok{trymax =} \DecValTok{50}\NormalTok{, }\AttributeTok{autotransform =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{trace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{nmds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
metaMDS(comm = D, k = 2, trymax = 50, autotransform = FALSE,      trace = FALSE) 

global Multidimensional Scaling using monoMDS

Data:     D 
Distance: euclidean 

Dimensions: 2 
Stress:     0.1467223 
Stress type 1, weak ties
Best solution was repeated 1 time in 20 tries
The best solution was from try 16 (random start)
Scaling: centring, PC rotation 
Species: scores missing
\end{verbatim}

Lots of information here, for example we have:

\begin{itemize}
\tightlist
\item
  the call used
\item
  the type of distance
\item
  the number of dimensions
\item
  the number of random starts tried
\item
  the final stress value
\item
  the scaling of the data
\end{itemize}

\subsection*{Step 3: Assess NMDS fit with
stress}\label{step-3-assess-nmds-fit-with-stress}
\addcontentsline{toc}{subsection}{Step 3: Assess NMDS fit with stress}

The stress of an NMDS solution quantifies how well the low-dimensional
configuration preserves the rank order of the original dissimilarities.
That is, it is a measure of mismatch between the distances in the
original high-dimensional space and the distances in the reduced
low-dimensional space. Here is the stress plot for our NMDS solution:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{stressplot}\NormalTok{(nmds)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-24-1.pdf}}

We see a fairly good match between the original distances and the NMDS
distances. The correlations are reasonably high, indicating that the
NMDS solution captures the rank order of dissimilarities well.

The stress value is another measure of fit. Lower stress values indicate
a better fit. A common rule of thumb is that stress \textless{} 0.1 is a
good fit, stress between 0.1 and 0.2 is acceptable, and stress
\textgreater{} 0.2 indicates a poor fit. A poor fit suggests that the
data may not be well represented in two dimensions, and a
higher-dimensional solution may be needed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nmds}\SpecialCharTok{$}\NormalTok{stress}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1467223
\end{verbatim}

We are in the range of okay fit in two dimensions.

What improvement in stress do we get if we go to three dimensions?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{nmds\_3d }\OtherTok{\textless{}{-}} \FunctionTok{metaMDS}\NormalTok{(D, }\AttributeTok{k =} \DecValTok{3}\NormalTok{, }\AttributeTok{trymax =} \DecValTok{50}\NormalTok{, }\AttributeTok{autotransform =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{trace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{nmds\_3d}\SpecialCharTok{$}\NormalTok{stress}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.07009879
\end{verbatim}

The stress decreases when we move to three dimensions, indicating a
better fit. However, the improvement may not be substantial enough to
justify the added complexity of a three-dimensional solution. In
practice, we often prefer two-dimensional solutions for ease of
visualization and interpretation, unless the stress reduction is very
large.

We can plot the NMDS scores, colored by time:

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-27-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Axis directions are arbitrary}\\
Flipping or rotating an NMDS solution (or a PCA) does not change its
meaning. Only relative distances among points matter.

\end{tcolorbox}

If we wanted to know the biological interpretation of the NMDS axes, we
would need to look at correlations between the original variables and
the NMDS axes. This is just the same as we did for PCA. We could then
plot the correlations as arrows on the NMDS plot to help interpret the
axes.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cor\_nmds }\OtherTok{\textless{}{-}} \FunctionTok{round}\NormalTok{(}\FunctionTok{cor}\NormalTok{(X, nmds\_scores }\SpecialCharTok{|\textgreater{}} \FunctionTok{select}\NormalTok{(NMDS1, NMDS2)), }\DecValTok{2}\NormalTok{)}
\NormalTok{cor\_nmds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
             NMDS1 NMDS2
max.breadth   0.65  0.64
basi.height   0.80  0.03
basi.length   0.64 -0.57
nasal.height  0.86 -0.09
\end{verbatim}

And plot these as arrows:

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-29-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

This NMDS graph looks a lot like the PCA biplot. This is because our
data are fairly linear and Euclidean distances are appropriate. In more
complex datasets, NMDS can reveal patterns that PCA might miss.

\end{tcolorbox}

\section*{Hypothesis testing: avoid ``four separate
regressions''}\label{hypothesis-testing-avoid-four-separate-regressions}
\addcontentsline{toc}{section}{Hypothesis testing: avoid ``four separate
regressions''}

\markright{Hypothesis testing: avoid ``four separate regressions''}

If you test time against each skull measurement separately, you face a
\textbf{multiple testing} problem.

A better match to the question (``do skulls change through time in
multivariate space?'') is a \textbf{multivariate test}.

Here are four common approaches demonstrated but not deeply explained:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  MANOVA (parametric, multivariate normality assumptions)\\
\item
  PERMANOVA (distance-based, permutation test)\\
\item
  Dispersion checks (are groups equally variable?)
\item
  Fitting time onto an ordination (envfit, ordisurf)
\end{enumerate}

\subsection*{1) MANOVA (parametric)}\label{manova-parametric}
\addcontentsline{toc}{subsection}{1) MANOVA (parametric)}

Multivariate ANOVA (MANOVA) tests whether group \textbf{centroids}
differ in multivariate space, assuming multivariate normality. The
response is a matrix of multiple variables, and the explanatory
variables can be categorical or continuous.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{man\_mod }\OtherTok{\textless{}{-}} \FunctionTok{manova}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(max.breadth, basi.height, basi.length, nasal.height) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ thousand.years,}
                  \AttributeTok{data =}\NormalTok{ skull)}
\FunctionTok{summary}\NormalTok{(man\_mod, }\AttributeTok{test =} \StringTok{"Pillai"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                Df  Pillai approx F num Df den Df    Pr(>F)    
thousand.years   1 0.36751   21.064      4    145 1.042e-13 ***
Residuals      148                                             
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

If we bin years into categories, MANOVA becomes closer to multivariate
ANOVA:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skull }\OtherTok{\textless{}{-}}\NormalTok{ skull }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{year\_class =} \FunctionTok{cut}\NormalTok{(thousand.years, }\AttributeTok{breaks =} \DecValTok{8}\NormalTok{) }\SpecialCharTok{|\textgreater{}} \FunctionTok{fct\_inorder}\NormalTok{())}
\NormalTok{man\_mod\_cat }\OtherTok{\textless{}{-}} \FunctionTok{manova}\NormalTok{(}\FunctionTok{cbind}\NormalTok{(max.breadth, basi.height, basi.length, nasal.height) }\SpecialCharTok{\textasciitilde{}}\NormalTok{ thousand.years,}
                      \AttributeTok{data =}\NormalTok{ skull)}
\FunctionTok{summary}\NormalTok{(man\_mod\_cat, }\AttributeTok{test =} \StringTok{"Pillai"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
                Df  Pillai approx F num Df den Df    Pr(>F)    
thousand.years   1 0.36751   21.064      4    145 1.042e-13 ***
Residuals      148                                             
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{MANOVA assumptions matter}\\
MANOVA relies on multivariate normality and homogeneity of covariance
matrices. In many biological datasets, these are imperfect---so
distance-based alternatives are common.

\end{tcolorbox}

\subsection*{2) PERMANOVA
(distance-based)}\label{permanova-distance-based}
\addcontentsline{toc}{subsection}{2) PERMANOVA (distance-based)}

PERMANOVA tests whether group \textbf{centroids} differ in multivariate
space, using permutations. In vegan, use \texttt{adonis2()}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Use the same distance matrix D we used for NMDS}
\FunctionTok{adonis2}\NormalTok{(D }\SpecialCharTok{\textasciitilde{}}\NormalTok{ year\_class, }\AttributeTok{data =}\NormalTok{ skull, }\AttributeTok{permutations =} \DecValTok{999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Permutation test for adonis under reduced model
Permutation: free
Number of permutations: 999

adonis2(formula = D ~ year_class, data = skull, permutations = 999)
          Df SumOfSqs      R2      F Pr(>F)    
Model      7   138.47 0.23234 6.1396  0.001 ***
Residual 142   457.53 0.76766                  
Total    149   596.00 1.00000                  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsection*{3) PERMDISP / betadisper: are groups equally
variable?}\label{permdisp-betadisper-are-groups-equally-variable}
\addcontentsline{toc}{subsection}{3) PERMDISP / betadisper: are groups
equally variable?}

A common pitfall:

\begin{itemize}
\tightlist
\item
  PERMANOVA can be significant either because \textbf{centroids differ}
\item
  or because \textbf{dispersion differs} (groups have different spread)
\end{itemize}

So it's good practice to check dispersion when comparing groups.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bd }\OtherTok{\textless{}{-}} \FunctionTok{betadisper}\NormalTok{(D, skull}\SpecialCharTok{$}\NormalTok{year\_class)}
\CommentTok{\#bd}
\FunctionTok{anova}\NormalTok{(bd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: Distances
           Df Sum Sq Mean Sq F value Pr(>F)
Groups      7  0.929 0.13274  0.2284  0.978
Residuals 142 82.529 0.58119               
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{permutest}\NormalTok{(bd, }\AttributeTok{permutations =} \DecValTok{999}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Permutation test for homogeneity of multivariate dispersions
Permutation: free
Number of permutations: 999

Response: Distances
           Df Sum Sq Mean Sq      F N.Perm Pr(>F)
Groups      7  0.929 0.13274 0.2284    999   0.98
Residuals 142 82.529 0.58119                     
\end{verbatim}

\subsection*{4) Fitting time onto an ordination: envfit and
ordisurf}\label{fitting-time-onto-an-ordination-envfit-and-ordisurf}
\addcontentsline{toc}{subsection}{4) Fitting time onto an ordination:
envfit and ordisurf}

Even when you use an unconstrained ordination (PCA/NMDS), you may want
to show how an explanatory variable aligns with it.

\subsubsection*{envfit: linear fit + permutation
test}\label{envfit-linear-fit-permutation-test}
\addcontentsline{toc}{subsubsection}{envfit: linear fit + permutation
test}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_years }\OtherTok{\textless{}{-}} \FunctionTok{envfit}\NormalTok{(nmds }\SpecialCharTok{\textasciitilde{}}\NormalTok{ thousand.years, }\AttributeTok{data =}\NormalTok{ skull, }\AttributeTok{permutations =} \DecValTok{999}\NormalTok{)}
\NormalTok{fit\_years}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

***VECTORS

                   NMDS1     NMDS2     r2 Pr(>r)    
thousand.years  0.996500 -0.083553 0.3643  0.001 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Permutation: free
Number of permutations: 999
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(nmds, }\AttributeTok{type =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
species scores not available
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{points}\NormalTok{(nmds, }\AttributeTok{display =} \StringTok{"sites"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\FunctionTok{plot}\NormalTok{(fit\_years, }\AttributeTok{col =} \StringTok{"black"}\NormalTok{)  }\CommentTok{\# vector direction and significance}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-35-1.pdf}}

\subsubsection*{ordisurf: non-linear surface
(GAM)}\label{ordisurf-non-linear-surface-gam}
\addcontentsline{toc}{subsubsection}{ordisurf: non-linear surface (GAM)}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(nmds, }\AttributeTok{type =} \StringTok{"n"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
species scores not available
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{points}\NormalTok{(nmds, }\AttributeTok{display =} \StringTok{"sites"}\NormalTok{, }\AttributeTok{pch =} \DecValTok{16}\NormalTok{, }\AttributeTok{cex =} \FloatTok{0.8}\NormalTok{)}
\FunctionTok{ordisurf}\NormalTok{(nmds }\SpecialCharTok{\textasciitilde{}}\NormalTok{ thousand.years, }\AttributeTok{data =}\NormalTok{ skull, }\AttributeTok{add =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{10.1-ordination_files/figure-pdf/unnamed-chunk-36-1.pdf}}

\begin{verbatim}

Family: gaussian 
Link function: identity 

Formula:
y ~ s(x1, x2, k = 10, bs = "tp", fx = FALSE)

Estimated degrees of freedom:
2.98  total = 3.98 

REML score: 1092.962     
\end{verbatim}

\section*{PCA in multiple regression}\label{pca-in-multiple-regression}
\addcontentsline{toc}{section}{PCA in multiple regression}

\markright{PCA in multiple regression}

PCA can also be used to reduce multiple explanatory variable into a few
uncorrelated components, which can then be used in multiple regression
models. This is particularly useful when the predictors are highly
correlated, as it helps to avoid multicollinearity issues.

The steps for this would be:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Perform PCA on the explanatory variables to obtain principal
  components.
\item
  Make a linear regression model using the selected principal components
  as explanatory variables.
\item
  Interpret the results in terms of the original variables, if needed.
\end{enumerate}

The ``win'' here is that the principal components are uncorrelated,
which simplifies the regression analysis and interpretation. The
``lose'' is that the principal components may not have a straightforward
biological interpretation, so care is needed when explaining the
results.

\section*{Review}\label{review-8}
\addcontentsline{toc}{section}{Review}

\markright{Review}

Ordination is useful when we have multivariate data and when variables
within that data are correlated. The correlation structure means that we
can represent the data in a lower-dimensional space without losing too
much information. If there is a lot of correlation among variables, then
ordination becomes very useful. If there is little correlation, then
ordination may not help much since most of the variance is spread evenly
across many dimensions.

Summary:

\begin{itemize}
\tightlist
\item
  PCA is a linear ordination method that finds orthogonal axes capturing
  the most variance.
\item
  NMDS is a flexible, distance-based ordination method that preserves
  rank order of dissimilarities.
\item
  Both methods help visualize and summarize multivariate data.
\item
  Multivariate hypothesis tests (MANOVA, PERMANOVA) assess group
  differences in multivariate space.
\item
  Checking dispersion (PERMDISP) is important to interpret PERMANOVA
  results.
\item
  Fitting explanatory variables onto ordinations (envfit, ordisurf)
  helps interpret patterns.
\end{itemize}

\section*{Further reading}\label{further-reading-9}
\addcontentsline{toc}{section}{Further reading}

\markright{Further reading}

Evelyn Chrystalla ``E.C.'' Pielou was a Canadian statistical ecologist.
She began her career as a researcher for the Canadian Department of
Forestry and the Canadian Department of Agriculture. Pielou's books are
classics that helped establish quantitative ecology. They range from the
mathematics of ordination to kinds of (ecological) question ordination
helps answer. A great place to start is \emph{The Interpretation of
Ecological Data: A Primer on Classification and Ordination}

\bookmarksetup{startatroot}

\chapter*{Mixed models (L11-1)}\label{mixed-models-l11-1}
\addcontentsline{toc}{chapter}{Mixed models (L11-1)}

\markboth{Mixed models (L11-1)}{Mixed models (L11-1)}

\section*{Introduction}\label{introduction-8}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

So far in BIO144 we mostly used models where \textbf{each observation is
assumed independent}:

\begin{itemize}
\tightlist
\item
  linear models (\texttt{lm()})
\item
  generalized linear models (\texttt{glm()}), e.g.~Poisson and binomial.
\end{itemize}

By independent we mean that the value of one observation does not give
us any information about the value of another observation.

But many biological datasets violate independence because observations
come in \textbf{groups}:

\begin{itemize}
\tightlist
\item
  repeated measures on the same individual (before/after, time series,
  multiple tissues)
\item
  multiple individuals from the same plot / site / stream / lake / cage
  / family
\item
  students within classes, patients within hospitals, samples within
  batches
\end{itemize}

In these cases, treating all rows as independent often leads to
\textbf{false confidence} (too-small standard errors, too-small
p-values). Mixed models are one standard way to handle this.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-important-color!10!white, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, colframe=quarto-callout-important-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Core idea} A \emph{mixed model} extends regression by adding
\textbf{random effects} that represent grouping structure (clusters) in
the data.

\begin{itemize}
\tightlist
\item
  \textbf{fixed effects}: effects you want to estimate explicitly
  (treatments, temperature, time, \ldots)
\item
  \textbf{random effects}: variation among groups (individuals, sites,
  years, \ldots) that induces correlation within groups
\end{itemize}

\end{tcolorbox}

\section*{Why not just average?}\label{why-not-just-average}
\addcontentsline{toc}{section}{Why not just average?}

\markright{Why not just average?}

A common workaround is to average repeated measurements to a single
value per group and then use \texttt{lm()}.

Sometimes this is OK --- but often it throws away information.

Reasons \emph{not} to average include:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Imbalanced sampling} (groups have different numbers of
  observations): averaging changes the weighting.
\item
  \textbf{Which average?} (mean, median, mode): different choices answer
  different questions.
\item
  \textbf{False confidence}: pretending ``n rows'' are independent can
  make uncertainty look too small.
\item
  \textbf{You may want to study variation among groups} (some
  individuals respond more strongly than others).
\item
  \textbf{``Sharing information'' across groups}: mixed models partially
  pool group estimates toward the overall mean.
\item
  \textbf{Keeping information}: you can use all observations without
  collapsing the design.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

A mixed model gives you a principled compromise between:

\begin{itemize}
\tightlist
\item
  analysing each observation as a independent one, and
\item
  averaging everything (too coarse).
\end{itemize}

This is sometimes called \textbf{partial pooling}.

\end{tcolorbox}

\section*{The problem: non-independence and
pseudoreplication}\label{the-problem-non-independence-and-pseudoreplication}
\addcontentsline{toc}{section}{The problem: non-independence and
pseudoreplication}

\markright{The problem: non-independence and pseudoreplication}

Imagine we measure the same individual multiple times.

If we fit a simple linear model, the residuals are assumed independent:

\[\varepsilon_i \sim \text{Normal}(0, \sigma^2), \quad \text{independent across } i\]

But repeated measures induce correlation:

\begin{itemize}
\tightlist
\item
  measurements from the same individual tend to be more similar
\item
  so residuals are not independent
\end{itemize}

This is one common form of \textbf{pseudoreplication}: treating repeated
measurements as if they were separate independent replicates.

\section*{Random intercept models}\label{random-intercept-models}
\addcontentsline{toc}{section}{Random intercept models}

\markright{Random intercept models}

\subsection*{Random intercept idea}\label{random-intercept-idea}
\addcontentsline{toc}{subsection}{Random intercept idea}

Suppose we measure a response \(y\) (e.g.~growth) for individuals \(j\)
at observations \(i\). And that we measure each individual more than
once. And that we have some treatment, such as temperature \(x\).

This means that our data have a \textbf{grouping structure}:
observations are grouped by individual. If we want to calculate the mean
growth, we should account for this grouping.

Let's make an example dataset to illustrate:

Read in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat\_example }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/mixed\_model\_example.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50 Columns: 3
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): individual
dbl (2): temperature, growth

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Here are the first few rows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(dat\_example)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 3
  individual temperature growth
  <chr>            <dbl>  <dbl>
1 Ind01             9.15  16.1 
2 Ind01             9.37  17.4 
3 Ind01             2.86   7.58
4 Ind01             8.30  14.2 
5 Ind01             6.42  13.8 
6 Ind02             5.19  12.5 
\end{verbatim}

In this data we have two sources of variation in \texttt{growth}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{fixed effect} of \texttt{temperature} (same for all
  individuals)
\item
  \textbf{random effect} of \texttt{individual} (different baseline
  growth for each individual)
\end{enumerate}

A fixed effect is something we want to estimate explicitly (e.g.~how
growth changes with temperature). Fixed effects are variables in which
the values have specific meaning (e.g.~temperature = 5Â°C).

A random effect is something that varies among groups (individuals
here), but where we are not interested in the specific values for each
group. Instead, we want to estimate the amount of variation among
groups. Random effects are variables where the specific values are not
of interest, but rather the variation among them (e.g.~individual
identity). I.e., the value ``Ind01'' has no specific meaning; we just
want to know how much individuals differ from each other on average.
That value could be anything.

A random effect is often containing levels that are considered a random
sample from a larger population. We did not, for example, choose
specific individuals for a reason; they are just a sample of all
possible individuals.

An appropriate model for this data is a \textbf{random-intercept mixed
model}. It is a model in which each group (individual) has its own
intercept (baseline), but the intercepts are assumed to come from a
common distribution.

A random-intercept mixed model is:

\[y_{ij} = \beta_0 + \beta_1 x_{ij} + b_{0j} + \varepsilon_{ij}\]

where:

\begin{itemize}
\tightlist
\item
  \(\beta_0, \beta_1\) are \textbf{fixed effects}
\item
  \(x_{ij}\) is the explanatory variable for observation \(i\) in group
  \(j\). Also called a \textbf{fixed effect}.
\item
  \(b_{0j}\) is a \textbf{random intercept} for group \(j\), typically
  \(b_{0j} \sim \text{Normal}(0,\sigma_b^2)\)
\item
  \(\varepsilon_{ij}\) is residual error,
  \(\varepsilon_{ij} \sim \text{Normal}(0,\sigma^2)\)
\end{itemize}

Interpretation:

\begin{itemize}
\tightlist
\item
  each group gets its own intercept (baseline), but
\item
  those intercepts are assumed to come from a common distribution
\end{itemize}

\subsection*{Random intercept syntax in
R}\label{random-intercept-syntax-in-r}
\addcontentsline{toc}{subsection}{Random intercept syntax in R}

To make a mixed model in R we can no longer use \texttt{lm()} or
\texttt{glm()}. Instead, we use the \texttt{lmer()} function from the
\texttt{lme4} package.

In the \texttt{lmer} function we have to specify the random effects in a
special way. For example, a random-intercept model for \texttt{y} with
fixed effect \texttt{x} and random intercept by \texttt{group} is
specified as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ group)}
\end{Highlighting}
\end{Shaded}

Read it as: ``a model for \texttt{y} with fixed effect \texttt{x} and a
random intercept by \texttt{group}''.

\section*{Random slope models}\label{random-slope-models}
\addcontentsline{toc}{section}{Random slope models}

\markright{Random slope models}

Sometimes groups differ not only in baseline level, but also in how they
respond to an explanatory variable.

A random-slope model:

\[y_{ij} = \beta_0 + \beta_1 x_{ij} + b_{0j} + b_{1j}x_{ij} + \varepsilon_{ij}\]

In R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ x }\SpecialCharTok{|}\NormalTok{ group)}
\end{Highlighting}
\end{Shaded}

Here each group has its own intercept \textbf{and} its own slope, and
these can be correlated.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

Random slopes are powerful but can be hard to estimate with small
datasets. If the model struggles to fit (singular fit warnings),
consider simplifying.

\end{tcolorbox}

\section*{Nested and crossed random
effects}\label{nested-and-crossed-random-effects}
\addcontentsline{toc}{section}{Nested and crossed random effects}

\markright{Nested and crossed random effects}

\subsection*{Nested}\label{nested}
\addcontentsline{toc}{subsection}{Nested}

\textbf{Nested} means one grouping factor is contained within another.

Example: measurements within plants within plots:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ treatment }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ plot}\SpecialCharTok{/}\NormalTok{plant)}
\end{Highlighting}
\end{Shaded}

This expands to
\texttt{(1\ \textbar{}\ plot)\ +\ (1\ \textbar{}\ plot:plant)}.

\subsection*{Crossed}\label{crossed}
\addcontentsline{toc}{subsection}{Crossed}

\textbf{Crossed} means groups are not nested.

Example: repeated measures with multiple observers (each observer
measures many individuals; each individual is measured by many
observers):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ individual) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ observer)}
\end{Highlighting}
\end{Shaded}

\section*{Hands-on example: plant growth with repeated
measures}\label{hands-on-example-plant-growth-with-repeated-measures}
\addcontentsline{toc}{section}{Hands-on example: plant growth with
repeated measures}

\markright{Hands-on example: plant growth with repeated measures}

\subsection*{Biological story}\label{biological-story}
\addcontentsline{toc}{subsection}{Biological story}

You are studying how temperature affects plant growth.

\begin{itemize}
\tightlist
\item
  30 plants are grown at one of three temperatures: 10Â°C, 15Â°C, 20Â°C.
\item
  Each plant is measured weekly for 8 weeks.
\item
  Response: plant height (cm).
\end{itemize}

Because we repeatedly measure the \textbf{same plant}, the observations
are not independent. We will compare:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  a naive linear model (wrong independence assumption),
\item
  a mixed model with plant as a random effect.
\end{enumerate}

\subsection*{An example dataset}\label{an-example-dataset-1}
\addcontentsline{toc}{subsection}{An example dataset}

Read in the data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{dat\_example }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"datasets/plant\_growth\_repeated.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 240 Columns: 4
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): plant_id
dbl (3): temp, week, height

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  plant_id temp   week height
  <fct>    <fct> <int>  <dbl>
1 P01      10        0   6.79
2 P01      10        1   8.59
3 P01      10        2  12.1 
4 P01      10        3  11.0 
5 P01      10        4  13.5 
6 P01      10        5  16.7 
\end{verbatim}

\subsection*{Explore the data}\label{explore-the-data}
\addcontentsline{toc}{subsection}{Explore the data}

First we make a plot of the data:

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-8-1.pdf}}

We can see data from each individual plant because it is connected by a
line. We also see that plants at higher temperature tend to be taller.
And we see that plants differ in their baseline height (week 0).

There are two challenges here: 1. We have repeated measures on the same
plants (non-independence). 2. We have variation among plants in baseline
height.

A mixed model can handle both of these.

\subsection*{Wrong model: treat all rows as
independent}\label{wrong-model-treat-all-rows-as-independent}
\addcontentsline{toc}{subsection}{Wrong model: treat all rows as
independent}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ week }\SpecialCharTok{*}\NormalTok{ temp, }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{anova}\NormalTok{(m\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Response: height
           Df Sum Sq Mean Sq   F value    Pr(>F)    
week        1 4478.8  4478.8 1888.8551 < 2.2e-16 ***
temp        2  586.9   293.5  123.7678 < 2.2e-16 ***
week:temp   2   46.9    23.4    9.8838 7.572e-05 ***
Residuals 234  554.8     2.4                        
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This model pretends there are \(30 \times 8 = 240\) independent data
points. But most of that information is repeated measures on the same
plants.

We can see that this is pseudoreplication because there are many more
residual degrees of freedom (236) than plants (30). This is a classic
sign of pseudoreplication. We really cannot trust the p-values or
confidence intervals from this model.

\subsection*{Mixed model: random intercept for
plant}\label{mixed-model-random-intercept-for-plant}
\addcontentsline{toc}{subsection}{Mixed model: random intercept for
plant}

Now let's fit a mixed model with plant identity as a random effect:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_lmm1 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ week }\SpecialCharTok{*}\NormalTok{ temp }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ plant\_id), }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{anova}\NormalTok{(m\_lmm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
          Sum Sq Mean Sq NumDF  DenDF   F value    Pr(>F)    
week      4478.8  4478.8     1 207.00 3264.1929 < 2.2e-16 ***
temp        21.6    10.8     2  46.37    7.8666  0.001144 ** 
week:temp   46.9    23.4     2 207.00   17.0806 1.362e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

In the anova table, note that there are no degrees of freedom and no
p-values for fixed effects. This is because calculating these in mixed
models is complicated and there are multiple methods. We will look at
this more later in this chapter.

Interpretation:

\begin{itemize}
\tightlist
\item
  fixed effects describe the \textbf{average} relationship between
  height, week, and temperature
\item
  the random intercept captures baseline differences among plants
\end{itemize}

\subsection*{model checking}\label{model-checking}
\addcontentsline{toc}{subsection}{model checking}

Mixed-model diagnostics (i.e., model checking) can be more involved than
\texttt{lm()}, but you can still start with:

\begin{itemize}
\tightlist
\item
  residual vs fitted plot (nonlinearity / heteroscedasticity)
\item
  normal Q--Q of residuals (approximate)
\item
  check random effect estimates for extreme outliers
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Residuals vs fitted
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(m\_lmm1), }\FunctionTok{resid}\NormalTok{(m\_lmm1),}
     \AttributeTok{xlab =} \StringTok{"Fitted values"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Residuals"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-12-1.pdf}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Normal Q--Q of residuals
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qqnorm}\NormalTok{(}\FunctionTok{resid}\NormalTok{(m\_lmm1)); }\FunctionTok{qqline}\NormalTok{(}\FunctionTok{resid}\NormalTok{(m\_lmm1))}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-13-1.pdf}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Scale--location (sqrt(\textbar resid\textbar) vs fitted)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{fitted}\NormalTok{(m\_lmm1), }\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{abs}\NormalTok{(}\FunctionTok{resid}\NormalTok{(m\_lmm1))),}
     \AttributeTok{xlab =} \StringTok{"Fitted values"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"sqrt(|Residuals|)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-14-1.pdf}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Residuals vs explanatory variable (helpful for nonlinearity with a
  continuous explanatory variable)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(dat}\SpecialCharTok{$}\NormalTok{temp, }\FunctionTok{resid}\NormalTok{(m\_lmm1), }\AttributeTok{xlab =} \StringTok{"x"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Residuals"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \DecValTok{0}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-15-1.pdf}}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-caution-color!10!white, title=\textcolor{quarto-callout-caution-color}{\faFire}\hspace{0.5em}{Caution}, colframe=quarto-callout-caution-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

\textbf{Don't over-interpret p-values}\\
In mixed models, inference depends on how you handle degrees of freedom
and uncertainty. In BIO144, focus on: - correct model structure (what
must be random?) - effect sizes and uncertainty (CIs) - sensible plots
and biological interpretation

\end{tcolorbox}

\subsection*{Extension: random slopes for
week}\label{extension-random-slopes-for-week}
\addcontentsline{toc}{subsection}{Extension: random slopes for week}

If you believe plants differ in growth rate (not only baseline), add a
random slope:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_lmm2 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ week }\SpecialCharTok{*}\NormalTok{ temp }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{+}\NormalTok{ week }\SpecialCharTok{|}\NormalTok{ plant\_id), }\AttributeTok{data =}\NormalTok{ dat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
boundary (singular) fit: see help('isSingular')
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(m\_lmm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Linear mixed model fit by REML. t-tests use Satterthwaite's method [
lmerModLmerTest]
Formula: height ~ week * temp + (1 + week | plant_id)
   Data: dat

REML criterion at convergence: 822.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.43220 -0.64862  0.04439  0.63469  2.31852 

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 plant_id (Intercept) 0.9012460 0.94934      
          week        0.0006804 0.02608  1.00
 Residual             1.3682645 1.16973      
Number of obs: 240, groups:  plant_id, 30

Fixed effects:
             Estimate Std. Error        df t value Pr(>|t|)    
(Intercept)   8.12308    0.38358  29.45344  21.177  < 2e-16 ***
week          1.69948    0.05767 158.78156  29.469  < 2e-16 ***
temp15        1.45872    0.54247  29.45344   2.689 0.011676 *  
temp20        2.24843    0.54247  29.45344   4.145 0.000263 ***
week:temp15   0.10592    0.08156 158.78156   1.299 0.195916    
week:temp20   0.45169    0.08156 158.78156   5.538 1.24e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Correlation of Fixed Effects:
            (Intr) week   temp15 temp20 wk:t15
week        -0.404                            
temp15      -0.707  0.285                     
temp20      -0.707  0.285  0.500              
week:temp15  0.285 -0.707 -0.404 -0.202       
week:temp20  0.285 -0.707 -0.202 -0.404  0.500
optimizer (nloptwrap) convergence code: 0 (OK)
boundary (singular) fit: see help('isSingular')
\end{verbatim}

Compare the two mixed models:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(m\_lmm1, m\_lmm2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
refitting model(s) with ML (instead of REML)
\end{verbatim}

\begin{verbatim}
Data: dat
Models:
m_lmm1: height ~ week * temp + (1 | plant_id)
m_lmm2: height ~ week * temp + (1 + week | plant_id)
       npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)
m_lmm1    8 826.51 854.35 -405.25   810.51                     
m_lmm2   10 830.01 864.82 -405.00   810.01 0.4974  2     0.7798
\end{verbatim}

There is little evidence that adding random slopes improves the model
here (p = 0.77). But in other datasets it might.

\begin{tcolorbox}[enhanced jigsaw, left=2mm, colbacktitle=quarto-callout-note-color!10!white, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, colframe=quarto-callout-note-color-frame, arc=.35mm, rightrule=.15mm, toprule=.15mm, coltitle=black, opacityback=0, bottomrule=.15mm, breakable, titlerule=0mm, bottomtitle=1mm, colback=white, opacitybacktitle=0.6, toptitle=1mm, leftrule=.75mm]

This likelihood ratio test compares nested models. It is widely used,
but has subtleties for random effects. In BIO144 you can treat it as a
reasonable practical tool, while noting that ``testing random effects''
is an advanced topic.

\end{tcolorbox}

\subsection*{Visualising fitted values}\label{visualising-fitted-values}
\addcontentsline{toc}{subsection}{Visualising fitted values}

We can visualise the fitted values from both mixed models:

\pandocbounded{\includegraphics[keepaspectratio]{11.1-mixed-models_files/figure-pdf/unnamed-chunk-19-1.pdf}}

There is not much difference between the two models here, but in other
datasets random slopes can make a big difference.

\subsection*{Significance testing for fixed
effects}\label{significance-testing-for-fixed-effects}
\addcontentsline{toc}{subsection}{Significance testing for fixed
effects}

Calculating p-values for fixed effects in mixed models is complicated,
and there are multiple methods (Satterthwaite, Kenward-Roger, likelihood
ratio tests, bootstrapping, Bayesian credible intervals). It is
difficult because the degrees of freedom depend on the random effects
structure and the data. There is no clear and objective method to get
the degrees of freedom.

Neverthless, we can get p-values for terms using the \texttt{lmerTest}
package (optional). This changes the \texttt{lmer()} function to provide
p-values using Satterthwaite's method for degrees of freedom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m\_lmm1 }\OtherTok{\textless{}{-}} \FunctionTok{lmer}\NormalTok{(height }\SpecialCharTok{\textasciitilde{}}\NormalTok{ week }\SpecialCharTok{*}\NormalTok{ temp }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ plant\_id), }\AttributeTok{data =}\NormalTok{ dat)}
\FunctionTok{anova}\NormalTok{(m\_lmm1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Type III Analysis of Variance Table with Satterthwaite's method
          Sum Sq Mean Sq NumDF  DenDF   F value    Pr(>F)    
week      4478.8  4478.8     1 207.00 3264.1929 < 2.2e-16 ***
temp        21.6    10.8     2  46.37    7.8666  0.001144 ** 
week:temp   46.9    23.4     2 207.00   17.0806 1.362e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\subsection*{Reporting (template)}\label{reporting-template}
\addcontentsline{toc}{subsection}{Reporting (template)}

\begin{itemize}
\tightlist
\item
  We modelled plant height as a function of week, temperature, and their
  interaction using a linear mixed model with plant identity as a random
  effect.\\
\item
  Height increased with week (fixed effect of week), and plants at
  higher temperature were taller on average (fixed effect of
  temperature).
\item
  All fixed effects had a p-value \textless{} 0.05, calculate using
  Satterthwaite's method for degrees of freedom (lmerTest package).
\item
  Including plant as a random effect accounted for non-independence due
  to repeated measures.
\end{itemize}

\section*{Review}\label{review-9}
\addcontentsline{toc}{section}{Review}

\markright{Review}

\begin{itemize}
\tightlist
\item
  Mixed models are used when observations are \textbf{grouped}
  (non-independent).
\item
  They combine \textbf{fixed effects} (average relationships) with
  \textbf{random effects} (group-to-group variation).
\item
  Random intercepts model different baselines among groups; random
  slopes allow different responses.
\item
  Nested and crossed random effects reflect study design.
\item
  Mixed models let you keep all observations while avoiding
  pseudoreplication and false confidence.
\end{itemize}

\section*{Further reading (optional): mixed
models}\label{further-reading-optional-mixed-models}
\addcontentsline{toc}{section}{Further reading (optional): mixed models}

\markright{Further reading (optional): mixed models}

In this course, there is only a short introduction to mixed (multilevel)
models. Students who are curious and would like to explore this topic
further (purely for their own interest) may find the following resources
useful. Material from these resources will not be examined in the final
exam, unless it is also already present in the course book.

\textbf{McElreath, R. -- Statistical Rethinking} This book offers an
excellent conceptual introduction to hierarchical (mixed) models, with a
strong focus on understanding why they are useful. It introduces ideas
such as partial pooling and shrinkage in a very intuitive way. The book
uses a Bayesian perspective, but the conceptual insights are valuable
even if you later apply frequentist methods. Strongly recommended by
Owen for conceptual understanding.

\textbf{Gelman, A. \& Hill, J. -- Data Analysis Using Regression and
Multilevel/Hierarchical Models} A classic and very practical text on
regression and mixed models, with many applied examples. It complements
Statistical Rethinking well and is particularly useful if you want to
understand mixed models as they are commonly used in practice.

\textbf{R documentation and tutorials for mixed-model packages} If you
are interested in implementation in R, the vignettes and documentation
for packages such as lme4 (frequentist) or brms (Bayesian) provide
hands-on examples of fitting, interpreting, and extending mixed models.

These resources are entirely optional and intended for students who wish
to deepen their understanding beyond the scope of the course.

\bookmarksetup{startatroot}

\chapter*{What next (L11-2)}\label{what-next-l11-2}
\addcontentsline{toc}{chapter}{What next (L11-2)}

\markboth{What next (L11-2)}{What next (L11-2)}

During this course, you have learned a variety of data analysis
techniques using R, including data manipulation, visualization, and
statistical modeling. You have gained a solid foundation in using R for
data analysis, you can analyse data that meets the assumptions of linear
models, and some types of data that do not (e.g., count data and binary
data using GLMs).

Of course, there is much more to learn! There are many opportunities for
you to further develop your skills in R and data analysis. What is the
next step after this course? What are the options to further improve
your skills in data analysis in R? What other types of analyses could
you learn about, and when might you need them?

Here are a list of other types of problem / question that we might have,
and types of analysis that could be relevant. The list is by no means
exhaustive, but it should give you some ideas of what to explore next,
and what to explore when you encounter specific types of data or
research questions.

\begin{itemize}
\item
  \textbf{Time series analysis}: If your data are collected over time
  (e.g., daily, monthly, yearly), you might need to learn about time
  series analysis techniques such as ARIMA models, seasonal
  decomposition, and forecasting methods. A key feature of time series
  data is that observations are not independent, which violates
  assumptions of many standard statistical methods. This needs to be
  carefully handled in the analysis.
\item
  \textbf{Spatial analysis}: If your data have a spatial component
  (e.g., locations, regions), you might need to learn about spatial
  statistics, geostatistics, and spatial modeling techniques. This could
  include methods such as kriging, spatial autocorrelation analysis, and
  spatial regression models. Again, spatial data often violate
  independence assumptions, requiring specialized methods.
\item
  \textbf{Non-linear regression}: If the relationship between your
  explanatory variables and response variable is not linear, you might
  need to learn about non-linear regression techniques. These can
  estimate the parameters of specific non-linear functions, and to
  assess the goodness of fit.
\item
  \textbf{Breakpoint analysis}: If you suspect that there are changes in
  the relationship between variables at certain points (e.g., before and
  after an intervention), you might need to learn about breakpoint
  analysis techniques, such as piecewise regression or change point
  detection methods.
\item
  \textbf{Generalized Additive Models (GAMs)}: If you want to model
  complex, non-linear relationships between explanatory variables and
  response variables while maintaining some interpretability, you might
  need to learn about GAMs. These models use smooth functions to capture
  non-linear effects. They are rather elegant!
\item
  \textbf{Structural Equation Modeling (SEM)}: If you want to analyze
  complex relationships among multiple variables, including latent
  variables, you might need to learn about SEM techniques. SEM allows
  for the modeling of direct and indirect effects, as well as
  measurement error. Effectively, we can build and test complex causal
  models. Variables can be both explanatory variables and responses at
  the same time.
\item
  \textbf{Machine Learning}: If you want to make predictions or classify
  data based on patterns, you might need to learn about machine learning
  techniques such as decision trees, random forests, support vector
  machines, and neural networks. These methods can handle large datasets
  and complex relationships but may sacrifice some interpretability.
\item
  \textbf{Meta-analysis}: If you want to synthesize results from
  multiple studies to draw broader conclusions, you might need to learn
  about meta-analysis techniques. This involves combining effect sizes
  from different studies and assessing heterogeneity among them.
\item
  \textbf{Survival analysis}: If your data involve time-to-event
  response variables (e.g., time until failure, time until death), you
  might need to learn about survival analysis techniques such as
  Kaplan-Meier estimation, Cox proportional hazards models, and
  parametric survival models.
\item
  \textbf{Non-parametric methods}: If your data do not meet the
  assumptions of parametric tests (e.g., normality, homoscedasticity),
  and you really can't figure out how to make a parametric model (e.g.,
  LM or GLM) you might need to learn about non-parametric methods such
  as rank-based tests, bootstrapping, and permutation tests.
\item
  \textbf{Power analysis and sample size estimation}: If you want to
  design studies with adequate statistical power, you might need to
  learn about power analysis techniques. This involves calculating the
  required sample size based on effect sizes, significance levels, and
  desired power.
\item
  \textbf{Bayesian statistics}: If you want to incorporate prior
  knowledge and uncertainty into your analyses, you might need to learn
  about Bayesian statistical methods. This involves using Bayes' theorem
  to update prior beliefs based on observed data.
\end{itemize}

These are just a few examples of the many types of analyses that you
might encounter in your data analysis journey. The choice of which
techniques to learn next will depend on your specific research
questions, data characteristics, and goals. Ideally you will plan your
analyses when you design your study, so that you can collect the right
type of data to answer your questions. When you don't or when something
changes, you can then explore and discussion with experts which
techniques are most appropriate.

A final word of advice\ldots{} try to not be driven by techniques.
Instead, be driven by your research questions. After all, we are not
doing data analysis for its own sake, but to answer questions about the
world around us. Let your questions guide your learning journey!

\bookmarksetup{startatroot}

\chapter*{Review (L12)}\label{review-l12}
\addcontentsline{toc}{chapter}{Review (L12)}

\markboth{Review (L12)}{Review (L12)}

\textbf{You need to prepare in advance for this lecture.}

In the final lecture of the course, we will review and repeat any of the
content that you find challenging. Please write in the Forum which
topics you would like to revisit, so that we can focus on those during
the session. There may also be chances to ask during the lecture time
questions on topics that you find difficult.

So the preparation for this lecture is to think about which topics you
would like to discuss again, and to post these in the Forum.

Here are some ideas of topics that you might want to revisit or learn
about:

\begin{itemize}
\tightlist
\item
  Types of study designs in biology.
\item
  Framing research questions.
\item
  Observational studies.
\item
  Experimental studies.
\item
  Key modeling concepts: interactions, overfitting, degrees of freedom,
  variance explained, hypothesis testing, null hypothesis\\
\item
  Parametric vs non-parametric approaches: what they mean and when to
  use them.
\item
  Going through any of the practical exercises in the course.
\item
  AI Assistant prompt creation\ldots{} how much can we make it do, and
  how much can we trust it?
\end{itemize}


\backmatter


\end{document}
