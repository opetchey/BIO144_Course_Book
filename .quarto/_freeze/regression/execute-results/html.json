{
  "hash": "2719ad5acb4e49672801662c9935c845",
  "result": {
    "markdown": "# Regression\n\n## Introduction\n\nLinear regression is a common statistical method that models the relationship between a dependent (response) variable and one or more independent (explanatory) variables. The relationship is modeled with the equation for a straight line ($y = a + bx$).\n\nWith linear regression we can answer questions such as:\n\n* How does the dependent variable change with respect to the independent variable?\n* What amount of variation in the dependent variable can be explained by the independent variable?\n* Is there a statistically significant relationship between the dependent variable and the independent variable?\n* Does the linear model fit the data well?\n\nIn this chapter / lesson we will explore what is linear regression and how to use it to answer these questions. We'll cover the following topics:\n\n* Why use linear regression?\n* What is the linear regression model?\n* Fitting the regression model (= finding the intercept and the slope).\n* Is linear regression good enough model to use?\n* What to do when things go wrong?\n* Transformation of variables/the response.\n* Identifying and handling odd data points (aka outliers).\n\nIn this chapter / lesson we will not discuss the statistical significance of the model. We will cover this topic in the next chapter / lesson.\n\n### Accompanying reading materia\n\n* Chapter 2 of _Lineare Regression_, p.7-20 (Stahel script).\n\n### Why use linear regression?\n\n* It's a good starting point because it is a relatively simple model.\n* Relationships are sometimes close enough to linear.\n* It's easy to interpret.\n* It's easy to use.\n* It's actually quite flexible (e.g. can be used for non-linear relationships, e.g., a quadratic model is still a linear model!!! See @sec-kind-of-magic.).\n\n### An example - blood pressure and age\n\nThere are lots of situations in which linear regression can be useful. For example, consider hypertension. Hypertension is a condition in which the blood pressure in the arteries is persistently elevated. Hypertension is a major risk factor for heart disease, stroke, and kidney disease. It is estimated that hypertension affects about 1 billion people worldwide. Hypertension is a complex condition that is influenced by many factors, including age. In fact, it is well known that blood pressure increases with age. But how much does blood pressure increase with age? This is a question that can be answered using linear regression.\n\nHere is an example of a study that used linear regression to answer this question:\nhttps://journals.lww.com/jhypertension/fulltext/2021/06000/association_of_age_and_blood_pressure_among_3_3.15.aspx\n\nIn this study, the authors used linear regression to model the relationship between age and blood pressure. They found that systolic blood pressure increased by 0.28--0.85 mmHg/year. This is a small increase, but it is statistically significant. This means that the observed relationship between age and blood pressure is unlikely to be due to chance.\n\nLets look at some simualted example data:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nWell, that is pretty conclusive. We hardly need statistics. There is a clear positive relationship between age and systolic blood pressure. But how can we quantify this relationship? This is where linear regression comes in. Linear regression can be used to model the relationship between age and systolic blood pressure. With linear regression we can answer the following questions:\n\n* What is a good mathematical representation of the relationship?\n* Is the relationship different from what we would expect by chance?\n* How good is the mathematical representation?\n* How much uncertainty is there in any predictions?\n\nLets try to figure some of these out from the visualisation:\n\n* Guess the slope.\n* Guess the intercept (hint be careful, lots of people get this wrong).\n\n\n\n### Regression from a mathematical perspective\n\n* Given an **independent/explanatory variable** ($X$) and a **dependent/response variable** ($Y$) all points $(x_i,y_i)$, $i= 1,\\ldots, n$, on a  straight line follow the equation\n\n$$y_i = \\beta_0 + \\beta_1 x_i\\ .$$\n\n* $\\beta_0$ is the **intercept** - the value of $Y$ when $x_i = 0$\n* $\\beta_1$ the **slope** of the line, also known as the regression coefficient of $X$.\n* If $\\beta_0=0$ the line goes through the origin $(x,y)=(0,0)$.\n* **Interpretation** of linear dependency: proportional increase in $y$ with increase (decrease) in $x$.\n\n\n\n\n\n\n\n### But the line is not a perfect fit to the data\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nThe line is not a perfect fit to the data. There is some scatter around the line. \n\nIf we collected data about age and blood pressure from different people, we would expect to see a somewhat similar, but not identical, relationship between age and blood pressure. This is because there are other factors that influence blood pressure, such as diet, exercise, and genetics. Even if we collected the data again from the same people, we would expect to see different blood pressure readings, because blood pressure can vary throughout the day. Also, the there could be differences due to the measurement instrument (i.e., some measurement error).\n\nThese other factors are not included in the model, so they are captured in an error term.\n\nThe error term captures the difference between the observed value of the dependent variable and the value predicted by the model.\n\nIn the linear regression model the dependent variable $Y$ is related to the independent variable $x$ as\n\n$$Y = \\beta_0 + \\beta_1 x + \\epsilon \\ $$\nwhere\n\n* $\\epsilon$ is the error term\n* $\\beta_0$ is the intercept\n* $\\beta_1$ is the slope\n* $\\epsilon$ is the error term.\n\nThe error term captures the difference between the observed value of the dependent variable and the value predicted by the model. The error term includes the effects of other factors that influence the dependent variable, as well as measurement error.\n\n\n$$Y \\quad= \\quad \\underbrace{\\text{expected value}}_{E(Y) = \\beta_0 + \\beta_1 x} \\quad + \\quad \\underbrace{\\text{random error}}_{\\epsilon}  \\ .$$\n\n\nGraphically the error term is the vertical distance between the observed value of the dependent variable and the value predicted by the model.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe error term is also known as the residual. It is the variation that *resides* (is left over / is left unexplained) after accounting for the relationship between the dependent and independent variables.\n\n### Dealing with the error\n\nIf we use each of the residuals as an error term, we end up with a perfect fit to the data. This is because the model is fitted to the data, so the residuals are the difference between the observed value of the dependent variable and the value predicted by the model. We then have a model with no error -- everything is explained by the model. This is known as overfitting. In fact, we have gained nothing by fitting the model, because we have not learned anything about the relationship between the dependent and independent variables. We have simply memorized the data!!!\n\nIn order to avoid this, we need to assume something about the residuals -- we need to *model* the error term. The most common model for the error term is normally distributed with mean 0 and constant variance.\n\n$$\\epsilon \\sim N(0,\\sigma^2)$$\n\nThis is known as the normality assumption. The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.\n\nSo the error term in the following equation is assumed to be normally distributed with mean 0 and constant variance.\n\n$$Y = \\beta_0 + \\beta_1 x + N(0,\\sigma^2) \\ $$\n\nwhere $\\sigma^2$ is the variance of the error term. The variance of the error term is the amount of variation in the dependent variable that is not explained by the independent variable. The variance of the error term is also known as the residual variance.\n\nAn alternate and equivalent formulation is that $Y$ is a random variable that follows a normal distribution with mean $\\beta_0 + \\beta_1 x$ and variance $\\sigma^2$.\n\n$$Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)$$\n\nSo, the answer to the question \"how do we deal with the error term\" is that we model the error term as normally distributed with mean 0 and constant variance. Put another way, the error term is assumed to be normally distributed with mean 0 and constant variance. **This is known as the normality assumption.** The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.\n\n\n\n\n\n\n\n\n### Back to blood pressure and age\n\nThe mathematical model in this case is:\n\n$$SystolicBP = \\beta_0 + \\beta_1 \\times Age + \\epsilon$$\n\nwhere:\n*SystolicBP* is the dependent (response) variable,\n$\\beta_0$ is the intercept,\n$\\beta_1$ is the coefficient of Age,\n*Age* is the independent (explanatory) variable,\n$\\epsilon$ is the error term.\n\nLets just make sure we understand this, by thinking about the units of the variables in this model. This can be very useful because it can help us to understand the model better and to check that the model makes sense.\n\n* What are the units of blood pressure?\n* What are the units of age?\n* What are the units of the intercept?\n* What are the units of the coefficient of Age?\n* What are the units of the error term?\n\n(The units of blood pressure are mmHg (millimeters of mercury). The units of age are years. The units of the intercept are mmHg. The units of the coefficient of Age are mmHg per year. The units of the error term are mmHg.)\n\nWe already guessed that the coefficient of Age would be positive, because blood pressure tends to increase with age. But we could also have guessed that the coefficient of Age would be negative, because blood pressure tends to decrease with age. So, we need to estimate the coefficient of Age from the data. This is what linear regression does. It estimates the coefficients of the independent variables in the model.\n\n## Finding the intercept and the slope\n\nIn a regression analysis, one task is to estimate the intercept and the slope. These are known as the **regression coefficients** $\\beta_0$, $\\beta_1$. We also estimate the **residual variance** $\\sigma^2$ for a given set of $(x,y)$ data.\n\n* **Problem**: For more than two points $(x_i,y_i)$, $i=1,\\ldots, n$, there is generally no perfectly fitting line.\n\n* **Aim:** We want to estimate the parameters $(\\beta_0,\\beta_1)$ of the **best fitting** line $Y = \\beta_0 + \\beta_1 x$.\n\n* **Idea:** Find the **best fitting line** by minimizing the deviations between the data points $(x_i,y_i)$ and the regression line. I.e., minimising the residuals.\n\nBut which deviations?\n\nThese ones?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nOr these?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nOr maybe even these?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nWell, actually its none of these!!!\n\n### Least squares\n\nFor multiple reasons (theoretical aspects and mathematical convenience), the parameters are estimated using the **least squares** approach. In this, yet something else is minimized:\n\nThe parameters $\\beta_0$ and $\\beta_1$ are estimated such that the **sum of squared vertical distances** (sum of squared residuals / errors.)\n\n**SSE** means **S**um of **S**quared **E**rrors.\n\n$$SSE = \\sum_{i=1}^n e_i^2 $$\n\nwhere, as we already saw \n$$e_i = y_i - \\underbrace{(\\beta_0 + \\beta_1 x_i)}_{=\\hat{y}_i} $$\n**Note:** $\\hat y_i = \\beta_0 + \\beta_1 x_i$ are the *predicted values*.\n\nSo we minimize the sum of the areas shown below\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Least squares estimates\n\nFor a given sample of data $(x_i,y_i), i=1,..,n$, with mean values $\\overline{x}$ and $\\overline{y}$, the least squares estimates $\\hat\\beta_0$ and $\\hat\\beta_1$ are computed as \n\n\n$$ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n  (y_i - \\overline{y}) (x_i - \\overline{x})}{ \\sum_{i=1}^n (x_i - \\overline{x})^2 } = \\frac{cov(x,y)}{var(x)}$$\n\n$$\\hat\\beta_0 = \\overline{y} - \\hat\\beta_1 \\overline{x}  $$\n\nMoreover,\n\n\n$$ \\hat\\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2 \\quad \\text{with residuals  } e_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i) $$\n\nis an unbiased estimate of the residual variance $\\sigma^2$.\n\n\\small (Derivations of the equations above are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)\n\n### Why division by $n-2$ ensures an unbiased estimator\n\nWhen estimating parameters ($\\beta_0$ and $\\beta_1$), the regression model is fitted to minimize the residuals. This fitting process inherently *uses up* two *degrees of freedom*, as the model forces the residuals to sum to zero (due to the intercept) and aligns the slope to best fit the data.\n\nThe adjustment (division by $n-2$ instead of $n$) compensates for the loss of variability due to parameter estimation, ensuring the estimator of the residual variance is unbiased. Mathematically, dividing by n - 2 adjusts for this loss and gives an accurate estimate of the population variance when working with sample data.\n\n### Let's do it in R\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Systolic_BP ~ Age, data = bp_age_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2195  -3.4434  -0.0808   3.1383  12.6025 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 98.96874    1.46102   67.74   <2e-16 ***\nAge          0.82407    0.02771   29.74   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.971 on 98 degrees of freedom\nMultiple R-squared:  0.9002,\tAdjusted R-squared:  0.8992 \nF-statistic: 884.4 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nHow do our guesses of the intercept and slope compare to the guesses we made earlier?\n\nRecal that the units of the *Age* coefficient are in mmHg per year. This means that for each additional year of age, the systolic blood pressure increases by 0.82 mmHg.\n\n## Is the model good enough to use?\n\n* All models are wrong, but is ours good enough to be useful?\n* Are the assumption of the model justified?\n* It would be very unwise to use the model before we know if it is good enough to use.\n* Don't jump out of an aeroplane until you know your parachute is good enough!\n\n\n\n### What assumptions do we make?\n\nWe alread heard about one. We assume that the residuals follow a $N(0,\\sigma^2)$ distribution. We make this assumption because it is often well enough met, and it gives great mathematical tractability.\n\nThis assumption implies that:\n\n* (a) The $\\epsilon_i$ are normally distributed.\n* (b) All $\\epsilon_i$ have the same variance: $Var(\\epsilon_i)=\\sigma^2$.\n* (c) The $\\epsilon_i$ are independent of each other.\n\nFurthermore:\n\n* (d) we assumed a linear relationship.\n* (e) implies there are no outliers (implied by (a) above)\n\n\nLets go through each five assumptions.\n\n### (a) Normally distributed residuals\n\nWhat does this mean? How can we check it?\n\nA normal distribution is symmetric and bell-shaped...\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nLets look at the frequency distribution of the residuals of the linear regression of blood pressure and age:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe normal distribution assumption (a) seems ok as well.\n\n\n### (a) Normally distributed residuals: The QQ-plot\n\nUsually, not the histogram of the residuals is plotted, but the\nso-called **quantile-quantile** (QQ) plot. The quantiles of the\nobserved distribution are plotted against the quantiles of the\nrespective theoretical (normal) distribution:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nIf the points lie approximately on a straight line, the data is fairly\nnormally distributed.\n\nThis is often \"tested\" by eye, and needs some experience.\n\n*But what on earth is a quantile???*\n\nImagine we make 21 measures of something, say 21 blood pressures:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nThe median of these is 127.8. The median is the\n50% or 0.5 quantile, because half the data points are above it, and half\nbelow.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n  50% \n127.8 \n```\n:::\n:::\n\n\n\nThe *theoretical quantiles* come from the normal distribution. The\n*sample quantiles* come from the distribution of our residuals.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n#### How do I know if a QQ-plot looks \"good\"?\n\nThere is **no quantitative rule** to answer this question, experience is needed. You can gain this experience from simulations. To this end, we can generate the same number of data points of a normally distributed variable and compare this simulated qqplot to our observed one.\n\nExample: Generate 100 points $\\epsilon_i \\sim N(0,1)$ each time:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### (b) Equal variance (all $\\epsilon_i$ have the same variance)\n\nBasically, we're interested if the size of the residuals tends to show a pattern with the fitted values. By *size* of the residuals we mean the *absolute value* of the residuals.\n\nWe have assumed that there is no relationship between the residuals and the fitted values.\n\nThat is, variance of the residuals is a constant: $\\text{Var}(\\epsilon_i) = \\sigma^2$. And not, for example $\\text{Var}(\\epsilon_i) = \\sigma^2 \\cdot x_i$.\n\nSo we're going to plot the size of the residuals against the fitted values.\n\nThis graph is known as the scale-location plot is particularly suited to check the assumption of equal variances (__homoscedasticity / Homoskedastizität__).\n\nThe idea is to plot the square root of the (standardized) residuals $\\sqrt{|R_i|}$ against the fitted values $\\hat{y_i}$. There should be __no trend__: \n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n#### How it looks with the variance increasing with the fitted values\n\nHere's a graphical example of how it would look if the variance of the residuals increases with the fitted values:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-17-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n### (c) Independence (the $\\epsilon_i$ are independent of each other)\n\nWe assume that the residuals are independent of each other. This means that the value of one residual is not somehow related to the value of another.\n\nThe dataset about blood pressure we looked at contained 100 observations, each one made from a different person. In such a study design, we could be safe in the assumption that the people are independent, and therefore the assumption that the residuals are independent.\n\nImagine, however, if we had 100 observations of blood pressure collected from 50 people, because we measured the blood pressure of each person twice. In this case, the residuals would not be independent, because the blood pressure of the same person is likely to be similar. A person is likely to have a high blood pressure in both measurements, or a low blood pressure in both measurements. This would mean they have a high residual in both measurements, or a low residual in both measurements.\n\nIn this case, we would need to account for the fact that the residuals are not independent. We would need to use a more complex model, such as a mixed effects model, to account for the fact that the residuals are not independent. We will talk about this again in the last week of this course.\n\nIn general, you should always think about the study design when you are analysing data. You should always think about whether the residuals are likely to be independent of each other. If they are not, you should think about how you can account for this in your analysis.\n\nA good way to assess if there could be dependencies in the residuals is to be critical about what is the unit of observation in the data. In the blood pressure example, the unit of observation is the person. Count the number of persons in the study. If there are fewer persons than observations, you must have measured some people at least twice. Repeating measures on the same person is a common way to get dependent residuals.\n\nSo, to check the assumption of independence, you should:\n  \n* Think carefully about the study design.\n* Think carefully about the unit of observation in the data.\n* Compare the number of observations to the number of units of observation.\n\n\n\n### (d) Linearity assumption\n\nThe linearity assumption states that the relationship between the independent variable and the dependent variable is linear. This means that the dependent variable changes by a constant amount for a one-unit change in the independent variable. And that this slope is does not change with the value of the independent variable.\n\nThe blood pressure data seems to be linear:\n  \n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nIn contrast, look at this linear regression through data that appears non-linear:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nAnd with the residuals shown as red lines:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nAt low values of $y$, the residuals are positive, at intermediate values of $y$ the residuals are negative, and at high values of $y$ the residuals are positive. This pattern in the residuals is a sign that the relationship between $x$ and $y$ is not linear.\n\nWe can plot the value of the residuals against the $y$ value directly, instead of looking at the pattern in the graph above. This is called a **Tukey-Anscombe plot**. It is a graph of the residuals versus the fitted $y$ values.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\nWe can very clearly see pattern in the residuals in this Tukey-Anscombe plot. The residuals are positive, then negative, then positive, as the fitted $y$ value gets larger.\n\nThe red line in the Tukey-Anscombe plot is a loess smooth. It is automatically added to the plot. It is a way of estimating the pattern in the residuals. If the red line is not flat, then there is a pattern in the residuals. However, the loess smooth is not always reliable. It is a good idea to look at the residuals directly, without this smooth.\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nThe data here is simulated to show a very clear pattern in the residuals. In real data, the pattern might not be so clear. But if you suspect you see a pattern in the residuals, it could be a sign that the relationship between the independent and dependent variable is not linear.\n\nHere is the Tukey-Anscombe plot for the blood pressure data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nThere is very little evidence of any pattern in the residuals. This data is simulated with a truly linear relationship, so we would not expect to see any pattern in the residuals.\n\n\n### (e) No outliers\n\nAn outlier is a data point that is very different from the other data points. Outliers can have a big effect on the results of a regression analysis. They can pull the line of best fit towards them, and make the line of best fit a poor representation of the data.\n\nLets again look at the blood pressure versus age data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\nThere are no obvious outliers in this data. The data points are all close to the line of best fit. This is a good sign that the line of best fit is a good representation of the data.\n\nWhere on this graph would you expect to see outliers?\n\nOutliers are data points that are far from the line of best fit. That is, they are data point that have unusually large residuals.\n\nBut just having a large residual does not mean a data point has a large effect on the value of the slope. Where on the graph would you expect to see data points that have a large effect on the value of the slope?\n\nData points that are far from the mean of the independent variable have a large effect on the value of the slope. These data points have a large leverage. They are data points that are far from the other data points in the $x$ direction.\n\nWe can think of this with the analogy of a seesaw. The slope of the line of best fit is like the pivot point of a seesaw. Data points that are far from the pivot point have a large effect on the slope. Data points that are close to the pivot point have a small effect on the slope.\n\nA measure of distance from the pivot point is called the $leverage$ of a data point. In simple regression, the leverage of individual $i$ is defined as\n\n$h_{i} = (1/n) + (x_i-\\overline{x})^2 / SSX$.\n\nwhere\n$SSX = \\sum_{i=1}^n (x_i - \\overline{x})^2$. (**S**um of **S**quares of **$X$**)\n\nSo, the leverage of a data point is inversely related to $n$ (the number of data points). The leverage of a data point is also inversely related to the sum of the squares of the $x$ values. The leverage of a data point is directly related to the square of the distance of the $x$ value from the mean of the $x$ values.\n\nMore intuitively perhaps, the leverage of a data point will be greater when the are fewer other data points. It will also be greater when the distance from the mean value of $x$ is greater.\n\nGoing back to the analogy of a seesaw, with data points as children on the seesaw, the leverage of a data point is like the distance from the pivot a child sits. But we also have children of different weights. A lighter child will have less effect on the tilt of the seesaw. A heavier one will have a greater effect on the tilt. A heavier child sitting far from the pivot will have a very large effect.\n\nWhat quantity that we already experienced is like the weight of the child?\n\nThe residuals are like the weight of the child. Data points with large residuals have a large effect on the slope of the line of best fit. Data points with small residuals have a small effect on the slope of the line of best fit.\n\nSo the overall effect of a data point on the slope of the line of best fit is a combination of the leverage and the residual. This quantity is called the $influence$ of a data point.\n\nLets add a rather extreme data point to the blood pressure versus age data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nThis is a bit ridiculous, but it is a good example of an outlier. The data point is far from the other data points. It has a large residual. And it is a long way from the pivot (the middle of the $x$ data) so has large leverage.\n\nWe can make a histogram of the residuals and see that the outlier has a large residual:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nAnd we can see that the leverage is large.\n\nThere is a graph that we can look at to see the influence of a data point. This is called a $Cook's$ $distance$ plot. The Cook's distance of a data point is a measure of how much the slope of the line of best fit changes when that data point is removed. The Cook's distance of a data point is defined as\n\n$D_i = \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 / (p \\times MSE)$.\n\nwhere\n$\\hat{y}_j$ is the predicted value of the dependent variable for data point $j$,\n$\\hat{y}_{j(i)}$ is the predicted value of the dependent variable for data point $j$ when data point $i$ is removed,\n$p$ is the number of parameters in the model (2 in this case),\n$MSE$ is the mean squared error of the model.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\nBut does it have a large influence on the value of the slope?\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nThe outlier has a large leverage. It is far from the pivot. But it does not have a large influence on the value of the slope? It is close to the line of best fit. It is close to the other data points. It is close to the pivot of the seesaw.\n\n\n#### Graphical illustration of the leverage effect\n\nData points with $x_i$ values far from the mean have a stronger leverage effect than when $x_i\\approx \\overline{x}$:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\nThe outlier in the middle plot \"pulls\" the regression line in its direction and biases the slope.\n\n\n\n#### Leverage plot (Hebelarm-Diagramm)\n\nIn the leverage plot, (standardized) residuals $\\tilde{R_i}$ are plotted against the leverage $H_{ii}$ :\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-31-1.png){fig-align='center' width=5cm}\n:::\n:::\n\n\nCritical ranges are the top and bottom right corners!!\n\nHere, observations 71, 85, and 87 are labelled as potential outliers.\n\n\nSome texts will give a rule of thumb that points with Cook’s distances greater than 1 should be considered influential, while other books claim a reasonable rule of thumb is  $4 / ( n - p - 1 )$\n where $n$ is the sample size, and $p$ is the number of $beta$ parameters.\n\n\n## What can go \"wrong\" during the modeling process?\n\nAnswer: a lot of things!\n\n* Non-linearity. We assumed a linear relationship between the response and the explanatory variables. But this is not always the case in practice. We might find that the relationship is curved and not well represtented by a straight line.\n* Non-normal distribution of residuals. The QQ-plot data might deviate from the straight line so much that we get worried!\n* Heteroscadisticity (non-constant variance). We assumed homoscadisticity, but the residuals might show a pattern.\n* Data point with high influence. We might have a data point that has a large influence on the slope of the line of best fit.\n\n\n### What to do when things \"go wrong\"?\n\n1. Now: Transform the response and/or explanatory variables.\n2. Now: Take care of outliers.\n3. Later in the course: Improve the model, e.g., by adding additional terms or interactions.\n4. Later in the course: Use another model family (generalized or nonlinear regression model).\n5. Not in this course: Use weighted regression.\n\n\n### Dealing with non-linearity\n\nHere's another example of $y$ and $x$ that are not linearly related:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-32-1.png){fig-align='center' width=10cm height=5cm}\n:::\n:::\n\n\nOne way to deal with this is to transform the response variable $Y$. Here we try two different transformations: $\\log_{10}(Y)$ and $\\sqrt{Y}$.\n\nSquare root transform of the response variable $Y$:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-33-1.png){fig-align='center' width=7cm}\n:::\n:::\n\n\nNot great.\n\nLog transformation of the response variable $Y$:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-34-1.png){fig-align='center' width=7cm}\n:::\n:::\n\n\nNope. Still some evidence of non-linearity.\n\nWhat about transforming the explanatory variable $X$ as well?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-35-1.png){fig-align='center' width=7cm}\n:::\n:::\n\n\n\n\n\nLet's look at the four diagnostic plots for the log-transformed data:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](regression_files/figure-html/unnamed-chunk-36-1.png){fig-align='center' width=7cm}\n:::\n:::\n\n\nAll looks pretty good. Pat on the back for us!\n\nBut... how to know which transformation to use...? It's a bit of trial and error. But we can use the diagnostic plots to help us decide.\n\nMost important is that we do this trial and error before we start using the model. E.g., we don't want to jump from the aeroplane and then find out that our parachute is not working properly! And then try to fix the parachute while we are falling....\n\nLikewise, we can't start using the model and then try to fix it. We need to make sure our model is in good working order before we start using it.\n\nOne of the traps we could fall into is called \"p-hacking\". This is when we try different transformation until we find one that gives us the **result** we want, for example significant relationship. This is a big no-no in statistics. We need to decide on the model (including any transformations) before we start using it, and then stick to it.\n\n\n### Common transformations\n\nWhich transformations could be considered? There is no simple answer. But some guidelines. E.g. if we see non-linearity and increasing variance with increasing fitted values, then a log transform may improve matter.\n\nSome common and useful transformations are:\n\n* The log transformation for concentrations and absolute values.\n* The square-root transformation for count data.\n* The arcsin square-root $\\arcsin(\\sqrt{\\cdot})$ transformation for proportions/percentages.\n\nTransformations can also be applied on explanatory variables, as we saw in the example above.\n\n\n\n### Outliers\n\nWhat do we do when we identify the presence of one or more outliers?\n\n1. Start by checking the \"correctness\" of the data. Is there a typo or a digital point that was shifted by mistake? Check both the response and explanatory variables.\n2. If not, ask whether the model could be improved. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?\n3. Sometimes, an outlier may be the most interesting observation in a dataset! Was the outlier created by some interesting but different process from the other data points?\n4. Consider that outliers can also occur just by chance! If you have a large dataset, you are more likely to observe an outlier than if you have a small dataset. But in a larger dataset, the outlier is also less likely to have a large impact on the results.\n5. Only if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.\n\n\n### Removing outliers\n\nIt might seem tempting to remove observations that apparently don't fit into the picture. However:\n\n* Do this __only with absolute care__ e.g., if an observation has extremely implausible values!  \n* Before deleting outliers, check points 1-5 above.\n* When removing outliers, **you must mention this in your report**.\n\n\nDuring the course we'll see many more examples of things going at least a bit wrong. And we'll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do in the next lesson. But before we wrap up, some good news...\n\n\n## Its a kind of magic... {#sec-kind-of-magic}\n\nAbove, we learned about linear regression, the equation for it, how to estimate the coefficients, and how to check the assumptions. There was a lot of information, and it might seem a bit overwhelming.\n\nYou might also be aware that there are quite a few other types of statistical test, such as multiple regression, t-test, ANOVA, two-way ANOVA, and ANCOVA. It could be worrying to think that you need to learn so much new information for each of these types of tests.\n\nBut this is where the kind-of-magic happens. The good news is that the linear regression model is a very general model. It can be used to perform all of these tests! This is because all of these tests, and linear regression, are special cases of the linear model. So, once you have learned about linear regression, you have learned a lot about all of these other tests as well.\n\nMoreover, the same function in R 'lm' is used to make them all. Awesome.\n\n### So what is a linear model?\n\nA linear model is a model where the relationship between the dependent variable and the independent variables is linear. That is, the dependent variable can be expressed as a linear combination of the independent variables. An example of a linear model is:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon$$\n\nwhere:\n$y$ is the dependent variable,\n$\\beta_0$ is the intercept,\n$\\beta_1, \\beta_2, \\ldots, \\beta_p$ are the coefficients of the independent variables,\n$x_1, x_2, \\ldots, x_p$ are the independent variables,\n$\\epsilon$ is the error term.\n\n\nIn contrast, a non-linear model is a model where the relationship between the dependent variable and the independent variables is non-linear. An example of a non-linear model is the exponential growth model:\n\n$$y = \\beta_0 + \\beta_1 e^{\\beta_2 x} + \\epsilon$$\n\nwhere:\ny is the dependent variable,\n$\\beta_0$ is the intercept,\n$\\beta_1, \\beta_2$ are the coefficients of the independent variables,\n$x$ is the independent variable,\n$\\epsilon$ is the error term.\n\nKeep in mind that a model with a quadratic term is still a linear model. For example:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\epsilon$$\n\nis still a linear model. We can see this if we substitute $x^2$ with a new variable $x_2$, where $x_2 = x^2$. The model then becomes:\n\n$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n\nThis is clearly a linear model.\n\n\n\n\n",
    "supporting": [
      "regression_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}