{
  "hash": "c9e204afe6cb79e8f9c2d8b7dc9ca0d1",
  "result": {
    "markdown": "::: {.cell}\n\n:::\n\n\n\n\n# ANOVA (L6) {.unnumbered}\n\nThis chapter contains the content of the sixth lecture of the course BIO144 Data Analysis in Biology at the University of Zurich.\n\n\n-   One-way ANOVA\n-   Post-hoc tests and contrasts\n-   Two-way ANOVA\n\nANOVA = ANalysis Of VAriance (Varianzanalyse)\n\n\n## Introduction\n\nAnalysis of variance is a method to compare the means of more than two groups. We already know a lot about analysing variance: we compared the total sum of squares (SST), model sum of squares (SSM) and the residual sum of squares (SSE) in the context of linear regression. We used these to calculated the $R^2$ value and the $F$-statistic. To calculate the F-statistic we used the formula $F = \\frac{MSM}{MSE}$, where $MSM$ is the mean square of the model and $MSE$ is the mean square of the residuals. The mean square is a measure of variance.\n\nAnalysis of variance is a special case of a linear model, so much of what we already learned about linear models still holds.\n\nThe defining characteristic of ANOVA is that we are comparing the means of more than two groups. Put another way, we will have a single categorical explanatory variable with more than two levels. We will test whether the means of the response variable are the same across all levels of the explanatory variable.\n\nWhen we have only one categorical explanatory variable, we will use a one-way ANOVA. When we have two categorical explanatory variables, we will use a two-way ANOVA (we'll look at this in the second half of this chapter).\n\nWe have already looked at categorical variables with more than two groups. Let us recap that material from lecture 5.\n\n\n\n\n## Understanding anlysis of variance (ANOVA)\n\nIn ANOVA, we often talk of *within-group variance*, *between-group variance*, and *total variance*. These are not new things:\n\n* Within-group variance is the variance of the residuals, $MS_{residual}$.\n* Between-group variance is the variance of the group means, $MS_{model}$.\n\n\nWe formulate a model as follows: \n$$y_{ij} = \\mu_j + \\epsilon_{i}$$\n\nwhere:\n\n-   $y_{ij}$ = Blood pressure of individual $i$ with diet $j$\n-   $\\mu_i$ = Mean blood pressure of an individual with diet $j$\n-   $\\epsilon_{i}\\sim N(0,\\sigma^2)$ is an independent error term.\n\nGraphically, with the blood pressure and diet data, this looks like:\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\n### Rewrite the model\n\nOne common way to rewrite the model is to define one of the groups as the reference group, and make the mean of that equal to the intercept of the model:\n\n$$\\mu_{meat} = \\beta_0$$\n\nAnd then to express the other group means as deviations from the reference group mean:\n\n$$\\mu_{Med} = \\beta_0 + \\beta_1$$\n$$\\mu_{vegan} = \\beta_0 + \\beta_2$$\n$$\\mu_{veggi} = \\beta_0 + \\beta_3$$\n\nWhen we write out the entire model, we get:\n\n$$y_i = \\beta_0 + \\beta_1 x_i^{1} + \\beta_2 x_i^{2} + \\beta_3 x_i^{3} + \\epsilon_i$$\nwhere:\n$y_i$ is the blood pressure of individual $i$.\n$x_i^{1}$ is a binary variable indicating whether individual $i$ is on the Mediterranean diet.\n$x_i^{2}$ is a binary variable indicating whether individual $i$ is on the vegan diet.\n$x_i^{3}$ is a binary variable indicating whether individual $i$ is on the vegetarian diet.\n\n\nGraphically, the model now looks like this:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n```\n:::\n\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-5-1.png){width=480}\n:::\n:::\n\n\n::: {.callout-important}\nThis looks exactly like the graph and model we made before while learning about categorical variables in Lecture 5. It is! There is nothing different here.\n:::\n\n### The ANOVA test: The $F$-test\n\n**Aim of ANOVA**: to test *globally* if the groups differ. That is we want to test the null hypothesis that all of the group means are equal:\n\n\n$$H_0: \\mu_1=\\mu_2=\\ldots = \\mu_g$$\nThis is equivalent to testing if all $\\beta$s that belong to a categorical variable are =0.\n\n$$H_0: \\beta_1 = \\ldots = \\beta_{g-1} = 0$$\nThe alternate hypothesis is that ${H_1}$: The group means are not all the same.\n\nA key point is that we are testing a null hypothesis that concerns all the groups. We are not testing if one group is different from another group (which we could do with a $t$-test on one of the non-intercept $\\beta$s).\n\nBecause we are testing a null hypothesis that concerns all the groups, we need to use an $F$-test. It asks if the model with the group means is better than a model with just the overall mean.\n\n\n\n### Calculating and analysing the variances\n\nTo derive the ingredients of the $F$-test, we look at the variances :\n\n**Total variability:** SST = $\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij}-\\overline{y})^2$\n\nwhere:\n\n* $y_{ij}$ is the blood pressure of individual $j$ in group $i$\n* $\\overline{y}$ is the overall mean blood pressure\n* $n_i$ is the number of individuals in group $i$\n* $k$ is the number of groups\n\n**Explained variability (between group variability)**: == SSM = $\\sum_{i=1}^k n_i (\\overline{y}_{i} - \\overline{y})^2$\n\nwhere:\n\n* $\\overline{y}_{i}$ is the mean blood pressure of group $i$\n\n*Residual variability (within group variability)**: = SSE = $\\sum_{i=1}^k \\sum_{j=1}^{n_i}  (y_{ij} - \\overline{y}_{i} )^2$\n\n\n\n*SST degrees of freedom**: $n - 1$ (total degrees of freedom is number of observations $n$ minus 1)\n\n*SSM degrees of freedom**: $k - 1$ (model degrees of freedom is number of groups $k$ minus 1)\n\n*SSE degrees of freedom**: $n - k$ (residual degrees of freedom is total degrees of freedom $n - 1$ minus model degrees of freedom $k - 1$)\n\n\nFrom these sums of squares and degrees of freedom we can calculate the mean squares and $F$-statistic:\n\n$$MS_{model} = \\frac{SS_{\\text{between}}}{k-1} = \\frac{SSM}{k-1}$$\n\n$$MS_{residual} = \\frac{SS_{\\text{within}}}{n-k} = \\frac{SSE}{n-k}$$\n\n$$F = \\frac{MS_{model}}{MS_{residual}}$$\n\n### Interpretation of the $F$ statistic\n\n-   $MS_{model}$: Quantifies the variability **between** groups.\n-   $MS_{residual}$: Quantifies the variability **within** groups.\n\n\nHere is an example with very low within group variability, and high between group variability:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n\nAnd here's an example with very high within group variability, and low between group variability:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-7-1.png){width=480}\n:::\n:::\n\n\n\n\n\n### Interpretation of the $F$ statistic II\n\n-   **$F$ increases**\n    -   when the group means become more different, or\n    -   when the variability within groups decreases.\n-   **$F$ decreases**\n    -   when the group means become more similar, or\n    -   when the variability within groups increases.\n\n$\\rightarrow$ The larger $F$, the less likely are the data seen under\n$H_0$.\n\n\n### Source of variance table\n\nThe **sources of variance table** is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the $F$-statistic.\n\n\n| Source | Sum of squares | Degrees of freedom | Mean square                       | F-statistic                       |\n|--------|----------------|--------------------|-----------------------------------|-----------------------------------|\n| Model  | $SSM$          | $k-1$                | $MSE_{model} = SSM / k-1$           | $\\frac{MSE_{model}}{MSE_{error}}$ |\n| Error  | $SSE$          | $n - 1 - (k-1)$        | $MSE_{error} = SSE / (n - 1 - (k-1))$ |                                   |\n| Total  | $SST$          | $n - 1$            |                                   |                                   |\n\n: Sources of variance table\n\n\n\n## Doing ANOVA in R\n\nLet's go back again the question of how diet effects blood pressure. Here is the data:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 3\n     bp diet          person_ID\n  <dbl> <chr>         <chr>    \n1   120 meat heavy    person_1 \n2    89 vegan         person_2 \n3    86 vegetarian    person_3 \n4   116 meat heavy    person_4 \n5   115 Mediterranean person_5 \n6   134 meat heavy    person_6 \n```\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-9-1.png){width=480}\n:::\n:::\n\n\nAnd here is how we fit a linear model to this data:\n\n\n::: {.cell}\n\n:::\n\n\nNext we check the diagnostic plots:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-11-1.png){width=480}\n:::\n:::\n\n\nNothing looks too bad.\n\nNow we can look at the ANOVA table:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(>F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nA suitable sentence to report our findings would be: \"Diet has a significant effect on blood pressure ($F(2, 27) = 20.7, p < 0.0001$)\". This means that the probability of observing such a large $F$ value under the null hypothesis is less than 0.01%.\n\n\n### Difference between pairs of groups\n\nIf the $F$-test of the null hypothesis that $\\beta_1=\\ldots= \\beta_{g-1}=0$ is rejected, a researcher might then be interested:\n\n1. in finding the actual group(s) that deviate(s) from the others.\n2. in estimates of the pairwise differences.\n\nThe summary table in R provides some of these comparison, specifically it contains the estimates for $\\beta_1$, $\\beta_2$, $\\beta_3$ (while the reference was set to $\\beta_0 = 0$). These three $\\beta$ values are the differences between the group means and the reference group mean. We can test if these differences are significantly different from zero using a $t$-test, as you've see before.\n\nHowever, there are two issues:\n\n1. The more tests you do, the more likely you are to find a significant result just by chance. This is called the problem of multiple comparisons. Many test can result in a type-I error: rejecting the null hypothesis when it is actually true. The more tests one does, the more likely one is to make a type-I error.\n\n2. The summary table does not provide all the possible pairwise comparisons. It does not, for example, provide the comparison between the \"vegan\" and the \"vegetarian\" group.\n\nSeveral methods to circumvent the problem of too many \"significant\" test results (type-I error) have been proposed. The most prominent ones are:\n\n* Bonferroni correction\n* Tukey **H**onest **S**ignificant **D**ifferences (HSD) approach\n* Fisher **L**east **S**ignificant **D**ifferences (LSD) approach\n\n\n\n\n\n\n\n\n### Bonferroni correction\n\n**Idea:** If a total of $m$ tests are carried out, simply divide the\ntype-I error level $\\alpha_0$ (often 5%) such that\n\n$$\\alpha = \\alpha_0 / m \\ .$$\n\n### Tukey HSD approach\n\n**Idea:** Take into account the distribution of \\emph{ranges} (max-min)\nand design a new test.\n\n### Fisher's LSD approach\n\n**Idea:** Adjust the idea of a two-sample test, but use a larger\nvariance (namely the pooled variance of all groups).\n\n### Other contrasts\n\nWe can design other contrasts, for example: are diets that contain meat different from diets that do not contain meat?\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 4\n     bp diet          person_ID meat_or_no_meat\n  <dbl> <chr>         <chr>     <chr>          \n1   120 meat heavy    person_1  no meat        \n2    89 vegan         person_2  no meat        \n3    86 vegetarian    person_3  no meat        \n4   116 meat heavy    person_4  no meat        \n5   115 Mediterranean person_5  meat           \n6   134 meat heavy    person_6  no meat        \n```\n:::\n:::\n\n\nHere we defined a new explanatory variable that groups the meat heavy and Mediterranean diet together into a single \"meat\" group and vegetarian and vegan into a single \"no meat\" group. We then fit a model with this explanatory variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_mnm <- lm(bp ~ meat_or_no_meat, data = bp_data_diet)\n```\n:::\n\n\n(We should not look at model diagnostics here, before using the model. But let us continue as if the assumptions are sufficiently met.)\n\nWe now do something a bit more complicated: we compare the variance explained by the model with four diets to the model with two diets. This is done by comparing the two models using an $F$-test. We are testing the null hypothesis that the two models are equally good at explaining the data, in which case the two diet model will explain as much variance as the four diet model.\n\nLet's look at the ANOVA table of the model comparison:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit, fit_mnm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: bp ~ diet\nModel 2: bp ~ meat_or_no_meat\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     46 3901.5                                  \n2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nWe see the residual sum of squares of the model with meat or no meat is over 9'000, while that of the four diet model is less than 4'000. That is, the four diet model explains much more variance in the data than the two diet model. The $F$-test is highly significant, so we reject the null hypothesis that the two models are equally good at explaining the data. And we conclude that its not just whether people eat meat or not, but rather what kind of diet they eat that affects their blood pressure.\n\nIdeally we do not make a lot of contrasts after we have collected and looked at our data. Rather, we would specify the contrasts we are interested in before we collect the data. This is called a priori contrasts. But sometimes we do exploratory data analysis and then we can make post hoc contrasts. In this case we should be careful to adjust for multiple comparisons.\n\n### Choosing the reference category\n\n**Question**: Why was the \"heavy meat\" diet chosen as the reference (intercept) category?\n\n**Answer**: Because R orders the categories alphabetically and takes the first level alphabetically as reference category.\n\nSometimes we may want to override this, for example if we have a treatment that is experimentally the control, then it will usually be useful to set this as the reference / intercept level.\n\nIn R we can set the reference level using the `relevel` function:\n\n\n::: {.cell}\n\n:::\n\n\nAnd now make the model and look at the estimated coefficients:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         95.857      3.481  27.538  < 2e-16 ***\ndietmeat heavy      26.768      4.173   6.414 6.92e-08 ***\ndietMediterranean   14.080      4.173   3.374  0.00151 ** \ndietvegetarian       3.143      4.453   0.706  0.48386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,\tAdjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n```\n:::\n:::\n\n\nNow we see the estimated coefficients for all diets except the vegan diet. The intercept is the mean individuals with vegan diet.\n\n\n## Two-way ANOVA (Zweiweg-Varianzanalyse)\n\nTwo-way ANOVA is used to analyse a specific type of study design. When we have a study with two categorical treatments and all possible combinations of them, we can use a two-way ANOVA.\n\nFor example, take the question of how diet and exercise affect blood pressure. Let's say we can have three levels of diet: meat heavy, Mediterranean, and vegetarian. And that we have two levels of exercise: low and high. And that we have all possible combinations of these two treatments: i.e., we have a total of $3 \\times 2 = 6$ **treatment combinations**.\n\nWe can also represent this study design in a table:\n\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-fymr\"></th>\n    <th class=\"tg-fymr\" colspan=\"2\">Exercise (G)</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0pky\">Diet (B)</td>\n    <td class=\"tg-0pky\">Low (1)</td>\n    <td class=\"tg-0pky\">High (2)</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Meat heavy (1)</td>\n    <td class=\"tg-0pky\"></td>\n    <td class=\"tg-0pky\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Mediterranean (2)</td>\n    <td class=\"tg-0pky\"></td>\n    <td class=\"tg-0pky\"></td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Vegetarian (3)</td>\n    <td class=\"tg-0pky\"></td>\n    <td class=\"tg-0pky\"></td>\n  </tr>\n</tbody>\n</table>\n\nThe six empty cells in the table represent the six treatment combinations.\n\nThis type of study, with all possible combinations, is known as a *factorial design*. The two treatments are called factors, and the levels of the factors are called factor levels. A *fully factorial design* is one where all possible combinations of the factor levels are present.\n\nLet's look at example data:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 4\n  diet          exercise  reps    bp\n  <chr>         <fct>    <int> <dbl>\n1 Mediterranean high         1  110.\n2 Mediterranean low          1  119.\n3 meat heavy    high         1  102.\n4 meat heavy    low          1  128.\n5 vegetarian    high         1  104.\n6 vegetarian    low          1  111.\n```\n:::\n:::\n\n\nWe can use the `xtabs` function to create a table of the data, by cross-tabulating the two treatments diet and exercise:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(~diet + exercise, data = bp_data_2cat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               exercise\ndiet            low high\n  meat heavy     10   10\n  Mediterranean  10   10\n  vegetarian     10   10\n```\n:::\n:::\n\n\nThis tells us there are 10 replicates in each of the six treatment combinations.\n\nAnd a visualisation of the data:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-20-1.png){width=480}\n:::\n:::\n\n\n\n::: {.callout-tip}\n## Think, Pair, Share (#twoway-plot)\n\nWhat do you conclude from this plot?\n:::\n\n\n### The model for 2-way ANOVA\n\n\nAssume we have a factorial design with two treatments (factors), factor $B$ and factor $G$.\n\nAnd that we can label the levels of factor $B$ as $i=1,2...$ and factor $\\G$ as $j=1,2...$.\n\nThen we can denote a particular treatment combination as $B_iG_j$.\n\nAnd let us set the one of the treatment combinations as the intercept of the model, and the let the intercept be equal to the mean of the observations in the treatment combination $B1G1$.\n\n$$intercept = \\frac{1}{n_{11}}\\sum_{k=1}^{n_{11}} y_{1,1,k}$$\n\nwhere:\n\n* $y_{1,1,k}$ is the $k$th observation in the treatment combination $B1G1$\n* $n_{11}$ is the number of observations in the treatment combination $B1G1$.\n\nAnd we will let all of the other treatment combinations be represented by the **effects** $\\beta_i$ and $\\gamma_j$.\n\nThe resulting linear model is:\n\n$$y_{ijk} = intercept + \\beta_i + \\gamma_j + (\\beta\\gamma)_{ij} + \\epsilon_{ijk} \\quad \\text{with} \\quad \\epsilon_{ijk} \\sim N(0,\\sigma^2)$$\n\nwhere\n\n* $y_{ijk}$ is the $k$th observation in the treatment combination of $i$ and $j$.\n* $(\\beta\\gamma)_{ij}$ is the interaction effect between the $i$th level of factor $\\beta$ and the $j$th level of factor $\\gamma$.\n\nIn this model, we set $\\beta_1=\\gamma_1=0$ and $(\\beta\\gamma)_{11}=0$ because they are already included in the intercept.\n\n\n\n\n### Using R for 2-way ANOVA\n\nIn R, a two-way ANOVA is as simple as one-way ANOVA, just add another\nvariable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod1 <- lm(bp ~ diet * exercise, data = bp_data_2cat)\n```\n:::\n\n\nNote that, as we saw in the chapter about interactions, we include the main effects of diet and exercise and the interaction term with the short hand `diet * exercise`.\n\nOf course we next check the model diagnostics:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](7-ANOVA_files/figure-html/unnamed-chunk-22-1.png){width=480}\n:::\n:::\n\n\nNo clear patterns: all is good.\n\n### Hypothesis testing\n\nAs is implied by the name \"Analysis of variance\" we analyse variances, here mean squares, to test hypotheses. And as before we use an $F$-test to do this. Remember that the $F$-test is a ratio of two mean squares (where mean squares are a kind of variance).\n\n\n\n::: {.callout-tip}\n## Think, Pair, Share (#full-degrees)\n\nHow many degrees of freedom for error will there be when we fit this model with both main effects and the interaction term? Hint: remember that the degrees of freedom for error is the number of observations minus the number of parameters estimated.\n:::\n\nWe can have have a null hypothesis of no effect for each of the two main effects and for the interaction. So we can do an $F$-test for each of these null hypotheses.\n\nHere is the ANOVA table:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: bp\n              Df  Sum Sq Mean Sq F value    Pr(>F)    \ndiet           2  390.74  195.37  9.9672 0.0002069 ***\nexercise       1 1297.52 1297.52 66.1966 5.952e-11 ***\ndiet:exercise  2  340.67  170.33  8.6901 0.0005346 ***\nResiduals     54 1058.46   19.60                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n**Interpretation**: All three of the null hypotheses are rejected. Importantly, we see that the interaction term is significant, which means that the effect of one treatment is different depending on the level of the other treatment. This means that even though the main effects are significant, we cannot interpret them without considering the interaction term. I.e., we cannot say anything general about the effect of diet or exercise alone on blood pressure. We have to qualify any statement about the effect of diet or exercise with \"depending on the level of the other treatment\". Or state something like \"the exercise reduces blood pressure greatly for people with a meat heavy diet, but reduces blood pressure only slightly for people with a vegetarian diet or Mediterranean diet\".\n\n### Interpreting coefficients\n\nWe can look at the estimated coefficients to see the size of the effects, but be aware that it contains only a subset of the possible effects; it would contain different values if a different treatment combination were set to be the intercept of the model. Also be aware that often we are mostly interested in the $F$-test and their hypotheses test, and are less interest in the coefficients and their $t$-tests (unlike in a regression model where we are often interested in the coefficients and their $t$-tests).\n\nFinally, be aware that it needs a bit of work to interpret the coefficients, because they are relative to the intercept. Let's try to figure out what that means. First, back to the table of the experimental design. This time we will put in the cells an expression for the mean of that treatment combination:\n\n\n<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}\n.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\"><thead>\n  <tr>\n    <th class=\"tg-fymr\"></th>\n    <th class=\"tg-fymr\" colspan=\"2\">Exercise (G)</th>\n  </tr></thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0pky\">Diet (B)</td>\n    <td class=\"tg-0pky\">Low (1)</td>\n    <td class=\"tg-0pky\">High (2)</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Meat heavy (1)</td>\n    <td class=\"tg-0pky\">$B_1G_1$</td>\n    <td class=\"tg-0pky\">$B_1G_2$</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Mediterranean (2)</td>\n    <td class=\"tg-0pky\">$B_2G_1$</td>\n    <td class=\"tg-0pky\">$B_2G_2$</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0pky\">Vegetarian (3)</td>\n    <td class=\"tg-0pky\">$B_2G_1$</td>\n    <td class=\"tg-0pky\">$B_3G_2$</td>\n  </tr>\n</tbody>\n</table>\n\n\nSo, for example, the mean of the treatment combination \"Meat heavy, Low\" is $B_1G_1$. And the mean of the treatment combination \"Mediterranean, High\" is $B_2G_2$.\n\nHowever, the coefficients in the summary table given by R are not like this. They are coefficients relative to an intercept / reference treatment combination. The reference treatment combination chosen by R is the first level of the first factor and the first level of the second factor. In this case, that is \"**Meat heavy, Low**\" -- $B_1G_1$.\n\nAll of the other coefficients are about differences from this reference treatment combination.\n\nSo, for example, the coefficient for \"High\" in the \"Exercise\" factor (appearing as `exercisehigh` in the summary table) is the difference in mean blood pressure between the treatment combination \"Meat heavy, High\" ($B_1G_2$) and the treatment combination \"Meat heavy, Low\". Put another way, $B_1G_2 = B_1G_1 + \\gamma_2$ where $\\gamma_2$ is the coefficient for \"High\" in the \"Exercise\" factor.\n\nAnd the coefficient for \"Mediterranean\" in the \"Diet\" factor (appearing as `dietMediterranean` in the summary table) is the difference in mean blood pressure between the treatment combination \"Mediterranean, Low\" and the treatment combination \"Meat heavy, Low\". Put another way, $B_2G_1 = B_1G_1 + \\beta_2$ where $\\beta_2$ is the coefficient for \"Mediterranean\" in the \"Diet\" factor.\n\n**Let us for a  moment assume that the effects of diet and exercise are additive.** If this is the case, then the mean for $B_2G_2$ = $B_1G_1 + \\beta_2 + \\gamma_2$. That is, the mean for \"Mediterranean, High\" is the mean for \"Meat heavy, Low\" plus the effect of \"Mediterranean\" plus the effect of \"High\".\n\nHowever, if the effects are not additive, then the mean for $B_2G_2$ is not $B_1G_1 + \\beta_2 + \\gamma_2$. Rather, it is $B_2G_2 = B_1G_1 + \\beta_2 + \\gamma_2 + (\\beta\\gamma)_{22}$. That is, the mean for \"Mediterranean, High\" is the mean for \"Meat heavy, Low\" $B_1G_1$ plus the effect of \"Mediterranean\" $\\beta_2$ plus the effect of \"High\" $\\gamma_2$ plus the non-additive effect between \"Mediterranean\" and \"High\" $(\\beta\\gamma)_{22}$.\n\nNon-additivity implies an interaction, therefore the non-additive effect is the interaction effect. In the summary table these interaction effects are those that contain a colon (:), e.g., `dietMediterranean:exercisehigh`.\n\nHere's a graphical representation of how the coefficients in the summary table relate to the means of the treatment combinations:\n\n![Understanding coefficients](assets/twowayanova1.png){width=300}\n\n::: {.callout-tip}\n## Think, Pair, Share (veghigh-estimate)\n\nFrom the values in the coefficients table, calculate the estimated mean of the treatment combination \"vegetarian, High\".\n::\n\n\n## Why not perform multiple $t$-tests?\n\nTo carry out pairwise $t$-tests between any two groups.\n\n-   How many tests would this imply?\n-   Why is this not a very clever idea?\n\n\n\n\n## Summing up\n\n* ANOVA is just another linear model.\n* It is used when we have categorical explanatory variables.\n* We use $F$-tests to test the null hypothesis of no difference among the means of the groups (categories).\n* We can use contrasts and post-hoc tests to test specific hypotheses about the means of the groups.\n* Two-way ANOVA is used when we have two categorical explanatory variables and can be used to test for interactions between them.\n\n\n## Additional reading\n\nPlease feel free to look at the follow resources for a slightly different perspective and some more information on ANOVA:\n\n-   Chapter 12 from Stahel book *Statistische Datenenalyse*\n-   *Getting Started with R* chapters 5.6 and 6.2\n\n\n",
    "supporting": [
      "7-ANOVA_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}