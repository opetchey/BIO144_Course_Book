```{r}
#| echo: false
source("_common.R")
```

# Multiple explanatory variables

In the previous chapters we covered simple linear regression and one-way analysis of variance. In both we had one response variable and one explanatory variable. In both cases we made a linear model to relate the response variable to the explanatory variable. The two cases differed in the type of explanatory variable. In the case of simple linear regression, the explanatory variable was continuous. In the case of one-way ANOVA, the explanatory variable was categorical.

We will now extend the linear model and analyses to cases with more than one (i.e., multiple) explanatory variables. The explanatory variables can be continuous or categorical, and can be a mixture of the two.

Some combinations of explanatory variables have special names:

* Multiple (more than one) continuous explanatory variables -> Multiple linear regression.
* Two categorical explanatory variables -> Two-way ANOVA.
* One continuous and one categorical explanatory variable -> Analysis of covariance (ANCOVA).

We will look at each of these, and start with multiple linear regression. As well as the details of a particular case (e.g., multiple linear regression), we will also cover general concepts that apply to all cases with multiple explanatory variables, such as:

* Visualising data and models with multiple explanatory variables.
* Correlations between explanatory variables and the implications.
* Interactions between explanatory variables, and the implications.
* The linear model equation for multiple explanatory variables.
* Specifying models in $R$ with multiple explanatory variables.
* Assessing model assumptions with multiple explanatory variables.
* Measuring model fit ($R^2$, adjusted $R^2$) with multiple explanatory variables.
* Hypothesis testing with multiple explanatory variables.
* Interpretation of model parameters with multiple explanatory variables.
* Model selection with multiple explanatory variables.

Let's get started with multiple linear regression, which is usually shortened to just multiple regression.

## Multiple regression (L4) {.unnumbered}

We previously looked at whether blood pressure is associated with age. This is an important question, because blood pressure has many health implications. However, blood pressure is not only associated with age, but also with other factors, such as weight, height, and lifestyle. In this chapter, we will look at how to investigate the association between blood pressure and multiple explanatory variables.

When we have multiple explanatory variables, we are often interested in questions such as:

* Question 1: Are each of the explanatory variables associated with the response?
* Question 2: What proportion of variability is explained?
* Question 3: Are some explanatory variables more important than others?

### An example dataset

Blood pressure is again the response variable, with age and lifestyle as two explanatory variables. Lifestyle is a continuous variable that is the number of minutes of exercise per week.

```{r}
bp_data <- tibble(age = ceiling(runif(100, 20, 80)),
                       mins_exercise = ceiling(runif(100, 0, 300)))
bp_data$bp <- ceiling(90 + 0.5 * bp_data$age - 0.1 * bp_data$mins_exercise + rnorm(100, 0, 10))
```

Here is a look at the dataset:

```{r}
bp_data
```

Since there are three variables, we can make three different scatter plots to visualise the relationships:

**1. Age vs blood pressure.** This is the graph of the response variable (blood pressure) against one of the explanatory variables (age). It looks like there is evidence of a positive relationship.

```{r}
ggplot2::ggplot(bp_data, aes(x = age, y = bp)) +
  geom_point() +
  labs(x = "Age", y = "Blood pressure")
```

Here we see a positive relationship between age and blood pressure. Blood pressure tends to increase with age.

**2. Minutes of exercise vs blood pressure.** This is a graph of the response variable (blood pressure) against the other explanatory variable (minutes of exercise). It looks like there is evidence of a negative relationship.

```{r}
ggplot2::ggplot(bp_data, aes(x = mins_exercise, y = bp)) +
  geom_point() +
  labs(x = "Minutes of exercise", y = "Blood pressure")
```

Here we see a negative relationship between minutes of exercise and blood pressure. Blood pressure tends to decrease with more minutes of exercise.

**3. Age vs minutes of exercise.** This is a graph of the two explanatory variables against each other. It looks like there is no relationship.

```{r}
ggplot2::ggplot(bp_data, aes(x = age, y = mins_exercise)) +
  geom_point() +
  labs(x = "Age", y = "Minutes of exercise")
```

And here we see no relationship between age and minutes of exercise. The two explanatory variables appear to be independent.

::: {.callout-important}
The lack of correlation between the two explanatory variables is very important. If the two explanatory variables were correlated, we would have a situation known as multicollinearity. Multicollinearity can greatly complicate the interpretation of the results of a multiple regression analysis. We will discuss multicollinearity later.
:::



## The multiple linear regression model

### The model

The multiple linear regression model is an extension of the simple linear regression model. Recall the simple linear regression model is:

$$y_i = \beta_0 + \beta_1 x_i + \epsilon_i$$

where:

* $y_i$ is the response variable
* $x_i$ is the explanatory variable
* $\beta_0$ is the intercept
* $\beta_1$ is the slope
* $\epsilon_i$ is the error term.

The multiple linear regression model with two explanatory variables is:

$$y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \epsilon_i$$

where:

* $x_i^{(1)}$ and $x_i^{(2)}$ are the two explanatory variables
* $\beta_0$ is the intercept
* $\beta_1$ is the slope for the first explanatory variable
* $\beta_2$ is the slope for the second explanatory variable

Note that the intercept $\beta_0$ is the value of the response variable when all explanatory variables are zero. In this example, it would be the blood pressure for someone that is 0 years old and does 0 minutes of exercise per week. This is not a particularly useful scenario, but it is a necessary mathematical construct that helps us to build the model.

We can extend the multiple regression model to have an arbitrary number of explanatory variables:

$$y_i = \beta_0 + \beta_1 x_i^{(1)} + \beta_2 x_i^{(2)} + \ldots + \beta_p x_i^{(p)} + \epsilon_i$$

Where:

$x_i^{(1)}, x_i^{(2)}, \ldots, x_i^{(p)}$ are the $p$ explanatory variables and all else is as before.

or with summation notation:

$$y_i = \beta_0 + \sum_{j=1}^p \beta_j x_i^{(j)} + \epsilon_i$$

Just like in simple linear regression, we can estimate the parameters $\beta_0, \beta_1, \ldots, \beta_p$ using the method of least squares. The least squares method minimizes the sum of the squared residuals:

$$\sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2$$

where $\hat{y}_i$ is the predicted value of the response variable for the $i$th observation:

$$\hat{y}_i = \hat{\beta}_0 + \sum_{j=1}^p \hat{\beta}_j x_i^{(j)}$$
where:

$\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p$ are the estimated parameters.

::: {.callout-tip}
## Think, Pair, Share (#two-shape)

Graphically, a linear regression with one explanatory variable is a line. What is a geometric representation of linear regression with two explanatory variables?
:::


```{r fig.width = 8, fig.height = 8}
# Define the x and y ranges
x <- seq(-5, 5, length.out = 50)
y <- seq(-5, 5, length.out = 50)

# Create a grid of x and y values
grid <- expand.grid(x = x, y = y)

# Define the z values in a matrix (needed for plotting)
z_matrix <- outer(x, y, function(x, y) 0.5* x - 0.6 * y)

# Create the 3D surface plot
fig <- plotly::plot_ly(
  x = ~x,
  y = ~y,
  z = ~z_matrix,
  type = "surface",
  colorscale = "Viridis" # Optional: choose a color scheme
)

# Show the plot
fig
```


### The blood pressure example

Let's write the equation for the blood pressure data:

$$bp_i = \beta_0 + \beta_1 \cdot age_i + \beta_2 \cdot mins\_exercise_i + \epsilon_i$$

where:

* $bp_i$ is the blood pressure for the $i$th observation
* $age_i$ is the age for the $i$th observation
* $mins\_exercise_i$ is the minutes of exercise for the $i$th observation
* $\beta_0$ is the intercept
* $\beta_1$ is the slope for age
* $\beta_2$ is the slope for minutes of exercise
* $\epsilon_i$ is the error term

and the error term is assumed to be normally distributed with mean 0 and constant variance, just as was the case for simple linear regression:

$$\epsilon_i \sim N(0, \sigma^2)$$


### We already know a lot!

**First**, the five assumptions of the multiple linear regression model are the same as for the simple linear regression model:

a) Normality of residuals.
b) Homoscedasticity = constant variance of residuals.
c) Independence of residuals.
d) Linearity.
e) No outliers.

::: {.callout-tip}
## Think, Pair, Share (#assump-match)

Review how we can check these assumptions in the simple linear regression model:

Match the following to the assumptions above:

1) Graph of size of residuals vs. fitted values.
2) QQ-plot.
3) Graph of residuals vs. fitted values.
4) Graph of leverage vs. standardized residuals.

And what is missing?

:::

**Second**, we know how to estimate the parameters $\beta_0, \beta_1, \ldots, \beta_p$ using the method of least squares.

**Third**, we know how to test the significance of the parameters using the $t$-test.

**Fourth**, we know how to calculate the confidence intervals for the parameters, and to make a confidence band.

**Fifth**, we know how to calculate the $R^2$ value to measure the goodness of fit of the model.

**Sixth**, we know how to infer the strength of the relationship between the response variable and an explanatory variable.

**Seventh**, we know how to make predictions using the model, and to make a prediction band.

**What we don't know** is how to answer the four questions already mentioned above:

* Question 1: Are each of the explanatory variables associated with the response?
* Question 2: What proportion of variability is explained?
* Question 3: Are some explanatory variables more important than others?

Let's answer these questions using the blood pressure example.

### Fitting the model

In R, we can fit a multiple linear regression model using the `lm()` function in a very similar way to the simple linear regression model. Here is the code for the blood pressure example. To fit two explanatory variables, we simply add the second variable to the formula using the `+` sign:

```{r eval=T,echo=T}
m1 <- lm(bp ~ age + mins_exercise, data = bp_data)
```

### Checking the assumptions

We can check the assumptions of the multiple linear regression model using the same methods as for the simple linear regression model. Here is the code for the blood pressure example:

```{r eval=T,echo=T}
# Check the assumptions
par(mfrow=c(2,2))
plot(m1, which = c(1,2,3,5), add.smooth = FALSE)
```

We see that the assumptions are met for the blood pressure example:

a) Normality of residuals: The QQ-plot shows that the residuals are normally distributed.
b) Homoscedasticity: The scale-location plot shows that the residuals have constant variance.
c) Independence of residuals: No evidence of pattern or clustering. But also need to know about study design to properly assess independence.
d) Linearity: The residuals vs. fitted values plot shows no clear pattern in the residuals.
e) No outliers: No points with high leverage or high residuals.


## Question 1: As an ensemble, are the explanatory variables associated with the response?

Recall that when we learned about ANOVA we saw that a single categorical explanatory variable with multiple levels can be represented as multiple binary (0/1) explanatory variables. In that case, we used the $F$-test to test the null hypothesis of no effect / relationship for all binary variables together.

Likewise, when we have multiple continuous explanatory variables, we use the $F$-test to test the null hypothesis that **together** the explanatory variables have no association with the response variable. That is, we use the $F$-test to test the null hypothesis that the ensemble of explanatory variables is not associated with the response variable.

Recall that the $F$-test compares the variance explained by the model to the variance not explained by the model (i.e., the variance of the residuals). If the variance explained by the model is significantly greater than the variance not explained by the model, then we can conclude that the explanatory variables are associated with the response variable.

If we reject the null hypothesis, we can conclude that some combination of the explanatory variables is associated with the response variable. However, we cannot conclude which specific explanatory variables are associated with the response variable. To determine which specific explanatory variables are associated with the response variable, we need to perform individual $t$-tests for each explanatory variable. We will do this in the next section.

OK, back to the $F$-test.

We know a lot already from the ANOVA chapter. Let's review how we calculate the $F$-statistic and how we interpret it.



The $F$-statistic is calculated as the ratio of two mean squares:

1. The mean square of the model ($MSE_{model}$).
2. The mean square of the residuals ($MSE_{residual}$).

Recall that a mean square is a sum of squares divided by the associated degrees of freedom. The formulas for these are the same as for ANOVA.

So, to calculate these two mean squares, we need to calculate three sums of squares:

1. The total sum of squares ($SST$).
2. The sum of squares of the model ($SSM$).
3. The sum of squares of the residuals ($SSE$).

We also need to calculate the degrees of freedom associated with each sum of squares.

1. The total degrees of freedom is $n-1$, where $n$ is the number of observations.
2. The model degrees of freedom is $p$, where $p$ is the number of explanatory variables. This is because for each explanatory variable we estimate one parameter (the slope), and each estimated parameter uses up one degree of freedom.
3. The residual degrees of freedom is $n-1-p$.

### The F-statistic in R

In R, we can get the degrees of freedom, sums of squares, mean squares, and 
$F$-statistic for the multiple linear regression model using the `anova()` function. Here is table for the blood pressure example:

```{r echo = TRUE}
anova(m1)
```


### Is my $F$-statistic large or small?

Recall that "The $F$-statistic is calculated as the ratio of the mean square error of the model to the mean square error of the residuals." And that a large $F$-statistic is evidence against the null hypothesis that the slopes of the explanatory variables are zero. And that a small $F$-statistic is evidence to not reject the null hypothesis that the slopes of the explanatory variables are zero.

But how big does the F-statistic need to be in order to confidently reject the null hypothesis?

The null hypothesis that the explained variance of the model is no greater than would be expected by chance. Here, "by chance" means that the slopes of the explanatory variables are zero.

$$H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0$$
The alternative hypothesis is that the explained variance of the model is greater than would be expected by chance. This would occur if the slopes of some or all of the explanatory variables are not zero.

$$H_1: \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{ or } \ldots \text{ or } \beta_p \neq 0$$
To test this hypothesis we are going to, as usual, calculate a $p$-value. The $p$-value is the probability of observing a test statistic as or more extreme as the one we observed, assuming the null hypothesis is true. To do this, we need to know the distribution of the test statistic under the null hypothesis. The distribution of the test statistic under the null hypothesis is known as the $F$-distribution.

The $F$-distribution has two degrees of freedom values associated with it: the degrees of freedom of the model and the degrees of freedom of the residuals. The degrees of freedom of the model are the number of parameters estimated by the model corresponding to the null hypothesis. The degrees of freedom of the residuals are the total degrees of freedom minus the degrees of freedom of the model.

Here is the $F$-distribution with 2 and 99 degrees of freedom:

```{r}
# Define parameters for the F-distribution
df1 <- 2   # Degrees of freedom for the numerator
df2 <- 99  # Degrees of freedom for the denominator
x_vals <- seq(0, 6, length.out = 500)  # Range of x values for the curve

# Calculate the F-distribution density
f_density <- data.frame(x = x_vals, y = df(x_vals, df1, df2))

# Plot the F-distribution
ggplot(f_density, aes(x = x, y = y)) +
  geom_line(color = "blue", linewidth = 1) +  # Line for the F-distribution
  geom_area(data = subset(f_density, x >= 3.89), aes(x = x, y = y), 
            fill = "red", alpha = 0.5) +  # Shade the area to the right of 3.89
  labs(
    title = "F-Distribution Curve",
    subtitle = "Shaded Area: Probability to the right of 3.89",
    x = "F Value",
    y = "Density"
  ) +
  theme_minimal() +
  annotate("text", x = 4.5, y = 0.05, label = "P(x >= 3.89)", color = "red") +
  xlim(0, 6)
```

The F-distribution is skewed to the right and has a long tail. The area to the right of 3.89 is shaded in red. This area represents the probability of observing an F-statistic as or more extreme as 3.89, assuming the null hypothesis is true. This probability is the $p$-value of the hypothesis test.


The $F$-statistic and $F$-test is briefly recaptured in 3.1.f) of the
Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact that

$$\frac{MSE_{model}}{MSE_{residual}} =  \frac{SSM/p}{SSE/(n-1-p)} \sim F_{p,n-1-p}$$

follows an $F$-distribution with $p$ and $(n-1-p)$ degrees of freedom,
where $p$ are the number of continuous variables, $n$ the number of data points.

* $SSE=\sum_{i=1} ^n(y_i-\hat{y}_i)^2$ is the residual sum of squares
* $SSM = SST - SSE$ is the sum of squares of the model
* $SST=\sum_{i=1}^n(y_i-\overline{y})^2$ is the total sum of squares
* $n$ is the number of data points
* $p$ is the number of explanatory variables in the regression model


### Source of variance table

The **sources of variance table** is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the $F$-statistic.


| Source | Sum of squares | Degrees of freedom | Mean square                       | F-statistic                       |
|--------|----------------|--------------------|-----------------------------------|-----------------------------------|
| Model  | $SSM$          | $p$                | $MSE_{model} = SSM / p$           | $\frac{MSE_{model}}{MSE_{error}}$ |
| Error  | $SSE$          | $n - 1 - p$        | $MSE_{error} = SSE / (n - 1 - p)$ |                                   |
| Total  | $SST$          | $n - 1$            |                                   |                                   |

: Sources of variance table

### The blood pressure example

How many observations?

```{r}
nrow(bp_data)
```


::: {.callout-tip}
## Think, Pair, Share (#three-degrees)

We have two explanatory variables (age and minutes of exercise per week).

* How many total degrees of freedom?
* How many degrees of freedom for the regression model?
* How many degrees of freedom for the residuals?
:::


Fit the model in R:

```{r echo = TRUE}
m1 <- lm(bp ~ age + mins_exercise, data = bp_data)
```

Get the F-statistic:

```{r echo = TRUE}
summary(m1)$fstatistic
```

Get the p-value:

```{r echo = TRUE}
p_value <- pf(summary(m1)$fstatistic[1],
              summary(m1)$fstatistic[2],
              summary(m1)$fstatistic[3],
              lower.tail = FALSE)
p_value
```

(The R-function `pf` calculates the cumulative probability distribution function of the F-distribution.)

And here is how we can get that information from the `summary` function. Look at the final line of the output:

```{r echo = TRUE}
summary(m1)
```




## Question 2: Which variables are associated with the response?

As we did for simple linear regression, we can perform a $t$-test for each explanatory variable to determine if it is associated with the response. As before, the null hypothesis for each $t$-test is that the slope of the explanatory variable is zero. The alternative hypothesis is that the slope of the explanatory variable is not zero.

```{r echo = TRUE, eval = TRUE}
summary(m1)$coef
```



Again, a 95% CI for each slope estimate $\hat\beta_j$ can be calculated with R:\


```{r echo = TRUE}
confint(m1)
```

Reminder: The 95% confidence interval is $[\hat\beta - c \cdot \sigma^{(\beta)} ; \hat\beta + c \cdot \sigma^{(\beta)}]$,
where $c$ is the 97.5% quantile of the $t$-distribution with $n-p$
degrees of freedom).

We can also use the `tbl_regression` function within the `gtsummary` package to get a publication ready table of the coefficients and confidence intervals:

```{r echo = TRUE, as.is = TRUE}
library(gtsummary)
tbl_regression(m1)
```


::: {.callout-important}
**However** Please insert a note into your brain that we are dealing here with an ideal case of **uncorrelated explanatory variables**. You'll learn later in the course about what happens when explanatory variables are correlated.
:::


## Question 3: What proportion of variability is explained?

### Multiple $R^2$

We can calculate the $R^2$ value for the multiple linear regression model just like we already did for a simple linear regression model. The $R^2$ value is the proportion of variability in the response variable that is explained by the model. As before, the $R^2$ value ranges from 0 to 1, where 0 indicates that the model does not explain any variability in the response variable, and 1 indicates that the model explains all the variability in the response variable.

For multiple linear regression, we often use the term "multiple $R^2$" to distinguish it from the $R^2$ value for simple linear regression. The multiple $R^2$ is the proportion of variability in the response variable that is explained by the model, taking into account all the explanatory variables in the model.

As before, for simple linear regression, the multiple $R^2$ value is calculated as the sum of squares explained by the model divided by the total sum of squares:

$$R^2 = \frac{SSM}{SST}$$

```{r}
sss <- anova(m1)
SSM <- sss$`Sum Sq`[1] + sss$`Sum Sq`[2]
SST <- sum(sss$`Sum Sq`)
R_squared <- SSM / SST
#R_squared
```


where $SSM$ is the sum of squares explained by the model and $SST$ is the total sum of squares, and $SSM = SST - SSE$.

For the blood pressure data:

```{r}
summary(m1)$r.squared
```


$R^2$ for multiple linear regression can also be calculated as the squared correlation between $(y_1,\ldots,y_n)$ and $(\hat{y}_1,\ldots,\hat{y}_n)$, where the $\hat y$ are the fitted values from the model. The fitted values are calculated as:

$$\hat{y}_i = \hat\beta_0 + \hat\beta_1 x^{(1)} + \ldots + \hat\beta_m x^{(m)}$$

In R:

```{r echo = TRUE}
r_squared <- cor(m1$fitted.values, bp_data$bp)^2
r_squared
```

Or:

```{r echo = TRUE}
sss <- anova(m1)
SSM <- sss$`Sum Sq`[1] + sss$`Sum Sq`[2]
SST <- sum(sss$`Sum Sq`)
R_squared <- SSM / SST
R_squared
```




### Adjusted $R^2$

However, we have a little problem to address. The $R^2$ value increases as we add more explanatory variables to the model, even if the additional variables are not associated with the response. This is because the $R^2$ value is calculated as the proportion of variability in the response variable that is explained by the model. As we add more explanatory variables to the model, the model will always explain more variability in the response variable, even if the additional variables are not associated with the response. Some of the variance will be explained by chance.

Here is an example of this problem. First, here's the explanatory power of the model with only age and minutes of exercise as the explanatory variables:

```{r}
m1 <- lm(bp ~ age + mins_exercise, data = bp_data)
summary(m1)$r.squared
```

Now, we can add a new explanatory variable to the blood pressure model that is not associated with the response:

```{r echo = TRUE}
bp_data$random_variable <- rnorm(nrow(bp_data))
m2 <- lm(bp ~ age + mins_exercise + random_variable, data = bp_data)
summary(m2)$r.squared
```


The $R^2$ value for the model with the random variable is higher than the $R^2$ value for the model without the random variable. This is because the model with the random variable explains more variability in the response variable, even though the random variable is not associated with the response.

To address this problem, we can use the adjusted $R^2$ value. The adjusted $R^2$ value is calculated as:

$$R^2_{\text{adj}} = 1 - \frac{SSE / (n - p - 1)}{SST / (n - 1)}$$

where
* $SSE$ is the sum of squared errors
* $SST$ is the total sum of squares
* $n$ is the number of observations
* $p$ is the number of explanatory variables in the model.

Or put another way:

$$R^2_{adj} = 1-(1-R^2 )\frac{n-1}{n-p-1}$$
In this form, we can see that as $p$ increases (as we add explanatory variables) the term $(n-1)/(n-p-1)$ increases, and the adjusted $R^2$ value will decrease if the additional variables are not associated with the response.


```{r}
#| eval = FALSE
n <- nrow(bp_data)
p <- seq(1, 20, 1)
R_squared <- summary(m1)$r.squared
R_squared_adj <- 1 - (1 - R_squared) * (n - 1) / (n - p - 1)
ggplot(data.frame(p = p, R_squared_adj = R_squared_adj, R_squared = R_squared), aes(x = p)) +
  geom_line(aes(y = R_squared_adj, color = "Adjusted R^2")) +
  geom_line(aes(y = R_squared, color = "R^2")) +
  labs(title = "Adjusted R^2 vs R^2",
       x = "Number of explanatory variables",
       y = "R^2 value") +
  scale_color_manual(values = c("Adjusted R^2" = "red", "R^2" = "blue"))
```

Take home: when we want to compare the explanatory power of models that differ in the number of explanatory variables, we should use the adjusted $R^2$ value.


## Question 4: Are some explanatory variables more important than others?

How important are the explanatory variables and how important are they relative to each other?

::: {.callout-tip}
## Think, Pair, Share (#variable-importance)

How might we assess how important is each of the explanatory variables, and how important they are relative to each other?
:::


The importance of an explanatory variable can be assessed by looking at the size of the coefficient for that variable. The larger the coefficient, the more important the variable is in explaining the response variable.

It is, however, important to remember that the size of the coefficient depends on the scale of the explanatory variable. If the explanatory variables are on different scales, then the coefficients will be on different scales and cannot be directly compared.

In our example, the age variable is measured in years, so the coefficient is in units mmHg (pressure) per year. The mins_exercise variable is measured in minutes, so the coefficient is in units mmHg per minute. The coefficients are on different scales and cannot be directly compared. Furthermore, the value of the coefficients would change if we measured age in months or minutes of exercise in hours.

There are other perspectives we can take when we're assessing importance. For example, we cannot change our age, but we can change the number of minutes of exercise. So, the practical importance of the two variables is quite different in that sense also.

To compare the importance of the explanatory variables that are measured on different scales, we can standardize the variables before fitting the model. This means that we subtract the mean of the variable and divide by the standard deviation. This puts all the variables on the same scale, so the coefficients can be directly compared. The coefficients are then in units of the response variable per standard deviation of the explanatory variable.

However, the coefficients are then not in the original units of the explanatory variables, so it is not always easy to interpret the coefficients. So while we can compare the coefficients, they have lost a bit of their original meaning and are not so easy to interpret.

One way to relate the coefficients in this case is to realise that to compensate for the blood pressure increase associated with one year of age, one would need to exercise for a certain number of minutes more.

::: {.callout-tip}
## Think, Pair, Share (#exercise-age)

How many minutes of exercise per week would we need to add to our fitness schedule to compensate for the blood pressure increase associated with one year of age?
:::


```{r}
extra_mins_exercise <- coef(m1)["age"] / -coef(m1)["mins_exercise"]
#extra_mins_exercise
```

## An elephant in the room: collinearity

In the blood pressure data there is no evidence of correlation (collinearity) between the explanatory variables. Recall that we can check for correlation between the explanatory variables by making a scatter plot of the two explanatory variables.

```{r fig.width = 4, fig.height = 4, fig.align = "center"}
ggplot(bp_data, aes(x = age, y = mins_exercise)) +
  geom_point()
```

The lack of pattern is good, because with no evidence of collinearity between the explanatory variables the model coefficients are stable and easy to interpret. They are stable in the sense that including or excluding one of the explanatory variables does not change the coefficients of the other variables much.

If there is correlation between the explanatory variables, then the model coefficients can be unstable and difficult to interpret. This is because the model cannot distinguish between the effects of the correlated variables.

Here is a new version of the blood pressure data in which the age and minutes of exercise variables are correlated:

```{r fig.width = 4, fig.height = 4, fig.align = "center"}
set.seed(123)
n <- 100
age <- ceiling(runif(n, 20, 80))
mins_exercise <- 100 - age + rnorm(n, 0, 10)
bp <- 100 + 0.5 * age - 0.1 * mins_exercise + rnorm(n, 0, 15)
bp_data_correlated <- data.frame(age = age, mins_exercise = mins_exercise, bp = bp)
ggplot(bp_data_correlated, aes(x = age, y = mins_exercise)) +
  geom_point()
```

Now we fit a multiple linear regression model to the correlated data:

```{r}
m2_both <- lm(bp ~ age + mins_exercise, data = bp_data_correlated)
summary(m2_both)$coefficients
```

There is a positive effect of age on blood pressure, and a negative effect of minutes of exercise. The age coefficient is just significant, with $p = 0.473$. The minutes of exercise coefficient is not significant, with $p = 0.125$.

Here is the model with only age included:

```{r}
m2_age <- lm(bp ~ age, data = bp_data_correlated)
summary(m2_age)$coefficients
```

This gives quite different results for age. The coefficient is much larger (0.57 compared to 0.34) and it is very very unlikely that the observed relationship could have occurred if the null hypothesis were true ($p < 0.001$). 

And here is the model with only minutes of exercise:

```{r}
m2_mins_exercise <- lm(bp ~ mins_exercise, data = bp_data_correlated)
summary(m2_mins_exercise)$coefficients
```

The same thing happens again. The coefficient for exercise is larger (-0.49 compared to -0.23) and it is very very unlikely that the observed relationship could have occurred if the null hypothesis were true ($p < 0.001$).  


When each of the explanatory variables is included in the model separately, the coefficients are quite different from when both variables are included in the model. And when they are included in the model separately, their p-values are very low (reject null hypothesis of slope of zero), but when they are included in the model together, their p-values are much larger and give quite weak or no evidence to reject the null hypothesis of slope of zero.

These phenomena occur due to the correlation/collinearity between the explanatory variables. The model cannot distinguish between the effects of the correlated variables, so the coefficients are unstable and difficult to interpret.

Unfortunately, collinearity is a common feature of real data, and it can make the interpretation of multiple linear regression models difficult. It is sometimes possible to design observational studies to reduced or avoid collinearity, but in practice it is often difficult to escape from.

In designed experiments, it is standard to make the design of the experiments so that explanatory variables are completely independent (zero collinearity). A fully-factorial design with balanced replication is a good way to ensure that explanatory variables are independent. Two-way analysis of variance is then an appropriate statistical model. We will look at this design and analysis in a later lecture.

Collinearity also affects the interpretation of the $R^2$ values. Collinearity will cause the collinear explanatory variables to share some of the explained variance. The $R^2$ value of the multiple regression will then be less than the sum of the $R^2$ values of the individual regressions of the response variable on each of the explanatory variables separately.

$R^2$ of the age only model:

```{r}
#| echo = TRUE
summary(m2_age)$r.squared
```

$R^2$ of the mins_exercise only model:

```{r}
#| echo = TRUE
summary(m2_mins_exercise)$r.squared
```

$R^2$ of the model with both age and mins_exercise:

```{r}
#| echo = TRUE
summary(m2_both)$r.squared
```

In this case the two explanatory variables are strongly correlated and so share a lot of the explained variance. The $R^2$ value of the model with both explanatory variables is much less than the sum of the $R^2$ values of the models with each explanatory variable separately. In fact, either of the models with only one explanatory variable is nearly as good as the model with both explanatory variables. We don't gain much from including another explanatory variable in the model when we already include one explanatory variable that is strongly correlated with the other.

## Recap

Simple regression:

* How well does the model describe the data: Correlation and $R^2$
* Are the parameter estimates compatible with some specific value ($t$-test)?
* What range of parameters values are compatible with the data (confidence intervals)?
* What regression lines are compatible with the data (confidence band)?
* What are plausible values of other data (prediction band)?

Multiple regression:

* Multiple linear regression $x_1$, $x_2$, \ldots, $x_m$
* Checking assumptions.
* $R^2$ in multiple linear regression
* $t$-tests, $F$-tests and $p$-values



## Extras

### 3D plot of multiple linear regression

We can also look at this in a plot with three axes. The y-axis is blood pressure, the x-axis is age, and the z-axis is minutes of exercise. Here is a 3d plot that we can interactive with and rotate:

```{r fig.width = 8, fig.height = 8}
plotly::plot_ly(bp_data, x = ~age, y = ~mins_exercise, z = ~bp, type = "scatter3d", mode = "markers", marker = list(size = 5))
```

All very good, and this makes sense. We can see that blood pressure is positively associated with age and negatively associated with minutes of exercise. But we need to be more quantitative about this. We need to build a model that relates blood pressure to age and minutes of exercise.

