[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIO144 Course Book (version 2026)",
    "section": "",
    "text": "Preface\nThis book contains the content of the course BIO144 Data Analysis in Biology at the University of Zurich. It is intended to be used as a companion to the lectures and practical exercises of the course. All of the required content of the course (i.e., what could be in the final exam) is included in this book. Additional content is included for those who want to learn more.\nBeware that Owen sometimes makes updates to the book during the semester, so if you have downloaded a copy or taken screenshots, your copy may not exactly match the most current version. However, all of the required content will be the same, and any changes will be correcting typos or improving explanations. If content does change in a way that would change the answer to a question in the final exam, Owen will announce this in the lectures and on OLAT.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-to-get-a-copy-of-this-book",
    "href": "index.html#how-to-get-a-copy-of-this-book",
    "title": "BIO144 Course Book (version 2026)",
    "section": "How to get a copy of this book",
    "text": "How to get a copy of this book\nIf you‚Äôd like a copy of this book for yourself, there are a few ways. But beware: if you take a local copy then it will not be updated when Owen makes changes to the online version!\n\nYou can download a PDF version of the entire book:  üìÑ Download PDF \nYou can download a complete local copy of the HTML version of the BIO144 course book from here:\nhttps://github.com/opetchey/BIO144_Course_Book/tree/main. The html files for the book are in the _book folder, and this is the only folder you need for your offline html copy of the book. You can open the index.html file in your web browser to read the book offline.\nYou can get all of the source code for the book from the GitHub repository. However, you may find it a little complicated to do anything useful with it!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#datasets-are-not-real",
    "href": "index.html#datasets-are-not-real",
    "title": "BIO144 Course Book (version 2026)",
    "section": "Datasets are not real",
    "text": "Datasets are not real\nThe datasets used in this book are not real datasets. They were created to illustrate the methods taught in the course. Any resemblance to real data is purely coincidental. Please do not use these datasets for any purpose other than learning the methods taught in this course. The patterns in the data may not reflect real-world patterns, and should not be used to draw any conclusions about real-world phenomena.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#getting-the-datasets",
    "href": "index.html#getting-the-datasets",
    "title": "BIO144 Course Book (version 2026)",
    "section": "Getting the datasets",
    "text": "Getting the datasets\nThe datasets used in this book are available for download as a zip file here: course_book_datasets.zip. You can download this file and unzip it to get all of the datasets used in the book. The datasets are in CSV format, which can be opened in R or other statistical software.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#packages-used-in-this-book",
    "href": "index.html#packages-used-in-this-book",
    "title": "BIO144 Course Book (version 2026)",
    "section": "Packages used in this book",
    "text": "Packages used in this book\nThis book uses a number of R add-on packages for data analysis and visualization. You will need to install these packages in order to run the code in the book. The required packages are listed in the _common.r file in the GitHub repository for the book. Here is a link to that file: _common.R. You can copy and paste the list of packages from that file into your R console to install them.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#if-you-think-you-found-a-mistake-in-this-book",
    "href": "index.html#if-you-think-you-found-a-mistake-in-this-book",
    "title": "BIO144 Course Book (version 2026)",
    "section": "If you think you found a mistake in this book",
    "text": "If you think you found a mistake in this book\nIf you think you have found a mistake in the book, please say. A really nice way is to submit an issue on the GitHub repository for the book: Issues page of the GitHub repository. You will need a GitHub account to do this, but they are free and easy to set up. Otherwise tell Owen in person sometime, or in the OLAT Forum, or by email.\nWhen reporting a mistake, please be as specific as possible about where the mistake is. A screenshot works well. Or give the chapter and section number, and copy a chunk of text, as well as a description of the issue problem of course!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-this-book-was-made",
    "href": "index.html#how-this-book-was-made",
    "title": "BIO144 Course Book (version 2026)",
    "section": "How this book was made",
    "text": "How this book was made\nThe book was written using a type of RMarkdown. It allows a script with a mix of normal text and R code to produce chapters and a book that has a mixture of text, R code, and R output. Rmarkdown is very useful for making reports, books, presentations, and even websites.\nThis book is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "BIO144 Course Book (version 2026)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content was based on lectures originally written by Dr Stefanie Muff.\nThe content of the book was written with the assistance of Github Copilot, an AI tool that helps write code and text.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "1.1-intro.html",
    "href": "1.1-intro.html",
    "title": "Introduction (L1)",
    "section": "",
    "text": "Notation and some definitions\nThe first lecture of the course introduces it, gives some important information, and sets the stage for the rest of the course. Some of the time in the lecture will be used to create a dataset for use during the course. It also gives an opportunity to review some of the things about R and statistics that it is very useful to already know.\nThe lecture includes:\nThroughout the course, we will use the following notation:",
    "crumbs": [
      "Introduction (L1)"
    ]
  },
  {
    "objectID": "1.1-intro.html#notation-and-some-definitions",
    "href": "1.1-intro.html#notation-and-some-definitions",
    "title": "Introduction (L1)",
    "section": "",
    "text": "\\(x\\) for a variable. Typically this variable contains a set of observations. These observations are said to represent a sample of all the possible observations that could be made of a population.\n\\(x_1, x_2, \\ldots\\) for the values of a variable\n\\(x_i\\) for the \\(i\\)th value of a scalar variable. This is often spoken as ‚Äúx sub i‚Äù or the ‚Äúi-th value of x‚Äù.\n\\(x^{(1)}\\) for variable 1, \\(x^{(2)}\\) for variable 2, etc.\nThe mean of the sample \\(x\\) is \\(\\bar{x}\\). This is usually spoken as ‚Äúx-bar‚Äù.\nThe mean of \\(x\\) is calculated as \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\).\n\\(n\\) is the number of observations in a sample.\nThe summation symbol \\(\\sum\\) is used to indicate that the values of \\(x\\) are summed over all values of \\(i\\) from 1 to \\(n\\).\nThe standard deviation of the sample is \\(s\\). The standard deviation of the population is \\(\\sigma\\).\nThe variance is \\(s^2\\). The variance of the population is \\(\\sigma^2\\).\nThe variance of the sample is calculated as \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\).\nThe standard deviation of the sample is calculated as \\(s = \\sqrt{s^2}\\).\n\\(y\\) is usually used to represent a dependent / response variable.\n\\(x\\) is usually used to represent an independent / predictor / explanatory variable.\n\\(\\beta_0\\) is usually used to denote the intercept of a linear model.\n\\(\\beta_1\\), \\(\\beta_2\\), etc. are usually used to denote the coefficients of the independent variables in a linear model.\nEstimates are denoted with a hat, so \\(\\hat{\\beta}_0\\) is the estimate of the intercept of a linear model.\nHence, the estimated value of \\(y_i\\) in a linear regression model is \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^{(1)}\\).\n\\(e_i\\) is the residual for the \\(i\\)th observation in a linear model. The residual is the difference between the observed value of \\(y_i\\) and the predicted value of \\(y_i\\) (\\(\\hat{y_i}\\)).\nOften we assume errors are normally distributed with mean 0 and variance \\(\\sigma^2\\). This is written as \\(e_i \\sim N(0, \\sigma^2)\\).\nSST is the total sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\).\nSSM is the model sum of squares. It is the sum of the squared differences between the predicted values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\).\nSSE is the error sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the predicted values of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\hat{y_i})^2\\).\nThe variance of \\(x\\) can be written as \\(Var(x)\\). The covariance between \\(x\\) and \\(y\\) can be written as \\(Cov(x, y)\\).\nCovariance is calculated as \\(Cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\).\n\\(H_0\\) is the null hypothesis.\n\\(\\alpha\\) is the significance level.\ndf is the degrees of freedom.\n\\(p\\) is the p-value.",
    "crumbs": [
      "Introduction (L1)"
    ]
  },
  {
    "objectID": "1.1-intro.html#data-analysis-workflow",
    "href": "1.1-intro.html#data-analysis-workflow",
    "title": "Introduction (L1)",
    "section": "Data analysis workflow",
    "text": "Data analysis workflow\nA general workflow for data analysis is as follows:\n\nDefine the question: What are you trying to find out?\nDefine the study: How will you answer the question? What subjects, what observations, what measurements? What experimental design? What treatments? What graphics and analyses will you use?\nCollect the data: Gather the necessary data to answer the question.\nExplore the data: Use summary statistics and graphics to understand the data.\nPrepare the data: Clean and format the data for analysis.\nVisualise the data: Create plots to visualise patterns and relationships.\nAnalyse the data: Use appropriate statistical methods to analyse the data, including checking model assumptions.\nInterpret the results: Draw conclusions from the analysis in the context of the original question.\nBe critical: Consider limitations, alternative explanations, and the robustness of your conclusions.\nCommunicate the results: Present the findings in a clear and concise manner, using tables, figures, and written summaries.",
    "crumbs": [
      "Introduction (L1)"
    ]
  },
  {
    "objectID": "1.1-intro.html#using-generative-ai-in-r-and-data-analysis-guidance-and-good-practice",
    "href": "1.1-intro.html#using-generative-ai-in-r-and-data-analysis-guidance-and-good-practice",
    "title": "Introduction (L1)",
    "section": "Using Generative AI in R and Data Analysis: Guidance and Good Practice",
    "text": "Using Generative AI in R and Data Analysis: Guidance and Good Practice\n\n\n\n\n\n\nImportant\n\n\n\nFor the final examination, you will use your own computer, but the test will run inside the Safe Exam Browser, which will be configured to block all access to generative AI tools, browser-based assistants, external software, online services, and any AI code copilots inside RStudio or other IDEs. This means no form of generative AI will be available during the exam. Because of this, please avoid becoming overly reliant on GenAI‚Äîsuch as ChatGPT, Claude, Gemini, Copilot, or similar tools for answering quiz questions, explaining results, fixing errors, guiding your analysis, or writing code. You must be able to perform all tasks independently. We also strongly recommend that you do not use RStudio with Copilot integration during the course, as it will not function in the exam environment and may leave you under prepared. Throughout the semester, be sure to practice writing your own R code, interpreting outputs yourself, and applying statistical reasoning without AI assistance, as your exam performance will depend entirely on your own knowledge and skills.\n\n\nGenerative AI (GenAI) tools can support learning, exploration, and coding in R. They can be powerful assistants, but they must be used with care. This section introduces the types of tools available, provides guidelines for responsible use, highlights red flags for problematic usage, and gives examples of good and poor practice.\nTypical uses:\n\nasking conceptual questions\n\nsummarizing methods\n\ngenerating example code\n\nexplaining error messages\n\nStrengths:\n\nflexible and conversational\n\ngood for brainstorming\n\ncan generate starter code\n\nLimitations:\n\noften wrong in subtle ways\n\nmay hallucinate functions\n\ncannot see your working R session\n\n\nGuidelines for Good Use of Generative AI\n\nUse GenAI as a Helper, Not a Source of Truth\nBest uses:\n\ndrafting\n\nexplanation\n\nsyntax reminders\n\nscaffolding\n\nNot reliable for:\n\nmodel choice\n\nstatistical inference\n\ninterpreting coefficients\n\ndesigning analysis workflows\n\nchecking assumptions\n\n\n\nAlways Verify AI-Generated Code and Explanations\nCheck:\n\ndoes the code run?\n\ndo variable names match?\n\nis the model appropriate?\n\nare assumptions addressed?\n\nis the explanation logically correct?\n\n\n\nKeep Human Judgement Central\nGenAI cannot:\n\nunderstand scientific questions\n\nevaluate model assumptions\n\nknow ecological/biological reasoning\n\ndetermine appropriate models\n\n\n\nProvide Context Carefully\nWhen asking GenAI:\n\ndescribe variables\n\nprovide example data\n\nspecify your goal\n\nshow your existing code\n\nBetter context = better answers.\n\n\nUse GenAI to Improve Understanding, Not Bypass It\nHelpful:\n\n‚ÄúExplain logistic regression.‚Äù\n\n‚ÄúWhy do residuals fan out?‚Äù\n\nNot helpful:\n\n‚ÄúDo my assignment for me.‚Äù\n\n\n\n\nIndicators of Problematic Usage\n\nCode That Does Not Reflect Ability\nSigns:\n\nunfamiliar advanced syntax\n\nunexplained packages\n\ninconsistent style\n\n\n\nHallucinated Functions or Nonsensical Code\nExamples:\n\nslope(x) in mixed models\n\nmissing arguments\n\nfabricated packages\n\n\n\nStatistical Errors Typical of AI\nCommon issues:\n\nwrong model family\n\nwrong inference logic\n\ninvented assumptions\n\nincorrect explanation of coefficients\n\n\n\nLack of Understanding\nIndicators:\n\ncannot explain model\n\ninconsistent interpretations\n\nidentical phrasing to AI output\n\n\n\nOver-Reliance on AI\nSigns:\n\nusing AI for every step\n\nno debugging effort\n\nstagnation in skill development\n\n\n\n\nExamples of Good and Problematic Use\n\nGood Use Examples\nA. Syntax help\n‚ÄúHow do I specify a random slope in lme4?‚Äù\nB. Clarification\n‚ÄúHow does adding an interaction change interpretation?‚Äù\nC. Debugging\n‚ÄúWhat does ‚Äòobject not found‚Äô usually mean?‚Äù\nD. Brainstorming\n‚ÄúHow can I visualise a logistic regression?‚Äù\n\n\nProblematic Use Examples\nA. Blindly copying model code\nlm(y ~ x1 + x2 * x3 * x1)\nB. Incorrect statistical logic\nAI code labelled as a bootstrap but is actually a permutation test.\nC. Misleading interpretation\nClaims that coefficients assume explanatory variable independence.\nD. Presenting AI-generated plots without understanding\nE. Outsourcing entire workflow\n‚ÄúWrite a script that loads data, cleans it, runs models, interprets, and writes the report.‚Äù\n\n\nSummary\nGenerative AI can:\n\nhelp learning\n\nsupport debugging\n\nprovide code scaffolds\n\nexplain concepts\n\nBut it can also:\n\nhallucinate\n\nproduce incorrect models\n\nmisinterpret statistics\n\nUse GenAI as a supportive tool‚Äînever as an unquestioned authority.\nGood use of GenAI supports learning. Problematic use replaces it.\n\n\n\nCommon GenAI Errors in R and Statistical Modelling\nGenerative AI tools can be helpful for writing R code, exploring ideas, and learning syntax.\nHowever, they sometimes produce plausible but incorrect code or explanations.\nThis section provides real examples of typical GenAI mistakes, with correct solutions and learning points.\nWhy this matters: GenAI is a pattern-matching system, not a statistical reasoning engine. It does not understand assumptions, inference, or modelling logic.Therefore, students should never accept code or explanations without checking them.\n\n\nIncorrect formula structure in lm()\nPrompt: Fit a linear model with main effects and a two-way interaction between x2 and x3.\nIncorrect GenAI output:\nlm(y ~ x1 + x2 * x3 * x1, data = df)\nThis includes an unintended three-way interaction and extra terms.\nCorrect:\nlm(y ~ x1 + x2 * x3, data = df)\nLearning point: Always check model formulas carefully. AI often adds or removes interactions.\n\n\nConfusing bootstrap and permutation tests\nDocumented case: GenAI was asked for a bootstrap t-test.\nIncorrect GenAI code (actually a permutation test):\nt_stats &lt;- replicate(1000, {\n  perm &lt;- sample(df$group)\n  t.test(df$value ~ perm)$statistic\n})\nCorrect bootstrap approach:\nt_stats &lt;- replicate(1000, {\n  sample_df &lt;- df[sample(nrow(df), replace = TRUE), ]\n  t.test(value ~ group, data = sample_df)$statistic\n})\nLearning point: The logic of inference matters. Code that runs is not necessarily correct.\n\n\n\nIncorrect explanation of linear-model coefficients\nIncorrect claim: ‚ÄúCoefficients assume independence among explanatory variable.‚Äù\nThis is false. Linear model coefficients describe conditional effects within the model, regardless of collinearity.\nLearning point: Interpretations come from the model structure, not from simplistic assumptions GenAI sometimes invents.\n\n\n\nHallucinated functions in mixed models\nIncorrect GenAI output:\nlmer(y ~ x + (slope(x) | group), data = df)\nslope() does not exist.\nCorrect random-slope specification:\nlmer(y ~ x + (x | group), data = df)\nLearning point: Always verify syntax in package documentation.\n\n\n\nWrong variable names\nThe dataset has variables height and age.\nIncorrect GenAI output:\nlm(Height ~ Age, data = df)\nCorrect:\nlm(height ~ age, data = df)\nLearning point: GenAI often guesses variable names. Check against your data.\n\n\n\nWrong model family for binary data\nIncorrect GenAI output (linear regression):\nlm(y ~ x, data = df)\nCorrect logistic regression:\nglm(y ~ x, data = df, family = binomial)\nLearning point: For binary response variables, specify the model family explicitly.\n\n\n\nIncorrect explanation of random intercepts\nIncorrect claim:\n‚ÄúRandom intercepts eliminate correlation among repeated measures.‚Äù\nIncorrect ‚Äî they model correlation, not eliminate it.\nLearning point: Random effects structure determines the implied correlation. AI explanations are often vague or wrong here.\n\n\n\nOmitting interaction terms in ANOVA\nPrompt: Two-way ANOVA with interaction.\nIncorrect:\naov(y ~ factor1 + factor2, data = df)\nCorrect:\naov(y ~ factor1 * factor2, data = df)\nLearning point: Confirm that the model matches the experimental design.\n\n\n\nIncorrect use of predict()\nPrompt: Predict for new x values.\nIncorrect GenAI output:\npredict(model)\nThis gives in-sample fitted values, not predictions for new data.\nCorrect:\npredict(model, newdata = data.frame(x = c(1, 2, 3)))\nLearning point: Always specify newdata for predictions.\n\n\n\nPoor explanations of multicollinearity\nIncorrect GenAI claim:\n‚ÄúMulticollinearity is indicated when the model p-value is low but the individual explanatory variable p-values are high.‚Äù\nThis is an unreliable and incomplete diagnostic.\nBetter diagnostics:\ncar::vif(model)\ncor(df)\nmodel.matrix(model)\nLearning point: AI often repeats common internet tropes rather than robust statistical principles.\n\n\n\nGenAI Summary\nGenAI can:\n\nwrite useful scaffolding code,\n\nprovide quick reminders,\n\nhelp with simple tasks.\n\nBut it can also:\n\nhallucinate functions,\n\ngive subtly incorrect models,\n\ninvent statistical logic,\n\nprovide plausible but wrong explanations.\n\nAdvice for students:\nUse GenAI as a starting point, not an authority.\nAlways check:\n\nfunction names,\n\nmodel formulas,\n\nassumptions,\n\ninterpretations,\n\nand logic.\n\nIn statistics, clarity of reasoning matters more than code that merely runs.",
    "crumbs": [
      "Introduction (L1)"
    ]
  },
  {
    "objectID": "1.1-intro.html#further-reading",
    "href": "1.1-intro.html#further-reading",
    "title": "Introduction (L1)",
    "section": "Further reading",
    "text": "Further reading\nStudents who are curious and would like to explore these topics further (purely for their own interest) may find the following resources useful. Material from these resources will not be examined in the final exam, unless it is also already present in the course book.\nIf you would like to read more about reaction time differences between men and women, this is a quite interesting paper: On the Implications of a Sex Difference in the Reaction Times of Sprinters at the Beijing Olympics, by Lipps et al (2011). The analyses are relatively simple, and the implications explored are quite interesting. Unfortunately, the data used in the paper is not publicly available, so you cannot use it for practice here.\nA more detailed data analysis workflow suggestion is here on the Insights from data website.\nHere is an article about Exploring The Ethical Implications Of Ai In Data Analytics: Challenges And Strategies For Responsible Implementation. It is rather brief and high-level, but may be of interest when you seek an ethical perspective on the use of AI tools in data analysis.",
    "crumbs": [
      "Introduction (L1)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html",
    "href": "2.1-R-and-RStudio.html",
    "title": "R and RStudio (L2)",
    "section": "",
    "text": "Getting R and RStudio\nR is a programming language and software environment for statistical computing and graphics. RStudio is an integrated development environment (IDE) for R. RStudio provides a user-friendly interface for working with R, including a console, a script editor, and tools for managing packages and projects.\nWe highly recommend using RStudio to work with R.\nThere are two ways to use RStudio:\nWhat do we recommend? Try the cloud first. If you like it then continue to use it.\nThink‚ÄìPair‚ÄìShare (#a_r_vs_rstudio_roles) What is one thing that R does, and one thing that RStudio does? Why is it useful that these are separate?",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#getting-r-and-rstudio",
    "href": "2.1-R-and-RStudio.html#getting-r-and-rstudio",
    "title": "R and RStudio (L2)",
    "section": "",
    "text": "RStudio Desktop: a standalone application that you can install on your computer. If you choose this option, you will need to install R first, and then install RStudio. This usually requires administrator privileges on your computer. If you have problems installing add-on packages, they will have to be fixed by you or with our help (rarely we cannot find a solution). Follow the instructions on this website about how to install R and RStudio: https://posit.co/download/rstudio-desktop/.\nRstudio Cloud: a web-based version of RStudio that you can use in your web browser. You don‚Äôt need to install anything on your computer, and you can access your work from any computer with an internet connection. The Faculty of Science has a RStudio Cloud here that you can use (and will have to use during the final exam).\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhether you use the RStudio application on your computer, or use RStudio on the Cloud, you are responsible for the safety and persistence of your files (data, code, etc.). Just because you‚Äôre using RStudio on the Cloud does not mean your files are automatically saved forever. Make sure to download and back up your important files regularly!",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#getting-to-know-the-rstudio-ide",
    "href": "2.1-R-and-RStudio.html#getting-to-know-the-rstudio-ide",
    "title": "R and RStudio (L2)",
    "section": "Getting to know the RStudio IDE",
    "text": "Getting to know the RStudio IDE\nWhen you open RStudio, you will see a window with four main panes:\n\nSource pane: where you can write and edit R scripts, R Markdown documents, and other files. This pane can have multiple tabs, so you can have several files open at the same time.\nConsole pane: where you can type and execute R commands directly. This pane has multiple tabs, including: Console, Terminal, and Jobs. During this course we will mostly use the Console tab.\nEnvironment pane: where you can see the objects (data frames, vectors, etc.) that are currently in your R session. This pane has multiple tabs, including: Environment, History, Connections, and Tutorial. During this course we will mostly use the Environment tab.\nFiles/Plots/Packages/Help pane: where you can manage files, view plots, manage packages, and access help documentation. This pane has multiple tabs, including: Files, Plots, Packages, Help, and Viewer. During this course we will mostly use the Files, Plots, Packages, and Help tabs.\n\nOur Scripts are in the Source pane tabs. The code / script we write in R is usually saved in a file with the extension .R. This file can be opened and edited in the Source pane. Creating a new R script: File &gt; New File &gt; R Script.\nYou can run code from the script by selecting the code and clicking the ‚ÄúRun‚Äù button, or by using the keyboard shortcut Ctrl + Enter (Windows) or Cmd + Enter (Mac).\nThere is so much more to learn about the RStudio IDE, but we will cover that as we go along in the course.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#getting-to-know-r",
    "href": "2.1-R-and-RStudio.html#getting-to-know-r",
    "title": "R and RStudio (L2)",
    "section": "Getting to know R",
    "text": "Getting to know R\nIn our newly opened script file, type the following code:\n\n# This is a comment. Comments are ignored by R.\n# They are useful for explaining what your code does.\n\nThen type the following code:\n\n1 + 1\n\n[1] 2\n\nexp(2)\n\n[1] 7.389056\n\nsqrt(16)\n\n[1] 4\n\n\nSelect all the code and run it (using the ‚ÄúRun‚Äù button or Ctrl + Enter / Cmd + Enter). You should see the results of the calculations in the Console pane.\nNow try assigning values to named object:\n\na &lt;- 5\nb &lt;- 10\nc &lt;- a + b\n\n\n\n\n\n\n\nNote\n\n\n\nIn fact, just about everything in R is an object! These objects live in your R session (i.e., in the memory of your computer), and you can see them in the Environment pane. You can create objects to store data, functions, and other information. Your data files are something different. They are just like other files on your computer, such as documents, images, and music files. They live on your hard drive (or in the cloud), and you can import them into R when you need to work with them.\n\n\nAnd then print the value of c:\n\nc\n\n[1] 15\n\n\nYou should see the value 15 printed in the Console pane.\nWe can also create vectors:\n\nmy_vector &lt;- c(1, 2, 3, 4, 5)\nmy_vector\n\n[1] 1 2 3 4 5\n\n\nAnd do maths on vectors:\n\nmy_vector * 2\n\n[1]  2  4  6  8 10\n\nmy_vector + 10\n\n[1] 11 12 13 14 15\n\nmy_vector ^ 2\n\n[1]  1  4  9 16 25\n\n\nWe can vectors of strings (text):\n\nmy_strings &lt;- c(\"apple\", \"banana\", \"cherry\")\nmy_strings\n\n[1] \"apple\"  \"banana\" \"cherry\"\n\n\nAnd can perform operations on strings:\n\npaste(\"I like\", my_strings)\n\n[1] \"I like apple\"  \"I like banana\" \"I like cherry\"\n\ntoupper(my_strings)\n\n[1] \"APPLE\"  \"BANANA\" \"CHERRY\"\n\n\nAnd we can create data frames, which are like tables of data\n\nmy_data &lt;- data.frame(\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Age = c(25, 30, 35),\n  Height = c(165, 180, 175)\n)\nmy_data\n\n     Name Age Height\n1   Alice  25    165\n2     Bob  30    180\n3 Charlie  35    175\n\n\nAbove we have numerous examples of functions: exp(), sqrt(), c(), paste(), toupper(), and data.frame(). Functions are a fundamental part of R programming. They are used to perform specific tasks, such as calculations, data manipulation, and data analysis. All functions have a name and can take arguments (inputs) and return values (outputs). They are called by writing the function name followed by parentheses, with any arguments inside the parentheses.\nYou likely guessed that there is much much more to learn about R, but we will cover that as we go along in the course.\nThink‚ÄìPair‚ÄìShare (#tps_a_objects_not_files) When you type a &lt;- ..., where does x live?",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#getting-help",
    "href": "2.1-R-and-RStudio.html#getting-help",
    "title": "R and RStudio (L2)",
    "section": "Getting help",
    "text": "Getting help\nR has a built-in help system that you can use to get information about functions, packages, and other topics. To access the help system, you can use the ? operator followed by the name of the function or topic you want to learn about. For example, to get help on the mean() function, you would type:\n\n?mean\n\nThis will open the help documentation for the mean() function in the Help pane of RStudio. The documentation includes a description of the function, its arguments, and examples of how to use it. Some of the help documentation is very useful and accessible, other is less so. Over time you will learn which functions and packages have good documentation, and you will get better and better at understanding R help files.\nOf course you can use any other resources to get help with R, including online forums, tutorials, and books. Some popular online resources for R help include:\n\nStack Overflow\nRStudio Community\nR-bloggers\nThe R Graph Gallery\n\nYou can also use search engines like Google to find answers to your R questions. Just be sure to include ‚ÄúR‚Äù in your search query to get relevant results.\nAI assistants like ChatGPT can also be useful for getting help with R programming. You can ask specific questions about R code, functions, and packages, and get instant responses.\nAnd of course there is always your course instructors and fellow students to help you out when you get stuck.\nThink‚ÄìPair‚ÄìShare (#a_errors_are_information) When we do something that R does not understand, it often gives us an error message. In red! What will you do when you get an error message?",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#add-on-packages",
    "href": "2.1-R-and-RStudio.html#add-on-packages",
    "title": "R and RStudio (L2)",
    "section": "Add-on packages",
    "text": "Add-on packages\nR has a vast ecosystem of add-on packages that extend its functionality. These packages are collections of functions, data, and documentation that can be installed and loaded into your R session. There are thousands of packages available on CRAN (the Comprehensive R Archive Network) and other repositories like Bioconductor and GitHub.\nWe will be using several packages throughout this course. To install a package, you can use the install.packages() function. For example, to install the ggplot2 package, you would type:\n\ninstall.packages(\"ggplot2\")\n\nYou can also use the RStudio interface to install packages. Go to the ‚ÄúPackages‚Äù tab in the bottom right pane, click on ‚ÄúInstall‚Äù, type the name of the package you want to install, and click ‚ÄúInstall‚Äù.\nYou can see which packages are currently installed by looking in the ‚ÄúPackages‚Äù tab.\n\n\n\n\n\n\nTip\n\n\n\nYou only need to install a package once. After it is installed, you can load it into your R session using the library() function. Do not install packages every time you want to use them; just load them with library().",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#r-version-and-add-on-package-versions",
    "href": "2.1-R-and-RStudio.html#r-version-and-add-on-package-versions",
    "title": "R and RStudio (L2)",
    "section": "R Version and add-on package versions",
    "text": "R Version and add-on package versions\n(This section concerns the Desktop version of R and RStudio, and not so much the Cloud version, because version management is handled for you in the Cloud.)\nR and its add-on packages are constantly being updated and improved. This can cause problems when trying to install or use packages that depend on specific versions of R or other packages.\nImagine that the online version of a package has been updated and now only works with the lastest version of R. If you are using an older version of R, you may not be able to install or use that package.\nOr if a package depends on another package that has been updated, you may need to update that package as well to use the first package.\nThis sounds complicated, but there are some simple steps you can take to reduce the chances of running into version-related problems:\n\nKeep your R version up to date. New versions of R are released every 6 months or so, and they often include important bug fixes and new features. You can check your current R version by typing R.version.string in the Console. To update R, you can download the latest version from the CRAN website.\nKeep your add-on packages up to date. You can update all your installed packages by using the update.packages() function. This will check for updates for all installed packages and install the latest versions. You can also use the RStudio interface to update packages by going to the ‚ÄúPackages‚Äù tab, clicking on ‚ÄúUpdate‚Äù, selecting the packages you want to update, and clicking ‚ÄúInstall Updates‚Äù.\nDo this well before critical deadlines or important events (e.g., exams). Updating R and packages can sometimes lead to unexpected issues, so it‚Äôs best to do it well in advance of when you need everything to work perfectly.\n\nNevertheless, even with these precautions, you may still encounter version-related issues from time to time. When this happens, don‚Äôt panic!\nA common problem you might see is an error message when trying to install or load a package, indicating that the package requires a newer version of R or another package. The error / warning message might look like:\nwarning: package 'xyz' requires R version &gt;= 4.2.0\nWarning in install.packages: package ‚ÄòXYZ‚Äô is not available (for R version 4.2.0)\nThese messages indicate that the package you are trying to install or load requires a newer version of R than the one you currently have. To fix this, you will need to update your R installation to the required version or higher. Then its also a good idea to update your packages as well.\n\n\n\n\n\n\nNote\n\n\n\nRStudio is also regularly updated, with new version released every several months or so. Your version of RStudio is independent of your version of R, so you can update RStudio without changing your R version. Note that usually your version of RStudio is not as important as your version of R and the packages you are using. So updating RStudio is usually not a high priority and doesn‚Äôt often help solve problems related to add on package versions.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#r-projects",
    "href": "2.1-R-and-RStudio.html#r-projects",
    "title": "R and RStudio (L2)",
    "section": "R Projects",
    "text": "R Projects\nI always work within R Projects. R Projects help you to organise your work and keep all files related to a project in one place. They also make importing data a breeze.\nBut what is an R Project? An R Project is a directory (folder) that contains all the files related to a specific project. When you open an R Project, RStudio automatically sets the working directory to the project directory, so you don‚Äôt have to worry about setting the working directory manually.\nTo see if you‚Äôre working within an R Project, look at the top right of the RStudio window. If you see the name of your project there, you‚Äôre good to go. If you see ‚ÄúProject: (None)‚Äù, then you‚Äôre not working within an R Project.\nIf you click on the project name, a dropdown menu will appear. From there, you can create a new project, open an existing project, or switch between projects.\nCreate a new R Project: File &gt; New Project &gt; New Directory or Existing Directory &gt; New Project &gt; Choose a name and location for your project &gt; Create Project.\n\n\n\n\n\n\nImportant\n\n\n\nGet organised! Put all files for a project in one folder. For example, I made a folder called BIO144_2026 and put all files related to this course in that folder. Within that folder, I have subfolders for data, scripts, and results. I then create an R Project in the BIO144_2026 folder. This way, all files related to the course are in one place, and I can easily find them later.\n\n\nNow, always open and ensure you‚Äôre working within the R Project for your project. As mentioned, you can see the project name at the top right of the RStudio window. And if its not the correct project, click on the name to get the drop-down list of available projects from which you can switch to the correct one.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#importing-data",
    "href": "2.1-R-and-RStudio.html#importing-data",
    "title": "R and RStudio (L2)",
    "section": "Importing data",
    "text": "Importing data\nFirst, get some data sets for us to work with. XYZ You can download them from the course website or use your own data sets. Save the data files in a folder called data within your R Project directory.\nWe will use the readr package to import data into R. The readr package provides functions to read data from various file formats, including CSV (comma-separated values) files, tab-separated values files, and others.\nTo read a CSV file, we can use the read_csv() function from the readr package. For example, to read a CSV file called my_data_file.csv, we can use the following code:\n\nlibrary(readr)\nmy_data &lt;- read_csv(\"datasets/my_data_file.csv\")\n\nThis code will read the data.csv file from the data folder within the current working directory (which should be the R Project directory) and store it in a data frame called data.\n\n\n\n\n\n\nTip\n\n\n\nEasily getting the file path In RStudio, you can easily get the file path by putting the cursor in the parentheses of the read_csv() function, the press the tab key. A drop-down menu will appear with options to navigate to the file. This way, you don‚Äôt have to type the file path manually.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#viewing-the-data",
    "href": "2.1-R-and-RStudio.html#viewing-the-data",
    "title": "R and RStudio (L2)",
    "section": "Viewing the data",
    "text": "Viewing the data\nOnce you‚Äôve imported your data, you can view it in several ways:\n\nClick on the data frame in the Environment tab in RStudio to open it in a new tab.\nUse the View() function to open the data frame in a new tab in RStudio.\nUse the head() function to view the first few rows of the data frame.\nUse the str() function to view the structure of the data frame, including the variable names and types.\nUse the summary() function to get a summary of the data frame, including basic statistics for each variable.\n\nAnother useful function is glimpse() from the dplyr package, which provides a quick overview of the data frame.\n\nlibrary(dplyr)\nglimpse(my_data)\n\nThere are many checks you can do to ensure your data was imported correctly. For example checking if there are duplicated values in a variable when there shouldn‚Äôt be:\n\nany(duplicated(my_data$Name))\n\n[1] FALSE\n\n\nThe function any() will return TRUE if there are any duplicated values in the Name variable, and FALSE otherwise. The function duplicated() returns a logical vector indicating which values are duplicates. We use the dollar sign $ to access a specific variable (column) in the data frame. A logical vector is a vector that contains only TRUE or FALSE values:\n\nduplicated(my_data$Name)\n\n[1] FALSE FALSE FALSE\n\n\nAll three logicals are FALSE, meaning none of the three are duplicates. If there were duplicates, the corresponding positions in the logical vector would be TRUE. For example:\n\nexample_vector &lt;- c(\"A\", \"B\", \"A\", \"C\", \"B\")\n\nWhat do you expect the output of duplicated(example_vector) to be?\nA final check (though not the final one we could do‚Äìthere are many others). Let us check for missing values and get a count of how many there are in each variable. We can do this with the following tidyverse code:\n\nlibrary(dplyr)\nmy_data |&gt; \n  summarise(across(everything(), ~ sum(is.na(.))))\n\n  Name Age Height\n1    0   0      0\n\n\nLooks complicated eh! Well, that‚Äôs because it is, for sure. But let‚Äôs break it down:\n\nsummarise() creates a new data frame with summary statistics.\nacross(everything(), ~ sum(is.na(.))) applies the function sum(is.na(.)) to every variable in the data frame.\nThe is.na() function returns a logical vector indicating which values are missing (NA), and the sum() function counts the number of TRUE values in that vector (i.e., the number of missing values).\n\n\n\n\n\n\n\nImportant\n\n\n\nLet‚Äôs assume your data was imported incorrectly. This means you have to inspect it carefully. Check that the variable names are correct, that the data types are correct (e.g., numeric, character, factor), that there are the correct number of rows and columns. If you find any issues, you need to find out what caused them, fix them, and re-import the data (see below).\n\n\nCommon data import problems:\n\nIncorrect delimiter: If your data file uses a different delimiter (e.g., tab, semicolon), you need to specify it in the read_csv() function using the delim argument (e.g., read_delim(\"data.csv\", delim = \"\\t\") for tab-delimited files).\nMissing values: If your data file uses a specific value to represent missing data (e.g., ‚ÄúNA‚Äù, ‚Äú-999‚Äù), you need to specify it in the read_csv() function using the na argument (e.g., read_csv(\"data.csv\", na = c(\"NA\", \"-999\"))).\nOnly one column: If your data file has only one column, it may be because the delimiter is incorrect. Check the delimiter and re-import the data with the correct delimiter.\nYou opened the downloaded file in Excel and then saved it: Excel may have changed the format of the file when you opened and saved it. Always work with the original downloaded file.\nWrong path or file name: Make sure the file path and name are correct. Remember, when you work in an R Project, you can place the cursor in the parentheses of the read_csv() function and press the tab key to navigate to the file.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#data-wrangling",
    "href": "2.1-R-and-RStudio.html#data-wrangling",
    "title": "R and RStudio (L2)",
    "section": "Data wrangling",
    "text": "Data wrangling\nNow we have our data imported and checked, and we‚Äôre ready to start working with it. This process is called data wrangling, and it involves cleaning, transforming, and reshaping the data to make it suitable for visualisation and analysis.\n\nClean the variable names\nThe first thing I like to do is standardise and clean up the variable names. I like to use the janitor package for this:\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nmy_data &lt;- my_data |&gt; \n  clean_names()\n\nThe clean_names() function from the janitor package will convert variable names to a consistent format (lowercase, spaces replaced by underscores, no special characters).\n\n\n\n\n\n\nNote\n\n\n\nWhen we ran the code library(janitor) we got the message: Attaching package: ‚Äòjanitor‚Äô The following objects are masked from ‚Äòpackage:stats‚Äô: chisq.test, fisher.test This sometimes happens when two packages have functions with the same name. In this case, the janitor package has functions called chisq.test() and fisher.test(), which are also in the base stats package. When we load the janitor package, it masks (hides) the functions from the stats package. This is usually not a problem, but if you want to use the functions from the stats package, you can specify the package name when calling the function, like this: stats::chisq.test().\n\n\n\n\nManipulate the data frame\nFunctions in the dplyr package are used to manipulate data frames:\n\nselect(): select columns by position, or by name, or by other methods\nfilter(): select rows that meet a logical condition\nslice(): select rows by position\narrange(): reorder rows\nmutate(): add new variables\n\nThe dplyr package also provides functions to group data frames and to summarize data:\n\ngroup_by(): add to a data frame a grouping structure\nsummarize(): summarize data, respecting any grouping structure specified by group_by()\n\nThe pipe operator |&gt; is used to chain together multiple operations on a data frame.\n\n\n\n\n\n\nTip\n\n\n\nNote that you will often see another pipe operator %&gt;% used in examples. The pipe operator |&gt; is a newer version of %&gt;% that is more efficient and easier to use. The pipe operator |&gt; is available in R version 4.1.0 and later.\n\n\nLets work through some examples with a sample data frame:\n\nmy_data1 &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eva\"),\n  age = c(25, 30, 35, 40, 45),\n  score = c(90, 85, 95, 80, 70))\n\nHere is the same dataset with 100 rows:\n\nset.seed(123)\nmy_data2 &lt;- tibble(name = paste0(\"Person_\", sprintf(\"%03d\", 1:100)),\n  age = sample(20:50, 100, replace = TRUE),\n  score = rnorm(100, mean = 75, sd = 10))\n\n\n\nSelect columns\nWe can select columns by name\n\nmy_data2 |&gt; \n  select(name, score)\n\n# A tibble: 100 √ó 2\n   name       score\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Person_001  91.9\n 2 Person_002  87.3\n 3 Person_003  77.8\n 4 Person_004  64.5\n 5 Person_005  69.8\n 6 Person_006  91.2\n 7 Person_007  64.3\n 8 Person_008  91.9\n 9 Person_009  72.6\n10 Person_010  70.3\n# ‚Ñπ 90 more rows\n\n\nWe can select columns by position\n\nmy_data2 |&gt; \n  select(1, 3)\n\n# A tibble: 100 √ó 2\n   name       score\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 Person_001  91.9\n 2 Person_002  87.3\n 3 Person_003  77.8\n 4 Person_004  64.5\n 5 Person_005  69.8\n 6 Person_006  91.2\n 7 Person_007  64.3\n 8 Person_008  91.9\n 9 Person_009  72.6\n10 Person_010  70.3\n# ‚Ñπ 90 more rows\n\n\nWe can select columns by a condition, for example select only the numeric columns:\n\nmy_data2 |&gt; \n  select(where(is.numeric))\n\n# A tibble: 100 √ó 2\n     age score\n   &lt;int&gt; &lt;dbl&gt;\n 1    50  91.9\n 2    34  87.3\n 3    38  77.8\n 4    33  64.5\n 5    22  69.8\n 6    29  91.2\n 7    37  64.3\n 8    41  91.9\n 9    30  72.6\n10    24  70.3\n# ‚Ñπ 90 more rows\n\n\nWe can select a column by pattern matching, using helper functions, for example select columns that contain the letter ‚Äúa‚Äù:\n\nmy_data2 |&gt; \n  select(contains(\"a\"))\n\n# A tibble: 100 √ó 2\n   name         age\n   &lt;chr&gt;      &lt;int&gt;\n 1 Person_001    50\n 2 Person_002    34\n 3 Person_003    38\n 4 Person_004    33\n 5 Person_005    22\n 6 Person_006    29\n 7 Person_007    37\n 8 Person_008    41\n 9 Person_009    30\n10 Person_010    24\n# ‚Ñπ 90 more rows\n\n\nOther helpers include starts_with(), ends_with(), matches(), and everything().\n\n\nFilter: Getting particular rows of data [#filter-rows]\nTo get particular rows of data, we can use the filter() function. This function takes a logical condition as an argument and returns only the rows that meet that condition. For example, to get all rows where the Age is greater than 30:\n\nmy_data2 |&gt; \n  filter(age &gt; 30)\n\n# A tibble: 66 √ó 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_002    34  87.3\n 3 Person_003    38  77.8\n 4 Person_004    33  64.5\n 5 Person_007    37  64.3\n 6 Person_008    41  91.9\n 7 Person_011    39  67.3\n 8 Person_012    33  96.5\n 9 Person_013    41  61.7\n10 Person_014    44  80.0\n# ‚Ñπ 56 more rows\n\n\nHere, the logical condition is age &gt; 30.\nWe can combine multiple conditions using the logical operators & (and), | (or), and ! (not). For example, to get all rows where the Age is greater than 30 and the Score is less than 90:\n\nmy_data2 |&gt; \n  filter(age &gt; 30 & score &lt; 90)\n\n# A tibble: 60 √ó 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_002    34  87.3\n 2 Person_003    38  77.8\n 3 Person_004    33  64.5\n 4 Person_007    37  64.3\n 5 Person_011    39  67.3\n 6 Person_013    41  61.7\n 7 Person_014    44  80.0\n 8 Person_015    45  87.3\n 9 Person_016    46  81.3\n10 Person_018    38  82.9\n# ‚Ñπ 50 more rows\n\n\nOther logical operators include == (equal to), != (not equal to), &lt;= (less than or equal to), and &gt;= (greater than or equal to).\n\n\nSlice: Getting rows by position [#slice-rows]\nThe slice() function allows us to get rows by their position in the data frame. For example, to get the first two rows:\n\nmy_data2 |&gt; \n  slice(1:2)\n\n# A tibble: 2 √ó 3\n  name         age score\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n1 Person_001    50  91.9\n2 Person_002    34  87.3\n\n\nI very rarely use this function, as I prefer to use filter() with logical conditions. I can‚Äôt think of a good use case for this function right now! Perhaps you can?\n\n\nArrange: Reordering rows\nThe arrange() function allows us to reorder the rows of a data frame based on the values in one or more columns. For example, to reorder the rows by Age in ascending order:\n\nmy_data2 |&gt; \n  arrange(age)\n\n# A tibble: 100 √ó 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_064    20  88.8\n 2 Person_056    21  79.5\n 3 Person_091    21  67.4\n 4 Person_005    22  69.8\n 5 Person_025    22  74.9\n 6 Person_033    23  67.3\n 7 Person_092    23  72.4\n 8 Person_010    24  70.3\n 9 Person_017    24  79.1\n10 Person_058    24  72.7\n# ‚Ñπ 90 more rows\n\n\nI f we want to reorder the rows by Age in descending order, we can use the desc() function:\n\nmy_data2 |&gt; \n  arrange(desc(age))\n\n# A tibble: 100 √ó 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_061    50  79.9\n 3 Person_075    50  67.3\n 4 Person_085    50  50.1\n 5 Person_096    50  86.8\n 6 Person_031    49  71.8\n 7 Person_078    49  72.6\n 8 Person_024    48  81.2\n 9 Person_057    48  80.3\n10 Person_021    47  66.0\n# ‚Ñπ 90 more rows\n\n\nIt‚Äôs unusual to need the rows of a dataset to be arranged in a specific order, but it can be useful when looking at the data directly.\n\n\n\n\n\n\nTip\n\n\n\nNote that when you view the data in RStudio, it will always be arranged by the row number. In the viewer you can sort by clicking on the column headers.\n\n\n\n\nMutate: Adding new variables [#mutate-variables]\nThe mutate() function allows us to add new variables to a data frame. For example, to add a new variable called Age_in_5_years that is the Age plus 5:\n\nmy_data2 |&gt; \n  mutate(age_in_5_years = age + 5)\n\n# A tibble: 100 √ó 4\n   name         age score age_in_5_years\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;\n 1 Person_001    50  91.9             55\n 2 Person_002    34  87.3             39\n 3 Person_003    38  77.8             43\n 4 Person_004    33  64.5             38\n 5 Person_005    22  69.8             27\n 6 Person_006    29  91.2             34\n 7 Person_007    37  64.3             42\n 8 Person_008    41  91.9             46\n 9 Person_009    30  72.6             35\n10 Person_010    24  70.3             29\n# ‚Ñπ 90 more rows\n\n\nWe can add multiple new variables at once:\n\nmy_data2 |&gt; \n  mutate(\n    age_in_5_years = age + 5,\n    percentage_score = score / 100\n  )\n\n# A tibble: 100 √ó 5\n   name         age score age_in_5_years percentage_score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n 1 Person_001    50  91.9             55            0.919\n 2 Person_002    34  87.3             39            0.873\n 3 Person_003    38  77.8             43            0.778\n 4 Person_004    33  64.5             38            0.645\n 5 Person_005    22  69.8             27            0.698\n 6 Person_006    29  91.2             34            0.912\n 7 Person_007    37  64.3             42            0.643\n 8 Person_008    41  91.9             46            0.919\n 9 Person_009    30  72.6             35            0.726\n10 Person_010    24  70.3             29            0.703\n# ‚Ñπ 90 more rows\n\n\n\n\nWorking with categorical variables\nVariables in a data frame in R have a type. The most common types of variables are numeric and categorical. Numeric variables are variables that take on numerical values, such as age or score. Categorical variables are variables that take on a limited number of values, often representing categories or groups. In R, categorical variables are typically have type &lt;chr&gt; which is character. Or they can be of type &lt;fct&gt; which is factor.\nWhen we import data categorical variable is usually imported as a character variable. For example, the variable name in our example dataset is a categorical variable of type character. Look at the first few rows of the dataset again, and see that below the variable name it says &lt;chr&gt; for the name variable:\n\nmy_data2\n\n# A tibble: 100 √ó 3\n   name         age score\n   &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt;\n 1 Person_001    50  91.9\n 2 Person_002    34  87.3\n 3 Person_003    38  77.8\n 4 Person_004    33  64.5\n 5 Person_005    22  69.8\n 6 Person_006    29  91.2\n 7 Person_007    37  64.3\n 8 Person_008    41  91.9\n 9 Person_009    30  72.6\n10 Person_010    24  70.3\n# ‚Ñπ 90 more rows\n\n\nThis is all totally fine. There are, however, use cases where we might want to convert a character variable to a factor variable. Factors are useful when we have a categorical variable with a fixed number of levels, and we want to specify the order of those levels. For example, if we had a variable called education_level with the values ‚ÄúHigh School‚Äù, ‚ÄúBachelor‚Äôs‚Äù, ‚ÄúMaster‚Äôs‚Äù, and ‚ÄúPhD‚Äù, we might want to convert this variable to a factor and specify the order of the levels.\nLet‚Äôs make a new dataset to illustrate this:\n\nmy_data3 &lt;- tibble(\n  name = c(\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"),\n  education_level = c(\"Bachelor's\", \"Master's\", \"PhD\", \"High School\", \"Bachelor's\"),\n  age = c(19, 23, 25, 16, 20)\n)\n\nLook at the structure of this new dataset:\n\nmy_data3\n\n# A tibble: 5 √ó 3\n  name    education_level   age\n  &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n1 Alice   Bachelor's         19\n2 Bob     Master's           23\n3 Charlie PhD                25\n4 David   High School        16\n5 Eve     Bachelor's         20\n\n\nWe can see that the education_level variable is of type &lt;chr&gt;, which is character.\nWe can convert the education_level variable to a factor:\n\nmy_data3 &lt;- my_data3 |&gt; \n  mutate(education_level = factor(education_level))\nmy_data3\n\n# A tibble: 5 √ó 3\n  name    education_level   age\n  &lt;chr&gt;   &lt;fct&gt;           &lt;dbl&gt;\n1 Alice   Bachelor's         19\n2 Bob     Master's           23\n3 Charlie PhD                25\n4 David   High School        16\n5 Eve     Bachelor's         20\n\n\nNow, the education_level variable is of type &lt;fct&gt;, which is factor.\nHere is a graph of age by education level:\n\nggplot(my_data3, aes(x = education_level, y = age)) +\n  geom_point()\n\n\n\n\n\n\n\n\nWe have a problem here: the education levels are not in a sensible order. The first level is ‚ÄúBachelor‚Äôs‚Äù, followed by ‚ÄúHigh School‚Äù, ‚ÄúMaster‚Äôs‚Äù, and ‚ÄúPhD‚Äù.\n\n\n\n\n\n\nNote\n\n\n\nWhy do you think the levels are in this order? We didn‚Äôt tell R to order them like this! The answer is that R orders factor levels alphabetically by default. So when we convert a character variable to a factor without specifying the order of the levels, R will order them alphabetically.\n\n\nIt would be much better to have the levels ordered as ‚ÄúHigh School‚Äù, ‚ÄúBachelor‚Äôs‚Äù, ‚ÄúMaster‚Äôs‚Äù, and ‚ÄúPhD‚Äù.\nWe can fix this by specifying the order of the levels when we convert the variable to a factor:\n\nmy_data3 &lt;- my_data3 |&gt; \n  mutate(education_level = factor(education_level,\n                                  levels = c(\"High School\", \"Bachelor's\", \"Master's\", \"PhD\")))\n\nNow when we plot the data again, the education levels are in the correct order:\n\nggplot(my_data3, aes(x = education_level, y = age)) +\n  geom_point()\n\n\n\n\n\n\n\n\nAnother use case is when we are making a linear model and want to specify the reference level for a categorical variable. We will look at this when we get to linear models. If you want to skip ahead, you can see how this works in a section at the end of this chapter.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#visualisation",
    "href": "2.1-R-and-RStudio.html#visualisation",
    "title": "R and RStudio (L2)",
    "section": "Visualisation",
    "text": "Visualisation\nThere are many many many types of data visualisation. We will not explore them all in this course! In fact, we will use only a few basic types of visualisation, but we will use them well and critically. The three types of visualisation we will focus on are scatter plots, histograms, and box and whisker plots.\n\nThree basic types of visualisation [#three-basic-visualisations]\nScatterplots are used to visualise the relationship between two continuous variables. Here is an example of a scatterplot:\n\nlibrary(ggplot2)\nggplot(my_data2, aes(x = age, y = score)) +\n  geom_point()\n\n\n\n\n\n\n\n\nHistograms are used to visualise the distribution of a single continuous variable. The axiss are different to scatterplots: the x-axis is the variable being measured, and the y-axis is the count (or frequency) of observations in each bin. A bin is a range of values. Here is an esample of a histogram:\n\nggplot(my_data2, aes(x = score)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nBox and whisker plots are used to visualise the distribution of a continuous variable across different categories. Here is an example of a box and whisker plot. First we add a new variable that is age group:\n\nmy_data2 &lt;- my_data2 |&gt; \n  mutate(age_group = case_when(\n    age &lt; 30 ~ \"20-29\",\n    age &gt;= 30 & age &lt; 40 ~ \"30-39\",\n    age &gt;= 40 ~ \"40-49\"\n  ))\n\nThe new variable age_group is a categorical variable with three levels: ‚Äú20-29‚Äù, ‚Äú30-39‚Äù, and ‚Äú40-49‚Äù. We make this using the case_when() function. This function works by checking each condition (which are given as the arguments to the function) in turn, and assigning the corresponding value when the condition is true. Now we can make the box and whisker plot:\n\nggplot(my_data2, aes(x = factor(age_group), y = score)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\nUnderstanding ggplot2 syntax [#understanding-ggplot2]\nWe have used the ggplot2 package to create visualisations. The ggplot2 package is based on the grammar of graphics, which provides a consistent way to create visualisations. It is amazing, and when it was created it revolutionised data visualisation in R.\nYou can see that for each of the three visualisations, we use the ggplot() function to create the base plot, and then we add layers to the plot using the + operator.\nThe first argument to the ggplot() function is the data frame that we want to visualise. The layers that we add to the plot each have two main components. The first component is the aesthetic mappings, which specify how the variables in the data frame are mapped to the visual properties of the plot (e.g., x-axis, y-axis, color, size). The second component is the geometric object, which defines how the data is represented in the plot (e.g., points, lines, bars).\nThe aesthetic mappings are specified using the aes() function, which takes arguments that define the mappings. Inside the aes() function, we specify the variables from the data frame that we want to map to the visual properties of the plot. For example, in the scatterplot, we map the age variable to the x-axis and the score variable to the y-axis using aes(x = age, y = score).\nThe geometric object is specified using functions that start with geom_, such as geom_point(), geom_histogram(), and geom_boxplot().\nYou will notice that for the scatterplot and the box and whisker plot, we specify both an x- and a y-variable, but for the histogram we only specify an x-variable. This is because histograms only have one variable, which is the variable being measured. The y-axis is automatically calculated as the count (or frequency) of observations in each bin.\nWe can customise many features of the graph using additional arguments to the ggplot() function and the geom_ functions. For example, we can add titles and labels to the axes using the labs() function:\n\nggplot(my_data2, aes(x = age, y = score)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Age vs Score\",\n    x = \"Age (years)\",\n    y = \"Score\"\n  )\n\n\n\n\n\n\n\n\nWe can also change the theme of the plot using the theme_ functions. For example, to use a minimal theme, and add it the customisations we already made:\n\nggplot(my_data2, aes(x = age, y = score)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Age vs Score\",\n    x = \"Age (years)\",\n    y = \"Score\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere are a million and one ways to customise visualisations in ggplot2. We will explore many of them during the course in a rather ad-hoc way. In this course we do not assess your skill and competence in making clear and beautiful visualisations. We will, however, be very happy to help you make beautiful and effective visualisations for your assignments and projects. And please be sure that making beautiful and effective visualisations is a skill that is very highly valued in the workplace.\n\n\nSaving ggplot visualisations [#saving-ggplot]\nAnother feature that is very useful is to save ggplot visualisations to objects and then save to a file (for example a pdf). First, here is how we save a ggplot to an object:\n\nplot1 &lt;- ggplot(my_data2, aes(x = age, y = score)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Age vs Score\",\n    x = \"Age (years)\",\n    y = \"Score\"\n  ) \n\nNow we can save the plot to a file using the ggsave() function:\n\nggsave(\"scatterplot_age_vs_score.pdf\", plot = plot1, width = 8, height = 6)\n\nNote two things about the ggsave() function. First, the first argument is the file name (including the file extension). The file extension determines the file type (e.g., pdf, png, jpeg). Second, we can specify the width and height of the plot in inches.\nAlso note that the file is saved to the current working directory. When you‚Äôre working in an R project, this is usually the base directory of the project. If you want to save your plots in a folder named plots you would first need to create the folder (if it doesn‚Äôt already exist) and then specify the path in the file name:\n\ndir.create(\"plots\")  # Create the folder if it doesn't exist\n\nWarning in dir.create(\"plots\"): 'plots' already exists\n\nggsave(\"plots/scatterplot_age_vs_score.pdf\", plot = plot1, width = 8, height = 6)",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#review",
    "href": "2.1-R-and-RStudio.html#review",
    "title": "R and RStudio (L2)",
    "section": "Review",
    "text": "Review\nIn this chapter we learned about using R and RStudio for data analysis. We covered the basics of R programming, including data types, variables, functions, and control structures. We also learned about importing data into R, cleaning and wrangling data using the dplyr package, and visualising data using the ggplot2 package. A great starting point for your journey into data analysis with R!",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#further-reading",
    "href": "2.1-R-and-RStudio.html#further-reading",
    "title": "R and RStudio (L2)",
    "section": "Further reading",
    "text": "Further reading\nThe data wrangling packages we use, such as dplyr and tidyr, are part of the tidyverse, a collection of R packages designed for data science. These packages were developed by Hadley Wickham and his team at RStudio. Moreover, Hadley Wickham is also the creator of ggplot2, the package we use for data visualisation.\nA great place to consolidate your learning so far, and to learn more about the tidyverse and data science with R is the book R for Data Science by Hadley Wickham and Garrett Grolemund (https://r4ds.had.co.nz/). This book is available for free online and covers many topics related to data science with R, including:\n\nScripts and projects\nData import\nData wrangling with dplyr and tidyr\nData visualisation with ggplot2\nAnd much more‚Ä¶\n\nBase R is a term used to describe the core functionality of R, without any additional packages. While the tidyverse packages are very useful and powerful, it can be useful to learn about the base R functions, especially if someone else is using base R in their code. A starting point for learning about base R is the chapter ‚ÄúA field guide to base R‚Äù in the R for Data Science book ‚Äì A field guide to base R.\n\nCheat sheets\nThere are a series of excellent cheat sheets for R and RStudio, and for packages in the tidyverse. They‚Äôre pretty dense and packed with information, but they can be very useful as a quick reference. You can find them here: RStudio Cheat Sheets.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "2.1-R-and-RStudio.html#extras",
    "href": "2.1-R-and-RStudio.html#extras",
    "title": "R and RStudio (L2)",
    "section": "Extras",
    "text": "Extras\n\nMaking reports directly using Quarto [#quarto-reports]\nWe don‚Äôt explicitly ask you to make reports using Quarto in this course, but it is a very useful skill to have, and I highly recommend you explore it further in your own time. Here are a few basics to get you started.\nOne of the great features of R and RStudio is the ability to create reports that combine text, code, and visualisations. One of the most popular tools for this is Quarto (https://quarto.org/), which allows you to create documents in various formats (HTML, PDF, Word, etc.) using a combination of Markdown and R code.\n**Why is this so great???* If you want to show someone your analysis and visualisation, say a team member or supervisor, it is often good to prepare a report that explains what you did, perhaps shows the code you used, and presents the results (including visualisations). One way to go about this is to prepare a powerpoint presentation or a word document, and then copy and paste code and visualisations into the document. Its what I used to do. It works. But it is tedious, error prone, and when you change something in your code or data, you have to remember to go back and update the powerpoint or word document.\nWith Quarto, you can create a report that automatically includes the code and visualisations directly from your R script. This way, if you change the code or data, you can simply re-render the report and everything is automatically updated. It takes away a lot of the tediousness and potential for errors. And it makes updating reports much easier.\nIf you‚Äôd like to get started with Quarto, check out the Quarto website (https://quarto.org/) and the RStudio Quarto documentation (https://quarto.org/docs/get-started/). There are also many tutorials and resources available online to help you learn how to use Quarto effectively.\nIf you have questions about Quarto, feel free to ask me or TAs during the practicals, though note that any particular TAs may or may not be experienced with Quarto themselves.\nQuarto reports are also covered in the R for Data Science book ‚Äì Quarto.\n\n\nCombining ggplots with patchwork [#combining-ggplots]\nWe often make multiple ggplots in our analyses. Sometimes it is useful to combine multiple plots into a single figure for easier comparison or presentation. We can do with ggplots and the lovely add-on package called patchwork. The patchwork package allows us to combine multiple ggplots into a single plot layout. Here is an example of how to use patchwork to combine the three plots we made earlier (scatterplot, histogram, and boxplot):\nFirst, load the patchwork package:\n\nlibrary(patchwork)\n\nNext make the first plot and assign it to an object:\n\nplot1 &lt;- ggplot(my_data2, aes(x = age, y = score)) +\n  geom_point() +\n  labs(\n    title = \"Scatterplot of Age vs Score\",\n    x = \"Age (years)\",\n    y = \"Score\"\n  ) \n\nNow make the second plot and assign it to an object:\n\nplot2 &lt;- ggplot(my_data2, aes(x = score)) +\n  geom_histogram(binwidth = 5) +\n  labs(\n    title = \"Histogram of Scores\",\n    x = \"Score\",\n    y = \"Count\"\n  ) \n\nNow make the third plot and assign it to an object:\n\nplot3 &lt;- ggplot(my_data2, aes(x = factor(age_group), y = score)) +\n  geom_boxplot() +\n  labs(\n    title = \"Boxplot of Scores by Age Group\",\n    x = \"Age Group\",\n    y = \"Score\"\n  ) \n\nNow we can combine the three plots into a single layout using the patchwork syntax. Here, we arrange plot1 on the top row, and plot2 and plot3 side by side on the bottom row:\n\ncombined_plot &lt;- plot1 / (plot2 | plot3)\ncombined_plot\n\n\n\n\n\n\n\n\nAmazing eh! OK, lets leave it there for now. We‚Äôll use ggplot2 throughout the course, and explore more features as we go along.\nIf you‚Äôd like to read more about the patchwork package, check out the package website ‚Äì patchwork.\n\n\nSetting a reference level in a linear model\nSometimes when fitting linear models with categorical explanatory (independent) variables, it is useful to set a specific reference level for the categorical variable. This can help in interpreting the model coefficients. In R, we can set the reference level using the relevel() function or by using the factor() function with the levels argument.\nFirst, let‚Äôs create a simple dataset:\n\nmy_data4 &lt;- tibble(\n  treatment = factor(c(\"Control\", \"Aspirin\", \"Ibuprofen\", \"Control\", \"Aspirin\", \"Ibuprofen\")),\n  response = c(5, 7, 6, 4, 8, 7)\n)\n\nBy default, R will set the first level of the factor (in alphabetical order) as the reference level. In this case, ‚ÄúAspirin‚Äù would be the reference level. Therefore when we visualise the data:\n\nggplot(my_data4, aes(x = treatment, y = response)) +\n  geom_point() +\n  labs(\n    title = \"Response by Treatment\",\n    x = \"Treatment\",\n    y = \"Response\"\n  ) \n\n\n\n\n\n\n\n\nIt would be nicer to have the ‚ÄúControl‚Äù group as the first level on the left of the x-axis.\nLikewise, when we make a linear model:\n\nmodel1 &lt;- lm(response ~ treatment, data = my_data4)\nsummary(model1)\n\n\nCall:\nlm(formula = response ~ treatment, data = my_data4)\n\nResiduals:\n   1    2    3    4    5    6 \n 0.5 -0.5 -0.5 -0.5  0.5  0.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          7.5000     0.5000  15.000 0.000643 ***\ntreatmentControl    -3.0000     0.7071  -4.243 0.023981 *  \ntreatmentIbuprofen  -1.0000     0.7071  -1.414 0.252215    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7071 on 3 degrees of freedom\nMultiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 \nF-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152\n\n\nThe (Intercept) term corresponds to the ‚ÄúAspirin‚Äù group, and the coefficients for ‚ÄúControl‚Äù and ‚ÄúIbuprofen‚Äù are relative to ‚ÄúAspirin‚Äù. R has done this because in the factor levels, ‚ÄúAspirin‚Äù comes first alphabetically and was therefore set as the reference level when the factor variable was created.\nIf we want to set ‚ÄúControl‚Äù as the reference level, we can do so using relevel():\n\nmy_data4 &lt;- my_data4 %&gt;%\n  mutate(treatment = relevel(treatment, ref = \"Control\"))\n\nNow when we visualise the data again:\n\nggplot(my_data4, aes(x = treatment, y = response)) +\n  geom_point() +\n  labs(\n    title = \"Response by Treatment\",\n    x = \"Treatment\",\n    y = \"Response\"\n  ) \n\n\n\n\n\n\n\n\nMagic! The ‚ÄúControl‚Äù group is now the first level on the left of the x-axis.\nAnd when we fit the linear model again:\n\nmodel2 &lt;- lm(response ~ treatment, data = my_data4)\nsummary(model2)\n\n\nCall:\nlm(formula = response ~ treatment, data = my_data4)\n\nResiduals:\n   1    2    3    4    5    6 \n 0.5 -0.5 -0.5 -0.5  0.5  0.5 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)          4.5000     0.5000   9.000   0.0029 **\ntreatmentAspirin     3.0000     0.7071   4.243   0.0240 * \ntreatmentIbuprofen   2.0000     0.7071   2.828   0.0663 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7071 on 3 degrees of freedom\nMultiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 \nF-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152\n\n\nThe (Intercept) term now corresponds to the ‚ÄúControl‚Äù group, and the coefficients for ‚ÄúAspirin‚Äù and ‚ÄúIbuprofen‚Äù are relative to ‚ÄúControl‚Äù. This makes interpretation of the model coefficients more intuitive.\nAlso see the chapter about factors in the R for Data Science book ‚Äì Factors.",
    "crumbs": [
      "R and RStudio (L2)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html",
    "href": "3.1-regression-part1.html",
    "title": "Regression Part 1 (L3)",
    "section": "",
    "text": "Introduction\nLinear regression is a common statistical method that models the relationship between a dependent (response) variable and one or more independent (explanatory) variables. The relationship is modeled with the equation for a straight line (\\(y = a + bx\\)).\nWith linear regression we can answer questions such as:\nIn this chapter / lesson we will explore what is linear regression and how to use it to answer these questions. We‚Äôll cover the following topics:\nIn this chapter / lesson we will not discuss the statistical significance of the model. We will cover this topic in the next chapter / lesson.",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#introduction",
    "href": "3.1-regression-part1.html#introduction",
    "title": "Regression Part 1 (L3)",
    "section": "",
    "text": "How does the dependent (response) variable change with respect to the independent (explanatory) variable?\nWhat amount of variation in the dependent variable can be explained by the independent variable?\nIs there a statistically significant relationship between the dependent variable and the independent variable?\nDoes the linear model fit the data well?\n\n\n\nWhy use linear regression?\nWhat is the linear regression model?\nFitting the regression model (= finding the intercept and the slope).\nIs linear regression a good enough model to use?\nWhat do we do when things go wrong?\nTransformation of variables/the response.\nIdentifying and handling odd data points (aka outliers).\n\n\n\nWhy use linear regression?\n\nIt‚Äôs a good starting point because it is a relatively simple model.\nRelationships are sometimes close enough to linear.\nIt‚Äôs easy to interpret.\nIt‚Äôs easy to use.\nIt‚Äôs actually quite flexible (e.g.¬†can be used for non-linear relationships, e.g., a quadratic model is still a linear model!!!).\n\n\n\nAn example - blood pressure and age\nThere are lots of situations in which linear regression can be useful. For example, consider hypertension. Hypertension is a condition in which the blood pressure in the arteries is persistently elevated. Hypertension is a major risk factor for heart disease, stroke, and kidney disease. It is estimated that hypertension affects about 1 billion people worldwide. Hypertension is a complex condition that is influenced by many factors, including age. In fact, it is well known that blood pressure increases with age. But how much does blood pressure increase with age? This is a question that can be answered using linear regression.\nHere is an example of a study that used linear regression to answer this question: https://journals.lww.com/jhypertension/fulltext/2021/06000/association_of_age_and_blood_pressure_among_3_3.15.aspx\nIn this study, the authors used linear regression to model the relationship between age and blood pressure. They found that systolic blood pressure increased by 0.28‚Äì0.85 mmHg/year. This is a small increase, but it is statistically significant. This means that the observed relationship between age and blood pressure is unlikely to be due to chance.\nLets look at some simulated example data:\n\n# Load the data\nbp_age_data &lt;- read.csv(\"datasets/Simulated_Blood_Pressure_and_Age_Data.csv\")\n\n# How many data points with both age and blood pressure?\nbp_age_data &lt;- na.omit(bp_age_data)\n# No rows with missing values\n\n# Visualize the data\nggplot(bp_age_data, aes(x = Age, y = Systolic_BP)) +\n  geom_point() +\n  labs(title = \"Systolic Blood Pressure vs. Age\",\n       x = \"Age\",\n       y = \"Systolic Blood Pressure\")\n\n\n\n\n\n\n\n\nWell, that is pretty conclusive. We hardly need statistics. There is a clear positive relationship between age and systolic blood pressure. But how can we quantify this relationship? And in less clear-cut cases what is the strength of evidence for a relationship? This is where linear regression comes in. Linear regression models the relationship between age and systolic blood pressure. With linear regression we can answer the following questions:\n\nWhat is the value of the intercept and slope of the relationship?\nIs the relationship different from what we would expect if there were no relationship?\nHow well does the mathematical representation match the observed values?\nHow much uncertainty is there in predictions?\n\nLets try to figure some of these out from the visualisation.\nThink-Pair-Share (#tps-guess-params) Make a guess of the slope. Make a guess of the intercept (hint be careful, lots of people get this wrong).",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#calculating-the-intercept-and-slope",
    "href": "3.1-regression-part1.html#calculating-the-intercept-and-slope",
    "title": "Regression Part 1 (L3)",
    "section": "Calculating the intercept and slope",
    "text": "Calculating the intercept and slope\n\nRegression from a mathematical perspective\nGiven an independent/explanatory variable (\\(X\\)) and a dependent/response variable (\\(Y\\)) all points \\((x_i,y_i)\\), \\(i= 1,\\ldots, n\\), on a straight line follow the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i\\ .\\]\n\n\\(\\beta_0\\) is the intercept - the value of \\(Y\\) when \\(x_i = 0\\)\n\\(\\beta_1\\) the slope of the line, also known as the regression coefficient of \\(X\\).\nIf \\(\\beta_0=0\\) the line goes through the origin \\((x,y)=(0,0)\\).\nInterpretation of linear dependency: proportional increase in \\(y\\) with increase (decrease) in \\(x\\).\n\n\n\nFinding the intercept and the slope\nIn a regression analysis, one task is to estimate the intercept and the slope. These are known as the regression coefficients \\(\\beta_0\\), \\(\\beta_1\\).\n\nProblem: For more than two points \\((x_i,y_i)\\), \\(i=1,\\ldots, n\\), there is generally no perfectly fitting line.\nAim: We want to estimate the parameters \\((\\beta_0,\\beta_1)\\) of the best fitting line \\(Y = \\beta_0 + \\beta_1 x\\).\nIdea: Find the best fitting line by minimizing the deviations between the data points \\((x_i,y_i)\\) and the regression line. I.e., minimising the residuals.\n\nBut which deviations?\nThese ones?\n\n\n\n\n\n\n\n\n\nOr these?\n\n\n\n\n\n\n\n\n\nOr maybe even these?\n\n\n\n\n\n\n\n\n\nWell, actually its none of these!!!\n\n\nLeast squares\nFor multiple reasons (theoretical aspects and mathematical convenience), the intercept and slope are estimated using the least squares approach. In this, yet something else is minimized:\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated such that the sum of squared vertical distances (sum of squared residuals / errors) is minimised.\nSSE means Sum of Squared Errors:\n\\[SSE = \\sum_{i=1}^n e_i^2 \\]\nwhere,\n\\[e_i = y_i - \\underbrace{(\\beta_0 + \\beta_1 x_i)}_{=\\hat{y}_i} \\] Note: \\(\\hat y_i = \\beta_0 + \\beta_1 x_i\\) are the predicted values.\nIn the graph just below, one of these squares is shown in red.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nResiduals are model-based quantities, not properties of the raw data.\n\n\n\n\nLeast squares estimates\nWith a linear model, we can calculate the least squares estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) directly using the following formulas.\nFor a given sample of data \\((x_i,y_i), i=1,..,n\\), with mean values \\(\\overline{x}\\) and \\(\\overline{y}\\), the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are computed as\n\\[ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n  (y_i - \\overline{y}) (x_i - \\overline{x})}{ \\sum_{i=1}^n (x_i - \\overline{x})^2 } = \\frac{cov(x,y)}{var(x)}\\]\n\\[\\hat\\beta_0 = \\overline{y} - \\hat\\beta_1 \\overline{x}  \\]\nMoreover,\n\\[ \\hat\\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2 \\quad \\text{with residuals  } e_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i) \\]\nis an unbiased estimate of the residual variance \\(\\sigma^2\\).\n(Derivations of the equations above are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)\n\n\nWhy division by \\(n-2\\) ensures an unbiased estimator\nWhen estimating parameters (\\(\\beta_0\\) and \\(\\beta_1\\)), the square of the residuals is minimised. This fitting process inherently uses up two degrees of freedom, as the model forces the residuals to sum to zero and aligns the slope to best fit the data. I.e., one degree of freedom is lost due to the estimation of the intercept, and another due to the estimation of the slope.\nThe adjustment (division by \\(n-2\\) instead of \\(n\\)) compensates for the loss of variability due to parameter estimation, ensuring the estimator of the residual variance is unbiased. Mathematically, dividing by n - 2 adjusts for this loss and gives an accurate estimate of the population variance when working with sample data.\nWe‚Äôll look at degrees of freedom in more detail later, so don‚Äôt worry if this is a bit confusing right now.\n\n\nLet‚Äôs do it in R\nFirst we read in the dataset:\n\nbp_age_data &lt;- read.csv(\"datasets/Simulated_Blood_Pressure_and_Age_Data.csv\")\n\nThe we make a graph of the data:\n\nggplot(bp_age_data, aes(x = Age, y = Systolic_BP)) +\n  geom_point() +\n  labs(x = \"Age (years)\", y = \"Systolic Blood Pressure (mmHg)\")\n\n\n\n\n\n\n\n\nThen we make the linear model, using the lm() function:\n\nbp_age_model &lt;- lm(Systolic_BP ~ Age, data = bp_age_data)\n\nThen we can look at the summary of the model. It contains a lot of information, so can be a bit confusing at first.\n\nsummary(bp_age_model)\n\n\nCall:\nlm(formula = Systolic_BP ~ Age, data = bp_age_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2195  -3.4434  -0.0808   3.1383  12.6025 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 98.96874    1.46102   67.74   &lt;2e-16 ***\nAge          0.82407    0.02771   29.74   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.971 on 98 degrees of freedom\nMultiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 \nF-statistic: 884.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nHow do our guesses of the intercept and slope compare to the guesses we made earlier?\nRecal that the units of the Age coefficient are in mmHg per year. This means that for each additional year of age, the systolic blood pressure increases by ¬¥r round(coef(bp_age_model)[2],2)¬¥ mmHg.\nThink‚ÄìPair‚ÄìShare (#a_interpreting_slope) In words, what does the slope represent in this model? What does it not tell you?",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#dealing-with-the-error",
    "href": "3.1-regression-part1.html#dealing-with-the-error",
    "title": "Regression Part 1 (L3)",
    "section": "Dealing with the error",
    "text": "Dealing with the error\n\n\n\n\n\n\n\n\n\nThe line is not a perfect fit to the data. There is scatter around the line.\nSome of this scatter could be caused by other factors that influence blood pressure, such as diet, exercise, and genetics. Also, the there could be differences due to the measurement instrument (i.e., some measurement error).\nThese other factors are not included in the model (only age is in the model), so they create variation that can only appear in error term.\nIn the linear regression model the dependent variable \\(Y\\) is related to the independent variable \\(x\\) as\n\\[Y = \\beta_0 + \\beta_1 x + \\epsilon \\ \\] where\n\n\\(\\epsilon\\) is the error term\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope\n\\(\\epsilon\\) is the error term.\n\nThe error term captures the difference between the observed value of the dependent variable and the value predicted by the model. The error term includes the effects of other factors that influence the dependent variable, as well as measurement error.\n\\[Y \\quad= \\quad \\underbrace{\\text{expected value}}_{E(Y) = \\beta_0 + \\beta_1 x} \\quad + \\quad \\underbrace{\\text{random error}}_{\\epsilon}  \\ .\\]\nGraphically the error term is the vertical distance between the observed value of the dependent variable and the value predicted by the model.\n\n\n\n\n\n\n\n\n\nThe error term is also known as the residual. It is the variation that resides (is left over / is left unexplained) after accounting for the relationship between the dependent and independent variables.\n\nExample in R\nLet‚Äôs look at observed values, expected (predicted) values, and residuals (error) in R.\nThe observed values of the response (dependent) variable are already in the dataset:\n\nhead(bp_age_data$Systolic_BP)\n\n[1] 150.3277 170.0801 139.7174 135.4089 151.9041 122.0296\n\n\nTo get the expected values, we need to find the intercept and slope of the linear model. We can do this using the lm() function in R.\n\nlm1 &lt;- lm(Systolic_BP ~ Age, data = bp_age_data)\n\nAnd we can get the intercept and slope using the coef() function:\n\ncoef(lm1)\n\n(Intercept)         Age \n 98.9687381   0.8240678 \n\n\nWe can then use the mutate function from the dplyr package to add the expected values to the dataset:\n\nbp_age_data &lt;- bp_age_data %&gt;%\n  mutate(Expected_BP = coef(lm1)[1] + coef(lm1)[2] * Age)\n\nAnd we can get the residuals by subtracting the expected values from the observed values:\n\nbp_age_data &lt;- bp_age_data %&gt;%\n  mutate(Residuals = Systolic_BP - Expected_BP)\n\n\n\n\n\n\n\nTip\n\n\n\nWe can also get the expected values and residuals directly from the lm object using the fitted() (or predicted()) and residuals() functions:\n\nbp_age_data &lt;- bp_age_data %&gt;%\n  mutate(Expected_BP = fitted(lm1),\n         Residuals = residuals(lm1))\n\n\n\nNow we have a model that gives the expected values (on the regression line) and that gives us a residual. Because the expected value plus the residual equals the observed value, if we use each of the residuals as the error for each respective data point, we end up with a perfect fit to the data. All we are doing is describing the observed data in a different way. This is known as over-fitting. In fact, we have gained very little by fitting the model. We have simply memorized / copied the data!!!\nIn order to avoid this, we need to assume something about the residuals ‚Äì we need to model the residuals. The most common model for the residuals is a normal distribution with mean 0 and constant variance.\n\\[\\epsilon \\sim N(0,\\sigma^2)\\]\nThis is known as the normality assumption. The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.\nThe linear regression model then becomes:\n\\[Y = \\beta_0 + \\beta_1 x + N(0,\\sigma^2) \\ \\]\nwhere \\(\\sigma^2\\) is the variance of the error term. The variance of the error term is the amount of variation in the dependent variable that is not explained by the independent variable. The variance of the error term is also known as the residual variance.\nAn alternate and equivalent formulation is that \\(Y\\) is a random variable that follows a normal distribution with mean \\(\\beta_0 + \\beta_1 x\\) and variance \\(\\sigma^2\\).\n\\[Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\]\nSo, the answer to the question ‚Äúhow do we deal with the error term‚Äù is that we model the error term as normally distributed with mean 0 and constant variance. Put another way, the error term is assumed to be normally distributed with mean 0 and constant variance.\n\n\nBack to blood pressure and age\nThe mathematical model in this case is:\n\\[SystolicBP = \\beta_0 + \\beta_1 \\times Age + \\epsilon\\]\nwhere: SystolicBP is the dependent (response) variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient of Age, Age is the independent (explanatory) variable, \\(\\epsilon\\) is the error term.\nLet‚Äôs ensure we understand this, by thinking about the units of the variables in this model. This can be very useful because it can help us to understand the model better and to check that the model makes sense.\nThink-Pair-Share (#tps-what-units)\n\nWhat are the units of blood pressure?\nWhat are the units of age?\nWhat are the units of the intercept?\nWhat are the units of the coefficient of Age?\nWhat are the units of the error term?",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#is-the-model-good-enough-to-use",
    "href": "3.1-regression-part1.html#is-the-model-good-enough-to-use",
    "title": "Regression Part 1 (L3)",
    "section": "Is the model good enough to use?",
    "text": "Is the model good enough to use?\n\nAll models are wrong, but is ours good enough to be useful?\nAre the assumption of the model justified?\nIt would be very unwise to use the model before we know if it is good enough to use.\nDon‚Äôt jump out of an aeroplane until you know your parachute is good enough!\n\n\nWhat assumptions do we make?\nWe already heard about one. We assume that the residuals follow a \\(N(0,\\sigma^2)\\) distribution (that is, a Gaussian / Normal distrution with mean of zero and variance of \\(\\sigma^2\\)). We make this assumption because it is often well enough met, and it gives great mathematical tractability.\nThis assumption implies that:\n\nThe \\(\\epsilon_i\\) are normally distributed.\n\\(\\epsilon_i\\) has constant variance: \\(Var(\\epsilon_i)=\\sigma^2\\).\nThe \\(\\epsilon_i\\) are independent of each other.\n\n\n\n\n\n\n\nNote\n\n\n\nThese assumptions are about the residuals, not the observed data!!!\n\n\nFurthermore:\n\nwe assumed a linear relationship.\nimplies there are no outliers (implied by (a) above)\n\nLets go through each five assumptions.\n\n\n(a) Normally distributed residuals\nRecall that we make the assumption that the residuals are normally distributed with mean 0 and constant variance:\n\\[\\epsilon \\sim N(0,\\sigma^2)\\]\nHere we are concerned with the first part of this assumption, that the residuals are normally distributed.\nWhat does this mean? How can we check it?\nA normal distribution is symmetric and bell-shaped‚Ä¶\n\n\n\n\n\n\n\n\n\nLets look at the frequency distribution of the residuals of the linear regression of blood pressure and age:\n\n\n\n\n\n\n\n\n\nThe normal distribution assumption (a) seems ok as well.\n\n\n(a) Normally distributed residuals: The QQ-plot\nUsually, not the histogram of the residuals is plotted, but the so-called quantile-quantile (QQ) plot. The quantiles of the observed distribution are plotted against the quantiles of the respective theoretical (normal) distribution:\n\nqqnorm(residuals(bp_age_model))\nqqline(residuals(bp_age_model))\n\n\n\n\n\n\n\n\nIf the points lie approximately on a straight line, the data is fairly normally distributed.\nThis is often ‚Äútested‚Äù by eye, and needs some experience.\nBut what on earth is a quantile???\nImagine we make 21 measures of something, say 21 blood pressures:\n\n\n\n\n\n\n\n\n\nThe median of these is 127.8. The median is the 50% or 0.5 quantile, because half the data points are above it, and half below.\n\nquantile(dd$Blood_Pressure, 0.5)\n\n  50% \n127.8 \n\n\nThe theoretical quantiles come from the normal distribution. The sample quantiles come from the distribution of our residuals.\n\n\n\n\n\n\n\n\n\n\nHow do I know if a QQ-plot looks ‚Äúgood‚Äù?\nThere is no quantitative rule to answer this question. Instead experience is needed. You can gain this experience from simulations. To this end, we can generate the same number of data points of a normally distributed variable and compare this simulated qqplot to our observed one.\nExample: Generate 100 points \\(\\epsilon_i \\sim N(0,1)\\) each time:\n\n\n\n\n\n\n\n\n\nEach of the graphs above has data points that are randomly generated from a normal distribution. In all cases the data points are close to the line. This is what we would expect if the data were normally distributed. The amount of deviation from the line is what we would expect from random variation, and so seeing this amount of variation in a QQ-plot of your model should not be cause for concern.\n\n\n\n(b) Constant error variance (homoscedasticity)\nRecall that we assume the errors are normally distributed with constant variance \\(\\sigma^2\\):\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nHere we‚Äôre concerned with the second part of this assumption, that the variance is constant.That is, variance of the residuals is a constant: \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). And not, for example \\(\\text{Var}(\\epsilon_i) = \\sigma^2 \\cdot x_i\\).\nPut another way, we‚Äôre interested if the size of the residuals tends to show a pattern with the fitted values. By size of the residuals we mean the absolute value of the residuals. In fact, we often look at the square root of the absolute value of the standardized residuals:\n\\[R_i = \\frac{\\epsilon_i}{\\hat{\\sigma}}\\] Where \\(\\hat{\\sigma}\\) is the estimated standard deviation of the residuals:\n\\[\\hat{\\sigma} = \\sqrt{\\frac{1}{n-2} \\sum_{i=1}^n \\epsilon_i^2}\\]\nSo that the full equation of the square root of the standardised residuals is:\n\\[\\sqrt{|R_i|} = \\sqrt{\\left|\\frac{\\epsilon_i}{\\hat{\\sigma}}\\right|}\\]\nTo look to see if the variance of the residuals is constant, we need to see if there is any relationship between the size of the residuals and the fitted values. A commonly used visualistion for this is a plot of the size of the residuals against the fitted values.\nLets first calculated the \\(\\sqrt{|R_i|}\\) values for our blood pressure model:\n\nbp_age_data &lt;- bp_age_data %&gt;%\n  mutate(fitted = predict(bp_age_model),\n         residuals = residuals(bp_age_model),\n         sigma_hat = sqrt(sum(residuals^2)/(n()-2)),\n         R_i = residuals / sigma_hat,\n         sqrt_abs_R_i = sqrt(abs(R_i)))\n\nAnd now visualise the relationship between the fitted values and the size of the residuals:\n\nggplot(bp_age_data, aes(x = fitted, y = sqrt_abs_R_i)) +\n  geom_point() +\n  labs(x = \"Fitted values\", y = expression(sqrt(abs(R[i])))) \n\n\n\n\n\n\n\n\nThis graph is known as the scale-location plot. It is particularly suited to check the assumption of equal variances (homoscedasticity / Homoskedastizit√§t). There should be no trend or pattern.\n\n\n\n\n\n\nTip\n\n\n\nWe can also use the built-in plot function for linear models to create this plot. It is the third plot in the set of model checking plots.\n\nplot(bp_age_model, which=3, add.smooth = FALSE)\n\n\n\n\n\n\n\n\n\n\n\nHow it looks with the variance increasing with the fitted values\nHere‚Äôs a graphical example of how it would look if the variance of the residuals increases with the fitted values.\nFirst here is a graph of the relationship:\n\nset.seed(2)\nx &lt;- runif(100, 1, 10)\ny &lt;- 100 + 5*x + 2*x*rnorm(100,0,1)\nggplot(data.frame(x=x,y=y), aes(x=x,y=y)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\nAnd here the scale-location plot for a linear model of that data:\n\nm &lt;- lm(y~x)\nplot(m,which=3)\n\n\n\n\n\n\n\n\n\n\n\n(c) Independence (residuals are independent of each other)\nWe assume that the residuals (\\(\\epsilon_i\\)) are independent of each other. This means that the value of one residual is not somehow related to the value of another.\nThe dataset about blood pressure we looked at contained 100 observations, each one made from a different person. In such a study design, we could be safe in the assumption that the people are independent, and therefore the assumption that the residuals are independent.\nImagine, however, if we had 100 observations of blood pressure collected from 50 people, because we measured the blood pressure of each person twice. In this case, the residuals would not be independent, because two measures of the blood pressure of the same person are likely to be similar. A person is likely to have a high blood pressure in both measurements, or a low blood pressure in both measurements. This would mean they have a high residual in both measurements, or a low residual in both measurements.\nIn this case, we would need to account for the fact that the residuals are not independent. We would need to use a more complex model, such as a mixed effects model, to account for the fact that the residuals are not independent. We will talk about this again in the last week of this course.\nIn general, you should always think about the study design when you are analysing data. You should always think about whether the residuals are likely to be independent of each other. If they are not, you should think about how you can account for this in your analysis.\nA good way to assess if there could be dependencies in the residuals is to be critical about what is the unit of observation in the data. In the blood pressure example, the unit of observation is the person. Count the number of persons in the study. If there are fewer persons than observations, then at least some people must have been measured at least twice. Repeating measures on the same person is a common way to get dependent residuals.\nSo, to check the assumption of independence, you should:\n\nThink carefully about the study design.\nThink carefully about the unit of observation in the data.\nCompare the number of observations to the number of units of observation.\n\n\n\n(d) Linearity assumption\nThe linearity assumption states that the relationship between the independent variable and the dependent variable is linear. This means that the dependent variable changes by a constant amount for a one-unit change in the independent variable. And that this slope is does not change with the value of the independent variable.\nThe blood pressure data seems to be linear:\n\n\nWarning: `fortify(&lt;lm&gt;)` was deprecated in ggplot2 3.6.0.\n‚Ñπ Please use `broom::augment(&lt;lm&gt;)` instead.\n‚Ñπ The deprecated feature was likely used in the ggplot2 package.\n  Please report the issue at &lt;https://github.com/tidyverse/ggplot2/issues&gt;.\n\n\n\n\n\n\n\n\n\n\nggplot(bp_age_model, aes(x = Age, y=Systolic_BP)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\", se=FALSE)\n\nIn contrast, look at this linear regression through data that appears non-linear:\n\nset.seed(1)\nx &lt;- runif(100, 1, 10)\ny &lt;- 2*x + rnorm(100,0,2) + 0.5*x^2\nm &lt;- lm(y~x)\nggplot(data.frame(x=x,y=y), aes(x=x,y=y)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\", se=FALSE)\n\n\n\n\n\n\n\n\nAnd with the residuals shown as red lines:\n\nggplot(data.frame(x=x,y=y), aes(x=x,y=y)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\", se=FALSE) +\n  geom_segment(aes(xend=x, yend=m$fitted), color=\"red\")\n\n\n\n\n\n\n\n\nAt low values of \\(y\\), the residuals are positive, at intermediate values of \\(y\\) the residuals are negative, and at high values of \\(y\\) the residuals are positive. This pattern in the residuals is a sign that the relationship between \\(x\\) and \\(y\\) is not linear.\nWe can plot the value of the residuals against the \\(y\\) value directly, instead of looking at the pattern in the graph above. This is called a Tukey-Anscombe plot. It is a graph of the residuals versus the fitted \\(y\\) values:\n\nggplot(mapping = aes(x = fitted(m), y = resid(m))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype=\"dashed\", color=\"red\")\n\n\n\n\n\n\n\n\nWe can very clearly see pattern in the residuals in this Tukey-Anscombe plot. The residuals are positive, then negative, then positive, as the fitted \\(y\\) value gets larger.\nWe can also make this Tukey-Anscombe plot using the built-in plot function for linear models in R:\n\nplot(m, which=1)\n\n\n\n\n\n\n\n\nThe red line in the Tukey-Anscombe plot is a loess smooth. It is automatically added to the plot. It is a way of estimating the pattern in the residuals. If the red line is not flat, then there is a pattern in the residuals. However, the loess smooth is not always reliable. It is a good idea to look at the residuals directly, without this smooth.\n\nplot(m, which=1, add.smooth = FALSE)\n\n\n\n\n\n\n\n\nThe data here is simulated to show a very clear pattern in the residuals. In real data, the pattern might not be so clear. But if you suspect you see a pattern in the residuals, it could be a sign that the relationship between the independent and dependent variable is not linear.\nHere is the Tukey-Anscombe plot for the blood pressure data:\n\nplot(bp_age_model, which=1, add.smooth = FALSE)\n\n\n\n\n\n\n\n\nThere is very little evidence of any pattern in the residuals. This data is simulated with a truly linear relationship, so we would not expect to see any pattern in the residuals.\n\n\n(e) No outliers\nAn outlier is a data point that is very different from the other data points. Outliers can have a big effect on the results of a regression analysis. They can pull the line of best fit towards them, and make the line of best fit a poor representation of the data.\nLets again look at the blood pressure versus age data:\n\nggplot(bp_age_model, aes(x = Age, y=Systolic_BP)) +\n  geom_point() +\n  geom_smooth(formula = y ~ x, method=\"lm\", se=FALSE) +\n  xlim(10, 85) +\n  ylim(100, 180)\n\n\n\n\n\n\n\n\nThere are no obvious outliers in this data. The data points are all close to the line of best fit. This is a good sign that the line of best fit is a good representation of the data.\nThink-Pair-Share (#tps-odd-data) Where on this graph would you expect to see particularly influential outliers? Influential in the sense that they would have a large effect on the slope of the line of best fit.\nData points that are far from the mean of the independent variable have a large effect on the value of the slope. These data points have a large leverage. They are data points that are far from the other data points in the \\(x\\) direction.\nWe can think of this with the analogy of a seesaw. The slope of the line of best fit is like the pivot point of a seesaw. Data points that are far from the pivot point have a large effect on the slope. Data points that are close to the pivot point have a small effect on the slope.\nA measure of distance from the pivot point is called the \\(leverage\\) of a data point. In simple regression, the leverage of individual \\(i\\) is defined as\n\\(h_{i} = (1/n) + (x_i-\\overline{x})^2 / SSX\\).\nwhere \\(SSX = \\sum_{i=1}^n (x_i - \\overline{x})^2\\). (Sum of Squares of \\(X\\))\nSo, the leverage of a data point is inversely related to \\(n\\) (the number of data points). The leverage of a data point is also inversely related to the sum of the squares of the \\(x\\) values. The leverage of a data point is directly related to the square of the distance of the \\(x\\) value from the mean of the \\(x\\) values.\nMore intuitively perhaps, the leverage of a data point will be greater when the are fewer other data points. It will also be greater when the distance from the mean value of \\(x\\) is greater.\nGoing back to the analogy of a seesaw, with data points as children on the seesaw, the leverage of a data point is like the distance from the pivot a child sits. But we also have children of different weights. A lighter child will have less effect on the tilt of the seesaw. A heavier one will have a greater effect on the tilt. A heavier child sitting far from the pivot will have a very large effect.\nThink-Pair-Share (#tps-like-weight) What quantity that we already experienced is like the weight of the child?\nThe size of the residuals are like the weight of the child. Data points with large residuals have a large effect on the slope of the line of best fit. Data points with small residuals have a small effect on the slope of the line of best fit.\nSo the overall effect of a data point on the slope of the line of best fit is a combination of the leverage and the residual. This quantity is called the \\(influence\\) of a data point.\nLet‚Äôs add a rather extreme data point to the blood pressure versus age data:\n\n\n\n\n\n\n\n\n\nThis is a bit ridiculous, but it is a good example of an outlier. The data point is far from the other data points. It has a large residual. And it is a long way from the pivot (the middle of the \\(x\\) data) so has large leverage.\nWe can make a histogram of the residuals and see that the outlier has a large residual:\n\n\n\n\n\n\n\n\n\nAnd we can see that the leverage is large.\nThere is a graph that we can look at to see the influence of a data point. This is called a \\(Cook's\\) \\(distance\\) plot. The Cook‚Äôs distance of a data point is a measure of how much the slope of the line of best fit changes when that data point is removed. The Cook‚Äôs distance of a data point is defined as\n\\(D_i = \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 / (p \\times MSE)\\).\nwhere \\(\\hat{y}_j\\) is the predicted value of the dependent variable for data point \\(j\\), \\(\\hat{y}_{j(i)}\\) is the predicted value of the dependent variable for data point \\(j\\) when data point \\(i\\) is removed, \\(p\\) is the number of parameters in the model (2 in this case), \\(MSE\\) is the mean squared error of the model.\n\nplot(bp_age_model_outlier, which=5)\n\n\n\n\n\n\n\n\nBut does it have a large influence on the value of the slope? In the next graph we show the line of best fit with the outlier (blue line) and without the outlier (red line).\n\n\n\n\n\n\n\n\n\nNo, the outlier doesn‚Äôt have much influence on the slope. The outlier has a large leverage. It is far from the pivot. But it does not have such a large effect (influence) on the slope. This is in large part because there are a lot data points (100) that are quite tightly arranged around the regression line.\n\nGraphical illustration of the leverage effect\nData points with \\(x_i\\) values far from the mean have a stronger leverage effect than when \\(x_i\\approx \\overline{x}\\):\n\n\n\n\n\n\n\n\n\nThe outlier (red circle) in the middle plot ‚Äúpulls‚Äù the regression line in its direction and has large influence on the slope. THe outlier (red circle) in the right plot has less influence on the slope because it is closer to the mean of \\(x\\).\n\n\nLeverage plot (Hebelarm-Diagramm)\nIn the leverage plot, (standardized) residuals \\(\\tilde{R_i}\\) are plotted against the leverage \\(H_{ii}\\) :\n\nplot(bp_age_model, which=5, add.smooth = FALSE)\n\n\n\n\n\n\n\n\nCritical ranges are the top and bottom right corners!!\nHere, observations 71, 85, and 87 are labelled as potential outliers.\nSome texts will give a rule of thumb that points with Cook‚Äôs distances greater than 1 should be considered influential, while others claim a reasonable rule of thumb is \\(4 / ( n - p - 1 )\\) where \\(n\\) is the sample size, and \\(p\\) is the number of \\(beta\\) parameters.\nThink‚ÄìPair‚ÄìShare (#a_linear_model_assumptions) Which assumption of linear regression feels most fragile in real biological data? Why?\n\n\n\nThe autoplot() function from the ggfortify package\nSo far we have used the built-in plot() function for linear models to create the model checking plots. Another option is to use the autoplot() function from the ggfortify package. This function creates the same four model checking plots, but doesn‚Äôt require the par(mfrow=c(2,2)) command to arrange the plots in a 2x2 grid, so is slightly more convenient to use. Of course, you do need to ensure that the ggfortify package is installed and loaded.\n\nlibrary(ggfortify)\nautoplot(bp_age_model, smooth.colour = NA)\n\n(At the time of writing, this use of autoplot() causes a warning message to be printed. This is a known issue with the ggfortify package. You can safely ignore this warning message for now.)",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#what-can-go-wrong-during-the-modeling-process",
    "href": "3.1-regression-part1.html#what-can-go-wrong-during-the-modeling-process",
    "title": "Regression Part 1 (L3)",
    "section": "What can go ‚Äúwrong‚Äù during the modeling process?",
    "text": "What can go ‚Äúwrong‚Äù during the modeling process?\nAnswer: a lot of things!\n\nNon-linearity. We assumed a linear relationship between the response and the explanatory variables. But this is not always the case in practice. We might find that the relationship is curved and not well represtented by a straight line.\nNon-normal distribution of residuals. The QQ-plot data might deviate from the straight line so much that we get worried!\nHeteroscadisticity (non-constant variance). We assumed homoscadisticity, but the residuals might show a pattern.\nData point with high influence. We might have a data point that has a large influence on the slope of the line of best fit.\n\n\nWhat to do when things ‚Äúgo wrong‚Äù?\n\nNow: Transform the response and/or explanatory variables.\nNow: Take care of outliers.\nLater in the course: Improve the model, e.g., by adding additional terms or interactions.\nLater in the course: Use another model family (generalized or nonlinear regression model).\n\n\n\nDealing with non-linearity\nHere‚Äôs another example of \\(y\\) and \\(x\\) that are not linearly related:\n\neg_data &lt;- tibble(x = runif(50)) %&gt;%\n  mutate(log_y = log(0.1) + 0.5* log(x) + rnorm(50,0,0.1),\n         y = exp(log_y),\n         log_y = log(y),\n         log_x = log(x),\n         sqrt_y = sqrt(y))\n\np1 &lt;- ggplot(eg_data, aes(x = x, y = y)) +\n  geom_point() + \n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\n\nm1 &lt;- lm(y ~ x, eg_data)\n\np2 &lt;- ggplot(mapping = aes(x = fitted(m1), y = residuals(m1))) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\np1 + p2\n\n\n\n\n\n\n\n\nOne way to deal with this is to transform the response variable \\(Y\\). Here we try two different transformations: \\(\\log_{10}(Y)\\) and \\(\\sqrt{Y}\\).\nSquare root transform of the response variable \\(Y\\):\n\np3 &lt;- ggplot(eg_data, aes(x = x, y = sqrt_y)) +\n  geom_point() + \n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\nm2 &lt;- lm(sqrt_y ~ x, eg_data)\np4 &lt;- ggplot(mapping = aes(x = fitted(m2), y = residuals(m2))) +\n  geom_point() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\nNot great.\nLog transformation of the response variable \\(Y\\):\n\np5 &lt;- ggplot(eg_data, aes(x = x, y = log_y)) +\n  geom_point() + \n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\nm3 &lt;- lm(log_y ~ x, eg_data)\np6 &lt;- ggplot(mapping = aes(x = fitted(m3), y = residuals(m3))) +\n  geom_point() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\n(p1 + p2) / (p3 + p4) / (p5 + p6)\n\n\n\n\n\n\n\n\nNope. Still some evidence of non-linearity.\nWhat about transforming the explanatory variable \\(X\\) as well?\n\np7 &lt;- ggplot(eg_data, aes(x = log_x, y = log_y)) +\n  geom_point() + \n  geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE)\nm4 &lt;- lm(log_y ~ log_x, eg_data)\np8 &lt;- ggplot(mapping = aes(x = fitted(m4), y = residuals(m4))) +\n  geom_point() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth(formula = y ~ x, method = \"loess\")\n\np7 + p8\n\n\n\n\n\n\n\n\nLet‚Äôs look at the four model checking plots for the log-log-transformed data:\n\n\n\n\n\n\n\n\n\nAll looks pretty good except for the scale-location plot, which shows a bit of a pattern. But overall, this looks much better than our original model.\nBut‚Ä¶ how to know which transformation to use‚Ä¶? It‚Äôs a bit of trial and error. But we can use the model checking plots to help us.\nVery very important is that we do this trial and error before we start using the model. E.g., we don‚Äôt want to jump from the aeroplane and then find out that our parachute is not working properly! And then try to fix the parachute while we are falling‚Ä¶.\nLikewise, we must not start using the model and then try to fix it. We need to make sure our model is in good working order before we start using it.\nOne of the traps we could fall into is called ‚Äúp-hacking‚Äù. This is when we try different transformation until we find one that gives us the result we want, for example significant relationship. This is a big no-no in statistics. We need to decide on the model (including any transformations) before we start using it.\n\n\nCommon transformations\nWhich transformations could be considered? There is no simple answer. But some guidelines. E.g. if we see non-linearity and increasing variance with increasing fitted values, then a log transform may improve matter.\nSome common and useful transformations are:\n\nThe log transformation for concentrations and absolute values.\nThe square-root transformation for count data.\nThe arcsin square-root \\(\\arcsin(\\sqrt{\\cdot})\\) transformation for proportions/percentages.\n\nTransformations can also be applied on explanatory variables, as we saw in the example above.\n\n\nOutliers\nWhat do we do when we identify the presence of one or more outliers?\n\nStart by checking the ‚Äúcorrectness‚Äù of the data. Is there a typo or a decimal point that was shifted by mistake? Check both the response and explanatory variables.\nIf not, ask whether the model could be improved. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?\nSometimes, an outlier may be the most interesting observation in a dataset! Was the outlier created by some interesting but different process from the other data points?\nConsider that outliers can also occur just by chance!\nOnly if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.\n\n\n\nRemoving outliers\nIt might seem tempting to remove observations that apparently don‚Äôt fit into the picture. However:\n\nDo this only with greatest care e.g., if an observation has extremely implausible values!\n\nBefore deleting outliers, check points 1-5 above.\nWhen removing outliers, you must mention this in your report.\n\nDuring the course we‚Äôll see many more examples of things going at least a bit wrong. And we‚Äôll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do in the next lesson. But before we wrap up, some good news‚Ä¶",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#its-a-kind-of-magic",
    "href": "3.1-regression-part1.html#its-a-kind-of-magic",
    "title": "Regression Part 1 (L3)",
    "section": "Its a kind of magic‚Ä¶",
    "text": "Its a kind of magic‚Ä¶\nAbove, we learned about linear regression, the equation for it, how to estimate the coefficients, and how to check the assumptions. There was a lot of information, and it might seem a bit overwhelming.\nYou might also be aware that there are quite a few other types of statistical model, such as multiple regression, t-test, ANOVA, two-way ANOVA, and ANCOVA. It could be worrying to think that you need to learn so much new information for each of these types of tests.\nBut this is where the kind-of-magic happens. The good news is that the linear regression model is a special case of what is called a general linear model, or just linear model for short. And that all the tests mentioned above are also types of linear model. So, once you have learned about linear regression, you have learned a lot about linear models, and therefore also a lot about all of these other tests as well.\nMoreover, the same function in R ‚Äòlm‚Äô is used to make all those statistical models Awesome.\n\nSo what is a linear model?\nA linear model is a model where the relationship between the dependent variable and the independent variables is linear. That is, the dependent variable can be expressed as a linear combination of the independent variables. An example of a linear model is:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon\\]\nwhere: \\(y\\) is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients of the independent variables, \\(x_1, x_2, \\ldots, x_p\\) are the independent variables, \\(\\epsilon\\) is the error term.\nIn contrast, a non-linear model is a model where the relationship between the dependent variable and the independent variables is non-linear. An example of a non-linear model is the exponential growth model:\n\\[y = \\beta_0 + \\beta_1 e^{\\beta_2 x} + \\epsilon\\]\nwhere: y is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2\\) are the coefficients of the independent variables, \\(x\\) is the independent variable, \\(\\epsilon\\) is the error term.\nKeep in mind that a model with a quadratic term is still a linear model. For example:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\epsilon\\]\nis still a linear model. We can see this if we substitute \\(x^2\\) with a new variable \\(x_2\\), where \\(x_2 = x^2\\). The model then becomes:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nThis is clearly a linear model.",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#review",
    "href": "3.1-regression-part1.html#review",
    "title": "Regression Part 1 (L3)",
    "section": "Review",
    "text": "Review\nIn this chapter we learned about first steps of regression analysis. Specifically, we learned about to:\n\nMotivation for regression analysis.\nThe simple linear regression model.\nEstimation of the coefficients (intercept and slope) using the least squares method.\nHow to model the errors (residuals) and the assumptions of linear regression.\nHow to check the assumptions of linear regression using model checking plots.\nWhat can go wrong during the modeling process, and how to deal with it.\n\nWith all that in place, we can be sure that when we use a linear regression model, we can trust the results it gives us. In the next chapter, we will start to use linear regression models to answer questions! This involves things such as hypothesis testing, confidence intervals, and prediction. Exciting times ahead!",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#further-reading",
    "href": "3.1-regression-part1.html#further-reading",
    "title": "Regression Part 1 (L3)",
    "section": "Further reading",
    "text": "Further reading\nThere are many good books and online resources about regression models, fitting them, and checking their assumptions. Here are some suggestions for further reading:\n\nFor a more mathematical treatment, including matrix representation, see Chapter 2 of Faraway, J. J. (2016). Linear models with R (2nd ed.). Chapman and Hall/CRC. Link\nThe same book (Faraway) covers Diagnostics and model checking in Chapter 5.\nFor a complementary perspective on regression, see Chapter 7 of The New Statistics with R, by Andy Hector.",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "3.1-regression-part1.html#extras",
    "href": "3.1-regression-part1.html#extras",
    "title": "Regression Part 1 (L3)",
    "section": "Extras",
    "text": "Extras\n\nTests for normality\nWe often want to know if our data or our residuals are not normally distributed. You‚Äôve now done a lot with QQ-plots to examine how normally distributed your data are. Some people also like to have a statistical test for normality. Here I‚Äôll show some tests.\n\nShapiro‚ÄìWilk test on normally distributed data\nThe Shapiro‚ÄìWilk test is frequently used.\n\nNull hypothesis: the data are normally distributed\n\nAlternative hypothesis: the data are not normally distributed\n\nLet‚Äôs make some data and look at its distribution and QQ-plot:\n\nset.seed(145)\nx &lt;- rnorm(100)\nggplot(data.frame(x=x), aes(x=x)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nYes, this is the distribution of data that come from a normal distribution.\nAnd the QQ-plot:\n\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nLooks pretty normal. Let‚Äôs see what the Shapiro‚ÄìWilk test tells us:\n\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99467, p-value = 0.9658\n\n\nThe p-value for the test is large (e.g.¬†p ‚âà 0.99). When we get a large p-value like this, it means we cannot reject the null hypothesis. (A small p-value, say p &lt; 0.05, would cause us to reject the null hypothesis.) Hence, here, we cannot reject the null hypothesis that the data are normally distributed. (Note that, as usual, technically we cannot accept the null hypothesis.)\nSo we can happily continue with any work that assumes normally distributed residuals.\n\n\nShapiro‚ÄìWilk test on non-normally distributed data\nNow let‚Äôs repeat this exercise using data that we know are not normally distributed.\n\nx &lt;- runif(100)\nggplot(data.frame(x=x), aes(x=x)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nThis is clearly not normally distributed data.\nAnd the QQ-plot:\n\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nOK, we don‚Äôt need to do a test to see that this is not normally distributed data. But let‚Äôs do it anyway:\n\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.92167, p-value = 1.728e-05\n\n\nNow the p-value is small, so we reject the null hypothesis that the data are normally distributed. Now we can‚Äôt continue with methods, such as linear models, that assume normally distributed residuals.\n\n\nWhat if we have fewer data points?\nWhat about if we have fewer data points, say 30? And let‚Äôs do the test lots of times and count how often we reject the null hypothesis.\n\nx &lt;- replicate(1000, shapiro.test(runif(30))$p.value)\nsum(x &lt; 0.05)\n\n[1] 371\n\n\nSo we reject the null hypothesis in about 38% of the trials. That is, we don‚Äôt reject the hypothesis that the data are normally distributed in nearly 40% of cases, even though the data came from a uniform distribution!\n\n\nShapiro‚ÄìWilk test on real data\nHow useful this (or any) test of normality is depends on our data.\n\nIf we have a large sample, the test has very high power, and we can easily reject the null hypothesis due to tiny deviations from normality.\nIf we have a small sample, we could easily fail to reject the null hypothesis even when the generating process does not create normally distributed data. The test has low power for small samples.\n\nHere‚Äôs an example of the latter point. We make data that has 5000 numbers sampled from a normal distribution, and then add a bit of non-normality:\n\nx &lt;- rnorm(5000) #+ c(1, 0, 2, 0, 1)\nggplot(data.frame(x=x), aes(x=x)) +\n  geom_histogram(bins = 30)\n\n\n\n\n\n\n\n\nLooks pretty normal.\nAnd the QQ-plot:\n\nqqnorm(x)\nqqline(x)\n\n\n\n\n\n\n\n\nAnd the test.\n\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99966, p-value = 0.5779\n\n\nUh oh! The test rejects the null hypothesis.\n\n\nSo what do we do?\nActually, we might be focusing on the wrong question. Instead of asking how close to normally distributed our data are, we should consider:\n\nhow robust our analysis is to deviations from normality, and\n\nwhich types of deviations from normality actually matter.\n\nGenerally speaking, linear models are robust to deviations from normality.\nThis means:\n\nwe can be relatively happy that the Shapiro‚ÄìWilk test lacks power with small samples, and\n\nwe should be cautious about rejecting normality when sample sizes are large.\n\n\n\n\nMore insight from QQ-plots\nIt is quite subjective to decide if a QQ-plot looks ok, or not. And if the tests are not so useful, what can we do?\nOne option is to make use of simulation to see what kind of variation we can expect in QQ-plots, even when the data really do come from a normal distribution.\nHere is an example of such a plot (the code to make it is below):\n\n\n\n\n\n\n\n\n\nThe red dots are is the qqplot of the observed data. The grey lines are qqplots of normally distributed random data, with characteristics equal to those assumed by the model. Each grey line is a separate draw of random numbers. So the grey lines give an idea of what kind of spread one can expect even when the data really do come from a normal distribution. If the red points lie within the grey region, we‚Äôre good to go!\nSome of the upper red dots are at the edge of grey region, suggesting there might be minor deviations from normality. Hence, even with some help given by showing what we can expect to see, we still have to make an arbitrary / rather subjective decision about when deviations are bad enough to warrant doing something, and when they are not so bad, and we can continue with the assumption of normality. Much of data analysis is more subjective that we might expect given that it is quantitative!\nHere is the code to make the graph, using some example data:\n\n## function adapted from\n## http://www.nate-miller.org/1/post/2013/03/how-normal-is-normal-a-q-q-plot-approach.html\nqqfunc &lt;- function(model, num.reps) {\n  \n  N &lt;- length(resid(model))\n  sigma &lt;- summary(model)$sigma\n  \n  x &lt;- rnorm(N, 0, sigma)\n  xx &lt;- qqnorm(x, plot.it=F)\n  xx$y &lt;- xx$y[order(xx$x)]\n  xx$x &lt;- xx$x[order(xx$x)]\n  plot(xx$x, scale(xx$y), pch=19, col=\"#00000011\", type=\"l\",\n       xlab=\"Theoretical quantiles\",\n       ylab=\"Standardised residuals\")\n  ##qqline(x)\n  \n  for(i in 2:num.reps) {\n    \n    x &lt;- rnorm(N, 0, sigma)\n    xx &lt;- qqnorm(x, plot.it=F)\n    xx$y &lt;- xx$y[order(xx$x)]\n    xx$x &lt;- xx$x[order(xx$x)]\n    points(xx$x, scale(xx$y), pch=19, col=\"#00000011\", type=\"l\")\n    \n  }\n  \n  xx &lt;- qqnorm(m1$residuals, plot.it=F)\n  points(xx$x, scale(xx$y), col=\"red\", pch=19)\n  \n}\n\n## load some useful packages\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(ggfortify)\n\n## load the data\n## Here I'm loading it direct from online where the dataset are stored\nfhc &lt;- suppressMessages(readr::read_csv(\"https://raw.githubusercontent.com/opetchey/BIO144_Practicals_WAs/refs/heads/main/assets/datasets/financing_healthcare.csv\",show_col_types = FALSE))\n\n## tell me which countries are in the dataset\n#unique(fhc$country)\n\n## tell me which years are in the dataset\n#unique(fhc$year)\n\n## give me the rows in which both child mortality and health care expenditure are not NA\n#filter(fhc, !is.na(child_mort) & !is.na(health_exp_total))\n\n## Wrange the data\nfhc1 &lt;- fhc %&gt;%\n  filter(year==2013) %&gt;% ## only data fro 2013 please\n  select(year:continent, health_exp_total, child_mort, life_expectancy) %&gt;% ## only these columns please\n  drop_na() ## drop rows with any NAs\n\n## From last weeks work we know we need to log transform the data\nfhc1 &lt;- mutate(fhc1,\n               log10_health_exp_total=log10(health_exp_total),\n               log10_child_mort=log10(child_mort))\n\n## plot the relationship between health care expenditure and child mortality\n#ggplot(data=fhc1, aes(x=health_exp_total, y=child_mort)) + geom_point()\n\n## fit the linear model of the log transformed data and assign it to object named m1\nm1 &lt;- lm(log10_child_mort ~ log10_health_exp_total, data=fhc1)\n\n## Here we run the function that makes the qqplot with some random samples from a normal distribution\nqqfunc(m1, 100)\nabline(0,1)",
    "crumbs": [
      "Regression Part 1 (L3)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html",
    "href": "4.1-regression-part2.html",
    "title": "Regression Part 2 (L4)",
    "section": "",
    "text": "Introduction\nThis chapter builds on the previous chapter on simple linear regression. There we learned how to fit a regression model to data, and how to check if the assumptions of the regression model are met. In this chapter we will learn how to interpret the results of a regression model, and how to use the model to make inferences about the relationship between the dependent and independent variables.\nNow that we have a satisfactory model, we can start to use it. In the following material, you will learn:",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#introduction",
    "href": "4.1-regression-part2.html#introduction",
    "title": "Regression Part 2 (L4)",
    "section": "",
    "text": "How to measure how good is the regression (correlation and \\(R^2\\)).\nHow to test if the parameter estimates are compatible with some specific value (\\(t\\)-test).\nHow to find the range of parameters values are compatible with the data (confidence intervals).\nHow to find the regression lines compatible with the data (confidence band).\nHow to calculate plausible values of newly collected data (prediction band).",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#how-good-is-the-regression-model",
    "href": "4.1-regression-part2.html#how-good-is-the-regression-model",
    "title": "Regression Part 2 (L4)",
    "section": "How good is the regression model?",
    "text": "How good is the regression model?\nWhat would a good regression model look like? What would a bad one look like? One could say that a good regression model is one that explains the dependent variable well. But what could we mean by ‚Äúexplains the data well‚Äù?\nTake these two examples.\n\n\n\n\n\n\n\n\n\nThink-Pair-Share (#tps-better-model) In which of these two would you say the model is better, and in which is it worse?\nThe first model seems to fit the data well, while the second one does not. But how can we quantify this?\nLet‚Äôs say that we will measure the goodness of the model by the amount of variability of the dependent variable that is explained by the independent variable. To do this we need to do the following:\n\nMeasure the total variability of the dependent variable (total sum of squares, \\(SST\\)).\nMeasure the amount of variability of the dependent variable that is explained by the independent variable (model sum of squares, \\(SSM\\)).\nMeasure the variability of the dependent variable that is not explained by the independent variable (error sum of squares, \\(SSE\\)).\nCalculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\), pronounced as ‚Äúr-squared‚Äù) (also known as the coefficient of determination) (\\(R^2\\) = \\(SSM/SST\\)).\n\nImportantly, note that we will calculate \\(SSM\\) and \\(SSE\\) so that they sum up to \\(SST\\). I.e., \\(SST = SSM + SSE\\). That is, the total variability is the sum of what is explained by the model and what remains unexplained.\nLet‚Äôs take each in turn:\n\n\\(SST\\)\n1. The total variability of the dependent variable is the sum of the squared differences between the dependent variable and its mean. This is called the total sum of squares (\\(SST\\)).\n\\[SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nwhere: \\(y_i\\) is the dependent variable, \\(\\bar{y}\\) is the mean of the dependent variable, \\(n\\) is the number of observations.\nNote that sometimes \\(SST\\) is referred to as \\(SSY\\) (sum of squares of \\(y\\)).\nGraphically, this is the sum of the square of the blue residuals as shown in the following graph, where the horizontal dashed line is at the value of the mean of the dependent variable.\n\n\n\n\n\n\n\n\n\nWe can calculate this in R as follows:\n\nSST &lt;- sum((y1 - mean(y1))^2)\n\n\n\nSSM and SSE\nNow the next two steps, that is getting the model sum of squares (SSM) and the error sum of squares (SSE) are a bit more complicated. To do this we need to fit a regression model to the data. Let‚Äôs see this graphically, and divide the data into the explained and unexplained parts.\nMake a graph with vertical lines connecting the data to the mean of the data, but with each line two parts, one from the mean to the data, and one from the data to the predicted value.\n\n\n\n\n\n\n\n\n\nIn this graph, the square of the length of the green lines is the model sum of squares (\\(SST\\)). The square of the length of the red lines is the error sum of squares (\\(SSE\\)).\nIn a better model the length of the green lines will be longer (the square of these gives the \\(SMM\\), the variability explained by the model). And the length of the red lines will be shorter (the square of these gives the \\(SSE\\), the variability not explained by the model).\n\n\n\\(SSM\\)\nNext we will do the second step, that is calculate the model sum of squares (\\(SSM\\)).\n2. The amount of variability of the dependent variable that is explained by the independent variable is called the model sum of squares (\\(SSM\\)).\nThis is the difference between the predicted value of the dependent variable and the mean of the dependent variable, squared and summed:\n\\[SSE = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nwhere: \\(\\hat{y}_i\\) is the predicted value of the dependent variable,\nIn R, we calculate this as follows:\n\nm1 &lt;- lm(y1 ~ x)\ny1_predicted &lt;- predict(m1)\nSSM &lt;- sum((y1_predicted - mean(y1))^2)\nSSM\n\n[1] 371.237\n\n\n\n\n\\(SSE\\)\nThird, we calculate the error sum of squares (\\(SSE\\)) with either of two methods. We could calculate it as the sum of the squared residuals, or as the difference between the total sum of squares and the model sum of squares:\n\\[SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = SST - SSM\\] Let‚Äôs calculate this in R uses both approaches:\n\nSSE &lt;- sum((y1 - y1_predicted)^2)\nSSE\n\n[1] 6.323436\n\n\nOr‚Ä¶\n\nSSE &lt;- SST - SSM\nSSE\n\n[1] 6.323436\n\n\n\n\n\\(R^2\\)\nFinally, we calculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\)):\n\\[R^2 = \\frac{SSM}{SST}\\]\n\nR.squared &lt;- SSM/SST\nR.squared\n\n[1] 0.9832519\n\n\n\n\nIs my R squared good?\nWhat value of \\(R^2\\) is considered good? In ecological research, \\(R^2\\) values are often low (less than 0.3), because ecological systems are complex and many factors influence the dependent variable. However, in other fields, such as physiology, \\(R^2\\) values are often higher. Therefore, the answer of what values of \\(R^2\\) are good depends on the field of research.\nHere are the four examples and their r-squared.\n\n\n\n\n\n\n\n\n\nThink-Pair-Share (#tps-what-minimised) What is minimised when we fit a regression model? And therefore what is maximised?",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "href": "4.1-regression-part2.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "title": "Regression Part 2 (L4)",
    "section": "How unlikey is the observed data given the null hypothesis?",
    "text": "How unlikey is the observed data given the null hypothesis?\nWe often hear this expressed as ‚Äúis the relationship significant?‚Äù And maybe we heard that the relationship is significant if the p-value is less than 0.05. But what does all this actually mean? In this section we‚Äôll figure all this out. The first step to is to formulate a null hypothesis.\nWhat is a meaningful null hypothesis for a regression model?\nAs mentioned, often we‚Äôre interested in whether there is a relationship between the dependent (response) and independent (explanatory) variable. Therefore, the null hypothesis is that there is no relationship between the dependent and independent variable. This means that the null hypothesis is that the slope of the regression line is zero.\nRecall the regression model: \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\nThink-Pair-Share (#tps-null-hypothesis) Write down the null hypothesis of no relationship between \\(x\\) and \\(y\\) in terms of a \\(\\beta\\) parameter.\nThe null hypothesis is that the slope of the regression line is zero: \\[H_0: \\beta_1 = 0\\]\nWhat is the alternative hypothesis?\n\\[H_1: \\beta_1 \\neq 0\\]\nSo, how do we test the null hypothesis? More precisely, we are going to calculate the probability of observing the data we have, given that the null hypothesis is true. If this probability is very low, then we can reject the null hypothesis.\nDoes that make sense? Does it seem a bit convoluted? It is a bit!!!\nBut this is how hypothesis testing works. We never prove the null hypothesis is true. Instead, we calculate the probability of observing our data given that the null hypothesis is true. If this probability is very low, we reject the null hypothesis.\nTo make the calculation we can use the fact that the slope of the regression line is an estimate of the true slope. This estimate has uncertainty associated with it. We can use this uncertainty to calculate the probability of observing the data we have, given the null hypothesis is true.\nWe can see that the slope estimate (the \\(x\\) row) has uncertainty by looking at the regression output:\n\nsummary(m1)$coefficients[1:2, 1:2]\n\n              Estimate Std. Error\n(Intercept) -0.7638353  0.6652233\nx            2.1161160  0.1072104\n\n\nThe estimate is the mean of the distribution of the parameter (slope) and the standard error is a measure of the uncertainty of the estimate.\nThe standard error is calculated as:\n\\[\\sigma^{(\\beta_1)} = \\sqrt{ \\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}}\\]\nWhere \\(\\hat\\sigma^2\\) is the expected residual variance of the model. This is calculated as:\n\\[\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat y_i)^2}{n-2}\\]\nWhere \\(\\hat y_i\\) is the predicted value of \\(y_i\\) from the regression model.\nOK, let‚Äôs take a look at this intuitively. We have the estimate of the slope and the standard error of the estimate.\nHere is a graph of the value of the slope estimate versus the standard error of the estimate:\n\n\n\n\n\n\n\n\n\nThink-Pair-Share (#tps-chance-area) In what areas of the graph is the slope estimate more likely to have been observed by chance? And what regions is it less likely to have been observed by chance? Think about this before you look at the end of this chapter for an answer.\nWhen the slope estimate is larger, it is less likely to have been observed by chance. And when the standard error is larger, it is more likely to have been observed by chance. How can we put these together into a single measure?\nIf we divide the slope estimate by the standard error, we get a measure of how many standard errors the slope estimate is from the null hypothesis slope of zero. This is the \\(t\\)-statistic:\n\\[t = \\frac{\\hat\\beta_1 - \\beta_{1,H_0}}{\\sigma^{(\\beta_1)}}\\]\nWhere \\(\\beta_{1,H_0}\\) is the null hypothesis value of the slope, usually zero, so that\n\\[t = \\frac{\\hat\\beta_1}{\\sigma^{(\\beta_1)}}\\]\nThe \\(t\\)-statistic is a measure of how many standard errors the slope estimate is from the null hypothesis value of the slope. The larger the \\(t\\)-statistic, the less likely the slope estimate was observed by chance.\nHow can we transform the value of a \\(t\\)-statistic into a p-value? We can use the \\(t\\)-distribution, which quantifies the probability of observing a value of the \\(t\\)-statistic under the null hypothesis.\nBut what is the \\(t\\)-distribution? It is a distribution of the \\(t\\)-statistic under the null hypothesis. It is a bell-shaped distribution that is centered on zero. The shape of the distribution is determined by the degrees of freedom, which is \\(n-2\\) for a simple linear regression model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBy the way, it is named the \\(t\\)-distribution by it‚Äôs developer, William Sealy Gosset, who worked for the Guinness brewery in Dublin, Ireland. In his 1908 paper, Gosset introduced the \\(t\\)-distribution but he didn‚Äôt explicitly explain his choice of the letter \\(t\\). The choice of the letter \\(t\\) could be to indicate ‚ÄúTest‚Äù, as the \\(t\\)-distribution was developed specifically for hypothesis testing.\n\n\nNow, recall that the p-value is the probability of observing the value of the test statistic (so here the \\(t\\)-statistic) at least as extreme as the one we have, given the null hypothesis is true. We can calculate this probability by integrating the \\(t\\)-distribution from the observed \\(t\\)-statistic to the tails of the distribution.\nHere is a graph of the \\(t\\)-distribution with 100 degrees of freedom with the tails of the distribution shaded so that the area of the shaded region is 0.05 (i.e., 5% of the total area).\n\n\n\n\n\n\n\n\n\nAnd here‚Äôs a graph of the \\(t\\)-distribution with 1000 degrees of freedom (blue line) and the normal distribution (green dashed line):\n\n\n\n\n\n\n\n\n\nSo, with a large number of observations, the \\(t\\)-distribution approaches the normal distribution. For the normal distribution, the 95% area is between -1.96 and 1.96.\n\n## x value for 95% area of normal distribution\nx_value &lt;- qnorm(0.975)\nx_value\n\n[1] 1.959964\n\n\nqnorm is a function that calculates the \\(x\\) value for a given quantile (probability) of the normal distribution. In simpler terms, it finds the value \\(x\\) at which the area under the normal curve (up to \\(x\\)) equals the given probability \\(p\\) (0.975 in the example immediately above here).\nLet‚Äôs go back to the age - blood pressure data and calculate the p-value for the slope estimate.\nRead in the data:\n\nbp_data &lt;- read_csv(here::here(\"datasets/Simulated_Blood_Pressure_and_Age_Data.csv\"))\n\nRows: 100 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (2): Age, Systolic_BP\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nMake a graph:\n\nggplot(bp_data, aes(x = Age, y = Systolic_BP)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x)\n\n\n\n\n\n\n\n\nHere‚Äôs the model:\n\nmod1 &lt;- lm(Systolic_BP ~ Age, data = bp_data)\n\nHere we calculate the \\(t\\)-statistic for the slope estimate:\n\nt_stat &lt;- mod1$coefficients[2] / summary(mod1)$coefficients[2, 2]\n\nAnd here we calculate the one-tailed and two-tailed \\(p\\)-values:\n\none_tailed_p_value &lt;- pt(-abs(t_stat), df = nrow(bp_data) - 2)\ntwo_tailed_p_value &lt;- 2 * one_tailed_p_value\none_tailed_p_value\n\n         Age \n3.746958e-51 \n\ntwo_tailed_p_value\n\n         Age \n7.493917e-51 \n\n\nWe can get the \\(p\\)-value directly from the summary function:\n\nsummary(mod1)$coefficients[2, ]\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 \n\n\nConclusion: there is very strong evidence that the blood pressure is associated with age, because the \\(p\\)-value is extremely small (thus it is very unlikely that the observed slope value or a large one would be seen if there was really no association). Thus, we can reject the null hypothesis that the slope is zero.\nThis basically answers question 1: ‚ÄúAre the parameters compatible with some specific value?‚Äù\n\nRecap: Formal definition of the \\(p\\)-value\nThe formal definition of \\(p\\)-value is the probability to observe a data summary (e.g., an average or a slope) that is at least as extreme as the one observed, given that the null hypothesis is correct.\nExample (normal distribution): Assume that we calculated that \\(t\\)-value = -1.96\n\\(\\Rightarrow\\) \\(Pr(|t|\\geq 1.96)=0.05\\) (two-tailed) and \\(Pr(t\\leq-1.96)=0.025\\) (one-tailed).\nAnd here is a graph showing this:\n\n\n\n\n\n\n\n\n\n\n\nA cautionary note on the use of \\(p\\)-values\nMaybe you have seen that in statistical testing, often the criterion \\(p\\leq 0.05\\) is used to test whether \\(H_0\\) should be rejected. This is often done in a black-or-white manner. However, we will put a lot of attention to a more reasonable and cautionary interpretation of \\(p\\)-values in this course!",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#how-strong-is-the-relationship",
    "href": "4.1-regression-part2.html#how-strong-is-the-relationship",
    "title": "Regression Part 2 (L4)",
    "section": "How strong is the relationship?",
    "text": "How strong is the relationship?\nThe actual value of the slope has practical meaning. The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit. The slope is one measure of the strength of the relationship between the two variables.\nWe can ask what values of a parameter estimate are compatible with the data (confidence intervals)? To answer this question, we can determine the confidence intervals of the regression parameters.\nThe confidence interval of a parameter estimate is defined as the interval that contains the true parameter value with a certain probability. So the 95% confidence interval of the slope is the interval that contains the true slope with a probability of 95%.\nWe can then imagine two cases. The 95% confidence interval of the slope includes 0:\n\n\nWarning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.\n‚Ñπ Please use the `orientation` argument of `geom_errorbar()` instead.\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\nOr where the confidence interval does not include zero:\n\n\n`height` was translated to `width`.\n\n\n\n\n\n\n\n\n\nHow do we calculate the lower and upper limits of the 95% confidence interval of the slope?\nRecall that the \\(t\\)-value for a null hypothesis of slope of zero is defined as:\n\\[t = \\frac{\\hat\\beta_1}{\\hat\\sigma^{(\\beta_1)}}\\]\nThe first step is to calculate the \\(t\\)-value that corresponds to a p-value of 0.05. This is the \\(t\\)-value that corresponds to the 97.5% quantile of the \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\n\\(t_{0.975} = t_{0.025} = 1.96\\), for large \\(n\\).\nThe 95% confidence interval of the slope is then given by:\n\\[\\hat\\beta_1 \\pm t_{0.975} \\cdot \\hat\\sigma^{(\\beta_1)}\\]\nIn our blood pressure example the estimated slope is 0.8240678 and the standard error of the slope is 0.0277096. We can calculate the 95% confidence interval of the slope in R as follows:\n\nn &lt;- 100\nt_0975 &lt;- qt(0.975, df = n - 2)\nhalf_interval &lt;- t_0975 * summary(mod1)$coef[2,2]\nlower_limit &lt;- coef(mod1)[2] - half_interval\nupper_limit &lt;- coef(mod1)[2] + half_interval\nci_slope &lt;- c(lower_limit, upper_limit)\nslope &lt;- coef(mod1)[2]\nslope\n\n      Age \n0.8240678 \n\nci_slope\n\n      Age       Age \n0.7690791 0.8790565 \n\n\nOr, using the confint function:\n\n## 95% confidence interval of the slope of mod1\nci_slope_2 &lt;- confint(mod1, level=c(0.95))[2,]\nci_slope_2\n\n    2.5 %    97.5 % \n0.7690791 0.8790565 \n\n\nOr we can do it using values from the coefficients table:\n\ncoefs &lt;- summary(mod1)$coef\nbeta &lt;- coefs[2,1]\nsdbeta &lt;- coefs[2,2] \nbeta + c(-1,1) * qt(0.975,241) * sdbeta \n\n[1] 0.7694840 0.8786516\n\n\nInterpretation: for an increase in the age by one year, roughly 0.82 mmHg increase in blood pressure is expected, and all true values for \\(\\beta_1\\) between 0.77 and 0.88 are compatible with the observed data.",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#confidence-and-prediction-bands",
    "href": "4.1-regression-part2.html#confidence-and-prediction-bands",
    "title": "Regression Part 2 (L4)",
    "section": "Confidence and Prediction Bands",
    "text": "Confidence and Prediction Bands\n\nRemember: If another sample from the same population was taken, the regression line would look slightly different.\nThere are two questions to be asked:\n\n\nWhich other regression lines are compatible with the observed data? This leads to the confidence band.\nWhere do future observations (\\(y\\)) with a given \\(x\\) coordinate lie? This leads to the prediction band.\n\nNote: The prediction band is much broader than the confidence band.",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#calculation-of-the-confidence-band",
    "href": "4.1-regression-part2.html#calculation-of-the-confidence-band",
    "title": "Regression Part 2 (L4)",
    "section": "Calculation of the confidence band",
    "text": "Calculation of the confidence band\nGiven a fixed value of \\(x\\), say \\(x_0\\). The question is:\nWhere does \\(\\hat y_0 = \\hat\\beta_0 + \\hat\\beta_1 x_0\\) lie with a certain confidence (i.e., 95%)?\nThis question is not trivial, because both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are estimates from the data and contain uncertainty.\nThe details of the calculation are given in Stahel 2.4b.\nPlotting the confidence interval around all \\(\\hat y_0\\) values one obtains the confidence band or confidence band for the expected values of \\(y\\).\nNote: For the confidence band, only the uncertainty in the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) matters.\nHere is the confidence band for the blood pressure data:\n\n\n\n\n\n\n\n\n\nVery narrow confidence bands indicate that the estimates are very precise. In this case the estimated intercept and slope are precise because the sample size is large and the data points are close to the regression line.",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#calculations-of-the-prediction-band",
    "href": "4.1-regression-part2.html#calculations-of-the-prediction-band",
    "title": "Regression Part 2 (L4)",
    "section": "Calculations of the prediction band",
    "text": "Calculations of the prediction band\nWe can easily predicted an expected value of \\(y\\) for a given \\(x\\) value. But we can also ask w where does a future observation lie with a certain confidence (i.e., 95%)?\nTo answer this question, we have to consider not only the uncertainty in the predicted value caused by uncertainty in the parameter estimates \\(\\hat y_0 =  \\hat\\beta_0 + \\hat\\beta_1 x_0\\), but also the error term \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)}.\nThis is the reason why the prediction band is wider than the confidence band.\nHere‚Äôs a graph showing the prediction band for the blood pressure data:\n\n\n\n\n\n\n\n\n\nAnother way to think of the 95% confidence band is that it is where we would expect 95% of the regression lines to lie if we were to collect many samples from the same population. The 95% prediction band is where we would expect 95% of the future observations to lie.\nThink‚ÄìPair‚ÄìShare (#a_prediction_vs_estimation) Which interval answers: ‚ÄúWhat is the mean response?‚Äù Which answers: ‚ÄúWhat might a new observation look like?‚Äù",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#review",
    "href": "4.1-regression-part2.html#review",
    "title": "Regression Part 2 (L4)",
    "section": "Review",
    "text": "Review\nThat is regression done (at least for our current purposes). Here is a summary of what we have covered:\nPrevious chapter:\n\nWhy use (linear) regression?\nFitting the line (= parameter estimation)\nIs linear regression good enough model to use?\nWhat to do when things go wrong?\nTransformation of variables/the response.\nHandling of outliers.\n\nThis chapter:\n\nSums of squares: \\(SST\\), \\(SSM\\), \\(SSE\\)\n\\(R^2\\) as a measure of goodness of fit\nHypothesis testing in regression\nNull and alternative hypotheses\nt-statistic and p-values\nTests and confidence intervals\nConfidence and prediction bands",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#further-reading",
    "href": "4.1-regression-part2.html#further-reading",
    "title": "Regression Part 2 (L4)",
    "section": "Further reading",
    "text": "Further reading\nAgain, there are many good books and online resources about regression models. The same ones as last week are recommended, also for interpreting results of regression models.\n\nFaraway, J. J. (2016). Linear models with R (2nd ed.). Chapman and Hall/CRC. Link.\nChapter 7 of The New Statistics with R, by Andy Hector.",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "4.1-regression-part2.html#extras",
    "href": "4.1-regression-part2.html#extras",
    "title": "Regression Part 2 (L4)",
    "section": "Extras",
    "text": "Extras\n\nRandomisation test for the slope of a regression line\nLet‚Äôs use randomisation as another method to understand how likely we are to observe the data we have, given the null hypothesis is true.\nIf the null hypothesis is true, we expect no relationship between \\(x\\) and \\(y\\). Therefore, we can shuffle the \\(y\\) values and fit a regression model to the shuffled data. We can repeat this many times and calculate the slope of the regression line each time. This will give us a distribution of slopes we would expect to observe if the null hypothesis is true.\nFirst, we‚Äôll make some data and get the slope of the regression line. Here is the observed slope and relationship:\n\nset.seed(123)\nn &lt;- 100\nx &lt;- 1:n\ny &lt;- 0.1*x + rnorm(n, 0, 10)\nm &lt;- lm(y ~ x)\ncoef(m)[2]\n\n        x \n0.1251108 \n\n\n\nggplot(data.frame(x = x, y = y), aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, formula = y ~ x) \n\n\n\n\n\n\n\n\nNow we‚Äôll use randomisation to test the null hypothesis. We can create lots of examples where the relationship is expected to have a slope of zero by shuffling randomly the \\(y\\) values. Here are 20:\n\npar(mfrow = c(5, 4))\nfor (i in 1:20) {\n  y_rand &lt;- sample(y)\n  m_rand &lt;- lm(y_rand ~ x)\n  plot(x, y_rand, main = paste(\"Slope = \", round(coef(m_rand)[2], 2)))\n  abline(m_rand)\n}\n\n\n\n\n\n\n\n\nNow let‚Äôs create 19 and put the real one in there somewhere random. Here‚Äôs a case where the real data has a quite strong relationship:\n\n\n\n\n\n\n\n\n\nWe can confidently find the real data amount the shuffled data. But what if the relationship is weaker?\n\n\n\n\n\n\n\n\n\nNow its less clear which is the real data. We can use this idea to test the null hypothesis.\nWe do the same procedure of but instead of just looking at the graphs, we calculate the slope of the regression line each time. This gives us a distribution of slopes we would expect to observe if the null hypothesis is true. We can then see where the observed slope lies in this distribution of null hypothesis slopes.\n\n# repeat 10000 time a randomisation test\ny &lt;- 0.15*x + rnorm(n, 0, 15)\nrand_slopes &lt;- replicate(10000, {\n  y_rand &lt;- sample(y)\n  m_rand &lt;- lm(y_rand ~ x)\n  coef(m_rand)[2]\n})\n\nggplot(data.frame(slopes = rand_slopes), aes(x = slopes)) +\n  geom_histogram(bins = 50) +\n  geom_vline(xintercept = coef(m)[2], color = \"red\")\n\n\n\n\n\n\n\n\nWe can now calculate the probability of observing the data we have, given the null hypothesis is true.\n\np_value &lt;- mean(abs(rand_slopes) &gt;= abs(coef(m)[2]))\np_value\n\n[1] 0.0172\n\n\n\n\nVisualising p-values for regression slopes",
    "crumbs": [
      "Regression Part 2 (L4)"
    ]
  },
  {
    "objectID": "5.1-anova.html",
    "href": "5.1-anova.html",
    "title": "Analysis of variance (L5)",
    "section": "",
    "text": "Introduction\nThe previous two chapters were about linear regression. Linear regression is a type of linear model ‚Äì recall that in R we used the function lm() to make the regression model. In this chapter we will look at a different type of linear model: analysis of variance (ANOVA).\nRecall that linear regression is a linear model with one continuous explanatory (independent) variable. A continuous explanatory variable is a variable in which values can take any value within a range (e.g., height, weight, temperature).\nIn contrast, analysis of variance (ANOVA) is a linear model with one or more categorical explanatory variables. We will first look at a one-way ANOVA, which has one categorical explanatory variable. Later (in a following chapter) we will look at two-way ANOVA, which has two categorical explanatory variables.\nWhat is a categorical variable? A categorical explanatory variable is a variable that contains values that fall into distinct groups or categories. For example, habitat type (e.g., forest, grassland, wetland), treatment group (e.g., control, low dose, high dose), or diet type (e.g., vegetarian, vegan, omnivore).\nThis means that each observation belongs to one of a limited number of categories or groups. For example, in a study of how blood pressure varies with diet type, diet type is a categorical variable with several levels (e.g., vegetarian, vegan, omnivore). A person can only belong to one diet type category.\nHere are the first several rows of a dataset that contains blood pressure measurements for individuals following different diet types:\nReading in the dataset:\nbp_data_diet &lt;- read_csv(\"datasets/bp_data_diet.csv\")\n\nRows: 50 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): diet, person_ID\ndbl (1): bp\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nbp_data_diet &lt;- select(bp_data_diet, bp, diet, person_ID)\nhead(bp_data_diet)\n\n# A tibble: 6 √ó 3\n     bp diet          person_ID\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;    \n1   120 meat heavy    person_1 \n2    89 vegan         person_2 \n3    86 vegetarian    person_3 \n4   116 meat heavy    person_4 \n5   115 Mediterranean person_5 \n6   134 meat heavy    person_6\nThere are three variables: - bp: blood pressure (continuous response variable) - diet: diet type (categorical explanatory variable) - person_ID: unique identifier for each individual (not used in the analysis)\nNote that the diet variable is of type &lt;chr&gt; which is short for character. In R, categorical variables are often represented as factors.\nAs usual, its a really good idea to visualise the data in as close to ‚Äúraw‚Äù form as possible before doing any analysis. We‚Äôll make a scatterplot of blood pressure versus diet type.\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_jitter(width = 0.1, height = 0, alpha = 0.7) +\n  labs(title = \"Blood Pressure by Diet Type\",\n       x = \"Diet Type\",\n       y = \"Blood Pressure (mm Hg)\")\nLooking at this graph it certainly looks like diet type has an effect on blood pressure. But is this effect statistically significant? In other words, are the differences in mean blood pressure between diet types larger than we would expect due to random variation alone?\nAnalysis of variance (ANOVA) is a statistical method that can help us answer this question, and also others.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#introduction",
    "href": "5.1-anova.html#introduction",
    "title": "Analysis of variance (L5)",
    "section": "",
    "text": "Tip\n\n\n\nWe just used geom_jitter() instead of geom_point() to make a scatterplot. This is because geom_jitter() adds a small amount of random noise to the points, which helps to prevent overplotting when multiple points have the same value (which is common when the x-axis is categorical).\nWhen we use geom_jitter(), we can specify the amount of noise to add in the x and y directions using the width and height arguments, respectively. We must be very careful to not add noise to the y direction if we care about the actual y values (e.g., blood pressure). In this case, we only added noise in the x direction by setting height = 0 to separate the points just enough, but not so much that we could get confused about which of the diets they belong to.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#how-does-it-look-like-in-r",
    "href": "5.1-anova.html#how-does-it-look-like-in-r",
    "title": "Analysis of variance (L5)",
    "section": "How does it look like in R?",
    "text": "How does it look like in R?\nWe can fit a one-way ANOVA model in R using the same lm() function that we used for linear regression. The only difference is that the explanatory variable is categorical.\n\nanova_model &lt;- lm(bp ~ diet, data = bp_data_diet)\n\nThen instead of using summary() to look at the results, we use the anova() function.\n\nanova(anova_model)\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis is an ANOVA table. It shows us the sources of variation in the data, along with their associated degrees of freedom (Df), sum of squares (Sum Sq), mean square (Mean Sq), F value, and p-value (Pr(&gt;F)) associate with a getting a F value the same as or greater than the observed F value if the null hypothesis were true.\nThe challenge now is to understand what all of these values mean! Let‚Äôs take it step by step.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#what-is-anova",
    "href": "5.1-anova.html#what-is-anova",
    "title": "Analysis of variance (L5)",
    "section": "What is ANOVA?",
    "text": "What is ANOVA?\nAnalysis of variance is a method to compare whether the observations (e.g., of blood pressure) differ according to some grouping (e.g., diet) that the subjects (e.g., people) belong to.\nWe already know a lot about analysing variance: we compared the total sum of squares (SST), model sum of squares (SSM) and the residual sum of squares (SSE) in the context of linear regression. We used these to calculated the \\(R^2\\) value. The \\(R^2\\) value tells us how much of the total variance in the response variable (e.g., blood pressure) is explained by the explanatory variable (e.g., diet).\nThe same applies to analysis of variance (ANOVA) (as well as regression) because ANOVA is a special case of a linear model, just like regression is also a special case of a linear model.\nThe defining characteristic of ANOVA is that we are comparing the means of groups by analysing variances. Put another way, we will have a single categorical explanatory variable with two or more levels. We will test whether the means of the response variable are the same across all levels of the explanatory variable, and we test this by analysing the variances.\nWhen we have only one categorical explanatory variable, we use a one-way ANOVA. When we have two categorical explanatory variables, we will use a two-way ANOVA (we‚Äôll look at this in a subsequent chapter).",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#anova-as-a-linear-model",
    "href": "5.1-anova.html#anova-as-a-linear-model",
    "title": "Analysis of variance (L5)",
    "section": "ANOVA as a linear model",
    "text": "ANOVA as a linear model\nJust like linear regression, ANOVA can be expressed as a linear model. The key difference is that in ANOVA, the explanatory variable is categorical rather than continuous.We formulate the linear model as follows:\n\\[y_{ij} = \\mu_j + \\epsilon_{i}\\]\nwhere:\n\n\\(y_{ij}\\) = Blood pressure of individual \\(i\\) with diet \\(j\\)\n\\(\\mu_i\\) = Mean blood pressure of an individual with diet \\(j\\)\n\\(\\epsilon_{i}\\sim N(0,\\sigma^2)\\) is an independent error term.\n\nGraphically, with the blood pressure and diet data, this looks like:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is lots of hidden code used to create the data used in the graph above, and to make the graph itself. You can see the code by going to the Github repository for this book.\n\n\n\nRewrite the model\nWe usually use a different formulation of the linear model for ANOVA. This is because we usually prefer to express the estimated parameters in terms of differences between means (rather than the means themselves). The reason for this is that then the null hypothesis can be that the differences are zero.\nTo proceed with this formulation, we define one of the groups as the reference group, and make the mean of that equal to the intercept of the model. For example, if we choose the ‚Äúmeat heavy‚Äù diet as the reference group, we can write:\n\\[\\mu_{meat} = \\beta_0\\]\nAnd then to express the other group means as deviations from the reference group mean:\n\\[\\mu_{Med} = \\beta_0 + \\beta_1\\] \\[\\mu_{vegan} = \\beta_0 + \\beta_2\\] \\[\\mu_{veggi} = \\beta_0 + \\beta_3\\]\nWhen we write out the entire model, we get:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{1} + \\beta_2 x_i^{2} + \\beta_3 x_i^{3} + \\epsilon_i\\] where: \\(y_i\\) is the blood pressure of individual \\(i\\). \\(x_i^{1}\\) is a binary variable indicating whether individual \\(i\\) is on the Mediterranean diet. \\(x_i^{2}\\) is a binary variable indicating whether individual \\(i\\) is on the vegan diet. \\(x_i^{3}\\) is a binary variable indicating whether individual \\(i\\) is on the vegetarian diet.\nGraphically, the model now looks like this:\n\n\n\n\n\n\n\n\n\n\nHere is something to warp you mind‚Ä¶ we described one-way ANOVA as a linear model with one categorical explanatory variable. But as you can see above, we can also describe it as a linear model with multiple binary explanatory variables (one for each group except the reference group). And when we make a linear model in R it really does create multiple binary explanatory variables behind the scenes. So one-way ANOVA and multiple linear regression with multiple binary explanatory variables are really the same thing! And, even more mind-warping, one-way ANOVA and multiple regression (regression with multiple continuous explanatory variables) are also the same thing! So when we look at multiple regression later in the course, you can think of it as just an extension of one-way ANOVA.\n\n\n\nThe ANOVA test: The \\(F\\)-test\nAim of ANOVA: to test globally if the groups differ. That is we want to test the null hypothesis that all of the group means are equal:\n\\[H_0: \\mu_1=\\mu_2=\\ldots = \\mu_g\\] This is equivalent to testing if all \\(\\beta\\)s that belong to a categorical variable are = 0.\n\\[H_0: \\beta_1 = \\ldots = \\beta_{g-1} = 0\\] The alternate hypothesis is that \\({H_1}\\): The group means are not all the same.\nA key point is that we are testing a null hypothesis that concerns all the groups. We are not testing if one group is different from another group (which we could do with a \\(t\\)-test on one of the non-intercept \\(\\beta\\)s).\nBecause we are testing a null hypothesis that concerns all the groups, we need to use an \\(F\\)-test. It asks if the model with the group means is better than a model with just the overall mean.\n\n\n\n\n\n\nNote\n\n\n\nThe \\(F\\)-test is called the ‚Äú\\(F\\)-test‚Äù because it is based on the \\(F\\)-distribution, which was named after the statistician Sir Ronald A. Fisher. Fisher developed this statistical method as part of his pioneering work in analysis of variance (ANOVA) and other fields of experimental design and statistical inference.\n\n\nActually, the \\(F\\)-test does not directly test the null hypothesis that all the group means are equal. Instead, it tests whether the model that includes the group means explains significantly more variance in the data than a model that only includes the overall mean (i.e., without considering group differences).\nThe \\(F\\)-test does this by comparing two variance estimates: the variance explained by the group means (between-group variance) and the variance that remains unexplained within each group (within-group variance).\nThink‚ÄìPair‚ÄìShare (#a_what_f_test_answers) What question does the ANOVA F-test answer? What question does it not answer?\n\n\nInterpretation of the \\(F\\) statistic\nThe \\(F\\)-test involves calculating from the observed data the value of the \\(F\\) statistic, and then computing if that value is large enough to reject the null hypothesis.\nThe \\(F\\) statistic is a ratio of two variances: the variance between groups, and the variance within groups.\nHere is an example with very low within group variability, and high between group variability:\n\n\n\n\n\n\n\n\n\nAnd here‚Äôs an example with very high within group variability, and low between group variability:\n\n\n\n\n\n\n\n\n\nSo, when the ratio of between group variance to within group variance is large, the group means are very different compared to the variability within groups. This suggests that the groups are different.\nWhen the ratio is small, the group means are similar compared to the variability within groups. This suggests that the groups are not different.\n\n\\(F\\) increases\n\nwhen the group means become more different, or\nwhen the variability within groups decreases.\n\n\\(F\\) decreases\n\nwhen the group means become more similar, or\nwhen the variability within groups increases.\n\n\n\\(\\rightarrow\\) The larger \\(F\\), the less likely are the data seen under \\(H_0\\).\n\n\nCalculating the \\(F\\) statistic\nRecall that the \\(F\\) statistic is a ratio of two variances. Specifically, it is the ratio of two mean squares (MS):\n\n\\(MS_{model}\\): the variability between groups.\n\\(MS_{residual}\\): the variability within groups.\n\n\\(MS\\) stands for Mean Square, and is a variance estimate.\nThe \\(F\\) statistic is calculated as:\n\\[F = \\frac{MS_{model}}{MS_{residual}}\\]\nTo find the mean squares, we need to calculate the within and the between group sums of squares, and the corresponding degrees of freedom. Let‚Äôs go though this step by step.\n\n\nCalculating the sums of squares\nFirst we get the total sum of squares (SST), which quantifies the total variability in the data. This is then split into the explained variability (SSM), and the residual variability (SSE).\nTotal variability: SST = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij}-\\overline{y})^2\\)\nwhere:\n\n\\(y_{ij}\\) is the blood pressure of individual \\(j\\) in group \\(i\\)\n\\(\\overline{y}\\) is the overall mean blood pressure\n\\(n_i\\) is the number of individuals in group \\(i\\)\n\\(k\\) is the number of groups\n\nExplained variability (between group variability): = SSM = \\(\\sum_{i=1}^k n_i (\\overline{y}_{i} - \\overline{y})^2\\)\nwhere:\n\n\\(\\overline{y}_{i}\\) is the mean blood pressure of group \\(i\\)\n\nResidual variability (within group variability): = SSE = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i}  (y_{ij} - \\overline{y}_{i} )^2\\)\n\n\nCalculating the degrees of freedom\nAnd now we need the degrees of freedom for each sum of squares:\nSST degrees of freedom: \\(n - 1\\) (total degrees of freedom is number of observations \\(n\\) minus 1)\nSSM degrees of freedom: \\(k - 1\\) (model degrees of freedom is number of groups \\(k\\) minus 1)\nSSE degrees of freedom: \\(n - k\\) (residual degrees of freedom is total degrees of freedom \\(n - 1\\) minus model degrees of freedom \\(k - 1\\))\n\nTotal degrees of freedom\nThe total degrees of freedom are the degrees of freedom associated with the total sum of squares (\\(SST\\)).\nIn order to calculate the \\(SST\\), we need to calculate the mean of the response variable. This implies that we estimate one parameter (the mean of the response variable). As a consequence, we lose one degree of freedom and so there remain \\(n-1\\) degrees of freedom associated with the total sum of squares (where \\(n\\) is the number of observations).\nWhat do we mean by ‚Äúlose one degree of freedom‚Äù? Imagine we have ten observations. We can calculate the mean of these ten observations. But if we know the mean and nine of the observations, we can calculate the tenth observation. So, in a sense, once we calculate the mean, the value of one of the ten observations is fixed. This is what we mean by ‚Äúlosing one degree of freedom‚Äù. When we calculate and use the mean, one of the observations ‚Äúloses its freedom‚Äù.\nFor example, take the numbers 1, 3, 5, 7, 9. The mean is 5. The sum of the squared differences between the observations and the mean is \\((1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2 = 20\\). This is the total sum of squares. The degrees of freedom are \\(5-1 = 4\\).\nThe total degrees of freedom are the total number of observations minus one. That is, the total sum of squares is associated with \\(n-1\\) degrees of freedom.\nAnother perspective in which to think about the total sum of squares and total degrees of freedom is to consider the intercept only model. The intercept only model is a model that only includes the intercept term. The equation of this model would be:\n\\[y_i = \\beta_0 + \\epsilon_i\\] The sum of the square of the residuals for this model is minimised when the predicted value of the response variable is the mean of the response variable. That is, the least squares estimate of \\(\\beta_0\\) is the mean of the response variable:\n\\[\\hat{\\beta}_0 = \\bar{y}\\]\nHence, the predicted value of the response variable is the mean of the response variable. The equation is:\n\\[\\hat{y}_i = \\bar{y} + \\epsilon_i\\]\nThe error term is therefore:\n\\[\\epsilon_i = y_i - \\bar{y}\\] And the total sum of squares is:\n\\[SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\]\nwhere \\(\\hat{y}_i\\) is the predicted value of the response variable for the \\(i\\)th observation, \\(\\bar{y}\\) is the mean of the response variable, and \\(\\epsilon_i\\) is the residual for the \\(i\\)th observation.\nThe intercept only model involves estimating only one parameter, so the total degrees of freedom are the total number of observations minus one \\(n - 1\\).\nTherefore, the total degrees of freedom are the total number of observations minus one.\nBottom line: \\(SST\\) is the residual sum of squares when we fit the intercept only model. The total degrees of freedom are the total number of observations minus one.\n\n\nModel degrees of freedom\nThe model degrees of freedom are the degrees of freedom associated with the model sum of squares (\\(SSM\\)).\nIn the case of the intercept only model, we estimated one parameter, the mean of the response variable.\nIn the case of a categorical variable with \\(k\\) groups, we need \\(k-1\\) parameters (non intercept \\(\\beta\\) parameters), so we lose \\(k-1\\) degrees of freedom. Put another way, when we fit a model with a categorical explanatory variable with \\(k\\) groups, we estimate \\(k-1\\) parameters in addition to the intercept. That is, we estimate the difference between each group and the reference group.\nEach time we estimate a new parameter, we lose a degree of freedom.\n\n\nResidual degrees of freedom\nThe residual degrees of freedom are the total degrees of freedom (\\(n-1\\)) minus the model degrees of freedom (\\(k-1\\)).\nTherefore, the residual degrees of freedom are the degrees of freedom remaining after we estimate the intercept and the other \\(\\beta\\) parameters. There is one intercept and \\(k-1\\) other \\(\\beta\\) parameters, so the residual degrees of freedom are \\(n-1- ( k-1) = n - k\\).\n\n\n\nCalculating the mean square and \\(F\\) statistic\nFrom these sums of squares and degrees of freedom we can calculate the mean squares and \\(F\\)-statistic:\n\\[MS_{model} = \\frac{SS_{\\text{between}}}{k-1} = \\frac{SSM}{k-1}\\]\n\\[MS_{residual} = \\frac{SS_{\\text{within}}}{n-k} = \\frac{SSE}{n-k}\\]\n\\[F = \\frac{MS_{model}}{MS_{residual}}\\]\n\n\n\n\n\n\nNote\n\n\n\nWhy divide by the degrees of freedom? The more observations we have, the greater will be the total sum of squares. The more observations we have, the greater will be the residual sum of squares. So it is not very informative to compare totals. Rather, we need to compare the mean of the sums of squares. Except we don‚Äôt calculate the mean by dividing by the number of observations. Rather we divide by the degrees of freedom. The total mean square is an estimate of the variance of the response variable. And the residual mean square is an estimate of the variance of the residuals.\n\n\n\n\n\\(SST\\), \\(SSM\\), \\(SSE\\), and degrees of freedom\nJust a reminder and a summary of some of the material above:\n\n\\(SST\\): degrees of freedom = \\(n-1\\)\n\\(SSM\\): degrees of freedom = \\(k-1\\)\n\\(SSE\\): degrees of freedom = \\(n-k\\)\n\nThe sum of squares add up:\n\\[SST = SSM + SSE\\]\nand the degrees of freedom add up\n\\[(n-1) = (k-1) + (n - k)\\]\n\n\nSource of variance table\nNow we have nearly everything we need. We often express all of this (and a few more quantities) in a convenient table called the sources of variance table (or ANOVA table).\nThe sources of variance table is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the \\(F\\)-statistic.\n\nSources of variance table\n\n\n\n\n\n\n\n\n\nSource\nSum of squares\nDegrees of freedom\nMean square\nF-statistic\n\n\n\n\nModel\n\\(SSM\\)\n\\(k-1\\)\n\\(MSE_{model} = SSM / k-1\\)\n\\(\\frac{MSE_{model}}{MSE_{error}}\\)\n\n\nError\n\\(SSE\\)\n\\(n - 1 - (k-1)\\)\n\\(MSE_{error} = SSE / (n - 1 - (k-1))\\)\n\n\n\nTotal\n\\(SST\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\nIs my \\(F\\)-statistic large or small?\nOK, so we have calculated the \\(F\\) statistic. But how do we use it to test our hypothesis?\nWe can use the \\(F\\) statistic to calculate a \\(p\\)-value, which tells us how likely our data is under the null hypothesis.\nSome key points:\n\n\\(F\\)-Distribution: The test statistic of the \\(F\\)-test (that is, the \\(F\\)-statistic) follows the \\(F\\)-distribution under the null hypothesis. This distribution arises when comparing the ratio of two independent sample variances (or mean squares).\nRonald Fisher‚Äôs Contribution: Fisher introduced the \\(F\\)-distribution in the early 20th century as a way to test hypotheses about the equality of variances and to analyze variance in regression and experimental designs. The ‚Äú\\(F\\)‚Äù in \\(F\\)-distribution honours him.\nVariance Ratio: The test statistic for the \\(F\\)-test is the ratio of two variances (termed mean squares in this case), making the \\(F\\)-distribution the natural choice for modeling this ratio when the null hypothesis is true.\n\nThe \\(F\\)-test is widely used, including when comparing variances, assessing the significance of multiple regression models (see later chapter), conducting ANOVA to test for differences among group means, and for comparing different models.\nRecall that ‚ÄúThe \\(F\\)-statistic is calculated as the ratio of the mean square error of the model to the mean square error of the residuals.‚Äù And that a large \\(F\\)-statistic is evidence against the null hypothesis that the slopes of the explanatory variables are zero. And that a small \\(F\\)-statistic is evidence to not reject the null hypothesis that the slopes of the explanatory variables are zero.\nBut how big does the F-statistic need to be in order to confidently reject the null hypothesis?\nThe null hypothesis that the explained variance of the model is no greater than would be expected by chance. Here, ‚Äúby chance‚Äù means that the slopes of the explanatory variables are zero.\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0\\]\nThe alternative hypothesis is that the explained variance of the model is greater than would be expected by chance. This would occur if the slopes of some or all of the explanatory variables are not zero.\n\\[H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0 \\text{ or } \\ldots \\text{ or } \\beta_p \\neq 0\\]\nTo test this hypothesis we are going to, as usual, calculate a \\(p\\)-value. The \\(p\\)-value is the probability of observing a test statistic as or more extreme as the one we observed, assuming the null hypothesis is true. To do this, we need to know the distribution of the test statistic under the null hypothesis. The distribution of the test statistic under the null hypothesis is known as the \\(F\\)-distribution.\nThe \\(F\\)-distribution has two degrees of freedom values associated with it: the degrees of freedom of the model and the degrees of freedom of the residuals. The degrees of freedom of the model are the number of parameters estimated by the model corresponding to the null hypothesis. The degrees of freedom of the residuals are the total degrees of freedom minus the degrees of freedom of the model.\nHere is the \\(F\\)-distribution with 2 and 99 degrees of freedom:\n\n\n\n\n\n\n\n\n\nThe F-distribution is skewed to the right and has a long tail. The area to the right of 3.89 is shaded in red. This area represents the probability of observing an F-statistic as or more extreme as 3.89, assuming the null hypothesis is true. This probability is the \\(p\\)-value of the hypothesis test.\nThe \\(F\\)-statistic and \\(F\\)-test is briefly recaptured in 3.1.f) of the Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact that\n\\[\\frac{MSE_{model}}{MSE_{residual}} =  \\frac{SSM/p}{SSE/(n-1-p)} \\sim F_{p,n-1-p}\\]\nfollows an \\(F\\)-distribution with \\(p\\) and \\((n-1-p)\\) degrees of freedom, where \\(p\\) are the number of continuous variables, \\(n\\) the number of data points.\n\n\\(SSE=\\sum_{i=1} ^n(y_i-\\hat{y}_i)^2\\) is the residual sum of squares\n\\(SSM = SST - SSE\\) is the sum of squares of the model\n\\(SST=\\sum_{i=1}^n(y_i-\\overline{y})^2\\) is the total sum of squares\n\\(n\\) is the number of data points\n\\(p\\) is the number of explanatory variables in the regression model\n\nWell, that is ANOVA conceptually. But how does it actually look like in R?",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#doing-anova-in-r",
    "href": "5.1-anova.html#doing-anova-in-r",
    "title": "Analysis of variance (L5)",
    "section": "Doing ANOVA in R",
    "text": "Doing ANOVA in R\nLet‚Äôs go back again the question of how diet effects blood pressure. Here is the data:\n\nbp_data_diet &lt;- select(bp_data_diet, bp, diet, person_ID)\nhead(bp_data_diet)\n\n# A tibble: 6 √ó 3\n     bp diet          person_ID\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;    \n1   120 meat heavy    person_1 \n2    89 vegan         person_2 \n3    86 vegetarian    person_3 \n4   116 meat heavy    person_4 \n5   115 Mediterranean person_5 \n6   134 meat heavy    person_6 \n\n\n\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_point() +\n  labs(title = \"Blood pressure by diet\",\n       x = \"Diet\",\n       y = \"Blood pressure\")\n\n\n\n\n\n\n\n\nAnd here is how we fit a linear model to this data:\n\nfit &lt;- lm(bp ~ diet, data = bp_data_diet)\n\nIMPORTANT: Since ANOVA is a linear model, it is important to check the assumptions of linear models before interpreting the results. These are some of the same assumptions we checked for simple linear regression, including: independence of errors, normality of residuals, and homoscedasticity (constant variance of residuals).\nAs with linear regression, we check the assumptions are not too badly broken by looking at model checking plots:\n\npar(mfrow = c(2, 2))\nplot(fit, add.smooth = FALSE)\n\n\n\n\n\n\n\n\nNothing looks too bad.\n** Think-pair-share**: Which of the four plots above would you use to check each of the three assumptions listed above?\n** Think-pair-share**: Before we look at the ANOVA table, lets figure out the total degrees of freedom, the degrees of freedom for the model, and the degrees of freedom for the residuals. Have a think-pair-share about each of these. Write you ideas down. Chat with you neighbour. Then share with the class.\nNow we can look at the ANOVA table:\n\nanova(fit)\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe ANOVA table shows the sum of squares, degrees of freedom, mean square, F value, and p-value for the model and residuals. As we know, the \\(F\\) value (\\(F\\) statistics) is calculated as the mean square of the model divided by the mean square of the residuals. The p-value is calculated based on the F-distribution with the appropriate degrees of freedom.\nA suitable sentence to report our findings would be: ‚ÄúDiet has a significant effect on blood pressure (\\(F(2, 27) = 20.7, p &lt; 0.0001\\))‚Äù. This means that the probability of observing such a large \\(F\\) value under the null hypothesis is less than 0.01%.\nThink-pair-share: You know that the \\(R^2\\) value is a measure of how much variance in the response variable is explained by the model. How would you calculate the \\(R^2\\) value from the ANOVA table above?",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#difference-between-pairs-of-groups",
    "href": "5.1-anova.html#difference-between-pairs-of-groups",
    "title": "Analysis of variance (L5)",
    "section": "Difference between pairs of groups",
    "text": "Difference between pairs of groups\n‚ÄúANOVA does not tell you which groups differ.‚Äù\nRecall that the \\(F\\) test is a global test. It tests the null hypothesis that all group means are equal. It does not tell us which groups are different from each other. It just tells us that at least one group mean is different. Sometimes researchers are interested in more specific questions such as:\n\nfinding the actual group(s) that deviate(s) from the others.\nin estimates of the pairwise differences.\n\nThe summary table in R provides some of these comparison, specifically it contains the estimates for \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) (while the reference was set to \\(\\beta_0 = 0\\)).\nFor example, here is the summary table for our diet data:\n\nsummary(fit)\n\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        122.625      2.302  53.260  &lt; 2e-16 ***\ndietMediterranean  -12.688      3.256  -3.897 0.000314 ***\ndietvegan          -26.768      4.173  -6.414 6.92e-08 ***\ndietvegetarian     -23.625      3.607  -6.549 4.33e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n\n\nIn this table we have the intercept (\\(\\beta_0\\)) and the three \\(\\beta\\) values for the diet groups: ‚ÄúdietMeat‚Äù, ‚ÄúdietVegetarian‚Äù, and ‚ÄúdietVegan‚Äù. The Estimate column shows the estimated coefficients for each group. The intercept (\\(\\beta_0\\)) represents the mean blood pressure for the reference group (in this case, the ‚Äúmeat‚Äù diet group). The other three coefficients represent the difference in mean blood pressure between each diet group and the reference group.\nAll well and good up to a point. But there are two issues with using the results from this table:\n\nThe greater the number of individual tests, the more likely one will be significant just by chance. This is called the problem of multiple comparisons. Many test can result in a type-I error: rejecting the null hypothesis when it is actually true. The more tests one does, the more likely one is to make a type-I error.\n\n** Think-pair-share**: Imagine that when our threshold p-value each individual test is 0.05 (5%) so that if it is less than 0.05 we call it ‚Äúsignificant‚Äù and if it is greater than 0.05 we call it ‚Äúnot significant‚Äù (this is the standard practice in many fields). When we make 20 hypothesis tests, how many would we expect to be ‚Äúsignificant‚Äù just by chance (i.e., when we assume that all null hypotheses is true.)\n\nThe summary table does not provide all the possible pairwise comparisons. It does not, for example, provide the comparison between the ‚Äúvegan‚Äù and the ‚Äúvegetarian‚Äù group.\n\nSeveral methods to circumvent the problem of too many ‚Äúsignificant‚Äù test results (type-I error) have been proposed. The most prominent ones are:\n\nBonferroni correction\nTukey Honest Significant Differences (HSD) approach\nFisher Least Significant Differences (LSD) approach\n\nThe second two when implemented in R also provide all possible pairwise comparisons.\n\nBonferroni correction\nIdea: If a total of \\(m\\) tests are carried out, simply divide the type-I error level \\(\\alpha_0\\) (often 5%) such that\n\\[\\alpha = \\alpha_0 / m \\ .\\]\nBut this still leaves the problem of how to efficiently get all of the possible pairwise comparisons. We can do this using the pairwise.t.test function in R:\n\npairwise.t.test(bp_data_diet$bp,\n                bp_data_diet$diet,\n                p.adjust.method = \"bonferroni\")\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  bp_data_diet$bp and bp_data_diet$diet \n\n              meat heavy Mediterranean vegan \nMediterranean 0.0019     -             -     \nvegan         4.2e-07    0.0091        -     \nvegetarian    2.6e-07    0.0239        1.0000\n\nP value adjustment method: bonferroni \n\n\nHere we can see that all pairwise comparisons have a p-value less than 0.05, except for the comparison of vegan versus vegetarian, which has a p-value that rounds to 1.0000.\nWe also see in the output the note that ‚ÄúP value adjustment method: bonferroni‚Äù, indicating that the Bonferroni correction has been applied to the p-values.\n\n\nTukey HSD approach\nIdea: Take into account the distribution of (max-min) and design a new test.\nIn R we can use the multcomp package to do Tukey HSD tests:\n\nbp_data_diet &lt;- bp_data_diet %&gt;%\n  mutate(diet = as.factor(diet))\nfit &lt;- lm(bp ~ diet, data = bp_data_diet)\nsuppressMessages(library(multcomp))\ntukey_test &lt;- glht(fit, linfct = mcp(diet = \"Tukey\"))\nsummary(tukey_test)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lm(formula = bp ~ diet, data = bp_data_diet)\n\nLinear Hypotheses:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \nMediterranean - meat heavy == 0  -12.688      3.256  -3.897  0.00168 ** \nvegan - meat heavy == 0          -26.768      4.173  -6.414  &lt; 0.001 ***\nvegetarian - meat heavy == 0     -23.625      3.607  -6.549  &lt; 0.001 ***\nvegan - Mediterranean == 0       -14.080      4.173  -3.374  0.00759 ** \nvegetarian - Mediterranean == 0  -10.938      3.607  -3.032  0.01951 *  \nvegetarian - vegan == 0            3.143      4.453   0.706  0.89305    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nWe get all the pairwise comparisons, along with their estimates, standard errors, t-values, and p-values. We also get a note Adjusted p values reported -- single-step method, indicating that the Tukey HSD adjustment has been applied to the p-values.\nAgain, all pairwise comparisons have a p-value less than 0.05, except for the comparison of vegan versus vegetarian, which has a p-value of 0.89305.\n\n\nFisher‚Äôs LSD approach\nIdea: Adjust the idea of a two-sample test, but use a larger variance (namely the pooled variance of all groups).\n\n\nOther contrasts\nA contrast is a specific comparison between groups. So far we have only considered pairwise contrasts (i.e., comparing two groups at a time). But we can also design more complex contrasts. For example: are diets that contain meat different from diets that do not contain meat?\n\nbp_data_diet &lt;- mutate(bp_data_diet,\n                       meat_or_no_meat = ifelse(diet == \"meat\" |\n                                                diet == \"Mediterranean\",\n                                                \"meat\", \"no meat\"))\nhead(bp_data_diet)\n\n# A tibble: 6 √ó 4\n     bp diet          person_ID meat_or_no_meat\n  &lt;dbl&gt; &lt;fct&gt;         &lt;chr&gt;     &lt;chr&gt;          \n1   120 meat heavy    person_1  no meat        \n2    89 vegan         person_2  no meat        \n3    86 vegetarian    person_3  no meat        \n4   116 meat heavy    person_4  no meat        \n5   115 Mediterranean person_5  meat           \n6   134 meat heavy    person_6  no meat        \n\n\nHere we defined a new explanatory variable that groups the meat heavy and Mediterranean diet together into a single ‚Äúmeat‚Äù group and vegetarian and vegan into a single ‚Äúno meat‚Äù group. We then fit a model with this explanatory variable:\n\nfit_mnm &lt;- lm(bp ~ meat_or_no_meat, data = bp_data_diet)\n\n(We should not look at model checking plots here, before using the model. But let us continue as if the assumptions are sufficiently met.)\nWe now do something a bit more complicated: we compare the variance explained by the model with four diets to the model with two diets. This is done by comparing the two models using an \\(F\\)-test. We are testing the null hypothesis that the two models are equally good at explaining the data, in which case the two diet model will explain as much variance as the four diet model.\nLet‚Äôs look at the ANOVA table of the model comparison:\n\nanova(fit, fit_mnm)\n\nAnalysis of Variance Table\n\nModel 1: bp ~ diet\nModel 2: bp ~ meat_or_no_meat\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 3901.5                                  \n2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see the residual sum of squares of the model with meat or no meat is over 9‚Äô000, while that of the four diet model is less than 4‚Äô000. That is, the four diet model explains much more variance in the data than the two diet model. The \\(F\\)-test is highly significant, so we reject the null hypothesis that the two models are equally good at explaining the data. And we conclude that its not just whether people eat meat or not, but rather what kind of diet they eat that affects their blood pressure.\nIdeally we do not make a lot of contrasts after we have collected and looked at our data. Rather, we would specify the contrasts we are interested in before we collect the data. This is called a priori contrasts. But sometimes we do exploratory data analysis and then we can make post hoc contrasts. In this case we should be careful to adjust for multiple comparisons.\n\n\nChoosing the reference category\nQuestion: Why was the ‚Äúheavy meat‚Äù diet chosen as the reference (intercept) category?\nAnswer: Because R orders the categories alphabetically and takes the first level alphabetically as reference category.\nSometimes we may want to override this, for example if we have a treatment that is experimentally the control, then it will usually be useful to set this as the reference / intercept level.\nIn R we can set the reference level using the relevel function:\n\nbp_data_diet$diet &lt;- relevel(factor(bp_data_diet$diet), ref = \"vegan\")\n\nAnd now make the model and look at the estimated coefficients:\n\nfit_vegan_ref &lt;- lm(bp ~ diet, data = bp_data_diet)\nsummary(fit_vegan_ref)\n\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         95.857      3.481  27.538  &lt; 2e-16 ***\ndietmeat heavy      26.768      4.173   6.414 6.92e-08 ***\ndietMediterranean   14.080      4.173   3.374  0.00151 ** \ndietvegetarian       3.143      4.453   0.706  0.48386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n\n\nNow we see the estimated coefficients for all diets except the vegan diet. The intercept is the mean individuals with vegan diet.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#communicating-the-results-of-anova",
    "href": "5.1-anova.html#communicating-the-results-of-anova",
    "title": "Analysis of variance (L5)",
    "section": "Communicating the results of ANOVA",
    "text": "Communicating the results of ANOVA\nWhen communicating the results of an ANOVA, we usually report the \\(F\\)-statistic, the degrees of freedom of the numerator and denominator, and the p-value. For example, we could say:\n\nBlood pressure differed significantly between groups, with the mean of a meat heavy diet being 123 mmHg, while the mean blood pressure of the vegan group was 27 mmHg lower (One-way ANOVA, \\(F(3, 46) = 20.7\\), \\(p &lt; 0.0001\\).\n\nAnd we would make a nice graph, in this case showing each individual observation since there are not too many to cause overplotting. We can also add the estimated means of each group if we like:\n\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_jitter(width = 0.1, height = 0) +\n  stat_summary(fun = mean, geom = \"point\", color = \"red\", size = 3) +\n  labs(title = \"Blood Pressure by Diet\",\n       x = \"Diet\",\n       y = \"Blood Pressure (mmHg)\")\n\n\n\n\n\n\n\n\nSome people like to see error bars as well, for example showing the 95% confidence intervals of the means:\n\nggplot(bp_data_diet, aes(x = diet, y = bp)) +\n  geom_jitter(width = 0.1, height = 0, col = \"grey\") +\n  stat_summary(fun = mean, geom = \"point\", color = \"black\", size = 3) +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.2, color = \"black\") +\n  labs(title = \"Blood Pressure by Diet\\nBlack points and error bars show mean ¬± 95% CI\",\n       x = \"Diet\",\n       y = \"Blood Pressure (mmHg)\") \n\n\n\n\n\n\n\n\nThere are many many plotting styles and preferences. The important thing is to clearly communicate the results, and to not mislead the reader. I find that plotting the individual data points is often a good idea, especially when the sample size is not too large.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#review",
    "href": "5.1-anova.html#review",
    "title": "Analysis of variance (L5)",
    "section": "Review",
    "text": "Review\nHere are the key points from this chapter:\n\nANOVA is just another linear model.\nIt is used when we have categorical explanatory variables.\nWe use \\(F\\)-tests to test the null hypothesis of no difference among the means of the groups (categories).\nWe can use contrasts and post-hoc tests to test specific hypotheses about the means of the groups.\n\nThink‚ÄìPair‚ÄìShare (#a_anova_is_regression) What changes when we move from regression to ANOVA? What stays exactly the same?",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#further-reading",
    "href": "5.1-anova.html#further-reading",
    "title": "Analysis of variance (L5)",
    "section": "Further reading",
    "text": "Further reading\nIf you‚Äôd like some further reading on ANOVA and ANOVA in R, here are some good resources. There is lots of overlap among them, however, and with the material in this chapter. I suggest to look at them mostly for different perspectives and examples. And only if you‚Äôd like to solidify your understanding.\n\nChapter 5, Section 5.6, of Getting Started with R by Beckerman et al.¬†This section is about 10 pages.\nChapter 11 of The New Statistics with R by Hector, not including the section on two-way ANOVA. This chapter is about 10 pages.\nChapter 14 of Linear Models with R by Faraway. This chapter is eight pages.\nChapter 9 of Statistics. An Introduction using R by Crawley, up to the section Factorial experiments. This section is about 16 pages.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "5.1-anova.html#extras",
    "href": "5.1-anova.html#extras",
    "title": "Analysis of variance (L5)",
    "section": "Extras",
    "text": "Extras\n\nANOVA and experimental design\nThis is not a course about experimental design (this is much too large a subject to cover). You may have noticed, however, that all of the example datasets and questions in your reading about ANOVA have been experiments, where researchers had various treatment groups containing the experimental subjects. That is, the researchers manipulated something to be different among the groups, and looked for any evidence of resulting differences.\nThis might lead you to wonder if we analyse experiments with ANOVA, and analyse observations (involving no manipulations) with regression. The answer is no, we can use either for either. It just happens that manipulations in experiments are often of the categorical nature: type of food, parasite species, light level (high or low), and so on. Of course some experiments contain manipulations that are continuous (e.g.¬†a range of temperatures or drug concentrations) and these are analysed with regression. And some studies include a manipulation (e.g., drug concentration) and observed variables like gender.\nSo there isn‚Äôt necessarily and association between type of explanatory variable (continuous or categorical) and whether an study is experimental or observational. Important, no essential, is for the researcher to clearly be aware of which variables are manipulation and which are observations. Because only the manipulations can be used to infer causation.",
    "crumbs": [
      "Analysis of variance (L5)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html",
    "href": "6.1-multiple-regression.html",
    "title": "Multiple regression (L6)",
    "section": "",
    "text": "Introduction\nIn the previous chapters we covered simple linear regression and one-way analysis of variance. In both we had one response variable and one explanatory variable. In both cases we made a linear model to relate the response variable to the explanatory variable. The two cases differed in the type of explanatory variable. In the case of simple linear regression, the explanatory variable was continuous. In the case of one-way ANOVA, the explanatory variable was categorical.\nWe will now extend the linear model and analyses to cases with more than one (i.e., multiple) explanatory variables. The explanatory variables can be continuous or categorical, and can be a mixture of the two.\nSome combinations of explanatory variables have special names:\nWe will look at each of these, and start with multiple linear regression which is usually shortened to just multiple regression.",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#introduction",
    "href": "6.1-multiple-regression.html#introduction",
    "title": "Multiple regression (L6)",
    "section": "",
    "text": "Multiple (more than one) continuous explanatory variables -&gt; Multiple linear regression.\nTwo categorical explanatory variables -&gt; Two-way ANOVA.\nOne continuous and one categorical explanatory variable -&gt; Analysis of covariance (ANCOVA).",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#multiple-regression",
    "href": "6.1-multiple-regression.html#multiple-regression",
    "title": "Multiple regression (L6)",
    "section": "Multiple regression",
    "text": "Multiple regression\nWe previously looked at whether blood pressure is associated with age. This is an important question, because blood pressure has many health implications. However, blood pressure is not only associated with age, but also with other factors, such as weight, height, and lifestyle. In this chapter, we will look at how to investigate the association between blood pressure and multiple explanatory variables.\nWhen we have multiple explanatory variables, we are often interested in questions such as:\n\nQuestion 1: As an ensemble (i.e., all together), are the explanatory variables ‚Äúuseful‚Äù?\nQuestion 2: Are each of the explanatory variables associated with the response?\nQuestion 3: What proportion of variability is explained?\nQuestion 4: Are some explanatory variables more important than others?\n\n\nAn example dataset\nBlood pressure is again the response variable, with age and lifestyle as two explanatory variables. Lifestyle is a continuous variable that is the number of minutes of exercise per week.\nReading in the dataset:\n\nbp_data_multreg &lt;- read_csv(\"datasets/bp_data_multreg.csv\")\n\nRows: 100 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (3): age, mins_exercise, bp\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere is a look at the dataset:\n\nhead(bp_data_multreg)\n\n# A tibble: 6 √ó 3\n    age mins_exercise    bp\n  &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1    74           288   110\n2    63           253    88\n3    70           190   121\n4    45           138    93\n5    69           184   104\n6    80           278    90\n\n\nSince there are three variables, we can make three different scatter plots to visualise the relationships:\n1. Age vs blood pressure. This is the graph of the response variable (blood pressure) against one of the explanatory variables (age). It looks like there is evidence of a positive relationship.\n\n\n\n\n\n\n\n\n\nHere we see a positive relationship between age and blood pressure. Blood pressure tends to increase with age.\n2. Minutes of exercise vs blood pressure. This is a graph of the response variable (blood pressure) against the other explanatory variable (minutes of exercise). It looks like there is evidence of a negative relationship.\n\n\n\n\n\n\n\n\n\nHere we see a negative relationship between minutes of exercise and blood pressure. Blood pressure tends to decrease with more minutes of exercise.\n3. Age vs minutes of exercise. This is a graph of the two explanatory variables against each other. It looks like there is no relationship.\n\n\n\n\n\n\n\n\n\nAnd here we see no relationship between age and minutes of exercise. The two explanatory variables appear to be independent.\n\n\n\n\n\n\nImportant\n\n\n\nThe lack of correlation between the two explanatory variables is very important. If the two explanatory variables were correlated, we would have a situation known as multicollinearity. Multicollinearity can greatly complicate the interpretation of the results of a multiple regression analysis. We will discuss multicollinearity later.\n\n\n\n\nThe multiple linear regression model\nThe multiple linear regression model is an extension of the simple linear regression model. Recall the simple linear regression model is:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nwhere:\n\n\\(y_i\\) is the response variable\n\\(x_i\\) is the explanatory variable\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope\n\\(\\epsilon_i\\) is the error term.\n\nThe multiple linear regression model with two explanatory variables is:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\epsilon_i\\]\nwhere:\n\n\\(x_i^{(1)}\\) and \\(x_i^{(2)}\\) are the two explanatory variables\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope for the first explanatory variable\n\\(\\beta_2\\) is the slope for the second explanatory variable\n\nNote that the intercept \\(\\beta_0\\) is the value of the response variable when all explanatory variables are zero. In this example, it would be the blood pressure for someone that is 0 years old and does 0 minutes of exercise per week. This is not a particularly useful scenario, but it is a necessary mathematical construct that helps us to build the model.\nWe can extend the multiple regression model to have an arbitrary number of explanatory variables:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\ldots + \\beta_p x_i^{(p)} + \\epsilon_i\\]\nWhere:\n\\(x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(p)}\\) are the \\(p\\) explanatory variables and all else is as before.\nor with summation notation:\n\\[y_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_i^{(j)} + \\epsilon_i\\]\nJust like in simple linear regression, we can estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the method of least squares. The least squares method minimizes the sum of the squared residuals:\n\\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere \\(\\hat{y}_i\\) is the predicted value of the response variable for the \\(i\\)th observation:\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\sum_{j=1}^p \\hat{\\beta}_j x_i^{(j)}\\] where:\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are the estimated parameters.\nThink-Pair-Share (#tps-two-shape) Graphically, a linear regression with one explanatory variable is a line. What is a geometric representation of linear regression with two explanatory variables?\nHere is a graph of the geometric representation of a multiple linear regression model with two explanatory variables (please note that this plot is best viewed in the HTML version of the book; in the PDF version, it will appear as a static image):\n\n\n\n\n\n\nLet‚Äôs write the equation for the blood pressure data:\n\\[bp_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot mins\\_exercise_i + \\epsilon_i\\]\nwhere:\n\n\\(bp_i\\) is the blood pressure for the \\(i\\)th observation\n\\(age_i\\) is the age for the \\(i\\)th observation\n\\(mins\\_exercise_i\\) is the minutes of exercise for the \\(i\\)th observation\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope for age\n\\(\\beta_2\\) is the slope for minutes of exercise\n\\(\\epsilon_i\\) is the error term\n\nand the error term is assumed to be normally distributed with mean 0 and constant variance, just as was the case for simple linear regression:\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\nSeventh, we know how to make predictions using the model, and to make a prediction band.\nWhat we don‚Äôt know is how to answer the four questions already mentioned above:\n\nQuestion 1: As an ensemble (i.e., all together), are the explanatory variables ‚Äúuseful‚Äù?\nQuestion 2: Are each of the explanatory variables associated with the response?\nQuestion 3: What proportion of variability is explained?\nQuestion 4: Are some explanatory variables more important than others?\n\nLet‚Äôs answer these questions using the blood pressure example.\n\n\nFitting the model\nWe know how to estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the method of least squares.\nIn R, we can fit a multiple linear regression model using the lm() function in a very similar way to the simple linear regression model. Here is the code for the blood pressure example. To fit two explanatory variables, we simply add the second variable to the formula using the + sign:\n\nm1 &lt;- lm(bp ~ age + mins_exercise, data = bp_data_multreg)\n\n\n\nChecking the assumptions\nGreat news ‚Äì&gt; the five assumptions of the multiple linear regression model are the same as for the simple linear regression model:\n\nNormality of residuals.\nHomoscedasticity = constant variance of residuals.\nIndependence of residuals.\nLinearity.\nNo outliers.\n\nThink-Pair-Share (#tps-assump-match) Review how we can check these assumptions in the simple linear regression model. Match the following to the assumptions above:\n\nGraph of size of residuals vs.¬†fitted values.\nQQ-plot.\nGraph of residuals vs.¬†fitted values.\nGraph of leverage vs.¬†standardized residuals.\n\nAnd what is missing?\nWe can check the assumptions of the multiple linear regression model using the same methods as for the simple linear regression model. Here is the code for the blood pressure example:\n\n# Check the assumptions\npar(mfrow=c(2,2))\nplot(m1, which = c(1,2,3,5), add.smooth = FALSE)\n\n\n\n\n\n\n\n\nWe see that the assumptions are met for the blood pressure example:\n\nNormality of residuals: The QQ-plot shows that the residuals are normally distributed.\nHomoscedasticity: The scale-location plot shows that the residuals have constant variance.\nIndependence of residuals: No evidence of pattern or clustering. But also need to know about study design to properly assess independence.\nLinearity: The residuals vs.¬†fitted values plot shows no clear pattern in the residuals.\nNo outliers: No points with high leverage or high residuals.",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#question-1-as-an-ensemble-are-the-explanatory-variables-useful",
    "href": "6.1-multiple-regression.html#question-1-as-an-ensemble-are-the-explanatory-variables-useful",
    "title": "Multiple regression (L6)",
    "section": "Question 1: As an ensemble, are the explanatory variables useful?",
    "text": "Question 1: As an ensemble, are the explanatory variables useful?\nRecall that when we learned about ANOVA we saw that a single categorical explanatory variable with multiple levels can be represented as multiple binary (0/1) explanatory variables. In that case, we used the \\(F\\)-test to test the null hypothesis of no effect / relationship for all binary variables together.\nLikewise, when we have multiple continuous explanatory variables, we use the \\(F\\)-test to test the null hypothesis that together the explanatory variables have no association with the response variable. That is, we use the \\(F\\)-test to test the null hypothesis that the ensemble of explanatory variables is not associated with the response variable.\nThis corresponds to the same null hypothesis as we used in one-way ANOVA: The null hypothesis that the explained variance of the model is no greater than would be expected by chance. Here, ‚Äúby chance‚Äù means that the slopes of the explanatory variables are zero:\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0\\]\nAnd the alternative hypothesis (just as in one-way ANOVA) is that the explained variance of the model is greater than would be expected by chance. This would occur if the slopes of some or all of the explanatory variables are not zero:\n\\[H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0 \\text{ or } \\ldots \\text{ or } \\beta_p \\neq 0\\]\nRecall that the \\(F\\)-test compares the variance explained by the model to the variance not explained by the model (i.e., the variance of the residuals). If the variance explained by the model is significantly greater than the variance not explained by the model, then we can conclude that the explanatory variables are associated with the response variable.\nIf we reject the null hypothesis, we can conclude that some combination of the explanatory variables is associated with the response variable. However, we cannot conclude which specific explanatory variables are associated with the response variable. To determine which specific explanatory variables are associated with the response variable, we need to perform individual \\(t\\)-tests for each explanatory variable. We will do this in the next section.\nOK, back to the \\(F\\)-test.\nWe know a lot already from the ANOVA chapter. Let‚Äôs review how we calculate the \\(F\\)-statistic.\nThe \\(F\\)-statistic is calculated as the ratio of two mean squares:\n\nThe mean square of the model (\\(MSE_{model}\\)).\nThe mean square of the residuals (\\(MSE_{residual}\\)).\n\nRecall that a mean square is a sum of squares divided by the associated degrees of freedom. The formulas for these are the same as for ANOVA.\nSo, to calculate these two mean squares, we need to calculate three sums of squares:\n\nThe total sum of squares (\\(SST\\)).\nThe sum of squares of the model (\\(SSM\\)).\nThe sum of squares of the residuals (\\(SSE\\)).\n\nWe also need to calculate the degrees of freedom associated with each sum of squares.\n\nThe total degrees of freedom is \\(n-1\\), where \\(n\\) is the number of observations.\nThe model degrees of freedom is \\(p\\), where \\(p\\) is the number of explanatory variables. This is because for each explanatory variable we estimate one parameter (the slope), and each estimated parameter uses up one degree of freedom.\nThe residual degrees of freedom is \\(n-1-p\\).\n\n\nThe F-statistic in R\nWe could do all these calculations ourselves (and you might be asked to in the exam), but also we can just ask R! For question 1 we need to know the \\(F\\)-statistic for the multiple linear regression model. We can easily get this from R using the summary() function, and by looking in the right place in the output:\n\nsummary(m1)\n\n\nCall:\nlm(formula = bp ~ age + mins_exercise, data = bp_data_multreg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.179  -7.592   1.064   7.608  23.756 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   95.28056    3.76302  25.320  &lt; 2e-16 ***\nage            0.51240    0.06551   7.822 6.39e-12 ***\nmins_exercise -0.12574    0.01403  -8.960 2.36e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.06 on 97 degrees of freedom\nMultiple R-squared:  0.541, Adjusted R-squared:  0.5316 \nF-statistic: 57.17 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nIn the final line of output we see ‚ÄúF-statistic: 57.17 on 2 and 97 DF, p-value: 3.95^{-17}‚Äù.\nThe model degrees of freedom is 2 (because we have two explanatory variables), and the residual degrees of freedom is 97 (because we have 100 observations and 2 explanatory variables, so \\(100 - 1 - 2 = 97\\)).\nSo the summary() function gives us everything we need to answer question 1. It even gives us the \\(p\\)-value for the \\(F\\)-test.\nHow to report the result. We could write somthing like this: ‚ÄúThe combination of age and minutes of exercise is significantly associated with blood pressure (F(2, 97) = 57.17, p = 3.95^{-17}).‚Äù Note that this is rather an undesirable statement, because it focuses too much on the statistics and not enough on the science. Indeed, perhaps we care more about the association of each explanatory variable with blood pressure, which we will look at next.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to review how to calculate a p-value from an F-statistic, see the corresponding section of the one-way ANOVA chapter.",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#question-2-which-variables-are-associated-with-the-response",
    "href": "6.1-multiple-regression.html#question-2-which-variables-are-associated-with-the-response",
    "title": "Multiple regression (L6)",
    "section": "Question 2: Which variables are associated with the response?",
    "text": "Question 2: Which variables are associated with the response?\nAs we did for simple linear regression, we can perform a \\(t\\)-test for one explanatory variable to determine if it is associated with the response. And we can do this for each of the explanatory variables. As before, the null hypothesis for each \\(t\\)-test is that the slope of the explanatory variable is zero. The alternative hypothesis is that the slope of the explanatory variable is not zero.\nHere is the coefficients table, which includes the results of the \\(t\\)-tests for each explanatory variable:\n\nsummary(m1)$coef\n\n                Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   95.2805567 3.76301963 25.320239 1.555233e-44\nage            0.5124011 0.06550667  7.822122 6.394789e-12\nmins_exercise -0.1257352 0.01403351 -8.959641 2.362231e-14\n\n\nAnd we can get the 95% CI for each slope estimate \\(\\hat\\beta_j\\) as follows:\n\nconfint(m1)\n\n                   2.5 %       97.5 %\n(Intercept)   87.8120044 102.74910897\nage            0.3823885   0.64241372\nmins_exercise -0.1535878  -0.09788255\n\n\nReminder: The 95% confidence interval is \\([\\hat\\beta - c \\cdot \\sigma^{(\\beta)} ; \\hat\\beta + c \\cdot \\sigma^{(\\beta)}]\\), where \\(c\\) is the 97.5% quantile of the \\(t\\)-distribution with \\(n-p\\) degrees of freedom).\n\n\n\n\n\n\nImportant\n\n\n\nHowever Please insert a note into your brain that we are dealing here with an ideal case of uncorrelated explanatory variables. You‚Äôll learn later in the course about what happens when explanatory variables are correlated. Hint: interpretation is difficult and unstable!\n\n\nThink‚ÄìPair‚ÄìShare (#a_partial_effects) What does ‚Äúholding other variables constant‚Äù mean here? Is this always biologically plausible?",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#question-3-what-proportion-of-variability-is-explained",
    "href": "6.1-multiple-regression.html#question-3-what-proportion-of-variability-is-explained",
    "title": "Multiple regression (L6)",
    "section": "Question 3: What proportion of variability is explained?",
    "text": "Question 3: What proportion of variability is explained?\n\nMultiple \\(R^2\\)\nWe can calculate the \\(R^2\\) value for the multiple linear regression model just like we already did for a simple linear regression model. The \\(R^2\\) value is the proportion of variability in the response variable that is explained by the model. As before, the \\(R^2\\) value ranges from 0 to 1, where 0 indicates that the model does not explain any variability in the response variable, and 1 indicates that the model explains all the variability in the response variable.\nFor multiple linear regression, we often use the term ‚Äúmultiple \\(R^2\\)‚Äù to distinguish it from the \\(R^2\\) value for simple linear regression. The multiple \\(R^2\\) is the proportion of variability in the response variable that is explained by the model, taking into account all the explanatory variables in the model.\nAs before, for simple linear regression, the multiple \\(R^2\\) value is calculated as the sum of squares explained by the model divided by the total sum of squares:\n\\[R^2 = \\frac{SSM}{SST}\\]\n\nsss &lt;- anova(m1)\nSSM &lt;- sss$`Sum Sq`[1] + sss$`Sum Sq`[2]\nSST &lt;- sum(sss$`Sum Sq`)\nR_squared &lt;- SSM / SST\n#R_squared\n\nwhere \\(SSM\\) is the sum of squares explained by the model and \\(SST\\) is the total sum of squares, and \\(SSM = SST - SSE\\).\nFor the blood pressure data:\n\nsummary(m1)$r.squared\n\n[1] 0.541017\n\n\n\\(R^2\\) for multiple linear regression can also be calculated as the squared correlation between \\((y_1,\\ldots,y_n)\\) and \\((\\hat{y}_1,\\ldots,\\hat{y}_n)\\), where the \\(\\hat y\\) are the fitted values from the model. The fitted values are calculated as:\n\\[\\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x^{(1)} + \\ldots + \\hat\\beta_m x^{(m)}\\]\nIn R:\n\nr_squared &lt;- cor(m1$fitted.values, bp_data_multreg$bp)^2\nr_squared\n\n[1] 0.541017\n\n\nOr:\n\nsss &lt;- anova(m1)\nSSM &lt;- sss$`Sum Sq`[1] + sss$`Sum Sq`[2]\nSST &lt;- sum(sss$`Sum Sq`)\nR_squared &lt;- SSM / SST\nR_squared\n\n[1] 0.541017\n\n\n\n\nAdjusted \\(R^2\\)\nHowever, we have a little problem to address. The \\(R^2\\) value increases as we add more explanatory variables to the model, even if the additional variables are not associated with the response. This is because the \\(R^2\\) value is calculated as the proportion of variability in the response variable that is explained by the model. As we add more explanatory variables to the model, the model will always explain more variability in the response variable, even if the additional variables are not associated with the response. Some of the variance will be explained by chance.\nHere is an example of this problem. First, here‚Äôs the explanatory power of the model with only age and minutes of exercise as the explanatory variables:\n\nm1 &lt;- lm(bp ~ age + mins_exercise, data = bp_data_multreg)\nsummary(m1)$r.squared\n\n[1] 0.541017\n\n\nNow, we can add a new explanatory variable to the blood pressure model that is not associated with the response:\n\nbp_data_multreg$random_variable &lt;- rnorm(nrow(bp_data_multreg))\nm2 &lt;- lm(bp ~ age + mins_exercise + random_variable, data = bp_data_multreg)\nsummary(m2)$r.squared\n\n[1] 0.5486269\n\n\nThe \\(R^2\\) value for the model with the random variable is higher than the \\(R^2\\) value for the model without the random variable. This is because the model with the random variable explains more variability in the response variable, even though the random variable is not associated with the response.\nTo address this problem, we can use the adjusted \\(R^2\\) value. The adjusted \\(R^2\\) value is calculated as:\n\\[R^2_{\\text{adj}} = 1 - \\frac{SSE / (n - p - 1)}{SST / (n - 1)}\\]\nwhere * \\(SSE\\) is the sum of squared errors * \\(SST\\) is the total sum of squares * \\(n\\) is the number of observations * \\(p\\) is the number of explanatory variables in the model.\nOr put another way:\n\\[R^2_{adj} = 1-(1-R^2 )\\frac{n-1}{n-p-1}\\] In this form, we can see that as \\(p\\) increases (as we add explanatory variables) the term \\((n-1)/(n-p-1)\\) increases, and the adjusted \\(R^2\\) value will decrease if the additional variables are not associated with the response.\nTake home: when we want to compare the explanatory power of models that differ in the number of explanatory variables, we should use the adjusted \\(R^2\\) value.",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#question-4-are-some-explanatory-variables-more-important-than-others",
    "href": "6.1-multiple-regression.html#question-4-are-some-explanatory-variables-more-important-than-others",
    "title": "Multiple regression (L6)",
    "section": "Question 4: Are some explanatory variables more important than others?",
    "text": "Question 4: Are some explanatory variables more important than others?\nWe know how to test the significance of the parameters using the \\(t\\)-test. We also know how to calculate the confidence intervals for the parameters, and to make a confidence band.\nHow important are the explanatory variables and how important are they relative to each other?\nThink-Pair-Share (#tps-variable-importance) How might we assess how important is each of the explanatory variables, and how important they are relative to each other?\nThe importance of an explanatory variable can be assessed by looking at the size of the coefficient for that variable. The larger the coefficient, the more important the variable is in explaining the response variable.\nIt is, however, important to remember that the size of the coefficient depends on the scale of the explanatory variable. If the explanatory variables are on different scales, then the coefficients will be on different scales and cannot be directly compared.\nIn our example, the age variable is measured in years, so the coefficient is in units mmHg (pressure) per year. The mins_exercise variable is measured in minutes, so the coefficient is in units mmHg per minute. The coefficients are on different scales and cannot be directly compared. Furthermore, the value of the coefficients would change if we measured age in months or minutes of exercise in hours.\nThere are other perspectives we can take when we‚Äôre assessing importance. For example, we cannot change our age, but we can change the number of minutes of exercise. So, the practical importance of the two variables is quite different in that sense also.\nTo compare the importance of the explanatory variables that are measured on different scales, we can standardize the variables before fitting the model. This means that we subtract the mean of the variable and divide by the standard deviation. This puts all the variables on the same scale, so the coefficients can be directly compared. The coefficients are then in units of the response variable per standard deviation of the explanatory variable.\nHowever, the coefficients are then not in the original units of the explanatory variables, so it is not always easy to interpret the coefficients. So while we can compare the coefficients, they have lost a bit of their original meaning and are not so easy to interpret.\nOne way to relate the coefficients in this case is to realise that to compensate for the blood pressure increase associated with one year of age, one would need to exercise for a certain number of minutes more.\nThink-Pair-Share (#tps-exercise-age) How many minutes of exercise per week would we need to add to our fitness schedule to compensate for the blood pressure increase associated with one year of age?\n\nextra_mins_exercise &lt;- coef(m1)[\"age\"] / -coef(m1)[\"mins_exercise\"]\n#extra_mins_exercise",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#question-5-how-do-we-make-predictions",
    "href": "6.1-multiple-regression.html#question-5-how-do-we-make-predictions",
    "title": "Multiple regression (L6)",
    "section": "Question 5: How do we make predictions?",
    "text": "Question 5: How do we make predictions?\nWe already made predictions from a simple linear regression model?\nRecall that the equation for multiple linear regression is:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\ldots + \\beta_p x_i^{(p)} + \\epsilon_i\\]\nTherefore to get a predicted value of \\(y_i\\), we can use the estimated parameters (\\(\\hat\\beta_0, \\hat\\beta_1, \\ldots, \\hat\\beta_p\\)) and the values of the explanatory variables (\\(x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(p)}\\)):\n\\[\\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x_i^{(1)} + \\hat\\beta_2 x_i^{(2)} + \\ldots + \\hat\\beta_p x_i^{(p)}\\]\nIn R, we can use the predict() function to make predictions from a multiple linear regression model. Here is an example of how to make predictions for the values of the explanatory variables in the original dataset:\n\npredictions &lt;- predict(m1)\n\nWhere m1 is the multiple linear regression model fitted earlier. The predict function automatically uses the original data because we did not provide any new data.\nWe can also make predictions for new values of the explanatory variables by providing a new data frame to the predict() function. Here is an example of how to make predictions for new values of age and minutes of exercise:\nFirst we need to make some new values of age and minutes of exercise:\n\nnew_data &lt;- data.frame(age = c(30, 40, 50),\n                       mins_exercise = c(50, 100, 150))\n\nThen we can use the predict() function to make predictions for these new values:\n\nnew_predictions &lt;- predict(m1, newdata = new_data)\n\n\n\n\n\n\n\nImportant\n\n\n\nThe new data frame must have the all the explanatory variables used in the model, and the variable names must match exactly those used in the model.\n\n\nWe can take this to the next level and make what is called a ‚Äúconditional effects plot‚Äù or ‚Äúeffect plot‚Äù. This is a plot that shows the predicted values of the response variable for different values of one explanatory variable, while holding the other explanatory variables constant. Here is an example of how to make a conditional effects plot for age, while holding minutes of exercise constant at 100:\n\nnew_data_effects &lt;- data.frame(age = seq(20, 80, by = 1),\n                                mins_exercise = 100)\nnew_data_effects &lt;- new_data_effects |&gt;\n  mutate(new_predictions_effects = predict(m1, newdata = new_data_effects))\nggplot(data = new_data_effects, aes(x = age, y = new_predictions_effects)) +\n  geom_line() +\n  labs(x = \"Age\", y = \"Predicted blood pressure\") +\n  ggtitle(\"Conditional effects plot for\\nage (mins_exercise = 100)\")\n\n\n\n\n\n\n\n\nAnd lets take this to the next level again and make a conditional effects plot for three different levels of mins_exercise:\n\nnew_data_effects_3 &lt;- expand.grid(age = seq(20, 80, by = 1),\n                                   mins_exercise = c(0, 150, 300))\n\nA new function! And it‚Äôs one of Owen‚Äôs favourites: expand.grid(). This function creates a data frame from all combinations of the supplied vectors or factors. Here, we are creating a data frame with all combinations of age (from 20 to 80) and minutes of exercise (0, 150, and 300).\nWe then give the new data frame to the predict() function to get the predicted values for each combination of age and minutes of exercise:\n\nnew_data_effects_3 &lt;- new_data_effects_3 |&gt; \n  mutate(prediction = predict(m1, newdata = new_data_effects_3),\n         mins_exercise_fac = as.factor(mins_exercise)) \n\n(Note that we changed)\nAnd make a graph:\n\nggplot(data = new_data_effects_3, aes(x = age, y = prediction,\n                                             col = mins_exercise_fac)) +\n  geom_line() +\n  labs(x = \"Age\", y = \"Predicted blood pressure\", color = \"Minutes\n of exercise\") +\n  ggtitle(\"Conditional effects plot for age\\nat three levels of minutes of exercise\")",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#collinearity",
    "href": "6.1-multiple-regression.html#collinearity",
    "title": "Multiple regression (L6)",
    "section": "Collinearity",
    "text": "Collinearity\nIn the blood pressure example data used previously in this chapter there is no evidence of correlation between the two explanatory variables. However, in practice, it is common for explanatory variables to be correlated with each other. This is known as collinearity. It can be quite problematic for us!\nThink-Pair-Share (#tps-collinearity1) Imagine if there was perfect correlation between age and minutes of exercise. This would mean that a graph of age vs minutes of exercise would show a perfect line. What would be the implications for the multiple linear regression model? For example, what would be the \\(R^2\\) value for a model with one explanatory variable compared to a model with both explanatory variables?\nCollinearity, specifically harmful collinearity, is extremely common in real dataset that result from observational studies. This is because in observational studies there are often numerous explanatory variables, and they are often correlated with each other. This is a situation that is ripe for collinearity problems. (Collinearity can also happen in data resulting from designed manipulative experiments, but is hopefully relatively rare there because a well-designed experiment will try to avoid collinearity by ensuring that the explanatory variables are independent.)\nSo what is collinearity, and why is it a problem?\nPut simply, collinearity is when one explanatory variable is predictable from a linear combination of others.\nThis can happen due to strong correlation among pairs of explanatory variables, or due to more complex relationships involving three or more explanatory variables.\nFor example, if we have three explanatory variables, \\(x_1, x_2,\\) and \\(x_3\\), and if \\(x_3\\) can be predicted from a linear combination of \\(x_1\\) and \\(x_2\\), then we have collinearity. For example, if:\n\\[x_3 = 2 \\cdot x_1 + 3 \\cdot x_2 + \\text{small random noise}\\]\nIn this case, variable \\(x_3\\) is a linear combination of \\(x_1\\) and \\(x_2\\), plus some small random noise. This means that if we know the values of \\(x_1\\) and \\(x_2\\), we can predict the value of \\(x_3\\) quite accurately. Also, in this case we might not have strong correlation between any pair of the explanatory variables, but there is still collinearity because \\(x_3\\) is predictable from \\(x_1\\) and \\(x_2\\). So lack of correlation between pairs of explanatory variables does not guarantee that there is no collinearity.\nIt is a problem because it makes the slope estimates unstable and therefore difficult to interpret.\nLet us see this instability in practice. First let‚Äôs look at the really extreme example of perfect collinearity. Here‚Äôs a new version of the blood pressure data in which the minutes of exercise variable is perfectly predicted from age:\n\nset.seed(123)\nn &lt;- 100\nage &lt;- ceiling(runif(n, 20, 80))\n\nHere is the line of code that makes the perfect correlation between age and minutes of exercise:\n\nmins_exercise &lt;- 100 - age\n\nNow we generate the blood pressure variable as before:\n\nbp &lt;- 100 + 0.5 * age + rnorm(n, 0, 15)\nbp_data_perfect &lt;- data.frame(age = age,\n                              mins_exercise = mins_exercise,\n                              bp = bp)\n\nRead in the data:\n\nbp_data_perfect &lt;- read_csv(\"datasets/bp_data_perfect.csv\")\n\nRows: 100 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (3): age, mins_exercise, bp\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow we fit a multiple linear regression model with only age:\n\nm1_age &lt;- lm(bp ~ age, data = bp_data_perfect)\nsummary(m1_age)\n\n\nCall:\nlm(formula = bp ~ age, data = bp_data_perfect)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.546  -9.147  -0.327   8.938  33.213 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 100.43331    4.54751  22.085  &lt; 2e-16 ***\nage           0.47541    0.08549   5.561 2.33e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.54 on 98 degrees of freedom\nMultiple R-squared:  0.2399,    Adjusted R-squared:  0.2321 \nF-statistic: 30.92 on 1 and 98 DF,  p-value: 2.325e-07\n\n\nThe estimated slope (coefficient) is 0.48, which is close to the true value of 0.5. The 95% confidence interval is 0.31 to 0.65, which includes the true value of 0.5. All good then.\nNow we fit the multiple linear regression model with both age and minutes of exercise included:\n\nm1_both &lt;- lm(bp ~ mins_exercise + age, data = bp_data_perfect)\nsummary(m1_both)\n\n\nCall:\nlm(formula = bp ~ mins_exercise + age, data = bp_data_perfect)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.546  -9.147  -0.327   8.938  33.213 \n\nCoefficients: (1 not defined because of singularities)\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   147.97398    4.48276  33.010  &lt; 2e-16 ***\nmins_exercise  -0.47541    0.08549  -5.561 2.33e-07 ***\nage                  NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.54 on 98 degrees of freedom\nMultiple R-squared:  0.2399,    Adjusted R-squared:  0.2321 \nF-statistic: 30.92 on 1 and 98 DF,  p-value: 2.325e-07\n\n\nThis output is a bit strange. The estimate for age is now NA. This is because the model cannot distinguish between the effects of age and minutes of exercise, since they are perfectly correlated. The model is unable to estimate the coefficient for age of exercise, so it returns NA.\nThis is an example of instability of coefficients due to collinearity. The coefficient for age is completely unstable, as it changes from a number to NA depending on whether minutes of exercise is included in the model or not.\nActually, you can see that the estimate for mins_exercise are the same (except the sign of the coefficient) as the estimate for age in the previous model. This is because minutes of exercise is perfectly correlated with age, so the model is essentially using minutes of exercise as a proxy for age. With perfect collinearity, the model cannot distinguish between the effects of the two variables‚Ä¶ they are effectively identical.\nYou can also see that the \\(R^2\\) value doesn‚Äôt change when the second variable is added. That is, the model with only age included is identical to the model with both age and minutes of exercise included. This is because minutes of exercise is perfectly correlated age, so including minutes of exercise in the model does not add any new information.\nThat is a pretty extreme example of perfect collinearity. In practice, collinearity is often not perfect, but still strong enough to cause problems.\nLet‚Äôs look at the less extreme example of collinearity we had earlier, with three explanatory variables, \\(x_1, x_2,\\) and \\(x_3\\), and where \\(x_3\\) can be predicted from a linear combination of \\(x_1\\) and \\(x_2\\) plus some random noise:\n\nset.seed(123)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nx3 &lt;- 2 * x1 + 3 * x2 + rnorm(n, 0, 5)\ny &lt;- 5 + 1.5 * x1 - 2 * x2 + 0.5 * x3 + rnorm(n, 0, 1)\ndata_collinear &lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y)\ncor(data_collinear)\n\n            x1          x2        x3          y\nx1  1.00000000 -0.04953215 0.1877534  0.6079515\nx2 -0.04953215  1.00000000 0.5194100 -0.1488812\nx3  0.18775342  0.51940999 1.0000000  0.6434430\ny   0.60795153 -0.14888117 0.6434430  1.0000000\n\n\nWe see that there is not strong correlation between any pair of the explanatory variables, but there is still collinearity because \\(x_3\\) is predictable from \\(x_1\\) and \\(x_2\\).\nLet‚Äôs look for evidence of instability of the coefficients. First, we fit the multiple linear regression model with all three explanatory variables included:\n\nm_collinear_123 &lt;- lm(y ~ x1 + x2 + x3, data = data_collinear)\nsummary(m_collinear_123)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  4.9806739 0.10734366  46.39933 1.490805e-67\nx1           1.4675042 0.11972274  12.25752 2.384482e-21\nx2          -1.9193496 0.12990373 -14.77517 1.832330e-26\nx3           0.4885226 0.02244616  21.76419 6.723265e-39\n\n\nDo these change much if we fit the model with only \\(x_1\\) and \\(x_2\\) included?\n\nm_collinear_12 &lt;- lm(y ~ x1 + x2, data = data_collinear)\nsummary(m_collinear_12)$coefficients\n\n              Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  5.3105865  0.2575325 20.621034 3.113931e-37\nx1           2.1192629  0.2809162  7.544111 2.464756e-11\nx2          -0.3956201  0.2651792 -1.491897 1.389714e-01\n\n\nYes, they do change quite a bit. The coefficients for \\(x_1\\) and \\(x_2\\) are quite different when \\(x_3\\) is included in the model compared to when it is not included. This is because \\(x_3\\) is predictable from \\(x_1\\) and \\(x_2\\), so including \\(x_3\\) in the model changes the interpretation of the coefficients for \\(x_1\\) and \\(x_2\\).\n\n\n\n\n\n\nNote\n\n\n\nIn a not so extreme case such as this one, the coefficients will not be completely unstable (i.e., they will not change from a number to NA) but they can still change quite a bit depending on which other collinear variables are included in the model.\n\n\nThe bottom line is that collinearity between explanatory variables complicates the interpretation of the model coefficients. If there is collinearity/correlation between the explanatory variables, then the model coefficients can be unstable and difficult to interpret.\nLet‚Äôs have one more look at this instability that is caused by collinearity. And make this demonstration a bit more general. We‚Äôll simulate data with two explanatory variables that are correlated with each other to varying degrees. We‚Äôll then fit multiple linear regression models with both explanatory variables included, and see how the stability of the coefficients changes as we change the correlation between the explanatory variables.\n\n\n\n\n\n\n\n\n\nIn the left panel we see the slope estimates for \\(x_1\\) from the multiple linear regression model with both \\(x_1\\) and \\(x_2\\) included, for different levels of correlation between \\(x_1\\) and \\(x_2\\). When there is no correlation between \\(x_1\\) and \\(x_2\\) (i.e., no collinearity), the slope estimates are quite stable and close to the true value of 1.\nAs the correlation between \\(x_1\\) and \\(x_2\\) increases, the slope estimates become more variable (= less stable). At correlation of 0.9, the slope estimates are quite variable and can be very far from the true value of 1.\nThe right panel shows the variance of the slope estimates as a function of the correlation between \\(x_1\\) and \\(x_2\\). Variance here is a measure of the amount of vertical spread in the left panel.\nWe see that the variance of the slope estimates increases as the correlation between \\(x_1\\) and \\(x_2\\) increases. This shows that greater collinearity between explanatory variables leads to greater instability (variance) of the slope estimates.\nThis increase in variance caused by collinearity is known as the variance inflation effect. The variance inflation effect makes it difficult to interpret the coefficients of the model, because the coefficients can be quite unstable and can change a lot depending on which other collinear variables are included in the model.\n\nCollinearity and interpretation of \\(R^2\\)\nCollinearity also affects the interpretation of the \\(R^2\\) values. Collinearity will cause the collinear explanatory variables to share some of the explained variance. The \\(R^2\\) value of the multiple regression will then be less than the sum of the \\(R^2\\) values of the individual regressions of the response variable on each of the explanatory variables separately.\n\\(R^2\\) of the age only model:\n\nsummary(m2_age)$r.squared\n\n[1] 0.1611723\n\n\n\\(R^2\\) of the mins_exercise only model:\n\nsummary(m2_mins_exercise)$r.squared\n\n[1] 0.2515\n\n\n\\(R^2\\) of the model with both age and mins_exercise:\n\nsummary(m2_both)$r.squared\n\n[1] 0.541017\n\n\nIn this case the two explanatory variables are strongly correlated and so share a lot of the explained variance. The \\(R^2\\) value of the model with both explanatory variables is much less than the sum of the \\(R^2\\) values of the models with each explanatory variable separately. In fact, either of the models with only one explanatory variable is nearly as good as the model with both explanatory variables. We don‚Äôt gain much from including another explanatory variable in the model when we already include one explanatory variable that is strongly correlated with the other.\nThink‚ÄìPair‚ÄìShare (#a_collinearity_intuition) Why can strong correlation among explanatory variables make estimates unstable even if the model fits the data well?\n\n\nDo I have a problem (with collinearity)?\nThere are several ways to measuring collinearity between explanatory variables and to assess if it is a problematic. One way is to look at the correlation matrix of the explanatory variables. If there are strong correlations between any pair of explanatory variables, then there is likely to be collinearity.\nBut recall that collinearity can also occur without strong pairwise correlations. So another way to detect collinearity is to calculate the Variance Inflation Factor (VIF). This is so named because it measures how much the variance of the estimated regression coefficients is increased due to collinearity. (Recall that we saw the variance inflation effect in the simulation above.)\nRecall the definition of collinearity: it is when an explanatory variable is predictable from a linear combination of others. To calculate the VIF for a specific explanatory variable, we fit a linear regression model with that explanatory variable as the response variable, and all the other explanatory variables as the explanatory variables. We then calculate the \\(R^2\\) value for this model. The VIF is then calculated as:\n\\[VIF_j = \\frac{1}{1 - R^2_j}\\]\nwhere \\(R^2_j\\) is the \\(R^2\\) value from the model with explanatory variable \\(j\\) as the response variable, and all other explanatory variables as the explanatory variables.\nIn the case where the explanatory variable \\(j\\) is not predictable from the other explanatory variables at all, then \\(R^2_j = 0\\), and the VIF is 1. This indicates that there is no collinearity.\nIn the case where the explanatory variable \\(j\\) is perfectly predictable from the other explanatory variables, then \\(R^2_j = 1\\), and the VIF is infinite. This indicates that there is perfect collinearity.\nTo get the VIF for a multiple linear regression model with multiple explanatory variables, we calculate the VIF for each explanatory variable separately.\nThe VIF measures how much the variance of the estimated regression coefficients is increased due to collinearity.\nIn R, we can calculate the VIF using the vif() function from the car package. Here is the code to calculate the VIF for the blood pressure model:\n\nvif(m1)\n\n          age mins_exercise \n     1.061564      1.061564 \n\n\nWe see that the VIF values for both explanatory variables are close to 1, indicating that there is no collinearity between the explanatory variables.\nFor the previous example with collinearity among three explanatory variables, we can calculate the VIF as follows:\n\nvif(m_collinear_123)\n\n      x1       x2       x3 \n1.069365 1.412832 1.460863 \n\n\nWe see that the VIF values for all three explanatory variables are greater than 1, indicating that there is collinearity between the explanatory variables. The VIF for \\(x_3\\) is highest, indicating that \\(x_3\\) is predictable from \\(x_1\\) and \\(x_2\\). But none of the VIF values are extremely high, indicating that the collinearity is not severe.\nA VIF value greater than 5 or 10 is often used as a rule of thumb to indicate that there is collinearity between the explanatory variables.\nLet‚Äôs make an example of three explanatory variables with more severe collinearity:\n\nset.seed(123)\nn &lt;- 100\nx1 &lt;- rnorm(n)\nx2 &lt;- rnorm(n)\nx3 &lt;- 2 * x1 + 3 * x2 + rnorm(n, 0, 1)  # less noise, more collinearity\ny &lt;- 5 + 1.5 * x1 - 2 * x2 + 0.5 * x3 + rnorm(n, 0, 1)\ndata_collinear_severe &lt;- data.frame(x1 = x1, x2 = x2, x3 = x3, y = y)\nm_collinear_severe &lt;- lm(y ~ x1 + x2 + x3, data\n = data_collinear_severe)\nvif(m_collinear_severe)\n\n       x1        x2        x3 \n 4.277416 10.644760 13.360572 \n\n\nWe see that the VIF values of \\(x_2\\) and \\(x_3\\) are greater than 10, indicating that there is strong collinearity between the explanatory variables. The coefficients for \\(x_2\\) and \\(x_3\\) will be quite unstable and difficult to interpret.\n\n\nWhat to do about collinearity?\nImagine we see high VIFs, we can conclud that there is collinearity between explanatory variables, and that the coefficients for those variables are likely to be unstable and difficult to interpret. What can we do???\nThe answer is we should think!\n\nAre explanatory variables measuring the same concept?\nIs this collinearity expected from the study design?\nDo I care about interpretation, or am I only interested in prediction?\n\nAnd then consider some possible remedies:\n\nCombine explanatory variables (index, PCA, biologically meaningful composite) (covered later in the course).\nLook at the explanatory power of a variable once all other variables are accounted for (see below).\nRemove one of a set of redundant explanatory variables (not covered in this course).\nUse regularization if prediction is the goal (not covered in this course).\n\n\n\nAssessing the importance of an explanatory variables in the presence of collinearity\nImagine that we want to know the importance of an explanatory variable in the presence of collinearity. For example, we might want to know how important age is in explaining blood pressure. But we also know that we have collinearity with other explanatory variables, such as minutes of exercise. How can we assess the importance of a particular explanatory variable in the presence of collinearity? Let us call the explanatory variable of interest the ‚Äúfocal‚Äù explanatory variable.\nOne way to assess the importance of the focal explanatory variable is to compare two models. Both should have all other explanatory variables included, but one model should have the focal explanatory variable included, and the other model should have the focal explanatory variable removed. We can then make an \\(F\\)-test to compare the two models. The null hypothesis is that the focal explanatory variable does not explain any additional variance in the response variable, once all other explanatory variables are accounted for. The alternative hypothesis is that the focal explanatory variable does explain additional variance in the response variable, once all other explanatory variables are accounted for.\nThe question is essentially: does including the focal explanatory variable improve the model fit, once all other explanatory variables are already accounted for?\nHere is an example of how to do this in R. Lets use the data with three explanatory variables, \\(x_1, x_2,\\) and \\(x_3\\), where \\(x_3\\) is predictable from a linear combination of \\(x_1\\) and \\(x_2\\) plus some random noise. We will assess the importance of \\(x_1\\) in explaining the response variable \\(y\\), once \\(x_2\\) and \\(x_3\\) are accounted for. First, we fit the full model with all three explanatory variables included:\n\nm_full &lt;- lm(y ~ x1 + x2 + x3, data = data_collinear)\n\nThen, we fit the reduced model with \\(x_1\\) removed:\n\nm_reduced &lt;- lm(y ~ x2 + x3, data = data_collinear)\n\nNow, we can use the anova() function to compare the two models:\n\nanova(m_reduced, m_full)\n\nAnalysis of Variance Table\n\nModel 1: y ~ x2 + x3\nModel 2: y ~ x1 + x2 + x3\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     97 272.27                                  \n2     96 106.14  1    166.12 150.25 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the R output we see information about two models.\n\nModel 1 is the reduced model with \\(x_1\\) removed.\nModel 2 is the second model is the full model with all three explanatory variables included.\n\nThen we have a table that looks somewhat like an ANOVA table. The first column has no title and contain 1 and 2. This indicates the two models being compared.\nThe second column is Res.Df, which is the residual degrees of freedom for each model. We can see that the residual degrees of freedom for Model 2 is one less than that for Model 1, because Model 2 has one more explanatory variable (i.e., \\(x_1\\)) than Model 1.\nThe third column is ‚ÄúRSS‚Äù, which is the Residual Sum of Squares for each model. The residual sum of squares for Model 2 is less than that for Model 1, because Model 2 has one more explanatory variable (i.e., \\(x_1\\)) than Model 1, and because that explanatory variable helps to explain some of the variance in the response variable.\nThe fourth column is ‚ÄúDf‚Äù, which is the difference in degrees of freedom between the two models. This is one in this case, because Model 2 has one more explanatory variable than Model 1.\nThe fifth column is ‚ÄúSum of Sq‚Äù, which is the difference in residual sum of squares between the two models. This the amount of variance in the response variable that is explained by the focal explanatory variable (i.e., \\(x_1\\)) once all other explanatory variables are already accounted for.\nThe sixth column is ‚ÄúF value‚Äù, which is the F-statistic for the comparison of the two models. This is calculated as:\n\\[F = \\frac{(RSS_1 - RSS_2) / (df_1-df_2)}{RSS_2 / df_2}\\]\nwhere \\(RSS_1\\) and \\(RSS_2\\) are the residual sum of squares for Model 1 and Model 2, respectively, and \\(df_1\\) and \\(df_2\\) are the residual degrees of freedom for Model 1 and Model 2, respectively.\nPut another way, the numerator is the mean square difference between the two models, and the denominator is the mean square error for the full model.\nIn our example data, the numbers are: 272.3 - 106.1 = 166.1 divided by - 96 = 1 for the numerator.\nAnd for the denominator: 106.1 divided by 96.\nThis gives an F value of 150.25. This is a very large F value, indicating that the focal explanatory variable (i.e., \\(x_1\\)) explains a significant amount of variance in the response variable, even once all other explanatory variables are already accounted for. This means that even though there is collinearity between the explanatory variables, we can still find that \\(x_1\\) is important in explaining the response variable.\nWe can also find the partial \\(R^2\\) value for the focal explanatory variable (i.e., \\(x_1\\)) from this analysis. The partial \\(R^2\\) value is the proportion of variance in the response variable that is explained by the focal explanatory variable, as a fraction of the variance that is not explained by the other explanatory variables.\nThe partial \\(R^2\\) is calculated as:\n\\[R^2_{partial} = \\frac{RSS_1 - RSS_2}{RSS_1}\\]\nWe can find this in R with:\n\nrss1 &lt;- anova(m_reduced, m_full)$\"RSS\"[1]\nrss2 &lt;- anova(m_reduced, m_full)$\"RSS\"[2]\npartial_r2 &lt;- (rss1 - rss2) / rss1\npartial_r2\n\n[1] 0.6101473\n\n\nDon‚Äôt confuse this partial \\(R^2\\) with what is sometimes known as the semi-partial \\(R^2\\). The semi-partial \\(R^2\\) is the proportion of variance in the response variable that is explained by the focal explanatory variable, as a fraction of the total variance in the response variable. The semi-partial \\(R^2\\) is calculated as:\n\\[R^2_{semi-partial} = \\frac{RSS_1 - RSS_2}{TSS}\\]\nwhere \\(TSS\\) is the total sum of squares of the response variable.\nIn R in our example this is:\n\ntss &lt;- sum((data_collinear$y - mean(data_collinear$y))^2)\nsemi_partial_r2 &lt;- (rss1 - rss2) / tss\nsemi_partial_r2\n\n[1] 0.1625303\n\n\nThis means that about 16.25% of the total variance in the response variable is explained by the focal explanatory variable (i.e., \\(x_1\\)), even once all other explanatory variables are already accounted for. This is explanatory power that is unique to the focal explanatory variable.\nThink‚ÄìPair‚ÄìShare (#a_interpreting_coefficients_conditionally) How does the interpretation of a coefficient change when you add another explanatory variable to the model?",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#review",
    "href": "6.1-multiple-regression.html#review",
    "title": "Multiple regression (L6)",
    "section": "Review",
    "text": "Review\nSimple regression:\n\nHow well does the model describe the data: Correlation and \\(R^2\\)\nAre the parameter estimates compatible with some specific value (\\(t\\)-test)?\nWhat range of parameters values are compatible with the data (confidence intervals)?\nWhat regression lines are compatible with the data (confidence band)?\nWhat are plausible values of other data (prediction band)?\n\nMultiple regression:\n\nMultiple linear regression \\(x_1\\), \\(x_2\\), , \\(x_m\\)\nChecking assumptions.\nQuestion 1: As an ensemble, do the explanatory variables explain variation in the response variable? This is done with an overall \\(F\\)-test.\nQuestion 2: Which of the explanatory variables are important in explaining variation in the response variable? This is done by looking at the coefficients, t-tests, and confidence intervals for each explanatory variable.\nQuestion 3: How well does the model describe the data? This is done with \\(R^2\\) and adjusted \\(R^2\\).\nQuestion 4: Are some explanatory variables more important than others in explaining variation in the response variable? This is done by looking at the sizes of the coefficients, and comparing those for different explanatory variables. But caution is required.\nQuestion 5: How do we make predictions from the model? This is done with the predict() function, and often requires fixing the values of some explanatory variables to specific values.\nCollinearity between explanatory variables can make the coefficients unstable and difficult to interpret. This can be assessed with the Variance Inflation Factor (VIF). Remedies include combining explanatory variables, removing redundant predictors, or using regularization if prediction is the goal. The importance of an explanatory variable in the presence of collinearity can be assessed by comparing models with and without that variable, using an F-test.",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#further-reading",
    "href": "6.1-multiple-regression.html#further-reading",
    "title": "Multiple regression (L6)",
    "section": "Further reading",
    "text": "Further reading\nIf you‚Äôd like to solidify your understanding of multiple linear regression, you might like to look at Chapter 11 of Statistics. An Introduction using R by Crawley. This section is about 14 pages. You will also see that the chapter includes material not included in this course, such as interactions, polynomial regression, model selection, and regression trees. Fun stuff (of course it will not be in the examination for this course!).",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "6.1-multiple-regression.html#extras",
    "href": "6.1-multiple-regression.html#extras",
    "title": "Multiple regression (L6)",
    "section": "Extras",
    "text": "Extras\n\n3D plot of multiple linear regression\n3D plots can help us to visualise multiple linear regression models with two explanatory variables. They are also kind of cool. They can also be difficult to interpret. So use with caution!\nHere is the code to make a 3D plot of the blood pressure data. The y-axis is blood pressure, the x-axis is age, and the z-axis is minutes of exercise. Here is a 3d plot that we can interactive with and rotate (please note that this plot is best viewed in the HTML version of the book; in the PDF version, it will appear as a static image):\n\nplotly::plot_ly(bp_data_multreg, x = ~age, y = ~mins_exercise, z = ~bp,\n                type = \"scatter3d\", mode = \"markers\", marker = list(size = 5),\n                width=500, height=500) |&gt; \n  plotly::layout(scene = list(xaxis = list(title = \"Age\"),\n                             yaxis = list(title = \"Minutes of exercise\"),\n                             zaxis = list(title = \"Blood pressure\")))\n\n\n\n\n\n\n\nPublication ready table of results\nSometimes we need to put a table of coefficients and confidence intervals into a report or publication. One option is to make a table in Word, and to manually enter the coefficients and confidence intervals. This is tedious and error prone. A much better option is to use R to make the table for us. One way to do this is to use the broom package to tidy up the model output into a data frame, and then use the knitr package to make a table from the data frame.\n\ntidy_m1 &lt;- tidy(m1, conf.int = TRUE)\nkable(tidy_m1, digits = 3, caption = \"Multiple linear regression results\nfor blood pressure data\")\n\n\nMultiple linear regression results for blood pressure data\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n95.281\n3.763\n25.320\n0\n87.812\n102.749\n\n\nage\n0.512\n0.066\n7.822\n0\n0.382\n0.642\n\n\nmins_exercise\n-0.126\n0.014\n-8.960\n0\n-0.154\n-0.098\n\n\n\n\n\nAnother approach is to use tbl_regression function within the gtsummary package to get a publication ready table of the coefficients and confidence intervals:\n\ntbl_regression(m1)\n\n\n\n\n\n\n\nCharacteristic\nBeta\n95% CI1\np-value\n\n\n\n\nage\n0.51\n0.38, 0.64\n&lt;0.001\n\n\nmins_exercise\n-0.13\n-0.15, -0.10\n&lt;0.001\n\n\n\n1 CI = Confidence Interval\n\n\n\n\n\n\n\n\nTo use either approach most efficiently you will, however, need to write your report using R Markdown or Quarto so that the table is automatically created in your report document.\n\n\nA figure showing the coefficients and confidence intervals\nAs well as or rather than a table of coefficients, we could make a figure showing the coefficients and confidence intervals for each explanatory variable. This can be a nice way to visualise the results of the multiple linear regression model.\n\ntidy_m1 &lt;- tidy(m1, conf.int = TRUE)\ntidy_m1 |&gt; \n  filter(term != \"(Intercept)\") |&gt; \nggplot(aes(x = term, y = estimate)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width =\n                0.2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Explanatory variable\", y = \"Coefficient estimate\") +\n  ggtitle(\"Multiple linear regression coefficients\\nand confidence intervals\") +\n  coord_flip()",
    "crumbs": [
      "Multiple regression (L6)"
    ]
  },
  {
    "objectID": "7.1-interactions.html",
    "href": "7.1-interactions.html",
    "title": "Interactions (L7)",
    "section": "",
    "text": "Thought for the week:\n‚ÄúYou should arrive at answers on your own, and not rely upon what you get from someone else. Answers from others are nothing more than stopgap measures, they‚Äôre of no value.‚Äù\nThey instead encourage answers gained from curiosity, observation, and dialogue. Answers you develop for yourself are more likely to stick with you, and to be meaningful.\nWhen we mentor, ask questions that encourage others to think for themselves, rather than simply giving answers. This is hard, but worthwhile.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#thought-for-the-week",
    "href": "7.1-interactions.html#thought-for-the-week",
    "title": "Interactions (L7)",
    "section": "",
    "text": "Ichiro Kishimi & Fumitake Koga, The Courage to Be Disliked",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#introduction",
    "href": "7.1-interactions.html#introduction",
    "title": "Interactions (L7)",
    "section": "Introduction",
    "text": "Introduction\nIn the previous chapters we have looked at models with a special type of simplicity: the effect of each explanatory variable on the response variable is independent of the other explanatory variables. This is special because it means that we can understand the effect of each explanatory variable on the response variable separately, without considering the other explanatory variables. Nevertheless, this is often not the case in real life. The effect of one explanatory variable on the response variable may depend on the value of another explanatory variable. This is called an interaction effect, or simply an interaction.\nInteractions are some of the most interesting phenomena in science, including biology. We are not talking about interactions between species, like predation, though these are also very interesting. We are talking about effects of one thing, like diet, depending on another thing, like exercise. Let‚Äôs break that down a bit‚Ä¶\nImagine we make a study of the effect of exercise (minutes per week) on blood pressure for people with a meat heavy diet.\nRead in the dataset:\n\nbp_meatheavy &lt;- read.csv(\"datasets/bp_meatheavy.csv\")\n\nHere are the first few rows of the data:\n\nhead(bp_meatheavy)\n\n         bp mins_per_week       diet\n1  94.12854      53.10173 meat heavy\n2  90.99957      74.42478 meat heavy\n3  73.83541     114.57067 meat heavy\n4  77.05434     181.64156 meat heavy\n5 100.14578      40.33639 meat heavy\n6  95.61900     179.67794 meat heavy\n\n\nAnd here is a graph of the relationship between blood pressure and minutes of exercise for people with a meat heavy diet:\n\n\n\n\n\n\n\n\n\nWe see that exercise seems to lower blood pressure. But what if we look at the effect of exercise on blood pressure for people with a vegetarian diet? The relationship might look something like this:\n\n\n         bp mins_per_week       diet\n1  77.56704     122.92899 vegetarian\n2  84.30684     111.43191 vegetarian\n3  75.23323      65.75546 vegetarian\n4  85.67402      90.62629 vegetarian\n5  77.45327     100.08819 vegetarian\n6 102.31114      36.17327 vegetarian\n\n\nRead in the dataset:\n\nbp_vegetarian &lt;- read.csv(\"datasets/bp_vegetarian.csv\")\n\nHere is a graph of the relationship between blood pressure and minutes of exercise for people with a vegetarian diet:\n\n\n\n\n\n\n\n\n\nWe see that exercise seems to lower blood pressure for vegetarians too, but the effect seems to be weaker.\nTo summarise this finding, we can say that the effect of exercise on blood pressure is stronger for people with a meat heavy diet than for people with a vegetarian diet. This means that the effect of exercise on blood pressure depends on diet.\nThis is very clear when we look at the both diets in the same graph:\n\n\n\n\n\n\n\n\n\n(The dataset with both combined is in the file bp_1cont1cat.csv.)\nThink-Pair-Share (#tps-general-diet) Can we say anything general about the effect of diet on blood pressure?\nNo, we can‚Äôt. We cannot, for example, state that a vegetarian diet lowers blood pressure. We can say, however, that a vegetarian diet lowers blood pressure of people that do little exercise.\nThink-Pair-Share (#tps-general-exercise) Can we say anything general about the effect of exercise on blood pressure?\nWell, we can say that exercise lowers blood pressure, but we have to be careful. We have to say that exercise lowers blood pressure of people with a meat heavy diet more than it lowers blood pressure of people with a vegetarian diet.\nI think it is clear that the interaction was easier to see when we plotted all the data in one graph‚Ä¶ it is much easier to visually compare the slopes of the two regression lines when they are on the same graph.\nAs we will see later in this chapter, the same holds true for statistical tests of interactions: it is much easier to make a statistical test of the interaction when we make a single model with an interaction term.\nIt is harder and is not recommended to make a separate regression for each level of the second variable (diet) and then compare the slopes of the regression lines (it is possible, just not at all efficient).",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#parallel-and-non-parallel-effects",
    "href": "7.1-interactions.html#parallel-and-non-parallel-effects",
    "title": "Interactions (L7)",
    "section": "Parallel and non-parallel effects",
    "text": "Parallel and non-parallel effects\nIn the example above, the effect of exercise on blood pressure was stronger for people with a meat heavy diet than for people with a vegetarian diet. That is, the slope of the regression line was steeper for the meat heavy diet than for the vegetarian diet. Put another way, the regression lines are not parallel.\nParallel = no interaction: Parallel regression lines are evidence of no interaction. This means that the effect of one variable (exercise) is the same for all levels of another variable (diet).\nNon-parallel = interaction: When the regression lines are not parallel, there is evidence of an interaction. This means that the effect of one variable (exercise) depends on the level of another variable (diet).\nThat was an example of an interaction between a continuous explanatory variable (exercise) and a categorical explanatory variable (diet). Interactions can also occur between two categorical explanatory variables, or between two continuous explanatory variables. Let us look at some more examples.\nThink‚ÄìPair‚ÄìShare (#a_when_lines_cross) What does it mean biologically when lines cross in an interaction plot? What does it mean when they are parallel?",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#two-cats",
    "href": "7.1-interactions.html#two-cats",
    "title": "Interactions (L7)",
    "section": "Two cats",
    "text": "Two cats\nTwo categorical explanatory variables: diet (meat heavy or vegetarian) and exercise (low or high), and one continuous response variable (blood pressure).\nRead in the dataset:\n\nbp_2cat &lt;- read.csv(\"datasets/bp_2cat.csv\")\n\nHere are the first few rows of the data:\n\nhead(bp_2cat)\n\n        diet exercise reps        bp\n1 meat heavy     high    1  83.73546\n2 meat heavy      low    1 115.11781\n3 vegetarian     high    1  74.18977\n4 vegetarian      low    1 108.58680\n5 meat heavy     high    2  91.83643\n6 meat heavy      low    2 103.89843\n\n\nAnd here is the data in a graph, where we show the individual data points as well as the group means connected by lines. The lines connecting the means are only to help visualize whether there is an interaction or not. If we see non-parallel lines connecting the means, we have evidence of an interaction.\n\n\n\n\n\n\n\n\n\nIndeed, the lines connecting the means are not parallel, which is evidence of an interaction between diet and exercise on blood pressure.\nThink-Pair-Share (#tps-express-int) Express the nature of the interaction in words.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#two-continuous",
    "href": "7.1-interactions.html#two-continuous",
    "title": "Interactions (L7)",
    "section": "Two continuous",
    "text": "Two continuous\nTwo continuous explanatory variables (age and exercise minutes) and one continuous response variable (blood pressure).\nRead in the dataset:\n\nbp_2cont &lt;- read.csv(\"datasets/bp_2cont.csv\")\n\nHere is the first few rows of the data:\n\nhead(bp_2cont)\n\n         bp      age mins_per_week\n1  61.58319 35.93052     130.94479\n2  83.86716 42.32743      70.63945\n3  93.01438 54.37120      54.05203\n4  82.14413 74.49247     198.53681\n5  60.13102 32.10092     126.69865\n6 102.00306 73.90338      42.64163\n\n\nNow we have a little challenge, namely that we have two continuous explanatory variables. This means that we need to use three dimensions to visualize the data. Here is a standard 2D scatter plot of blood pressure against minutes of exercise, though with age represented by color grading from young (dark blue) to old (yellow):\n\n\n\n\n\n\n\n\n\nAnd we can make the complementary plot of blood pressure against age, with minutes of exercise represented by color grading from low (green) to high (orange):\n\n\n\n\n\n\n\n\n\nWhat do we see? If we look at a set of points of the similar colour (i.e., similar number of minutes of exercised), we can see that the slope of the relationship between blood pressure and age depends on exercise. The slope is steeper for people that exercise more.\nThink-Pair-Share (#tps-small-older) Another thing we could say is that the effect of exercise is smaller for older people. How is this shown in the graph?\nYet another way to visualise this interaction is to create categorical versions of age and minutes of exercise, and then plot the data with these categorical variables:\nHere is the first few rows of the modified data:\n\nhead(bp_2cont)\n\n         bp      age mins_per_week age_class mins_per_week_class\n1  61.58319 35.93052     130.94479     30-40             120-140\n2  83.86716 42.32743      70.63945     40-50               60-80\n3  93.01438 54.37120      54.05203     50-60               40-60\n4  82.14413 74.49247     198.53681     70-80             180-200\n5  60.13102 32.10092     126.69865     30-40             120-140\n6 102.00306 73.90338      42.64163     70-80               40-60\n\n\nAnd here is the plot of blood pressure against minutes of exercise, with age class represented by colour. Regression lines have been added, to help visualize any interaction:\n\n\n\n\n\n\n\n\n\nIt is as we saw in the previous version of the graph. The slope of the relationship between blood pressure and minutes of exercise depends on age. The slope is shallower for older people.\nIn the complementary graph of blood pressure against age, with minutes of exercise represented by colour, we see the same interaction:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is only one interaction We made two graphs of the same data, one with age as a categorical variable and one with minutes of exercise as a categorical variable. In both graphs we see an interaction. This is not us seeing two separate interactions, however. There is only one interaction here, namely that the effect of age on blood pressure depends on minutes of exercise, and equivalently, that the effect of minutes of exercise on blood pressure depends on age. The two graphs just look at the same interaction from different perspectives.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#other-perspectives",
    "href": "7.1-interactions.html#other-perspectives",
    "title": "Interactions (L7)",
    "section": "Other perspectives",
    "text": "Other perspectives\n\nInteractions and additivity of effects\nAnother way of thinking about interactions is from the perspective of additivity or non-additivity of effects. Imagine we made two separate studies, one of the effect of diet on blood pressure, and one of the effect of exercise on blood pressure. In the first study we only varied diet, and in the second study we only varied exercise. In the first study we found an effect size of diet on blood pressure of say 10 mmHg (e.g., difference between meat heavy and vegetarian). And in the second study we found an effect size of exercise on blood pressure of 15 mmHg (e.g., difference between low and high exercise).\nThink-Pair-Share (#tps-adding-effects) If the effects of diet and exercise were additive, what would we expect the effect size to be if we estimated the effect of diet and exercise on blood pressure in the same study?\nThink-Pair-Share (#tps-comb-non-add) What would we expect the effect size of combined diet and exercise to be if the effects were non-additive? (Hint: this is a bit of a trick question.)\nIf the effects are non-additive, we would expect the effect size to be different from additive. For example, if we found the combined effect of diet and exercise on blood pressure to be 40, we would say that the effects are non-additive. Their combined effect is more than the sum of their individual effects. This example is of a synergistic interaction because the combined effect (40) is greater than the sum of the individual effects (25).\n\n\nDrug interactions\nWhen a doctor is considering giving us a particular medication, we are asked if we are taking any other medications. This is because the effects of drugs can interact. For example, if we take two drugs that both lower blood pressure and they interfere with each other, the combined effect might be less than the sum of their individual effects. This is an antagonistic interaction. It could be worse than that though, the interaction might actually be harmful, which is why doctors are so careful about drug interactions.\nThink-Pair-Share (#tps-other-int-examples) Can you think of any other examples of interactions in biology or medicine?",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#the-maths-bit",
    "href": "7.1-interactions.html#the-maths-bit",
    "title": "Interactions (L7)",
    "section": "The maths bit",
    "text": "The maths bit\nLet us return to the example of the effects of number of minutes of exercise and diet on blood pressure:\n\n\n\n\n\n\n\n\n\nWe have one continuous explanatory variable (minutes of exercise) and one binary explanatory variable (diet) and one continuous response variable (blood pressure).\nThink-Pair-Share (#tps-without-interaction) What would a linear model without an interaction term be?\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\epsilon_i\\]\nwhere:\n\n\\(y_i\\) is the blood pressure of the \\(i\\)th participant\n$x_i^{(1)} is the number of minutes of exercise of the \\(i\\)th participant\n\\(x_i^{(2)}\\) is the diet of the \\(i\\)th participant\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the effect of exercise on blood pressure\n\\(\\beta_2\\) is the effect of diet on blood pressure\n\\(\\epsilon_i\\) is the error term for the \\(i\\)th participant.\n\nThis model is a multiple regression model in which one of the explanatory variables is binary.\nThink-Pair-Share (#tps-with-interaction) What might the model look like if we wanted to include an interaction between diet and exercise?\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\beta_3 (x_{i}^{(1)} x_{i}^{(2)}) + \\epsilon_i\\]\nwhere:\n\n\\(x_{i}^{(1)}x_{i}^{(2)}\\) is the product of the number of minutes of exercise and the diet of the \\(i\\)th participant.\n\\(\\beta_3\\) is the coefficient of the interaction term between diet and exercise.\n\nWe could also write this model as:\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\beta_3 x_{i}^{(3)} + \\epsilon_i\\]\nwhere:\n\n\\(x_{i}^{(3)} = x_{i}^{(1)} x_{i}^{(2)}\\)\n\nThis is again a multiple regression model, but now with three explanatory variables.\nThink-Pair-Share (#tps-sketch-interaction) Make sketches of the possible relationships between diet, exercise and blood pressure. Make a sketch compatible with \\(\\beta_3 = 0\\). Make a sketch compatible with \\(\\beta_3 \\neq 0\\).",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#hypothesis-testing",
    "href": "7.1-interactions.html#hypothesis-testing",
    "title": "Interactions (L7)",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nIf we want to test whether the effect of minutes of exercise on blood pressure is different for people with different diets, we need a null hypothesis to test.\nThink-Pair-Share (#tps-interaction-null) What is the null hypothesis in this case, verbally, and in terms of the coefficients of the model?\nThe null hypothesis is that the effect of minutes of exercise on blood pressure is the same for people with different diets. This is a null hypothesis of no interaction between diet and exercise. In terms of the coefficients of the model, the null hypothesis is that \\(\\beta_3 = 0\\).\nIf we reject the null hypothesis, we conclude that the effect of minutes of exercise on blood pressure is different for people with different diets. This is a non-additive effect.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#doing-it-in-r",
    "href": "7.1-interactions.html#doing-it-in-r",
    "title": "Interactions (L7)",
    "section": "Doing it in R",
    "text": "Doing it in R\nLet us fit the model with the interaction term in R. There are two methods to do this and they are equivalent:\n\nmod1 &lt;- lm(bp ~ mins_per_week + diet + mins_per_week:diet, data=bp_1cont1cat)\nmod2 &lt;- lm(bp ~ mins_per_week * diet, data=bp_1cont1cat)\n\nThe second is a shorthand for the first. The * operator includes the main effects (main effects are terms in the model that don‚Äôt include interactions) and the interaction term. The : operator includes only the interaction term.\nOf course, we check the model assumptions before we interpret the results:\n\npar(mfrow=c(2,2))\nplot(mod2, add.smooth=FALSE)\n\n\n\n\n\n\n\n\nAll of the plots look good. Now we can do hypothesis testing of the interaction term using an F-test:\n\nanova(mod2)\n\nAnalysis of Variance Table\n\nResponse: bp\n                   Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nmins_per_week       1 1133.6 1133.55 13.4800 0.0003964 ***\ndiet                1 1560.8 1560.75 18.5601 3.979e-05 ***\nmins_per_week:diet  1  340.4  340.36  4.0475 0.0470408 *  \nResiduals          96 8072.8   84.09                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the ANOVA table we see four rows. The first row is for the main effect of minutes of exercise, the second row is for the main effect of diet, the third row is for the interaction effect between diet and minutes of exercise, and the fourth row is for the residuals. As always, an interaction term in the R output is shown with a colon : between the two variables (here it looks like mins_per_week:diet).\nIn this example, the F-statistic for the interaction term is quite large (4.05), and the p-value is very small (0.047). This means that we reject the null hypothesis that there is no interaction between diet and minutes of exercise on blood pressure. We conclude that the effect of minutes of exercise on blood pressure is different for people with different diets.\nIf we like (and we don‚Äôt have to), we can look at the coefficients of the model:\n\nsummary(mod2)$coefficients\n\n                                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                  100.62716134 2.87228930 35.033783 1.714199e-56\nmins_per_week                 -0.09661054 0.02406014 -4.015376 1.177969e-04\ndietvegetarian               -15.27591304 4.09882726 -3.726898 3.275630e-04\nmins_per_week:dietvegetarian   0.06907583 0.03433487  2.011828 4.704076e-02\n\n\nAs expected, there are four coefficients.\nThe first is (Intercept), which is the expected blood pressure for a person who does 0 minutes of exercise and is on diet ‚Äúmeat heavy‚Äù.\nThe second is mins_per_week, which is the effect (slope) of minutes of exercise on blood pressure for a person on diet ‚Äúmeat heavy‚Äù.\nThe third is dietvegetarian, which is the effect of being on a vegetarian diet on blood pressure for a person who does 0 minutes of exercise. This can be thought of as the change in the intercept for a person on a vegetarian diet compared to a person on a ‚Äúmeat heavy‚Äù diet.\nThe fourth is the interaction term mins_per_week:dietvegetarian, which is the difference in the effect (slope) of minutes of exercise on blood pressure for a person on a vegetarian diet compared to a person on a ‚Äúmeat heavy‚Äù diet.\nThink-Pair-Share (#tps-two-equations) Write two equations, one for each of the two diets. They would look something like this: \\(y_i = 0.1 - 0.1 x_{i}^{(1)}\\), but will have other numbers.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#reporting-our-findings",
    "href": "7.1-interactions.html#reporting-our-findings",
    "title": "Interactions (L7)",
    "section": "Reporting our findings",
    "text": "Reporting our findings\nOf course a nice graph is always helpful. We already have quite a nice one:\n\n\n\n\n\n\n\n\n\nWe also might want some tables summarizing the model results. Here is a table of the coefficients:\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n100.6271613\n2.8722893\n35.033783\n0.0000000\n\n\nmins_per_week\n-0.0966105\n0.0240601\n-4.015377\n0.0001178\n\n\ndietvegetarian\n-15.2759130\n4.0988273\n-3.726898\n0.0003276\n\n\nmins_per_week:dietvegetarian\n0.0690758\n0.0343349\n2.011828\n0.0470408\n\n\n\n\n\nWe could also report the \\(R^2\\) of the model:\n\nsummary(mod2)$r.squared\n\n[1] 0.2732093\n\n\nAnd also a table of the variances of the terms in the model:\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nmins_per_week\n1\n1133.554\n1133.55446\n13.47998\n0.0003964\n\n\ndiet\n1\n1560.753\n1560.75255\n18.56012\n0.0000398\n\n\nmins_per_week:diet\n1\n340.357\n340.35702\n4.04745\n0.0470408\n\n\nResiduals\n96\n8072.804\n84.09171\nNA\nNA\n\n\n\n\n\nWe also might use a sentence like this to report the results: ‚ÄúThe effect of minutes of exercise is generally negative, but the effect is stronger for people on a meat heavy diet than for people on a vegetarian diet (\\(F\\)-statistics of interaction term = 4.05, degrees of freedom = 1, degrees of freedom residuals = 96, \\(p\\)-value = 0.047).‚Äù\nThink‚ÄìPair‚ÄìShare (#a_interaction_vs_main_effect) How does an interaction change the meaning of a ‚Äúmain effect‚Äù? Can a main effect be misleading when an interaction is present?",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#multiple-regression-vs.-many-single-regressions",
    "href": "7.1-interactions.html#multiple-regression-vs.-many-single-regressions",
    "title": "Interactions (L7)",
    "section": "Multiple regression vs.¬†many single regressions",
    "text": "Multiple regression vs.¬†many single regressions\nQuestion: Why not just fit a separate simple regression model and then test whether the slopes are the same (i.e., if they are parallel)? That is, why not fit the two models:\n\\[y_i = \\beta_{0,veg} + \\beta_{1,veg} x_i^{(1)} + \\epsilon_i\\]\n\\[y_i = \\beta_{0,meat} + \\beta_{1,meat} x_i^{(2)} + \\epsilon_i\\]\nand compare the estimate of \\(\\beta_{1,veg}\\) to the estimate of \\(\\beta_{1,meat}\\)?\nWell, you could do that, and could probably find a way to test for whether the difference in the slopes is different from 0. This would be a test of the null hypothesis that the effect of minutes of exercise on blood pressure is the same for people with different diets. But, this would be a more complicated way to do it, and would not be as general as the model with the interaction term. The model with the interaction term is more general, more flexible, and more elegant.\n\n\n\n\n\n\nNote\n\n\n\nIt is usually better model the whole dataset to test a single hypothesis, rather dividing up the dataset into smaller parts, fitting a model to each part, and then comparing the results of the models. The latter approach is less efficient and less elegant. One hypothesis = one model.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#ancova",
    "href": "7.1-interactions.html#ancova",
    "title": "Interactions (L7)",
    "section": "ANCOVA",
    "text": "ANCOVA\nThe example we just worked through was with one continuous explanatory variable (minutes of exercise) and one categorical explanatory variable (diet). This is an example of an analysis of covariance (ANCOVA). ANCOVA is a type of linear model in which there are both continuous and categorical explanatory variables.\nANCOVA is often used in two main ways.\nFirst, it can be used to account for covariates (continuous variables). In this case, the main interest is in comparing groups (as in ANOVA), while one or more continuous variables are included to explain additional variation in the response. These covariates are not the main focus of interpretation; instead, they help adjust group means and improve the precision of group comparisons.\nSecond, ANCOVA can be used to test whether covariate effects differ between groups. Here, the continuous variable is of real interest, and the question is whether the relationship between the covariate and the response is the same across groups. This is done by including a group √ó covariate interaction, which allows the slope of the relationship to differ between groups.\nAn important distinction is that the first use assumes the covariate has the same effect in all groups (parallel slopes), while the second explicitly tests whether this assumption is valid.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#two-way-anova",
    "href": "7.1-interactions.html#two-way-anova",
    "title": "Interactions (L7)",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\nAbove we had an example with two categorical explanatory variables (diet [levels: meat heavy or vegetarian] and exercise [levels: low or high]) and one continuous response variable (blood pressure). This is an example of a two-way ANOVA. Two-way ANOVA is used to test for effects of two categorical explanatory variables, as well as their interaction effect on a continuous response variable.\nHere are the first few rows of the data again:\n\n\n        diet exercise reps        bp\n1 meat heavy     high    1  83.73546\n2 meat heavy      low    1 115.11781\n3 vegetarian     high    1  74.18977\n4 vegetarian      low    1 108.58680\n5 meat heavy     high    2  91.83643\n6 meat heavy      low    2 103.89843\n\n\nHere is the graph of the data again:\n\n\n\n\n\n\n\n\n\nWe can fit a two-way ANOVA model in R as follows:\n\nmod_2cat &lt;- lm(bp ~ diet * exercise, data=bp_2cat)\n\nRecall that this fits a model with main effects of diet and exercise, as well as their interaction effect. Recall that we could specify the model equivalently as:\n\nmod_2cat &lt;- lm(bp ~ diet + exercise + diet:exercise, data=bp_2cat)\n\nWe can check the model assumptions:\n\npar(mfrow=c(2,2))\nplot(mod_2cat, add.smooth=FALSE)\n\n\n\n\n\n\n\n\nThese look ok.\nHypothesis testing the interaction is just as before. We use an F-test on the interaction term to test the null hypothesis that there is no interaction between diet and exercise on blood pressure.\n\nanova(mod_2cat)\n\nAnalysis of Variance Table\n\nResponse: bp\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet           1 2879.8  2879.8  34.695 9.741e-07 ***\nexercise       1 4776.5  4776.5  57.546 5.680e-09 ***\ndiet:exercise  1 1142.5  1142.5  13.765  0.000696 ***\nResiduals     36 2988.1    83.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the ANOVA table we see four rows. The first row is for the main effect of diet, the second row is for the main effect of exercise, the third row is for the interaction effect between diet and exercise, and the fourth row is for the residuals. As always, an interaction term in the R output is shown with a colon : between the two variables (here it looks like diet:exercise).\nIn this example, the F-statistic for the interaction term is quite large (13.76 and the corresponding p-value is quite small (7^{-4}. Hence, we conclude that there is a strong evidence of an interaction between diet and exercise on blood pressure. The effect of exercise on blood pressure depends on diet.\nThink-Pair-Share (#tps-two-way-report) Write a sentence reporting the results of the two-way ANOVA. Focus on the biological interpretation of the results, rather than the statistical details. Include statistics in parentheses, in support of your statements.\n\n\n\n\n\n\nImportant\n\n\n\nReporting two-way ANOVA results When reporting the results of a two-way ANOVA, it is important to focus on the biological interpretation of the results, rather than the statistical details. Include statistics in parentheses, in support of your statements. For example, you might say something like: ‚ÄúThe effect of exercise was different for people on different diets, with a stronger effect for those on a vegetarian diet compared to those on a meat heavy diet (F(1, 36) = 13.77, p &lt; 0.001).‚Äù\n\n\n\nMore than two levels\nWhat if we had a categorical explanatory variable with more than two levels? For example, what if diet had three levels: meat heavy, vegetarian, and vegan?\nHere is an example dataset.\nRead in the dataset:\n\nbp_2cat_3levels &lt;- read.csv(\"datasets/bp_3cat.csv\")\n\nHere are the first few rows of the data:\n\nhead(bp_2cat_3levels)\n\n        diet exercise reps        bp\n1 meat heavy     high    1  83.73546\n2 meat heavy      low    1 115.11781\n3      vegan     high    1  59.18977\n4      vegan      low    1  98.58680\n5 vegetarian     high    1  68.35476\n6 vegetarian      low    1  98.98106\n\n\nHere is the graph of the data:\n\n\n\n\n\n\n\n\n\nOr plotted differently:\n\n\n\n\n\n\n\n\n\nThe hypothesis testing is the same as before. We fit the model with interaction term:\n\nmod_2cat_3levels &lt;- lm(bp ~ diet * exercise, data=bp_2cat_3levels)\n\nWe check the model assumptions:\n\npar(mfrow=c(2,2))\nplot(mod_2cat_3levels, add.smooth=FALSE)\n\n\n\n\n\n\n\n\nThese look ok.\nNow we do the ANOVA:\n\nanova(mod_2cat_3levels)\n\nAnalysis of Variance Table\n\nResponse: bp\n              Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet           2 8724.1  4362.1  55.636 7.644e-14 ***\nexercise       1 9078.3  9078.3 115.789 4.791e-15 ***\ndiet:exercise  2 1741.3   870.6  11.104 9.130e-05 ***\nResiduals     54 4233.8    78.4                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nImportant: Although we have a categorical variable with three rather than two levels, we still have only four rows in the ANOVA table. One for each main effect (diet and exercise), one for the interaction effect (diet:exercise), and one for the residuals. This is because the ANOVA table tests each effect as a whole, rather than testing each level of the categorical variable separately.\nIn this case, we see that there is strong evidence of main effects of diet and exercise on blood pressure, as well as strong evidence of an interaction effect between diet and exercise on blood pressure.\nReporting the patterns and statistics is similar to before, but now we have more levels to consider so the reporting is a bit more complex, and we have to be careful to not over-interpret the results. For example, when the hypothesis test is on the interaction term via an F-test, we can only say that there is evidence of an interaction between diet and exercise on blood pressure. We cannot say which diets have different effects of exercise on blood pressure. To do that, we would need to do post-hoc tests, such as pairwise comparisons between the levels of diet within each level of exercise.\n\n\n\n\n\n\nCaution\n\n\n\nDegrees of freedom Look at the ANOVA table and the degrees of freedom column. For diet, the degrees of freedom is 2, because there are three levels of diet (meat heavy, vegetarian, vegan), and the degrees of freedom is number of levels minus 1. For exercise, the degrees of freedom is 1, because there are two levels of exercise (low, high). For the interaction term diet:exercise, the degrees of freedom is 2, which is the product of the degrees of freedom for diet (2) and exercise (1).\nAnother way to think about this is how many parameters are estimated? The answer is as follows: six parameters are estimated in total, one for each of the combinations of diet and exercise (3 diets x 2 exercises = 6 combinations). Hence the residual degrees of freedom is total number of observations (60) minus 6.\nDon‚Äôt worry if this is a bit confusing at first. It will become clearer with practice, and you can ask for it to be explained again and in different ways.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#multiple-regression-with-interaction-term",
    "href": "7.1-interactions.html#multiple-regression-with-interaction-term",
    "title": "Interactions (L7)",
    "section": "Multiple regression with interaction term",
    "text": "Multiple regression with interaction term\nAbove we had the example of two continuous explanatory variables (age and minutes of exercise) and one continuous response variable (blood pressure). We saw evidence of an interaction between age and minutes of exercise on blood pressure.\nHere are the first few rows of the data again:\n\n\n         bp      age mins_per_week age_class mins_per_week_class\n1  61.58319 35.93052     130.94479     30-40             120-140\n2  83.86716 42.32743      70.63945     40-50               60-80\n3  93.01438 54.37120      54.05203     50-60               40-60\n4  82.14413 74.49247     198.53681     70-80             180-200\n5  60.13102 32.10092     126.69865     30-40             120-140\n6 102.00306 73.90338      42.64163     70-80               40-60\n\n\nHere is the data in a graph, with age represented by a colour gradient:\n\n\n\n\n\n\n\n\n\nWe can fit a multiple regression model with an interaction term in R as follows:\n\nmod_2cont &lt;- lm(bp ~ mins_per_week * age, data=bp_2cont)\n\nWe check the model assumptions:\n\npar(mfrow=c(2,2))\nplot(mod_2cont, add.smooth=FALSE)\n\n\n\n\n\n\n\n\nThese look ok.\nNow we do the F-test for the interaction term:\n\nanova(mod_2cont)\n\nAnalysis of Variance Table\n\nResponse: bp\n                  Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nmins_per_week      1 14307.2 14307.2 1534142 &lt; 2.2e-16 ***\nage                1 10220.0 10220.0 1095879 &lt; 2.2e-16 ***\nmins_per_week:age  1  1764.8  1764.8  189242 &lt; 2.2e-16 ***\nResiduals         96     0.9     0.0                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt looks the same as in the two-way ANOVA case! This is because we still have two variables and their interaction, so the ANOVA table has one row for each main effect (mins_per_week and age), one row for the interaction effect (mins_per_week:age), and one row for the residuals.\nThe F-statistic for the interaction term is very large (1.8924178^{5}) and the corresponding p-value is very small (5.6^{-160}). Hence, we conclude that there is very strong evidence of an interaction between minutes of exercise and age on blood pressure. The effect of minutes of exercise on blood pressure depends on age, with a stronger effect for younger people compared to older people.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#intepreting-main-effects-and-interaction-effects",
    "href": "7.1-interactions.html#intepreting-main-effects-and-interaction-effects",
    "title": "Interactions (L7)",
    "section": "Intepreting main effects and interaction effects",
    "text": "Intepreting main effects and interaction effects\nThe term main effect refers to the individual effect of each categorical explanatory variable on the response variable, ignoring the other variable. For example, the main effect of diet would be the difference in blood pressure between meat heavy and vegetarian diets, averaged over both levels of exercise.\nHowever, if there is an interaction between the two categorical explanatory variables, the main effects may not fully capture the relationship. The interaction effect indicates that the effect of one categorical variable on the response variable depends on the level of the other categorical variable. For example, the effect of diet on blood pressure may differ between low and high exercise groups.\nWe can see this in the example:\n\n\n\n\n\n\n\n\n\nThere is a main effect of diet on blood pressure, as well as a main effect of exercise on blood pressure. However, there is also an interaction effect between diet and exercise on blood pressure, as the effect of diet on blood pressure depends on the level of exercise.\nIt could look otherwise. For example, if we found a (albeit rather unlikely pattern) of higher blood pressure for vegetarians how exercise little, and lower blood pressure for vegetarians who exercise a lot, we would have a pattern as follows:\n\n\n\n\n\n\n\n\n\nHere there is no main effect of diet on blood pressure, as the average blood pressure for meat heavy and vegetarian diets is the same when averaged over both levels of exercise. There is still a main effect of exercise on blood pressure, as blood pressure is lower for high exercise compared to low exercise. And there is still an interaction effect between diet and exercise on blood pressure, as the effect of diet on blood pressure depends on the level of exercise.\nTake home: Interactions are very interesting, but also will require careful and nuanced interpretation.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#more-than-two-explanatory-variables",
    "href": "7.1-interactions.html#more-than-two-explanatory-variables",
    "title": "Interactions (L7)",
    "section": "More than two explanatory variables",
    "text": "More than two explanatory variables\nAll the examples above had two explanatory variables. What if we have more than two explanatory variables? The principles are the same, but the models and interpretations become more complex. For example, if we have three categorical explanatory variables (A, B, C), we can fit a model with main effects of A, B, and C, as well as all possible interaction effects (A:B, A:C, B:C, A:B:C). The ANOVA table will have one row for each main effect, one row for each two-way interaction effect, one row for the three-way interaction effect, and one row for the residuals.\nInterpretation of three-way interactions can be quite complex, as it involves understanding how the effect of one variable on the response variable depends on the levels of the other two categorical variables. Very careful consideration and visualization of the data are often necessary to fully understand and communicate the results.",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#review",
    "href": "7.1-interactions.html#review",
    "title": "Interactions (L7)",
    "section": "Review",
    "text": "Review\n\nInteractions are some of the most interesting effects in biology and medicine. They occur when the effect of one explanatory variable on the response variable depends on the level of another explanatory variable.\nInteractions can occur between continuous and categorical explanatory variables, or between two categorical explanatory variables, or between two continuous explanatory variables.\nVisualization is key to understanding interactions. Use graphs to explore and communicate interactions.\nHypothesis testing for interactions is done using \\(F\\)-tests on the interaction terms in linear models.\nThe degrees of freedom for interaction terms depend on the levels of the categorical variables involved. Each categorical variable takes degrees of freedom equal to the number of levels minus one. Each continuous variable takes one degree of freedom. The degrees of freedom for the interaction term is the product of the degrees of freedom of the individual variables.\nReport interactions carefully, focusing on the biological interpretation and including relevant statistics (i.e., the \\(F\\)-statistic, degrees of freedom for the interaction term, degrees of freedom for error, and p-value).",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#further-reading",
    "href": "7.1-interactions.html#further-reading",
    "title": "Interactions (L7)",
    "section": "Further reading",
    "text": "Further reading\n\nWikipedia is always a good place to start for understanding concepts: Interaction term (statistics)\nThis is a nice paper for clinicians on understanding interactions: How to interact with interactions: what clinicians should know about statistical interactions\nIf you want a challenge, and to see how nuanced interactions and interpretation can be, have a look at this paper Interactions in statistical models: Three things to know",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "7.1-interactions.html#extras",
    "href": "7.1-interactions.html#extras",
    "title": "Interactions (L7)",
    "section": "Extras",
    "text": "Extras\n\nDegrees of freedom\nIt can be useful to think about degrees of freedom in terms of how many parameters / coefficients have to be estimated by the model.\nIn one-way ANOVA, this is the number of groups one has. So if you have a categorical variable with five categories (in another language one would say a factor variable with five levels), there will be five means estimated, and so the model uses five degrees of freedom. So error degrees of freedom will be the total number of observation minus five.\nIn two-way ANOVA (which includes the interaction of the two categorical variables) we have to estimate a mean for each of the combinations of the categories. So if we have one categorical variable with three levels and another with four, there are 3*4 combinations, so 12 means to estimate. So the model takes 12 degrees of freedom, and the error degrees of freedom will be the number of observations minus 12.\nIf we made a two-way ANOVA without an interaction (i.e.¬†we included only the main effects) between the two explanatory variables, and we could have (i.e.¬†we had performed a fully factorial experiment), well, this is just weird. Why would we not do the analysis we planned when we designed the experiment?\n(I raised that in case anyone wants to know how to calculate degrees of freedom with only main effects. In that case, the two explanatory variables share the same reference level (intercept) so in the case above, only 6 things would need to be estimated: A shared intercept, and the other five means. Its a bit like when we previously made a regression model with two regression lines, both with the same slope. They shared the same slope ‚Äì so only one needed to be estimated. So often when we use only main effects, getting the degrees of freedom is a bit tricky.)\nBottom line: more explanatory variables, more interactions, more levels in categorical explanatory variables = more things have to be estimated = fewer degrees of freedom for error. And few degrees of freedom for error is not good, generally speaking.\n\n\n3d scatter plot\nHere is a 3D scatter plot of the data with two continuous explanatory variables (age and minutes of exercise) and one continuous response variable (blood pressure). This plot helps to visualise the interaction between age and minutes of exercise on blood pressure.\n(Please note that this plot is best viewed in the HTML version of the book; in the PDF version, it will appear as a static image.)\n\nplotly::plot_ly(bp_2cont, x = ~mins_per_week, y = ~age, z = ~bp, type = \"scatter3d\", mode = \"markers\", color = age, width=500, height=500) %&gt;%\n  plotly::layout(title = \"3D Scatter plot of Blood Pressure vs Exercise and Age\",\n         scene = list(xaxis = list(title = \"Minutes per week of exercise\"),\n                      yaxis = list(title = \"Age\"),\n                      zaxis = list(title = \"Blood Pressure\"))\n         )\n\n\n\n\n\n\n\nTypes of sums of squares\n\n\n\n\n\n\nImportant\n\n\n\nA note on anova() and ‚ÄúType I‚Äù (sequential) sums of squares\nIn this course we often use anova() to test terms in a linear model. In base R, anova() for an lm uses Type I (sequential) sums of squares. That means the test for each term is done in the order the terms appear in the model formula: each term is tested after the terms before it have already been included.\nIn balanced designs (equal sample sizes in each group/combination), the order usually does not matter much. But in unbalanced designs (unequal sample sizes), the p-value for a ‚Äú‚Äúmain effect‚Äù or an interaction can change if you change the order of terms in the model formula.\nThis is related to collinearity between explanatory variables. When explanatory variables are correlated, the variance they explain in the response variable overlaps. In that case, the order of terms matters because earlier terms get to ‚Äúclaim‚Äù more of the shared variance.\nSo: when you see an ANOVA table from anova(), remember that it is testing terms sequentially, not ‚Äúall at once‚Äù.\n\n\n\n\nBox and whisker plot for two cats\nOften you will see box and whisker plots used to visualise data with two categorical explanatory variables. Here is how to make such a plot in R:\n\nggplot(bp_2cat, aes(x=diet, y=bp, fill=exercise)) + \n  geom_boxplot(position=position_dodge(width=0.8)) +\n  labs(title=\"Blood pressure vs diet and exercise\",\n       x=\"Diet\",\n       y=\"Blood pressure\",\n       fill=\"Exercise\")\n\n\n\n\n\n\n\n\nI (Owen) prefer to show the individual data points whenever possible, so here is a box and whisker plot with the individual data points overlaid:\n\nggplot(bp_2cat, aes(x=diet, y=bp, fill=exercise)) + \n  geom_boxplot(position=position_dodge(width=0.8), alpha = 0.5) +\n  geom_jitter(aes(col=exercise), position=position_jitterdodge(jitter.width=0.2, dodge.width=0.8), size=2, alpha = 0.7) +\n  labs(title=\"Blood pressure vs diet and exercise\",\n       x=\"Diet\",\n       y=\"Blood pressure\",\n       fill=\"Exercise\",\n       col=\"Exercise\")\n\n\n\n\n\n\n\n\nI, and others, prefer to not use bar plots with error bars to visualise data with categorical explanatory variables. Bar plots hide the individual data points and can be misleading. Box and whisker plots are better, but still hide some of the data. Scatter plots with jittered points are often the best way to visualise such data.\nIn case you must use bar plots with error bars, here is how to make one in R:\n\ngrouped_data &lt;- group_by(bp_2cat, diet, exercise) %&gt;%\n  summarise(mean_bp = mean(bp), sd_bp = sd(bp), n = n(), se_bp = sd_bp / sqrt(n), .groups = \"drop\")\nggplot(grouped_data, aes(x=diet, y=mean_bp, fill=exercise)) + \n  geom_bar(stat=\"identity\", position=position_dodge(width=0.8), alpha = 0.5) +\n  geom_errorbar(aes(ymin=mean_bp - se_bp, ymax=mean_bp + se_bp), position=position_dodge(width=0.8), width=0.2) +\n  labs(title=\"Blood pressure vs diet and exercise\",\n       x=\"Diet\",\n       y=\"Blood pressure\",\n       fill=\"Exercise\")",
    "crumbs": [
      "Interactions (L7)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html",
    "href": "8.1-GLM1-count-data.html",
    "title": "Count data (L8)",
    "section": "",
    "text": "Introduction\nIn all the previous chapters, we have focused on linear models for continuous response variables. However, many biological response variables are counts (e.g., number of offspring, number of parasites). In this chapter, we introduce Generalized Linear Models (GLMs) for analysing count data, focusing on the Poisson distribution.\nSo far in BIO144, we have focused on linear models fitted using lm(). These models assume:\nLinear models are powerful, but these assumptions are often violated in biological data.\nIn this chapter we move beyond normal linear models to handle an important new type of response variable: counts.\nWe introduce Generalized Linear Models (GLMs), which extend linear models by allowing the response variable to follow distributions other than the normal distribution.\nBy the end of this chapter, you should be able to:\nThink‚ÄìPair‚ÄìShare (#tps-non-neg-kinds) What kinds of biological outcomes (response variables) can you think of that cannot be negative or continuous?",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#introduction",
    "href": "8.1-GLM1-count-data.html#introduction",
    "title": "Count data (L8)",
    "section": "",
    "text": "A continuous response variable\nNormally distributed residuals\nConstant variance (homoscedasticity)\n\n\n\n\n\n\nRecognise when linear regression is inappropriate\nUnderstand the core components of a GLM\nFit and interpret Poisson regression models\nDiagnose common problems such as overdispersion and zero inflation\n\n\n\nFrom LM to GLM\nLinear models (LM) describe the mean of a response variable as a linear function of explanatory variables. For example:\n\\(y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)} + \\epsilon_i\\)\nwhere the error term \\(\\epsilon_i\\) is normally distributed with mean 0 and constant variance. That is:\n\\(\\epsilon_i \\sim N(0, \\sigma^2)\\).\nThis works well when the response variable is continuous and approximately normally distributed.\nHowever, many biological response variables are:\n\nCounts (e.g.¬†offspring, parasites, species)\nBinary responses (e.g.¬†alive/dead)\nProportions\n\nIn these cases, forcing the data into a linear model often leads to invalid predictions and misleading inference.\nGeneralized Linear Models (GLMs) solve this problem by:\n\nKeeping the familiar linear predictor, but\nAllowing different distributions for the response, and\nLinking the predictor to the mean response using a link function.\n\n\n\n\n\n\n\nImportant\n\n\n\nKey idea\nGLMs are not a replacement for linear models ‚Äî they are a generalisation of them. Linear regression is a special case of a GLM.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-which-reg-assump) Which assumption of linear regression do you think is most problematic for count data?\n\n\nCount data\nCount data occur frequently in biology and medicine. Typical examples include:\n\nNumbers of animals, plants, or species\nNumbers of offspring\nNumbers of pathological structures (e.g.¬†polyps)\n\nCount data have four key properties:\n\nThey are discrete\nThey are non-negative\nTheir variance often increases with the mean\nThere is no known upper limit to the count.\n\nThese properties immediately suggest that standard linear regression may be inappropriate.\nThink‚ÄìPair‚ÄìShare (#tps-which-props-viol) Which of these properties is violated if we use a normal distribution for counts?",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#example-soay-sheep",
    "href": "8.1-GLM1-count-data.html#example-soay-sheep",
    "title": "Count data (L8)",
    "section": "Example: Soay sheep",
    "text": "Example: Soay sheep\nA feral population of Soay sheep on the island of Hirta (Scotland) has been studied extensively. Ecologists were interested in whether the body mass of female sheep influences their fitness, measured as lifetime reproductive success (number of offspring produced over a lifetime).\nQuestion: Are heavier females fitter than lighter females?\nRead in an example dataset:\n\nsoay &lt;- read.csv(\"datasets/soay_sheep.csv\")\n\nHere are the first few rows of the dataset:\n\nhead(soay)\n\n  body.size fitness\n1  53.99529      12\n2  32.69467       0\n3  33.55580       2\n4  43.20078       2\n5  43.34102       4\n6  59.52711       7\n\n\nAs always, we start by exploring the data visually:\n\n\n\n\n\n\n\n\n\nThink‚ÄìPair‚ÄìShare (#tps-what-suspicious) What features of this plot might already make you suspicious about using linear regression?\n\nThe wrong analysis\nA common mistake is to analyse count data using linear regression, treating counts as if they were continuous.\n\nmod_soay_lm &lt;- lm(fitness ~ body.size, data = soay)\n\nThe model checking plots show clear violations of linear regression assumptions:\n\npar(mfrow = c(2, 2))\nplot(mod_soay_lm, which = 1:4, add.smooth = TRUE)\n\n\n\n\n\n\n\n\nThe qq-plot looks fine. The scale-location plot shows that variance increases with fitted values, violating homoscedasticity. Also, there is a clear non-linear pattern in the residuals vs fitted plot, suggesting that the linear model is not capturing the relationship well.\nAdding a quadratic term improves the fit slightly:\n\nmod2_soay_lm &lt;- lm(fitness ~ body.size + I(body.size^2), data = soay)\npar(mfrow = c(2, 2))\nplot(mod2_soay_lm, which = 1:4, add.smooth = TRUE)\n\n\n\n\n\n\n\n\nBut problems remain: variance increases with fitted values.\n\n\n\n\n\n\nCaution\n\n\n\nWhy this matters\nViolating model assumptions can lead to biased estimates, incorrect standard errors, and misleading p-values.\n\n\nAnother issue remains, however: linear regression can predict negative counts, which are impossible. We can see this in a plot of the data and the fitted regression line:\n\n\n\n\n\n\n\n\n\nNotice that for small body sizes the fitted line goes below 0 on the y-axis. That is, the moe predicts negative fitness values, which are biologically impossible.\n\n\n\n\n\n\nCaution\n\n\n\nWe have extrapolated beyond the data range here, largely for illustrative purposes. In practice we should be very cautious about extrapolating beyond the range of observed data. This is because the model may not hold outside the observed range, leading to nonsensical predictions.\n\n\n\n\nWhy linear regression fails for count data\n\nThe normal distribution is for continuous variables\nIt allows negative values\nIt assumes constant variance\nCount data are discrete, non-negative, and typically heteroscedastic\n\nThink‚ÄìPair‚ÄìShare (#a_why_poisson) Why is a normal distribution a poor choice for count data? Which assumptions can be expected to fail?",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#poisson-glm",
    "href": "8.1-GLM1-count-data.html#poisson-glm",
    "title": "Count data (L8)",
    "section": "Poisson GLM",
    "text": "Poisson GLM\nTo deal with count data, we need a different probability (error) model. A common probability model for counts is the Poisson distribution. The Poisson distribution is often the default starting point for modelling counts because it is the simplest distribution that respects discreteness, non-negativity, and increasing variance.\nThe Poisson distribution has the following probability mass function:\n\\(P(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\\)\nwhere:\n\n\\(y = 0, 1, 2, \\ldots\\)\n\\(\\lambda &gt; 0\\) is the mean (and variance) of the distribution\n\nHere are a couple of Poisson distributions with different means:\n\n\n\n\n\n\n\n\n\nThe Poisson distribution has two important properties:\n\nIt is defined only for non-negative integers (0, 1, 2, ‚Ä¶)\nThe mean and variance are equal.\n\n\n\n\n\n\n\nImportant\n\n\n\nMean‚Äìvariance relationship\nIn a Poisson distribution, the mean and variance are equal. This captures an important feature of count data, but it will later lead to the concept of overdispersion.\n\n\nThink‚ÄìPair‚ÄìShare (#a_mean_variance_link) Why does increasing variance with the mean cause problems for linear regression?\n\nGeneralized Linear Models (GLMs)\nGLMs extend linear models by combining three components:\n\nLinear predictor.\nLink function.\nProbability distribution (family).\n\n\n\n\n\n\n\nNote\n\n\n\nIf you choose a normal family and an identity link, a GLM is mathematically identical to the linear model you fit using lm().\n\n\n\nThe linear predictor\nThe linear predictor is the same as in linear regression:\n\\(\\eta_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)\nIt is just the linear combination of explanatory variables and coefficients. Nothing new here.\n\n\nThe link function\nIn the linear models we already used (e.g., for linear regression) the link function is the identity link:\n\\(E(y_i) = \\eta_i\\)\nHere, \\(E(y_i)\\) refers to the expected value (mean) of many hypothetical observations, not the single observed value \\(y_i\\).\nIdentity link: the expected value of the response \\(E(y_i)\\) equals the linear predictor \\(\\eta_i\\).\nThat is:\n\\(E(y_i) = \\eta_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)\nFor count data, this can lead to negative predictions.\nThe solution is a different link function. For Poisson regression, the standard choice is the log link. It relates the linear predictor to the expected value of the response as follows:\n\\(\\eta_i = \\log(E(y_i))\\)\nwhich implies:\n\\(E(y_i) = \\exp(\\eta_i) &gt; 0\\)\nThe log link ensures that the expected count is always positive, regardless of the values of the explanatory variables. Whatever the value of the linear predictor \\(\\eta_i\\), the exponential of it \\(\\exp(\\eta_i)\\) is always positive.\n\nA good link function enforces the natural constraints of the data.\n\nThink‚ÄìPair‚ÄìShare (#tps-what-wrong-id) What would go wrong if we used an identity link for count data?\n\n\nThe probability distribution (family)\nThe final component of a GLM is the probability distribution (also called the family). This specifies how the response variable is distributed. In Poisson regression, we assume that the response variable follows a Poisson distribution. Hence, we say we are fitting a Poisson GLM.\nMathematically, a Poisson GLM can be summarised as:\n\\(y_i \\sim \\text{Poisson}(\\lambda_i)\\)\nAnd written out fully with the linear predictor and link function:\n\\(y_i \\sim \\text{Poisson}(\\lambda_i)\\)\nwhere \\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)\nor equivalently:\n\\(E(y_i) \\sim \\text{Poisson}(\\exp(\\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}))\\)",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#r---poisson-glm",
    "href": "8.1-GLM1-count-data.html#r---poisson-glm",
    "title": "Count data (L8)",
    "section": "R - Poisson GLM",
    "text": "R - Poisson GLM\nWhen fitting a Poisson GLM in R, we use the glm() function, specifying the family as poisson:\n\nsoay_glm &lt;- glm(fitness ~ body.size, data = soay, family = poisson)\n\nSpecifying family = poisson tells R to use the Poisson distribution with a log link function by default. We could specify the link explicitly as family = poisson(link = \"log\"), but this is unnecessary since the log link is the default for the Poisson family.\n\nChecking model assumptions\nAs always, we should check model assumptions:\n\npar(mfrow = c(2, 2))\nplot(soay_glm, which = 1:4, add.smooth = TRUE)\n\n\n\n\n\n\n\n\nThe QQ-plot is rather concerning. However, QQ-plots in GLMs are not testing normality of residuals in the same way as for linear models, so their interpretation differs. The deviance residuals should approximately follow a normal distribution if the model fits well. Here, there are some deviations from normality, which could be somewhat concerning, but for now we will focus on the other plots. (If you‚Äôre interested in what are deviance residuals, please see the Extras section at the end of this chapter.)\nThe other plots look much better than for the linear model. The residuals vs fitted plot shows no obvious pattern, and the scale-location plot shows more constant variance.\n\n\n\n\n\n\nImportant\n\n\n\nYou must specify the family in glm(). If you omit the link function, R uses the default link for that family.\n\n\n\n\nInterpreting coefficients\n\nsummary(soay_glm)\n\n\nCall:\nglm(formula = fitness ~ body.size, family = poisson, data = soay)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.253092   0.210186  -5.962 2.49e-09 ***\nbody.size    0.053781   0.003735  14.400  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 333.105  on 99  degrees of freedom\nResidual deviance:  97.783  on 98  degrees of freedom\nAIC: 378.16\n\nNumber of Fisher Scoring iterations: 5\n\n\nCoefficients are estimated on the log scale.\n\n\n\n\n\n\nImportant\n\n\n\nA one-unit increase in an explanatory variable multiplies the expected count by \\(\\exp(\\beta)\\).\n\n\nThink‚ÄìPair‚ÄìShare (#tps-what-change-beta) If \\(\\beta=0.05\\) what change in the expected count would be caused by an increase in body mass of 1 kg? As well as a numeric value, specify whether it is used additively or multiplicatively.\nTo interpret the coefficient for body size (\\(\\beta_1 = 0.05\\)), we exponentiate it: \\(\\exp(0.05) \\approx 1.65\\)\nThis means that for each additional kilogram of body size, the expected count of fitness increases by a factor of approximately 1.65 (i.e., a 65% increase). We use the number multiplicatively because of the log link function.\nTo make a prediction of the expected count for a given body size, we back-transform the linear predictor:\n\\(E(y) = \\exp(\\beta_0 + \\beta_1 \\times \\text{body.size})\\)\nFor example, for a body size of 40 kg:\n\nbody_size_example &lt;- 40\nlinear_predictor &lt;- coef(soay_glm)[1] + coef(soay_glm)[2] * body_size_example\nexpected_count &lt;- exp(linear_predictor)\nexpected_count\n\n(Intercept) \n   2.455011 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough counts can only be integers, the expected value from a Poisson model can be any positive real number. This is because the expected value is a mean over many possible counts.\n\n\n\n\nAnalysis of deviance\nThere is something technically different that we have glossed over until now: how model fit is assessed. In linear regression we estimate parameters by minimizing the sum of squared residuals. In GLMs, we use maximum likelihood estimation (MLE). In this course we will not go into the mathematical details of MLE, but the key idea is that we find the parameter values that make the observed data most probable under the assumed model (just like in least squares). Instead of minimising sums of squares, we maximise the likelihood of the data given the model. Maximising the likelihood is equivalent to minimising the deviance, which is a measure of model fit based on likelihoods. Hence, when we fit a GLM in R, we get output including the deviance (and not sums of squares):\n\nanova(soay_glm, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: fitness\n\nTerms added sequentially (first to last)\n\n          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                         99     333.11              \nbody.size  1   235.32        98      97.78 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the output we see the deviance for the null model and the fitted model, as well as the change in deviance when adding explanatory variables. We can use a chi-squared test to assess whether adding explanatory variables significantly improves model fit.\nWe use a chi-squared test here because, under certain regularity conditions, the change in deviance between nested models follows a chi-squared distribution with degrees of freedom equal to the difference in the number of parameters. If you are interested in what this means, you can read more about it in more advanced statistics textbooks.\nYou may be wondering if we can calculate an r-squared value for GLMs. While there is no direct equivalent of r-squared for GLMs, several pseudo-r-squared measures have been proposed. However, these are not as widely used or interpreted as r-squared in linear regression. If you‚Äôre interested, check out the Extras section at the end of this chapter for more information.\n\n\nReporting\nWhen reporting results from a Poisson GLM, we can make a graph and a sentence describing the pattern and related statistics.\nWhen we wish to make a graph of the fitted relationship, we can either make it with the y-axis on the log scale (linear predictor scale) or back-transform to the original count scale. I usually prefer the original scale, as it is easier to interpret.\nThe first step is to create a new data frame with a sequence of body sizes for prediction, and errors for the 95% confidence intervals:\n\nnew_data &lt;- data.frame(body.size = seq(min(soay$body.size), max(soay$body.size), length.out = 100))\npredictions &lt;- predict(soay_glm, newdata = new_data, se.fit = TRUE)\nnew_data$fit &lt;- exp(predictions$fit)\nnew_data$lower &lt;- exp(predictions$fit - 1.96 * predictions$se.fit)\nnew_data$upper &lt;- exp(predictions$fit + 1.96 * predictions$se.fit)\n\nNote that we back-transform the predictions and confidence intervals using the exponential function, since the link function is the log. And note that we must calculate the confidence intervals on the log scale first, and then back-transform them.\n\n\n\n\n\n\nNote\n\n\n\nIf we only wanted the fitted value (and not confidence intervals) we could have used type = \"response\" in the predict() function to get predictions on the original count scale (back-transformed from the log scale). When we don‚Äôt specified this, we would get predictions on the log scale‚Äìthis is the default behavior.\n\n\nNow we can plot the data and the fitted relationship with confidence intervals:\n\n\n\n\n\n\n\n\n\nExcellent! We can now write a nice sentence for our results:\nReproductive fitness (in terms of lifetime number of offspring) increased significantly with body mass, with a unit increase in body mass associated with a multiplicative increase in expected fitness of 1.06 (95% CI: 1.05, 1.06; \\(\\chi^2 =\\) 235.32, \\(df = 1\\), \\(p &lt;\\) 4.1^{-53}).",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#well-done",
    "href": "8.1-GLM1-count-data.html#well-done",
    "title": "Count data (L8)",
    "section": "Well done!",
    "text": "Well done!\nWe have made our first steps into the world of GLMs and Poisson regression for count data. You should now be able to:\n\nRecognise why linear regression is often inappropriate for count data.\nUnderstand the components of a GLM: linear predictor, link function, and family.\nFit a Poisson GLM in R using glm().\nInterpret coefficients from a Poisson GLM.\n\nNext, we will look at common issues that arise when fitting Poisson models, such as overdispersion and zero inflation.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#overdispersion",
    "href": "8.1-GLM1-count-data.html#overdispersion",
    "title": "Count data (L8)",
    "section": "Overdispersion",
    "text": "Overdispersion\nIn a Poisson model, mean and variance are assumed equal. In practice, variance often exceeds the mean, a phenomenon called overdispersion.\nCommon causes include:\n\nUnmeasured explanatory variables. This means important explanatory variables are missing from the model, leading to extra variability.\nIndividual heterogeneity. This can arise when individuals differ in ways not captured by measured explanatory variables, leading to extra variability in counts.\nCorrelated observations. This can occur when observations are not independent, such as repeated measures on the same individual.\n\nA simple check for overdispersion is to compare the residual deviance to the residual degrees of freedom:\n\\(\\text{Dispersion} = \\frac{\\text{Residual Deviance}}{\\text{Residual DF}}\\)\nIf this ratio is substantially greater than 1 (e.g., &gt; 1.5 or 2), it indicates overdispersion.\nCheck this for our Soay sheep model:\n\ndispersion_soay &lt;- deviance(soay_glm) / df.residual(soay_glm)\ndispersion_soay\n\n[1] 0.9977877\n\n\nHere, the dispersion is quite close to 1 (it is 1), indicating little overdispersion. However, in many real datasets, overdispersion is common.\n\n\n\n\n\n\nCaution\n\n\n\nIgnoring overdispersion leads to anti-conservative p-values (too small). This would be a typical example of Type I error inflation. Type I errors occur when we incorrectly reject a true null hypothesis, leading to false positives. In the context of statistical modeling, ignoring overdispersion can result in underestimating standard errors, which in turn leads to smaller p-values. Consequently, we may conclude that an effect is statistically significant when it is not, thereby increasing the likelihood of Type I errors.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-why-ind-diff) Why might individuals differ even after accounting for measured explanatory variables?\n\nQuasi-Poisson and negative binomial models\nOne solution is the quasi-Poisson model, which estimates an additional dispersion parameter:\nFor this, we can use family = quasipoisson in glm():\n\nsoay_quasi &lt;- glm(fitness ~ body.size, data = soay, family = quasipoisson)\nsummary(soay_quasi)\n\n\nCall:\nglm(formula = fitness ~ body.size, family = quasipoisson, data = soay)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.253092   0.209446  -5.983 3.59e-08 ***\nbody.size    0.053781   0.003722  14.450  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.992977)\n\n    Null deviance: 333.105  on 99  degrees of freedom\nResidual deviance:  97.783  on 98  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nThink‚ÄìPair‚ÄìShare (#a_overdispersion_consequences) What happens to standard errors and p-values if overdispersion is ignored?",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#zero-inflation",
    "href": "8.1-GLM1-count-data.html#zero-inflation",
    "title": "Count data (L8)",
    "section": "Zero inflation",
    "text": "Zero inflation\nA special case of overdispersion arises when there are more zeros than expected under a Poisson model. This is called zero inflation.\nExamples include:\n\nNumber of cigarettes smoked (many people do not smoke, more than can be modelled by a Poisson distribution)\nParasite counts (many host individual have no parasites, more than can be modelled by a Poission distribution)\n\n\n\n\n\n\n\nNote\n\n\n\nZero inflation often reflects two processes: whether an observation can be non-zero at all, and how large it is if it is. E.g., whether an individual smokes at all, and then how many cigarettes they smoke if they do.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-zero-inf-examples) Can you think of an ecological, biological, or medical process that would generate many structural zeros?\nIn this course we will not describe in detail or practice fitting zero-inflated models, but they are an important tool for count data with many zeros. Examples of models include zero-inflated Poisson (ZIP) and zero-inflated negative binomial (ZINB) models. These models combine a count model (e.g., Poisson or negative binomial) with a separate model for the probability of being a structural zero. There is also a model called the hurdle model, which is similar but has a different interpretation.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#multiple-explanatory-variables",
    "href": "8.1-GLM1-count-data.html#multiple-explanatory-variables",
    "title": "Count data (L8)",
    "section": "Multiple explanatory variables",
    "text": "Multiple explanatory variables\nGLMs can include multiple explanatory variables, just like linear models. And they can include only categorical variables, only continuous variables, or a mix of both.\nFor example, we could include parasite load as an additional predictor of fitness in the Soay sheep data:\nRead in the updated dataset:\n\nsoay &lt;- read.csv(\"datasets/soay_sheep_with_parasites.csv\")\n\nHere is the first few rows of the updated dataset:\n\nhead(soay)\n\n  body.size fitness parasite.load\n1  53.99529       1             2\n2  32.69467       1             6\n3  33.55580       0             5\n4  43.20078       1             6\n5  43.34102       0             7\n6  59.52711       3             6\n\n\nWe can visualise the relationship between parasite load and fitness:\n\n\n\n\n\n\n\n\n\nIt looks like higher parasite loads are associated with lower fitness.\nWe can fit a Poisson GLM with both body size and parasite load as explanatory variables:\n\nsoay_glm2 &lt;- glm(fitness ~ body.size + parasite.load, data = soay, family = poisson)\n\nAs always we check model assumptions:\n\npar(mfrow = c(2, 2))\nplot(soay_glm2, which = 1:4, add.smooth = TRUE)\n\n\n\n\n\n\n\n\nThe model checking plots look good. We can summarise the model:\n\nanova(soay_glm2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: poisson, link: log\n\nResponse: fitness\n\nTerms added sequentially (first to last)\n\n              Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                             99     226.57              \nbody.size      1   95.723        98     130.85 &lt; 2.2e-16 ***\nparasite.load  1   11.063        97     119.78 0.0008809 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see that both body size and parasite load significantly affect fitness. Body size has a positive effect, while parasite load has a negative effect. These effects are interpreted conditional on the other explanatory variable being held constant, just as in multiple linear regression.\nThink‚ÄìPair‚ÄìShare (#tps-how-vis-poisson) How would you visualise (make a graph) of the data and the modelled relationships? Make a sketch of what you could do.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#review",
    "href": "8.1-GLM1-count-data.html#review",
    "title": "Count data (L8)",
    "section": "Review",
    "text": "Review\nIn this chapter we have introduced Generalized Linear Models (GLMs) for count data, focusing on Poisson regression. Key points include:\n\nLinear regression is often inappropriate for count data due to violations of assumptions.\nGLMs extend linear models by allowing different distributions and link functions.\nPoisson GLMs use the Poisson distribution with a log link to model counts.\nCoefficients are interpreted on the log scale, with exponentiation giving multiplicative effects.\nOverdispersion and zero inflation are common issues that need to be addressed.\nGLMs can include multiple explanatory variables, just like linear models.\n\nWith this foundation, you are now equipped to analyse count data using GLMs in R. In the next chapter, we will explore further extensions and applications of GLMs.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#further-reading",
    "href": "8.1-GLM1-count-data.html#further-reading",
    "title": "Count data (L8)",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 14 of The R Book by Crawley (2012) provides a comprehensive introduction to GLMs in R, including Poisson regression. The R is a bit old fashioned but the concepts are well explained.\nGeneralized Linear Models With Examples in R (2018) by Peter K. Dunn and Gordon K. Smyth.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "8.1-GLM1-count-data.html#extras",
    "href": "8.1-GLM1-count-data.html#extras",
    "title": "Count data (L8)",
    "section": "Extras",
    "text": "Extras\n\nDeviance residuals\nIn linear models, residuals are simply the difference between observed and predicted values. In GLMs, residuals are more complex due to the non-normal distribution of the response variable.\nRecall from above that a Poisson GLM is mathematically summarised as:\n\\(y_i \\sim \\text{Poisson}(\\lambda_i)\\)\nwhere \\(\\log(\\lambda_i) = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)\nThe deviance residuals are a type of residual used in GLMs to assess model fit. They are derived from the concept of deviance, which measures the difference between the fitted model and a saturated model (a model that perfectly fits the data).\nThe deviance residual for each observation is calculated as: \\(r_i = \\text{sign}(y_i - \\hat{y}_i) \\sqrt{2 \\left( y_i \\log\\left(\\frac{y_i}{\\hat{y}_i}\\right) - (y_i - \\hat{y}_i) \\right)}\\)\nwhere: - \\(y_i\\) is the observed count - \\(\\hat{y}_i\\) is the predicted count from the model\nDeviance residuals have the following properties:\n\nThey are approximately normally distributed if the model fits well. Therefore they can be used in QQ-plots to assess model fit.\nThey can be positive or negative, indicating whether the observed count is above or below the predicted\nThey are used in diagnostic plots to assess model fit, similar to residuals in linear models.\n\n\n\nPseudo-R-squared for GLMs\nWhile there is no direct equivalent of r-squared for GLMs, several pseudo-r-squared measures have been proposed. These measures aim to provide an indication of model fit similar to r-squared in linear regression, but they do not have the same interpretation.\nTo understand how pseudo-r-squared works, we first need to understand the concept of deviance and log-likelihoods in GLMs. Deviance is a measure of model fit based on likelihoods. It compares the fitted model to a null model (a model with only an intercept) and a saturated model (a model that perfectly fits the data). Let‚Äôs break that down a bit further:\nResidual deviance is calculated as:\n\\(D = -2 \\left( \\log L(\\text{fitted model}) - \\log L(\\text{saturated model}) \\right)\\)\nwhere - \\(\\log L(\\text{fitted model})\\) is the log-likelihood of the fitted model and - \\(\\log L(\\text{saturated model})\\) is the log-likelihood of the saturated model.\nThe log-likelihood measures how well the model explains the observed data; higher values indicate better fit. The fitted model is the model we have estimated, while the saturated model is a hypothetical model that perfectly fits the data.\nNull deviance is calculated similarly, but for the null model:\n\\(D_{null} = -2 \\left( \\log L(\\text{null model}) - \\log L(\\text{fitted model}) \\right)\\)\nwhere - \\(\\log L(\\text{null model})\\) is the log-likelihood of the null model.\n(By the way, the -2 factor is included to make the deviance comparable to the chi-squared distribution, with degrees of freedom equal to the difference in the number of parameters between models. This is useful for hypothesis testing.)\nSo:\n\nNull deviance: This is the deviance of the null model, which includes only an intercept. It represents how well a model with no predictors fits the data.\nResidual deviance: This is the deviance of the fitted model. It represents how well the model with predictors fits the data relative to the null model.\n\nOne common pseudo-r-squared measure is McFadden‚Äôs R-squared, defined as:\n\\(R^2_{McFadden} = 1 - \\frac{D_{fitted}}{D_{null}}\\) where \\(D_{fitted}\\) is the residual deviance of the fitted model and \\(D_{null}\\) is the null deviance.\nThis measure ranges from 0 to 1, with higher values indicating better model fit. However, it is important to note that pseudo-r-squared values for GLMs are generally lower than r-squared values in linear regression, and they should be interpreted with caution.\nThere is much more to learn about the methods surrounding GLMs and their diagnostics, but this introduction should give you a solid foundation to start working with count data in R using Poisson regression.",
    "crumbs": [
      "Count data (L8)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html",
    "href": "9.1-GLM2-binary-data.html",
    "title": "Binary data (L9)",
    "section": "",
    "text": "Introduction\nIn the previous chapter we moved from linear models and normal errors to generalized linear models (GLMs) to handle count responses with a Poisson GLM. In this chapter we move to another common case: binary responses.\nBinary response variables take only two values, often coded as 0 and 1:\nThe central question becomes:\nWhich explanatory variables influence the probability \\(\\pi_i = P(y_i = 1)\\) of the response variable?\nThink‚ÄìPair‚ÄìShare (#tps-binary-examples) Name three response variables in biology, ecology, or medicine that are naturally binary (0/1). Why would a linear model be risky for each?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#introduction",
    "href": "9.1-GLM2-binary-data.html#introduction",
    "title": "Binary data (L9)",
    "section": "",
    "text": "1 = yes / success / present / dead\n0 = no / failure / absent / alive\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBridge from LM to GLM\nA GLM keeps the familiar linear predictor from linear models, but: 1) uses a distribution (family) that matches the response type, and\n2) uses a link function so predictions respect the natural constraints.\n\nCount data: Poisson family + log link\n\nBinary data: binomial family + logit link",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#overview",
    "href": "9.1-GLM2-binary-data.html#overview",
    "title": "Binary data (L9)",
    "section": "Overview",
    "text": "Overview\nIn this chapter we will cover:\n\nContingency tables and the \\(\\chi^2\\) test (a familiar starting point)\nOdds, odds ratios, and log-odds\nLogistic regression as a binomial GLM\nInterpreting coefficients (odds ratios and probabilities)\nModel assumptions, deviances, and (some) model checking\nOverdispersion in aggregated binomial data and the quasibinomial fix\nPractical complications for individual-level (non-aggregated) 0/1 data",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#a-warm-up-the-chi-square-test-for-a-22-table",
    "href": "9.1-GLM2-binary-data.html#a-warm-up-the-chi-square-test-for-a-22-table",
    "title": "Binary data (L9)",
    "section": "A warm-up: the Chi-square test for a 2√ó2 table",
    "text": "A warm-up: the Chi-square test for a 2√ó2 table\nBinary responses often first appear in a contingency table. For example:\n\nExplanatory variable \\(x\\): hormonal contraception (yes/no)\nResponse variable \\(y\\): heart attack (yes/no)\n\nThe \\(\\chi^2\\) (chi-squared) test asks whether the proportion (or frequency) of heart attacks is the same in the two contraception groups.\n\nA concrete example dataset\nWe will reconstruct the classic 2√ó2 table from some example and store it as a dataset:\nRead in the heart attack dataset:\n\nheart_attack &lt;- read_csv(\"datasets/heart_attack_2x2.csv\")\n\nRows: 4 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): contraception, heart_attack\ndbl (1): n\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere are the first few rows of the heart attack dataset:\n\nhead(heart_attack)\n\n# A tibble: 4 √ó 3\n  contraception heart_attack     n\n  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;\n1 yes           yes             23\n2 yes           no              34\n3 no            yes             35\n4 no            no             132\n\n\nWe can create the table and run the \\(\\chi^2\\) test.\nHere is the data ready for analysis:\n\ntab &lt;- xtabs(n ~ contraception + heart_attack, data = heart_attack)\ntab\n\n             heart_attack\ncontraception  no yes\n          no  132  35\n          yes  34  23\n\n\nAnd here is the \\(\\chi^2\\) test:\n\nchisq.test(tab, correct = FALSE)\n\n\n    Pearson's Chi-squared test\n\ndata:  tab\nX-squared = 8.3288, df = 1, p-value = 0.003902\n\n\n\n\n\n\n\n\nNote\n\n\n\nContinuity correction\nchisq.test() defaults to Yates‚Äô continuity correction for 2√ó2 tables. Here we set correct = FALSE to match the classic ‚Äúhand-calculation‚Äù style example.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-chisq-meaning) What is the null hypothesis of this \\(\\chi^2\\) test, in words?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#quantifying-an-association-risk-and-odds",
    "href": "9.1-GLM2-binary-data.html#quantifying-an-association-risk-and-odds",
    "title": "Binary data (L9)",
    "section": "Quantifying an association: risk and odds",
    "text": "Quantifying an association: risk and odds\nIf \\(\\pi\\) is the probability of ‚Äúyes‚Äù (e.g.¬†heart attack), then:\n\nRisk (probability) is \\(\\pi\\)\nOdds is \\(\\pi/(1-\\pi)\\)\n\nThe odds ratio (OR) compares the odds in two groups:\nOdds Ratio = OR = \\(\\frac{\\text{odds in group 1}}{\\text{odds in group 2}}\\).\nThe log odds ratio is \\(\\log(OR)\\). It is 0 when \\(OR=1\\).\n\n\n\n\n\n\nImportant\n\n\n\nOdds are not probabilities\nAn odds of 3 means ‚Äú3 to 1‚Äù, which corresponds to a probability of \\(3/(3+1)=0.75\\). Odds and probabilities are linked mathematically, but they are not the same thing.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-odds-intuition) Why might odds ratios be a convenient effect size in a regression model? (This is a difficult question‚Äîthink carefully, and Pair!)",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#from-tables-to-regression-models",
    "href": "9.1-GLM2-binary-data.html#from-tables-to-regression-models",
    "title": "Binary data (L9)",
    "section": "From tables to regression models",
    "text": "From tables to regression models\nContingency tables are a good start, but they are limited:\n\nthey typically compare groups (few categorical explanatory variable),\nthey don‚Äôt easily handle continuous explanatory variables (e.g.¬†dose),\nthey don‚Äôt naturally generalise to multiple explanatory variables.\n\nWhen \\(y\\) is binary or binomial, the standard regression approach is logistic regression, a binomial GLM.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#working-example-beetle-mortality-and-insecticide-dose-aggregated-data",
    "href": "9.1-GLM2-binary-data.html#working-example-beetle-mortality-and-insecticide-dose-aggregated-data",
    "title": "Binary data (L9)",
    "section": "Working example: beetle mortality and insecticide dose (aggregated data)",
    "text": "Working example: beetle mortality and insecticide dose (aggregated data)\nEight groups of beetles were exposed to an insecticide dose for 5 hours. For each dose level, we know how many beetles were tested and how many were killed. This is binomial (aggregated) data.\nRead in the beetle dataset:\n\nbeetle &lt;- read_csv(\"datasets/beetle_mortality.csv\")\n\nRows: 8 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\ndbl (4): Dose, Number_tested, Number_killed, Mortality_rate\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere are the first few rows of the beetle dataset:\n\nhead(beetle)\n\n# A tibble: 6 √ó 4\n   Dose Number_tested Number_killed Mortality_rate\n  &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;\n1  49.1            49             6          0.122\n2  53.0            60            13          0.217\n3  56.9            62            18          0.290\n4  60.8            56            28          0.5  \n5  64.8            63            52          0.825\n6  68.7            59            53          0.898\n\n\nIt contains one explanatory variable (Dose) and two pieces of information about the binary response variable (Number_killed and Number_tested). The mortality rate is also included, and is the proportion killed at each dose (Number_killed / Number_tested).\nAs always, start with a graph.\n\n\n\n\n\n\n\n\n\nThink‚ÄìPair‚ÄìShare (#tps-why-lm-wrong) What goes wrong if we fit a linear regression to a probability (mortality rate)? Name at least two problems.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#two-tempting-but-wrong-analyses",
    "href": "9.1-GLM2-binary-data.html#two-tempting-but-wrong-analyses",
    "title": "Binary data (L9)",
    "section": "Two tempting but wrong analyses",
    "text": "Two tempting but wrong analyses\n\nWrong analysis 1: linear regression on mortality rate\n\nmod_beetle_lm &lt;- lm(Mortality_rate ~ Dose, data = beetle)\n\nThis can predict probabilities below 0 or above 1 and does not respect the mean‚Äìvariance structure of binomial data.\nCheck out the model assumptions:\n\npar(mfrow = c(2, 2))\nplot(mod_beetle_lm)\n\n\n\n\n\n\n\n\nWe don‚Äôt have so many data points here, but the residuals vs fitted indicating non-linearity and the Q-Q plot indicating non-normality are both concerning. Also the scale-location plot suggests non-constant variance.\n\n\nWrong analysis 2: Poisson regression on counts killed\nA Poisson model ignores the fact that deaths are bounded by the number tested at each dose.\n\nmod_beetle_pois &lt;- glm(Number_killed ~ Dose, data = beetle, family = poisson)\nsummary(mod_beetle_pois)$coefficients\n\n               Estimate  Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -0.77418056 0.494639994 -1.565139 1.175502e-01\nDose         0.06678281 0.007240113  9.224002 2.862145e-20\n\n# Example of the \"impossible\" issue:\ndose_example &lt;- 76.54\npred_killed &lt;- predict(mod_beetle_pois,\n                       newdata = data.frame(Dose = dose_example),\n                       type = \"response\")\npred_killed\n\n       1 \n76.50652 \n\nnum_tested_example &lt;- beetle$Number_tested[beetle$Dose == dose_example]\nnum_tested_example\n\n[1] 60\n\n\nAt the highest dose (76.54), the model predicts about 76 deaths, but only 60 beetles were tested! Clearly this is an impossible prediction. We can‚Äôt go on with the Poisson model either.\n\n\n\n\n\n\nCaution\n\n\n\nPoisson vs binomial, when to use each\nUse a Poisson model when counts have no known upper limit.\nUse a binomial model when counts are ‚Äú\\(k\\) successes out of \\(n\\) trials‚Äù. Use a binomial model when each observation is a 0/1 response.\n\n\nThink‚ÄìPair‚ÄìShare (#a_probability_not_linear) Why is it dangerous to model probabilities directly using a linear model?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#the-probability-model-bernoulli-and-binomial",
    "href": "9.1-GLM2-binary-data.html#the-probability-model-bernoulli-and-binomial",
    "title": "Binary data (L9)",
    "section": "The probability model: Bernoulli and binomial",
    "text": "The probability model: Bernoulli and binomial\n\nBernoulli (binary) data\nFor a single 0/1 observation:\n\\(Y \\sim \\text{Bernoulli}(\\pi),\\quad P(Y=1)=\\pi,\\quad P(Y=0)=1-\\pi\\)\nIn words, this means that the probability of success (Y=1) is \\(\\pi\\), and the probability of failure (Y=0) is \\(1-\\pi\\). And that this probability is Bernoulli distributed. Bernoulli is a special case of the binomial distribution with \\(n=1\\).\nMean and variance are:\n\\(E(Y)=\\pi,\\quad Var(Y)=\\pi(1-\\pi)\\)\n\n\nBinomial (aggregated) data\nFor \\(k\\) successes out of \\(n\\) trials:\n\\(Y \\sim \\text{Binomial}(n,\\pi)\\)\nThis means that the number of successes \\(Y\\) in \\(n\\) independent Bernoulli trials, each with success probability \\(\\pi\\), follows a binomial distribution.\nMean and variance are:\n\\(E(Y)=n\\pi,\\quad Var(Y)=n\\pi(1-\\pi)\\)\n\n\n\n\n\n\nNote\n\n\n\nKey pattern\nFor Bernoulli/binomial data, the variance is determined by the mean. This is one reason ‚Äúconstant variance‚Äù fails for linear models.\n\n\nThink‚ÄìPair‚ÄìShare (#tps-binom-variance) For fixed \\(n\\), at what value of \\(\\pi\\) is \\(Var(Y)=n\\pi(1-\\pi)\\) largest? At what values of \\(\\pi\\) is it smallest?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#logistic-regression-the-binomial-glm",
    "href": "9.1-GLM2-binary-data.html#logistic-regression-the-binomial-glm",
    "title": "Binary data (L9)",
    "section": "Logistic regression: the binomial GLM",
    "text": "Logistic regression: the binomial GLM\nWe use the usual GLM structure:\n\nLinear predictor\n\n\\(\\eta_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\cdots + \\beta_p x_i^{(p)}\\)\n\nFamily: binomial (Bernoulli is the special case \\(n=1\\))\nLink: logit\n\n\\(\\eta_i = \\log\\left(\\frac{\\pi_i}{1-\\pi_i}\\right)\\)\nThe inverse link (back-transformation) is the logistic function:\n\\(\\pi_i = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)}\\)\n\n\n\n\n\n\nImportant\n\n\n\nWhy the logit link?\nIt maps probabilities \\((0,1)\\) to the whole real line \\((-\\infty,\\infty)\\), so the linear predictor can take any value while \\(\\pi_i\\) stays valid, i.e., [0,1].\n\n\nThink‚ÄìPair‚ÄìShare (#tps-identity-link-binary) What would go wrong if we used the identity link \\(E(y_i)=\\eta_i\\) for a binary response variable? And which link function is this, by the way?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#fitting-logistic-regression-in-r-aggregated-binomial-data",
    "href": "9.1-GLM2-binary-data.html#fitting-logistic-regression-in-r-aggregated-binomial-data",
    "title": "Binary data (L9)",
    "section": "Fitting logistic regression in R (aggregated binomial data)",
    "text": "Fitting logistic regression in R (aggregated binomial data)\nFor aggregated binomial data, we provide successes and failures:\n\nbeetle &lt;- beetle |&gt;\n  mutate(Number_survived = Number_tested - Number_killed)\nhead(beetle)\n\n# A tibble: 6 √ó 5\n   Dose Number_tested Number_killed Mortality_rate Number_survived\n  &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;           &lt;dbl&gt;\n1  49.1            49             6          0.122              43\n2  53.0            60            13          0.217              47\n3  56.9            62            18          0.290              44\n4  60.8            56            28          0.5                28\n5  64.8            63            52          0.825              11\n6  68.7            59            53          0.898               6\n\n\nNow we can fit the logistic regression model, being careful to specify family = binomial and to use cbind(successes, failures) for the response. Because we specify the response this way, R knows we are working with aggregated binomial data. And because we specify family = binomial, R knows to use the logit link (the default).\n\nbeetle_glm &lt;- glm(cbind(Number_killed, Number_survived) ~ Dose,\n                  data = beetle, family = binomial)\n\nAnd as usual we can check the model assumptions:\n\npar(mfrow = c(2, 2))\nplot(beetle_glm)\n\n\n\n\n\n\n\n\nNot great, but again we have only 8 dose levels. Let us continue in any case and look at the ANOVA table and the coefficients.\n\n\n\n\n\n\nImportant\n\n\n\nThe response in aggregated binomial data\nUse cbind(successes, failures) in glm(..., family = binomial).\n\nsuccesses: number of 1s\n\nfailures: number of 0s",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#interpreting-coefficients-log-odds-odds-ratios-and-probabilities",
    "href": "9.1-GLM2-binary-data.html#interpreting-coefficients-log-odds-odds-ratios-and-probabilities",
    "title": "Binary data (L9)",
    "section": "Interpreting coefficients: log-odds, odds ratios, and probabilities**",
    "text": "Interpreting coefficients: log-odds, odds ratios, and probabilities**\nIn logistic regression:\n\n\\(\\beta_1\\) is a log odds ratio for a one-unit change in the linear predictor\n\\(\\exp(\\beta_1)\\) is the odds ratio (multiplicative change in odds)\n\n\nbeta1 &lt;- coef(beetle_glm)[[\"Dose\"]]\nexp(beta1)\n\n[1] 1.278311\n\n\nInterpretation: increasing dose by 1 unit multiplies the odds of death by \\(\\exp(\\beta_1)\\).\n\n\n\n\n\n\nNote\n\n\n\nOdds vs probability\nA constant odds ratio does not mean a constant change in probability. The probability change depends on where you start (e.g.¬†\\(\\pi=0.05\\) vs \\(\\pi=0.80\\)).\n\n\nThink‚ÄìPair‚ÄìShare (#tps-or-vs-probability) Why might a treatment have a ‚Äúbig‚Äù odds ratio, but only a ‚Äúsmall‚Äù change in probability in one situation?\nThink‚ÄìPair‚ÄìShare (#a_odds_vs_probability) Why might odds be convenient mathematically,\neven if probabilities feel more intuitive?",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#analysis-of-deviance-likelihood-based-anova",
    "href": "9.1-GLM2-binary-data.html#analysis-of-deviance-likelihood-based-anova",
    "title": "Binary data (L9)",
    "section": "Analysis of deviance (likelihood-based ANOVA)",
    "text": "Analysis of deviance (likelihood-based ANOVA)\nAs for count-data GLMs, we use likelihood and deviance. Nested models can be compared with a \\(\\chi^2\\) test.\n\nanova(beetle_glm, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(Number_killed, Number_survived)\n\nTerms added sequentially (first to last)\n\n     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                     7    267.662              \nDose  1   259.23         6      8.438 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs with the Poisson GLM, the model is fit using maximum likelihood estimation, and the deviance is used to assess goodness-of-fit. So we see a table quite similar to that for the Poisson GLM. In this case the Dose row shows a huge amount of the deviance is explained by dose, and so the \\(p\\)-value is very small.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#plotting-the-fitted-relationship",
    "href": "9.1-GLM2-binary-data.html#plotting-the-fitted-relationship",
    "title": "Binary data (L9)",
    "section": "Plotting the fitted relationship",
    "text": "Plotting the fitted relationship\nWe will plot \\(P(Y=1)\\) (mortality probability) against dose using model predictions.\n\ndose_grid &lt;- tibble(Dose = seq(min(beetle$Dose)-20, max(beetle$Dose)+20, length.out = 400))\npreds &lt;- predict(beetle_glm, newdata = dose_grid, se.fit = TRUE) |&gt; \n  as_tibble() |&gt;\n         mutate(p_hat = exp(fit) / (1 + exp(fit)),\n         p_hat_upper_2se = exp(fit + 2*se.fit) / (1 + exp(fit + 2*se.fit)),\n         p_hat_lower_2se = exp(fit - 2*se.fit) / (1 + exp(fit - 2*se.fit))\n       )\ndose_grid &lt;- bind_cols(dose_grid, preds)\n\nAnd now the plot:\n\nggplot() +\n  geom_point(data = beetle, aes(x = Dose, y = Mortality_rate)) +\n  geom_line(data = dose_grid, aes(x = Dose, y = p_hat), color = \"blue\") +\n  geom_ribbon(data = dose_grid,\n              aes(x = Dose, ymin = p_hat_lower_2se, ymax = p_hat_upper_2se),\n              alpha = 0.2, fill = \"blue\") +\n  labs(x = \"Dose\", y = \"Predicted mortality probability\")\n\n\n\n\n\n\n\n\nThe plot is intentionally made to show what happens at very low and very high dose. At very low dose, the predicted mortality probability approaches 0, and at very high dose it approaches 1. This is a key feature of logistic regression: the predicted probabilities are always between 0 and 1. We also see that the standard errors are larger at intermediate doses, where the slope of the curve is steepest. And the standard errors are smaller at very low and very high doses, where the curve flattens out.\n\nReporting\nWhen reporting results from a logistic regression, you might say something like:\n‚ÄúA logistic regression was used to model the probability of beetle mortality as a function of insecticide dose. The odds of mortality increased by a factor of 1.28 (95% CI: 1.23, 1.34 per unit increase in dose. This indicates that higher doses are associated with higher odds of mortality. The model predicts that at a dose of 50, the probability of mortality is approximately 0.09 (95% CI: -2.34, -2.26 transformed to probability scale). The model explained a significant amount of deviance (Deviance = 8.4, df = 6, \\(\\chi^2\\) p &lt; 0.001).‚Äù",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#overdispersion-in-aggregated-binomial-data",
    "href": "9.1-GLM2-binary-data.html#overdispersion-in-aggregated-binomial-data",
    "title": "Binary data (L9)",
    "section": "Overdispersion in aggregated binomial data",
    "text": "Overdispersion in aggregated binomial data\nOverdispersion means the variability is larger than the binomial model expects. A quick (rough) check for aggregated binomial data is:\n\\(\\text{Residual deviance} \\approx \\text{df}\\)\nValues much larger than 1 for the ratio \\(\\frac{\\text{Residual deviance}}{\\text{df}}\\) suggest overdispersion. In practice we look for values above about 1.5 or 2.\nIn the beetle mortality example:\n\ndeviance(beetle_glm)\n\n[1] 8.437898\n\ndf.residual(beetle_glm)\n\n[1] 6\n\ndeviance(beetle_glm) / df.residual(beetle_glm)\n\n[1] 1.406316\n\n\n\n\n\n\n\n\nCaution\n\n\n\nType I error inflation (binomial overdispersion)\nIf there is overdispersion and we ignore it, standard errors are usually too small and \\(p\\)-values can be too small (anti-conservative). That increases false positives (Type I errors).\n\n\n\nQuasibinomial as a pragmatic fix\nIf we had found overdispersion, a simple fix is to use the quasibinomial family. This estimates an extra dispersion parameter from the data. For example:\n\nbeetle_glm_q &lt;- glm(cbind(Number_killed, Number_survived) ~ Dose,\n                    data = beetle, family = quasibinomial)\n\n\n\n\n\n\n\nNote\n\n\n\nquasibinomial estimates an extra dispersion parameter from the data. It is often a good ‚Äúfirst fix‚Äù when aggregated binomial data look overdispersed.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#individual-level-binary-data-non-aggregated",
    "href": "9.1-GLM2-binary-data.html#individual-level-binary-data-non-aggregated",
    "title": "Binary data (L9)",
    "section": "Individual-level binary data (non-aggregated)",
    "text": "Individual-level binary data (non-aggregated)\nIn some datasets we record a single 0/1 observation per individual, rather than aggregated counts for many individuals. The same logistic regression model is used, but:\n\nsimple plots of \\(y\\) against explanatory variables look uninformative (two bands)\nsome assumption and dispersion checks need extra care\n\n\nExample: blood screening (ESR)\nIndividuals with low ESR (ESR is erythrocyte sedimentation rate and is a measure of general inflammation and infection) are generally considered healthy; ESR &gt; 20 mm/hr indicates possible disease. We will model the probability of high ESR using fibrinogen and globulin concentration.\nRead in the plasma dataset:\n\nplasma &lt;- read_csv(\"datasets/plasma_esr_original_HSAUR3.csv\")\n\nRows: 32 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): ESR\ndbl (3): fibrinogen, globulin, y\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere are the first few rows of the data:\n\nhead(plasma)\n\n# A tibble: 6 √ó 4\n  fibrinogen globulin ESR          y\n       &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1       2.52       38 ESR &lt; 20     0\n2       2.56       31 ESR &lt; 20     0\n3       2.19       33 ESR &lt; 20     0\n4       2.18       31 ESR &lt; 20     0\n5       3.41       37 ESR &lt; 20     0\n6       2.46       36 ESR &lt; 20     0\n\n\nThe ESR response variable is in two variables. ESR is a factor with levels ‚Äúlow‚Äù and ‚Äúhigh‚Äù. It is also present as a numeric variable y, coded 0 (low) and 1 (high). The explanatory variables are fibrinogen and globulin, both continuous.\n\n\nComplication 1: graphical description\nSimple scatter plots of 0/1 data are not so informative:\n\n\n\n\n\n\n\n\n\nA more informative plot is a conditional density plot. This shows the proportion of 0s and 1s at each value of the explanatory variable.\n\npar(mfrow = c(1, 2))\ncdplot(factor(y) ~ fibrinogen, data = plasma, xlab = \"Fibrinogen\")\ncdplot(factor(y) ~ globulin, data = plasma, xlab = \"Globulin\")\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))\n\nIt looks like higher fibrinogen and higher globulin are associated with higher probability of high ESR. The pattern appears stronger for fibrinogen.\n\n\nFit the logistic regression model\nIn the case of non-aggregated individual-level 0/1 data, we simply use y as the response variable. We have to remember to specify family = binomial in glm().\n\nplasma_glm &lt;- glm(y ~ fibrinogen + globulin, data = plasma, family = binomial)\n\n\n\nComplication 2: model checking and dispersion\nResidual plots exist, but can be hard to interpret for 0/1 data:\n\npar(mfrow = c(2, 2))\nplot(plasma_glm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDispersion checks need caution for individual 0/1 data\nThe simple ‚Äúresidual deviance \\(\\approx\\) residual df‚Äù rule-of-thumb is most useful for aggregated binomial data. With individual-level 0/1 data, detecting overdispersion is more subtle and often requires additional structure (e.g.¬†grouping, random effects).",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#two-common-practical-issues",
    "href": "9.1-GLM2-binary-data.html#two-common-practical-issues",
    "title": "Binary data (L9)",
    "section": "Two common practical issues",
    "text": "Two common practical issues\n\n1. Separation\nSometimes a explanatory variable (or combination of explanatory variables) perfectly predicts the response variable (all 0s on one side, all 1s on the other). This is called (complete) separation and can cause extremely large coefficient estimates and warnings.\n\n\n\n\n\n\nCaution\n\n\n\nSeparation warning\nIf your logistic regression produces enormous standard errors or warnings about fitted probabilities being 0 or 1, check for separation. Remedies include more data, simpler models, or penalized methods (advanced topic).\n\n\n\n\n2. Probability vs odds ratio in reporting\nOdds ratios are the default ‚Äúnative‚Äù scale of logistic regression, but probabilities are often easier to interpret. In practice, you may report odds ratios and translate to probabilities at meaningful explanatory variable values.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#review",
    "href": "9.1-GLM2-binary-data.html#review",
    "title": "Binary data (L9)",
    "section": "Review",
    "text": "Review\nThere is a lot more to logistic regression and binary data. We can of course make models to analyse more complex data and questions, including multiple explanatory variables, different types of explanatory variable (continuous and categorical), interactions, and so on. But the key ideas are all above. To summarize the main points:\n\nData can be either aggregated binomial (successes out of trials) or individual-level 0/1.\nThe key questions are about how explanatory variables influence the probability of ‚Äúyes‚Äù.\nThe distribution family is binomial (Bernoulli for individual-level data).\nThe link function is the logit (log-odds).\nIf we find overdispersion in aggregated binomial data, quasibinomial is a pragmatic fix.\nWith non-aggregated individual-level 0/1 data, plotting and checking model assumptions require more care.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "9.1-GLM2-binary-data.html#further-reading",
    "href": "9.1-GLM2-binary-data.html#further-reading",
    "title": "Binary data (L9)",
    "section": "Further reading",
    "text": "Further reading\n\nChapter 17 of The R Book by Crawley (2012) provides a comprehensive introduction to GLMs in R, including binomial regression. The R is a bit old fashioned but the concepts are well explained.\nGeneralized Linear Models With Examples in R (2018) by Peter K. Dunn and Gordon K. Smyth.",
    "crumbs": [
      "Binary data (L9)"
    ]
  },
  {
    "objectID": "10.1-ordination.html",
    "href": "10.1-ordination.html",
    "title": "Ordination (L10)",
    "section": "",
    "text": "Introduction\nIn earlier chapters, we have analysed variability in one response variable at a time. But many biological questions are intrinsically multivariate:\nWhen we have many response variables, two practical problems appear:\nOrdination is a family of methods that helps with both.\nIt can also help in cases where we have many explanatory variables (e.g.¬†environmental gradients). This can be especially useful when predictors are collinear.\nThink‚ÄìPair‚ÄìShare (#tps-ordination-why) Name one dataset you work with (or could imagine working with) that has many response variables. What would you like to learn from it that a single-response analysis might miss?",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#introduction",
    "href": "10.1-ordination.html#introduction",
    "title": "Ordination (L10)",
    "section": "",
    "text": "morphology: multiple measurements describe shape (not just size)\ncommunities: abundance of many species describes composition\nphysiology / omics: many traits or genes change together\n\n\n\nHow do we visualize patterns across many variables?\nHow do we summarize the dominant patterns without losing the biology?\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCore idea\nOrdination finds a low-dimensional representation (often 2D) of multivariate data, so that points that are ‚Äúsimilar‚Äù in the original multivariate space end up close together in the ordination plot.\n\n\n\n\nWorking example: skull shape through time\nWe will use a classic dataset of human skull dimensions measured in millimeters. The biological question is:\n\nHas skull shape changed through time?\n\nFour measurements of skull were made:\n\nWe will treat the four skull measurements as multivariate responses, and time as an explanatory variable.\nFirst read the dataset from the csv file:\n\nskull &lt;- read_csv(here(\"datasets\", \"skull_shape_time.csv\"))\n\nRows: 150 Columns: 6\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): id\ndbl (5): max.breadth, basi.height, basi.length, nasal.height, thousand.years\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nWide vs long format (and why you should care)\nMultivariate data often come in wide format:\n\none row per object (skull)\none column per variable (measurement)\n\nThat‚Äôs great for many ordination functions.\nBut long format is often easier for: - grouped summaries - plotting multiple variables with a single ggplot call\nWe can easily make a long format version with pivot_longer():\n\nskull_long &lt;- skull |&gt;\n  pivot_longer(\n    cols = c(max.breadth, basi.height, basi.length, nasal.height),\n    names_to = \"dimension\",\n    values_to = \"value\"\n  )\n\nThis will make some of the following exploration easier.\n\n\nExplore first: univariate views of a multivariate problem\nA good habit:\n\nvisualize each variable,\n\nlook for patterns and surprises,\n\nthen use ordination to summarize.\n\n\n\nFour scatterplots (measurement vs time)\n\np_vars &lt;- skull_long |&gt;\n  ggplot(aes(x = thousand.years, y = value)) +\n  geom_point(alpha = 0.6) +\n  facet_wrap(~ dimension, scales = \"free_y\", ncol = 2) +\n  labs(x = \"Thousand years ago\", y = \"Measurement (mm)\")\np_vars\n\n\n\n\n\n\n\n\nInterpretation (qualitative):\nIf all dimensions change similarly through time, that‚Äôs mostly a size change. If some change differently, that suggests shape change.\nThink‚ÄìPair‚ÄìShare (#tps-size-shape) How would you distinguish ‚Äúsize change‚Äù from ‚Äúshape change‚Äù using these plots?\n\n\nWhy not four separate regressions?\nFitting separate regressions for each measurement has two problems: 1. Multiple testing: testing four times increases the chance of false positives. 2. Ignoring covariance: measurements may be correlated, so analyzing them separately misses joint patterns\nThink‚ÄìPair‚ÄìShare (#a_why_ordination) What information is lost when you analyze multivariate data one variable at a time? Why could this be important?",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#ordination-i-pca-principal-components-analysis",
    "href": "10.1-ordination.html#ordination-i-pca-principal-components-analysis",
    "title": "Ordination (L10)",
    "section": "Ordination I: PCA (Principal Components Analysis)",
    "text": "Ordination I: PCA (Principal Components Analysis)\nPCA is the most common entry point.\n\n\n\n\n\n\nImportant\n\n\n\nWhat PCA does\nPCA finds new axes (PC1, PC2, ‚Ä¶) that are: - linear combinations of the original variables - orthogonal (uncorrelated) - ordered so that PC1 captures the most variance, PC2 the next most, etc.\n\n\n\nIntuitive picture of PCA\nImagine that each skull is a point in a space with one axis per measurement (four dimensions here). Together, the skulls form a cloud of points.\nPCA asks from which direction does this cloud show the greatest spread?\nTo answer this, PCA rotates the coordinate system so that:\n\nPC1 points in the direction of greatest variation,\nPC2 points in the next greatest direction, orthogonal to PC1,\nand so on.\n\nYou can think of PCA as turning the data cloud until you find the view where the points are most spread out. That view becomes PC1. Then PCA finds the best second view at right angles to it (PC2).\nThe key idea is that PCA does not invent new information: it simply re-expresses the same data using new axes that make dominant patterns easier to see.\n\n\n\n\n\n\nNote\n\n\n\nUnder the hood, PCA is based on the covariance (or correlation) matrix of the variables. The principal components are eigenvectors of this matrix, and the amount of variance they capture is given by the corresponding eigenvalues. You do not need to compute these by hand, but this explains why PCA is fundamentally about variance and correlation.\n\n\n\n\nCentering and scaling\nBefore PCA, we usually center and scale the data, or at least think carefully if we should.\n\nCentering subtracts the mean of each variable (so mean = 0).\nScaling divides by the SD of each variable (so SD = 1).\n\nScaling matters if variables are on different scales or if you care about relative variation.\n\n\n\n\n\n\nNote\n\n\n\nNote that above it is written that PCA does not change the relative positions of the points. However, if we scale the data before performing PCA, this can affect the relative positions of the points in the PCA space. Scaling ensures that all variables contribute equally to the analysis, which can be important when variables are measured on different scales. If we do not scale the data, variables with larger variances can dominate the PCA results, potentially distorting the relative positions of the points in the PCA space.\n\n\nWe can make these transformed variables ourself, or use the built-in options in prcomp().\nSo, let us do our first PCA. We must be sure to only do the PCA on the four skull measurement variables. To do this we will select only those columns from the original data frame, and pipe these into the prcomp() function:\n\nskull_pca &lt;- skull |&gt;\n  dplyr::select(max.breadth, basi.height, basi.length, nasal.height) |&gt;\n  prcomp(center = TRUE, scale. = TRUE)\n\nWhat is this new dataset? Part of it is the new coordinates of each skull in the PC space:\n\nhead(skull_pca$x)\n\n           PC1         PC2        PC3        PC4\n[1,] -3.621758  0.87923353  0.2413718 -0.2421541\n[2,] -3.201426 -0.38205353  0.3833647 -0.3500371\n[3,] -3.004179 -1.00242513  1.5640064 -0.2348435\n[4,] -2.683398  0.08615036  1.6069216 -1.0534579\n[5,] -3.001132 -0.46314398 -0.2188543 -0.7276811\n[6,] -2.774762  0.09654899  0.2109694 -0.8941411\n\n\n\n\nVariance represented\nEach of the new PC axes captures some of the variance in the original data. We can summarize this with:\n\nsummary(skull_pca)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4\nStandard deviation     1.4916 0.8981 0.7643 0.62007\nProportion of Variance 0.5562 0.2016 0.1460 0.09612\nCumulative Proportion  0.5562 0.7578 0.9039 1.00000\n\n\nThis shows the standard deviation of each PC axis, the proportion of variance explained by each PC, and the cumulative proportion of variance explained.\nThe first PC axis captures 55.6% of the variance in the original data. The first two PC axes together capture 75.8% of the variance.\nThis is quite a lot, so we can visualize the data well with two PC axes. When we do so, we ignore the remaining axes, which capture less variance (24.2%).\n\n\nPCA scores plot (PC1 vs PC2)\nThe scores are the coordinates of each skull in the new PC space. We do a bit of data wrangling to combine these with the original data (id and time):\n\nscores &lt;- as_tibble(skull_pca$x) |&gt;\n  mutate(id = skull$id, thousand.years = skull$thousand.years)\nscores |&gt; select(id, PC1, PC2, thousand.years) |&gt; slice_head(n = 5)\n\n# A tibble: 5 √ó 4\n  id      PC1     PC2 thousand.years\n  &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1 S1    -3.62  0.879             888\n2 S2    -3.20 -0.382             112\n3 S3    -3.00 -1.00              854\n4 S4    -2.68  0.0862            171\n5 S5    -3.00 -0.463             544\n\n\nAnd then make the graph:\n\nggplot(scores, aes(x = PC1, y = PC2, color = thousand.years)) +\n  geom_point(alpha = 0.8) +\n  labs(color = \"Thousand\\nyears ago\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterpretation tip\nIf points separate along PC1 as time increases, that suggests the dominant multivariate trend is associated with time.\n\n\n\n\nWhat do the axes mean?\nThe loadings tell us how each original variable contributes to each PC. In fact, each PC is a linear combination of the original variables, weighted by the loadings.\n\nloadings &lt;- as_tibble(skull_pca$rotation, rownames = \"variable\")\nloadings\n\n# A tibble: 4 √ó 5\n  variable       PC1     PC2    PC3    PC4\n  &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 max.breadth  0.436  0.689  -0.568  0.107\n2 basi.height  0.536  0.108   0.643  0.537\n3 basi.length  0.440 -0.712  -0.474  0.272\n4 nasal.height 0.573 -0.0783  0.196 -0.792\n\n\nThese can help us interpret the PCs. For example, we see a strong positive loading of all four skull measurements on PC1. This suggests that PC1 represents overall size, as all measurements increase together. On the second PC axis, we see a mix of positive and negative loadings, indicating that PC2 captures shape differences where some measurements increase while others decrease. The positive PC2 loadings for head breadth and nasal height, combined with negative loadings for basi height and basi length, suggest that PC2 reflects a shape change where skulls become wider and taller in the nasal region while becoming shorter in the base dimensions.\nAnother way to interpret PCs is to look at the correlations between the original variables and the PCs. This can provide insights into how each original variable relates to the new PC axes. We can calculate these correlations as follows:\n\nX &lt;- skull |&gt;\n  dplyr::select(max.breadth, basi.height, basi.length, nasal.height)\nround(cor(X, skull_pca$x), 2)\n\n              PC1   PC2   PC3   PC4\nmax.breadth  0.65  0.62 -0.43  0.07\nbasi.height  0.80  0.10  0.49  0.33\nbasi.length  0.66 -0.64 -0.36  0.17\nnasal.height 0.86 -0.07  0.15 -0.49\n\n\nThink‚ÄìPair‚ÄìShare (#tps-pc-meaning) If PC1 is positively correlated with all four skull measurements, what biological interpretation is most natural?\n\n\nA simple biplot (scores + loadings)\nBelow is a lightweight biplot-style plot. (There are fancy versions in packages like factoextra, but we keep it minimal.)\n\n\n\n\n\n\n\n\n\nIn this graph we see all arrow point in the same horizontal direction, indicating that PC1 represents overall size. An increase in any of the four measurements will increase PC1.\nIn contrast, the arrows for PC2 point in different directions, indicating that PC2 represents shape differences where some measurements increase while others decrease. The two longest arrows are for basi.length and max.breadth, suggesting that these measurements contribute most strongly to shape variation captured by PC2.\nThink‚ÄìPair‚ÄìShare (#tps-biplot-interpretation) Make a sketch of how skull size / shape changes along the two PC axis.\nThink‚ÄìPair‚ÄìShare (#a_size_vs_shape) What pattern would indicate pure size change? What pattern would indicate shape change?",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#when-ordination-helps",
    "href": "10.1-ordination.html#when-ordination-helps",
    "title": "Ordination (L10)",
    "section": "When ordination helps",
    "text": "When ordination helps\nThink‚ÄìPair‚ÄìShare (#tps-when-ordination) In what situations would ordination methods like PCA be especially helpful? When would they be less useful? Think about the nature of the data, and, if you like, the biological questions.\nWe might think that ordination is more useful when we have a greater number of variables, because then the reducing down to two or three dimensions is more helpful. But the key factor is actually the correlation structure among the variables. If many variables are correlated, then ordination can capture most of the variance in a few dimensions. If variables are uncorrelated, then ordination may not help much since most of the variance is spread evenly across many dimensions. Let‚Äôs have a look a this with some simulated data. We will simulate three datasets with three variables each, but with different correlation structures: high correlation, moderate correlation, and low correlation.\nNow a 3D plot of the simulated data with different correlation structures (please note that these plots are best viewed in the HTML version of the book; in the PDF version, they will appear as static images):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot shows data with high correlation among variables, where the points are clustered along a diagonal line, indicating that the variables are strongly related. The second plot shows moderate correlation, where the points are more spread out but still show some clustering. The third plot shows low correlation, where the points are scattered randomly in space, indicating that the variables are largely independent.\nThink‚ÄìPair‚ÄìShare (#tps-ordination-correlation) What output of the PCA would you expect to differ among these three datasets?\nWe expect that in the high correlation case, the first principal component will capture a large proportion of the variance, while in the low correlation case, the variance will be more evenly distributed across all principal components. Let‚Äôs check this by performing PCA on each dataset and examining the variance explained by each principal component.\n\n\n[1] 0.925377783 0.065681354 0.008940863\n\n\n[1] 0.6571611 0.1978438 0.1449951\n\n\n[1] 0.3874361 0.3586825 0.2538815\n\n\nIt turns out that in the high correlation case, the first principal component captures a large proportion of the variance (92.5%). This is a large amount of variance explained by a single component. It is caused because we made a dataset where the variables are strongly correlated. There is only one axis along which the data vary strongly.\nIn contrast, in the zero correlation case, the variance is more evenly distributed across all principal components, with each component capturing around 38.7%, 35.9%, and 25.4% of the variance respectively. This indicates that there is no single dominant direction of variance in the data, as the variables are largely independent.\nIn the intermediate correlation case, the variance explained by each principal component is more balanced, with the first component capturing around 65.7% of the variance, and the remaining components capturing significant portions as well. This reflects the moderate correlation structure in the data.\nOrdination is most useful when it can reduce the dimensionality of the data while retaining most of the variance. This means that ordination methods like PCA will be more effective in summarizing the data in the high correlation case, as most of the information can be captured in just one or two dimensions. In contrast, in the low correlation case, ordination may not provide much dimensionality reduction, as each variable contributes independently to the overall variance.\nWhat determines if we have variables with high correlation in real datasets? Often, it is biological or physical relationships among the variables. For example, in morphological datasets, measurements of different body parts may be correlated due to overall size or shape factors. In ecological datasets, species abundances may be correlated due to shared environmental preferences or interactions. Understanding the underlying biology can help us anticipate when ordination methods will be most useful.",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#ordination-ii-nmds-non-metric-multidimensional-scaling",
    "href": "10.1-ordination.html#ordination-ii-nmds-non-metric-multidimensional-scaling",
    "title": "Ordination (L10)",
    "section": "Ordination II: NMDS (Non-metric Multidimensional Scaling)",
    "text": "Ordination II: NMDS (Non-metric Multidimensional Scaling)\nPCA is powerful, but it is a linear method and it relies on Euclidean geometry in the original variable space.\nNMDS is often used when:\n\nyou want ordination based on distances/dissimilarities\nthe relationships are not well represented by a linear method\nyou want flexibility in the choice of distance (e.g.¬†Bray‚ÄìCurtis, Gower, ‚Ä¶)\n\n\n\n\n\n\n\nImportant\n\n\n\nWhat NMDS does (conceptually)\n1) compute pairwise distances among objects\n2) place points in a low-dimensional space (usually 2D)\n3) try to preserve the rank order of distances (non-metric)\n4) report stress: lower is better (roughly: mismatch between original dissimilarities and ones in the lower-dimensional space)\n\n\n\nStep 1: Choose a distance measure\nHere our variables are numeric, so Euclidean distance is a reasonable default.\nWe will scale first (so variables contribute comparably), then compute distances:\n\nX_scaled &lt;- scale(X)\nD &lt;- dist(X_scaled, method = \"euclidean\")\n#D\n\n\n\nStep 2: Fit NMDS with multiple random starts\nIn NMDS, we need to choose some random starting configuration of points. And we want to make sure that we choose a good solution, not just a local optimum. Hence, we try multiple random starts and pick the best one.\nIn practice, use metaMDS() (from vegan) rather than calling the low-level optimizer directly. It tries multiple starting configurations and does useful housekeeping.\nWhen calling metaMDS(), we specify the following:\n\nk = 2 for a 2D solution.\ntrymax = 50 to try up to 50 random starts.\nautotransform = FALSE because we already scaled the data ourselves.\ntrace = FALSE to suppress output during fitting.\n\n\nset.seed(1)\nnmds &lt;- metaMDS(D, k = 2, trymax = 50, autotransform = FALSE, trace = FALSE)\nnmds\n\n\nCall:\nmetaMDS(comm = D, k = 2, trymax = 50, autotransform = FALSE,      trace = FALSE) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     D \nDistance: euclidean \n\nDimensions: 2 \nStress:     0.1467223 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 16 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n\n\nLots of information here, for example we have:\n\nthe call used\nthe type of distance\nthe number of dimensions\nthe number of random starts tried\nthe final stress value\nthe scaling of the data\n\n\n\nStep 3: Assess NMDS fit with stress\nThe stress of an NMDS solution quantifies how well the low-dimensional configuration preserves the rank order of the original dissimilarities. That is, it is a measure of mismatch between the distances in the original high-dimensional space and the distances in the reduced low-dimensional space. Here is the stress plot for our NMDS solution:\n\nstressplot(nmds)\n\n\n\n\n\n\n\n\nWe see a fairly good match between the original distances and the NMDS distances. The correlations are reasonably high, indicating that the NMDS solution captures the rank order of dissimilarities well.\nThe stress value is another measure of fit. Lower stress values indicate a better fit. A common rule of thumb is that stress &lt; 0.1 is a good fit, stress between 0.1 and 0.2 is acceptable, and stress &gt; 0.2 indicates a poor fit. A poor fit suggests that the data may not be well represented in two dimensions, and a higher-dimensional solution may be needed.\n\nnmds$stress\n\n[1] 0.1467223\n\n\nWe are in the range of okay fit in two dimensions.\nWhat improvement in stress do we get if we go to three dimensions?\n\nset.seed(1)\nnmds_3d &lt;- metaMDS(D, k = 3, trymax = 50, autotransform = FALSE, trace = FALSE)\nnmds_3d$stress\n\n[1] 0.07009879\n\n\nThe stress decreases when we move to three dimensions, indicating a better fit. However, the improvement may not be substantial enough to justify the added complexity of a three-dimensional solution. In practice, we often prefer two-dimensional solutions for ease of visualization and interpretation, unless the stress reduction is very large.\nWe can plot the NMDS scores, colored by time:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAxis directions are arbitrary\nFlipping or rotating an NMDS solution (or a PCA) does not change its meaning. Only relative distances among points matter.\n\n\nIf we wanted to know the biological interpretation of the NMDS axes, we would need to look at correlations between the original variables and the NMDS axes. This is just the same as we did for PCA. We could then plot the correlations as arrows on the NMDS plot to help interpret the axes.\n\ncor_nmds &lt;- round(cor(X, nmds_scores |&gt; select(NMDS1, NMDS2)), 2)\ncor_nmds\n\n             NMDS1 NMDS2\nmax.breadth   0.65  0.64\nbasi.height   0.80  0.03\nbasi.length   0.64 -0.57\nnasal.height  0.86 -0.09\n\n\nAnd plot these as arrows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis NMDS graph looks a lot like the PCA biplot. This is because our data are fairly linear and Euclidean distances are appropriate. In more complex datasets, NMDS can reveal patterns that PCA might miss.\n\n\nThink‚ÄìPair‚ÄìShare (#a_linear_vs_nonlinear_ordination) When might a linear method like PCA be misleading? What does NMDS relax?",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#hypothesis-testing-avoid-four-separate-regressions",
    "href": "10.1-ordination.html#hypothesis-testing-avoid-four-separate-regressions",
    "title": "Ordination (L10)",
    "section": "Hypothesis testing: avoid ‚Äúfour separate regressions‚Äù",
    "text": "Hypothesis testing: avoid ‚Äúfour separate regressions‚Äù\nIf you test time against each skull measurement separately, you face a multiple testing problem.\nA better match to the question (‚Äúdo skulls change through time in multivariate space?‚Äù) is a multivariate test.\nHere are four common approaches demonstrated but not deeply explained:\n\nMANOVA (parametric, multivariate normality assumptions)\n\nPERMANOVA (distance-based, permutation test)\n\nDispersion checks (are groups equally variable?)\nFitting time onto an ordination (envfit, ordisurf)\n\n\n1) MANOVA (parametric)\nMultivariate ANOVA (MANOVA) tests whether group centroids differ in multivariate space, assuming multivariate normality. The response is a matrix of multiple variables, and the explanatory variables can be categorical or continuous.\n\nman_mod &lt;- manova(cbind(max.breadth, basi.height, basi.length, nasal.height) ~ thousand.years,\n                  data = skull)\nsummary(man_mod, test = \"Pillai\")\n\n                Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nthousand.years   1 0.36751   21.064      4    145 1.042e-13 ***\nResiduals      148                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf we bin years into categories, MANOVA becomes closer to multivariate ANOVA:\n\nskull &lt;- skull |&gt;\n  mutate(year_class = cut(thousand.years, breaks = 8) |&gt; fct_inorder())\nman_mod_cat &lt;- manova(cbind(max.breadth, basi.height, basi.length, nasal.height) ~ thousand.years,\n                      data = skull)\nsummary(man_mod_cat, test = \"Pillai\")\n\n                Df  Pillai approx F num Df den Df    Pr(&gt;F)    \nthousand.years   1 0.36751   21.064      4    145 1.042e-13 ***\nResiduals      148                                             \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nCaution\n\n\n\nMANOVA assumptions matter\nMANOVA relies on multivariate normality and homogeneity of covariance matrices. In many biological datasets, these are imperfect‚Äîso distance-based alternatives are common.\n\n\n\n\n2) PERMANOVA (distance-based)\nPERMANOVA tests whether group centroids differ in multivariate space, using permutations. In vegan, use adonis2().\n\n# Use the same distance matrix D we used for NMDS\nadonis2(D ~ year_class, data = skull, permutations = 999)\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = D ~ year_class, data = skull, permutations = 999)\n          Df SumOfSqs      R2      F Pr(&gt;F)    \nModel      7   138.47 0.23234 6.1396  0.001 ***\nResidual 142   457.53 0.76766                  \nTotal    149   596.00 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n3) PERMDISP / betadisper: are groups equally variable?\nA common pitfall:\n\nPERMANOVA can be significant either because centroids differ\nor because dispersion differs (groups have different spread)\n\nSo it‚Äôs good practice to check dispersion when comparing groups.\n\nbd &lt;- betadisper(D, skull$year_class)\n#bd\nanova(bd)\n\nAnalysis of Variance Table\n\nResponse: Distances\n           Df Sum Sq Mean Sq F value Pr(&gt;F)\nGroups      7  0.929 0.13274  0.2284  0.978\nResiduals 142 82.529 0.58119               \n\npermutest(bd, permutations = 999)\n\n\nPermutation test for homogeneity of multivariate dispersions\nPermutation: free\nNumber of permutations: 999\n\nResponse: Distances\n           Df Sum Sq Mean Sq      F N.Perm Pr(&gt;F)\nGroups      7  0.929 0.13274 0.2284    999   0.98\nResiduals 142 82.529 0.58119                     \n\n\n\n\n4) Fitting time onto an ordination: envfit and ordisurf\nEven when you use an unconstrained ordination (PCA/NMDS), you may want to show how an explanatory variable aligns with it.\n\nenvfit: linear fit + permutation test\n\nfit_years &lt;- envfit(nmds ~ thousand.years, data = skull, permutations = 999)\nfit_years\n\n\n***VECTORS\n\n                   NMDS1     NMDS2     r2 Pr(&gt;r)    \nthousand.years  0.996500 -0.083553 0.3643  0.001 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 999\n\n\n\nplot(nmds, type = \"n\")\n\nspecies scores not available\n\npoints(nmds, display = \"sites\", pch = 16, cex = 0.8)\nplot(fit_years, col = \"black\")  # vector direction and significance\n\n\n\n\n\n\n\n\n\n\nordisurf: non-linear surface (GAM)\n\nplot(nmds, type = \"n\")\n\nspecies scores not available\n\npoints(nmds, display = \"sites\", pch = 16, cex = 0.8)\nordisurf(nmds ~ thousand.years, data = skull, add = TRUE)\n\n\n\n\n\n\n\n\n\nFamily: gaussian \nLink function: identity \n\nFormula:\ny ~ s(x1, x2, k = 10, bs = \"tp\", fx = FALSE)\n\nEstimated degrees of freedom:\n2.98  total = 3.98 \n\nREML score: 1092.962",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#pca-in-multiple-regression",
    "href": "10.1-ordination.html#pca-in-multiple-regression",
    "title": "Ordination (L10)",
    "section": "PCA in multiple regression",
    "text": "PCA in multiple regression\nPCA can also be used to reduce multiple explanatory variable into a few uncorrelated components, which can then be used in multiple regression models. This is particularly useful when the predictors are highly correlated, as it helps to avoid multicollinearity issues.\nThe steps for this would be:\n\nPerform PCA on the explanatory variables to obtain principal components.\nMake a linear regression model using the selected principal components as explanatory variables.\nInterpret the results in terms of the original variables, if needed.\n\nThe ‚Äúwin‚Äù here is that the principal components are uncorrelated, which simplifies the regression analysis and interpretation. The ‚Äúlose‚Äù is that the principal components may not have a straightforward biological interpretation, so care is needed when explaining the results.",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#review",
    "href": "10.1-ordination.html#review",
    "title": "Ordination (L10)",
    "section": "Review",
    "text": "Review\nOrdination is useful when we have multivariate data and when variables within that data are correlated. The correlation structure means that we can represent the data in a lower-dimensional space without losing too much information. If there is a lot of correlation among variables, then ordination becomes very useful. If there is little correlation, then ordination may not help much since most of the variance is spread evenly across many dimensions.\nSummary:\n\nPCA is a linear ordination method that finds orthogonal axes capturing the most variance.\nNMDS is a flexible, distance-based ordination method that preserves rank order of dissimilarities.\nBoth methods help visualize and summarize multivariate data.\nMultivariate hypothesis tests (MANOVA, PERMANOVA) assess group differences in multivariate space.\nChecking dispersion (PERMDISP) is important to interpret PERMANOVA results.\nFitting explanatory variables onto ordinations (envfit, ordisurf) helps interpret patterns.",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "10.1-ordination.html#further-reading",
    "href": "10.1-ordination.html#further-reading",
    "title": "Ordination (L10)",
    "section": "Further reading",
    "text": "Further reading\nEvelyn Chrystalla ‚ÄúE.C.‚Äù Pielou was a Canadian statistical ecologist. She began her career as a researcher for the Canadian Department of Forestry and the Canadian Department of Agriculture. Pielou‚Äôs books are classics that helped establish quantitative ecology. They range from the mathematics of ordination to kinds of (ecological) question ordination helps answer. A great place to start is The Interpretation of Ecological Data: A Primer on Classification and Ordination",
    "crumbs": [
      "Ordination (L10)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html",
    "href": "11.1-mixed-models.html",
    "title": "Mixed models (L11-1)",
    "section": "",
    "text": "Introduction\nSo far in BIO144 we mostly used models where each observation is assumed independent:\nBy independent we mean that the value of one observation does not give us any information about the value of another observation.\nBut many biological datasets violate independence because observations come in groups:\nIn these cases, treating all rows as independent often leads to false confidence (too-small standard errors, too-small p-values). Mixed models are one standard way to handle this.\nThink‚ÄìPair‚ÄìShare (#tps-why-mixed) Name one example in biology where multiple measurements come from the same ‚Äúunit‚Äù (individual, plot, lake, strain, batch‚Ä¶). What goes wrong if we pretend those measurements are independent?",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#introduction",
    "href": "11.1-mixed-models.html#introduction",
    "title": "Mixed models (L11-1)",
    "section": "",
    "text": "linear models (lm())\ngeneralized linear models (glm()), e.g.¬†Poisson and binomial.\n\n\n\n\nrepeated measures on the same individual (before/after, time series, multiple tissues)\nmultiple individuals from the same plot / site / stream / lake / cage / family\nstudents within classes, patients within hospitals, samples within batches\n\n\n\n\n\n\n\n\nImportant\n\n\n\nCore idea A mixed model extends regression by adding random effects that represent grouping structure (clusters) in the data.\n\nfixed effects: effects you want to estimate explicitly (treatments, temperature, time, ‚Ä¶)\nrandom effects: variation among groups (individuals, sites, years, ‚Ä¶) that induces correlation within groups",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#why-not-just-average",
    "href": "11.1-mixed-models.html#why-not-just-average",
    "title": "Mixed models (L11-1)",
    "section": "Why not just average?",
    "text": "Why not just average?\nA common workaround is to average repeated measurements to a single value per group and then use lm().\nSometimes this is OK ‚Äî but often it throws away information.\nReasons not to average include:\n\nImbalanced sampling (groups have different numbers of observations): averaging changes the weighting.\nWhich average? (mean, median, mode): different choices answer different questions.\nFalse confidence: pretending ‚Äún rows‚Äù are independent can make uncertainty look too small.\nYou may want to study variation among groups (some individuals respond more strongly than others).\n‚ÄúSharing information‚Äù across groups: mixed models partially pool group estimates toward the overall mean.\nKeeping information: you can use all observations without collapsing the design.\n\n\n\n\n\n\n\nNote\n\n\n\nA mixed model gives you a principled compromise between:\n\nanalysing each observation as a independent one, and\naveraging everything (too coarse).\n\nThis is sometimes called partial pooling.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#the-problem-non-independence-and-pseudoreplication",
    "href": "11.1-mixed-models.html#the-problem-non-independence-and-pseudoreplication",
    "title": "Mixed models (L11-1)",
    "section": "The problem: non-independence and pseudoreplication",
    "text": "The problem: non-independence and pseudoreplication\nImagine we measure the same individual multiple times.\nIf we fit a simple linear model, the residuals are assumed independent:\n\\[\\varepsilon_i \\sim \\text{Normal}(0, \\sigma^2), \\quad \\text{independent across } i\\]\nBut repeated measures induce correlation:\n\nmeasurements from the same individual tend to be more similar\nso residuals are not independent\n\nThis is one common form of pseudoreplication: treating repeated measurements as if they were separate independent replicates.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#random-intercept-models",
    "href": "11.1-mixed-models.html#random-intercept-models",
    "title": "Mixed models (L11-1)",
    "section": "Random intercept models",
    "text": "Random intercept models\n\nRandom intercept idea\nSuppose we measure a response \\(y\\) (e.g.¬†growth) for individuals \\(j\\) at observations \\(i\\). And that we measure each individual more than once. And that we have some treatment, such as temperature \\(x\\).\nThis means that our data have a grouping structure: observations are grouped by individual. If we want to calculate the mean growth, we should account for this grouping.\nLet‚Äôs make an example dataset to illustrate:\nRead in the data:\n\ndat_example &lt;- read_csv(\"datasets/mixed_model_example.csv\")\n\nRows: 50 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): individual\ndbl (2): temperature, growth\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHere are the first few rows:\n\nhead(dat_example)\n\n# A tibble: 6 √ó 3\n  individual temperature growth\n  &lt;chr&gt;            &lt;dbl&gt;  &lt;dbl&gt;\n1 Ind01             9.15  16.1 \n2 Ind01             9.37  17.4 \n3 Ind01             2.86   7.58\n4 Ind01             8.30  14.2 \n5 Ind01             6.42  13.8 \n6 Ind02             5.19  12.5 \n\n\nIn this data we have two sources of variation in growth:\n\nfixed effect of temperature (same for all individuals)\nrandom effect of individual (different baseline growth for each individual)\n\nA fixed effect is something we want to estimate explicitly (e.g.¬†how growth changes with temperature). Fixed effects are variables in which the values have specific meaning (e.g.¬†temperature = 5¬∞C).\nA random effect is something that varies among groups (individuals here), but where we are not interested in the specific values for each group. Instead, we want to estimate the amount of variation among groups. Random effects are variables where the specific values are not of interest, but rather the variation among them (e.g.¬†individual identity). I.e., the value ‚ÄúInd01‚Äù has no specific meaning; we just want to know how much individuals differ from each other on average. That value could be anything.\nA random effect is often containing levels that are considered a random sample from a larger population. We did not, for example, choose specific individuals for a reason; they are just a sample of all possible individuals.\nAn appropriate model for this data is a random-intercept mixed model. It is a model in which each group (individual) has its own intercept (baseline), but the intercepts are assumed to come from a common distribution.\nA random-intercept mixed model is:\n\\[y_{ij} = \\beta_0 + \\beta_1 x_{ij} + b_{0j} + \\varepsilon_{ij}\\]\nwhere:\n\n\\(\\beta_0, \\beta_1\\) are fixed effects\n\\(x_{ij}\\) is the explanatory variable for observation \\(i\\) in group \\(j\\). Also called a fixed effect.\n\\(b_{0j}\\) is a random intercept for group \\(j\\), typically \\(b_{0j} \\sim \\text{Normal}(0,\\sigma_b^2)\\)\n\\(\\varepsilon_{ij}\\) is residual error, \\(\\varepsilon_{ij} \\sim \\text{Normal}(0,\\sigma^2)\\)\n\nInterpretation:\n\neach group gets its own intercept (baseline), but\nthose intercepts are assumed to come from a common distribution\n\n\n\nRandom intercept syntax in R\nTo make a mixed model in R we can no longer use lm() or glm(). Instead, we use the lmer() function from the lme4 package.\nIn the lmer function we have to specify the random effects in a special way. For example, a random-intercept model for y with fixed effect x and random intercept by group is specified as:\ny ~ x + (1 | group)\nRead it as: ‚Äúa model for y with fixed effect x and a random intercept by group‚Äù.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#random-slope-models",
    "href": "11.1-mixed-models.html#random-slope-models",
    "title": "Mixed models (L11-1)",
    "section": "Random slope models",
    "text": "Random slope models\nSometimes groups differ not only in baseline level, but also in how they respond to an explanatory variable.\nA random-slope model:\n\\[y_{ij} = \\beta_0 + \\beta_1 x_{ij} + b_{0j} + b_{1j}x_{ij} + \\varepsilon_{ij}\\]\nIn R:\ny ~ x + (1 + x | group)\nHere each group has its own intercept and its own slope, and these can be correlated.\n\n\n\n\n\n\nCaution\n\n\n\nRandom slopes are powerful but can be hard to estimate with small datasets. If the model struggles to fit (singular fit warnings), consider simplifying.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#nested-and-crossed-random-effects",
    "href": "11.1-mixed-models.html#nested-and-crossed-random-effects",
    "title": "Mixed models (L11-1)",
    "section": "Nested and crossed random effects",
    "text": "Nested and crossed random effects\n\nNested\nNested means one grouping factor is contained within another.\nExample: measurements within plants within plots:\ny ~ treatment + (1 | plot/plant)\nThis expands to (1 | plot) + (1 | plot:plant).\n\n\nCrossed\nCrossed means groups are not nested.\nExample: repeated measures with multiple observers (each observer measures many individuals; each individual is measured by many observers):\ny ~ x + (1 | individual) + (1 | observer)",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#hands-on-example-plant-growth-with-repeated-measures",
    "href": "11.1-mixed-models.html#hands-on-example-plant-growth-with-repeated-measures",
    "title": "Mixed models (L11-1)",
    "section": "Hands-on example: plant growth with repeated measures",
    "text": "Hands-on example: plant growth with repeated measures\n\nBiological story\nYou are studying how temperature affects plant growth.\n\n30 plants are grown at one of three temperatures: 10¬∞C, 15¬∞C, 20¬∞C.\nEach plant is measured weekly for 8 weeks.\nResponse: plant height (cm).\n\nBecause we repeatedly measure the same plant, the observations are not independent. We will compare:\n\na naive linear model (wrong independence assumption),\na mixed model with plant as a random effect.\n\n\n\nAn example dataset\nRead in the data:\n\ndat_example &lt;- read_csv(\"datasets/plant_growth_repeated.csv\")\n\nRows: 240 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (1): plant_id\ndbl (3): temp, week, height\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nhead(dat)\n\n# A tibble: 6 √ó 4\n  plant_id temp   week height\n  &lt;fct&gt;    &lt;fct&gt; &lt;int&gt;  &lt;dbl&gt;\n1 P01      10        0   6.79\n2 P01      10        1   8.59\n3 P01      10        2  12.1 \n4 P01      10        3  11.0 \n5 P01      10        4  13.5 \n6 P01      10        5  16.7 \n\n\n\n\nExplore the data\nFirst we make a plot of the data:\n\n\n\n\n\n\n\n\n\nWe can see data from each individual plant because it is connected by a line. We also see that plants at higher temperature tend to be taller. And we see that plants differ in their baseline height (week 0).\nThere are two challenges here: 1. We have repeated measures on the same plants (non-independence). 2. We have variation among plants in baseline height.\nA mixed model can handle both of these.\n\n\nWrong model: treat all rows as independent\n\nm_lm &lt;- lm(height ~ week * temp, data = dat)\nanova(m_lm)\n\nAnalysis of Variance Table\n\nResponse: height\n           Df Sum Sq Mean Sq   F value    Pr(&gt;F)    \nweek        1 4478.8  4478.8 1888.8551 &lt; 2.2e-16 ***\ntemp        2  586.9   293.5  123.7678 &lt; 2.2e-16 ***\nweek:temp   2   46.9    23.4    9.8838 7.572e-05 ***\nResiduals 234  554.8     2.4                        \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis model pretends there are \\(30 \\times 8 = 240\\) independent data points. But most of that information is repeated measures on the same plants.\nWe can see that this is pseudoreplication because there are many more residual degrees of freedom (236) than plants (30). This is a classic sign of pseudoreplication. We really cannot trust the p-values or confidence intervals from this model.\n\n\nMixed model: random intercept for plant\nNow let‚Äôs fit a mixed model with plant identity as a random effect:\n\nm_lmm1 &lt;- lmer(height ~ week * temp + (1 | plant_id), data = dat)\nanova(m_lmm1)\n\nType III Analysis of Variance Table with Satterthwaite's method\n          Sum Sq Mean Sq NumDF  DenDF   F value    Pr(&gt;F)    \nweek      4478.8  4478.8     1 207.00 3264.1929 &lt; 2.2e-16 ***\ntemp        21.6    10.8     2  46.37    7.8666  0.001144 ** \nweek:temp   46.9    23.4     2 207.00   17.0806 1.362e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn the anova table, note that there are no degrees of freedom and no p-values for fixed effects. This is because calculating these in mixed models is complicated and there are multiple methods. We will look at this more later in this chapter.\nInterpretation:\n\nfixed effects describe the average relationship between height, week, and temperature\nthe random intercept captures baseline differences among plants\n\n\n\nmodel checking\nMixed-model diagnostics (i.e., model checking) can be more involved than lm(), but you can still start with:\n\nresidual vs fitted plot (nonlinearity / heteroscedasticity)\nnormal Q‚ÄìQ of residuals (approximate)\ncheck random effect estimates for extreme outliers\n\n\nResiduals vs fitted\n\n\nplot(fitted(m_lmm1), resid(m_lmm1),\n     xlab = \"Fitted values\", ylab = \"Residuals\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nNormal Q‚ÄìQ of residuals\n\n\nqqnorm(resid(m_lmm1)); qqline(resid(m_lmm1))\n\n\n\n\n\n\n\n\n\nScale‚Äìlocation (sqrt(|resid|) vs fitted)\n\n\nplot(fitted(m_lmm1), sqrt(abs(resid(m_lmm1))),\n     xlab = \"Fitted values\", ylab = \"sqrt(|Residuals|)\")\n\n\n\n\n\n\n\n\n\nResiduals vs explanatory variable (helpful for nonlinearity with a continuous explanatory variable)\n\n\nplot(dat$temp, resid(m_lmm1), xlab = \"x\", ylab = \"Residuals\")\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nDon‚Äôt over-interpret p-values\nIn mixed models, inference depends on how you handle degrees of freedom and uncertainty. In BIO144, focus on: - correct model structure (what must be random?) - effect sizes and uncertainty (CIs) - sensible plots and biological interpretation\n\n\n\n\nExtension: random slopes for week\nIf you believe plants differ in growth rate (not only baseline), add a random slope:\n\nm_lmm2 &lt;- lmer(height ~ week * temp + (1 + week | plant_id), data = dat)\n\nboundary (singular) fit: see help('isSingular')\n\nsummary(m_lmm2)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: height ~ week * temp + (1 + week | plant_id)\n   Data: dat\n\nREML criterion at convergence: 822.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.43220 -0.64862  0.04439  0.63469  2.31852 \n\nRandom effects:\n Groups   Name        Variance  Std.Dev. Corr\n plant_id (Intercept) 0.9012460 0.94934      \n          week        0.0006804 0.02608  1.00\n Residual             1.3682645 1.16973      \nNumber of obs: 240, groups:  plant_id, 30\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)   8.12308    0.38358  29.45344  21.177  &lt; 2e-16 ***\nweek          1.69948    0.05767 158.78156  29.469  &lt; 2e-16 ***\ntemp15        1.45872    0.54247  29.45344   2.689 0.011676 *  \ntemp20        2.24843    0.54247  29.45344   4.145 0.000263 ***\nweek:temp15   0.10592    0.08156 158.78156   1.299 0.195916    \nweek:temp20   0.45169    0.08156 158.78156   5.538 1.24e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) week   temp15 temp20 wk:t15\nweek        -0.404                            \ntemp15      -0.707  0.285                     \ntemp20      -0.707  0.285  0.500              \nweek:temp15  0.285 -0.707 -0.404 -0.202       \nweek:temp20  0.285 -0.707 -0.202 -0.404  0.500\noptimizer (nloptwrap) convergence code: 0 (OK)\nboundary (singular) fit: see help('isSingular')\n\n\nCompare the two mixed models:\n\nanova(m_lmm1, m_lmm2)\n\nrefitting model(s) with ML (instead of REML)\n\n\nData: dat\nModels:\nm_lmm1: height ~ week * temp + (1 | plant_id)\nm_lmm2: height ~ week * temp + (1 + week | plant_id)\n       npar    AIC    BIC  logLik deviance  Chisq Df Pr(&gt;Chisq)\nm_lmm1    8 826.51 854.35 -405.25   810.51                     \nm_lmm2   10 830.01 864.82 -405.00   810.01 0.4974  2     0.7798\n\n\nThere is little evidence that adding random slopes improves the model here (p = 0.77). But in other datasets it might.\n\n\n\n\n\n\nNote\n\n\n\nThis likelihood ratio test compares nested models. It is widely used, but has subtleties for random effects. In BIO144 you can treat it as a reasonable practical tool, while noting that ‚Äútesting random effects‚Äù is an advanced topic.\n\n\n\n\nVisualising fitted values\nWe can visualise the fitted values from both mixed models:\n\n\n\n\n\n\n\n\n\nThere is not much difference between the two models here, but in other datasets random slopes can make a big difference.\n\n\nSignificance testing for fixed effects\nCalculating p-values for fixed effects in mixed models is complicated, and there are multiple methods (Satterthwaite, Kenward-Roger, likelihood ratio tests, bootstrapping, Bayesian credible intervals). It is difficult because the degrees of freedom depend on the random effects structure and the data. There is no clear and objective method to get the degrees of freedom.\nNeverthless, we can get p-values for terms using the lmerTest package (optional). This changes the lmer() function to provide p-values using Satterthwaite‚Äôs method for degrees of freedom.\n\nm_lmm1 &lt;- lmer(height ~ week * temp + (1 | plant_id), data = dat)\nanova(m_lmm1)\n\nType III Analysis of Variance Table with Satterthwaite's method\n          Sum Sq Mean Sq NumDF  DenDF   F value    Pr(&gt;F)    \nweek      4478.8  4478.8     1 207.00 3264.1929 &lt; 2.2e-16 ***\ntemp        21.6    10.8     2  46.37    7.8666  0.001144 ** \nweek:temp   46.9    23.4     2 207.00   17.0806 1.362e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nReporting (template)\n\nWe modelled plant height as a function of week, temperature, and their interaction using a linear mixed model with plant identity as a random effect.\n\nHeight increased with week (fixed effect of week), and plants at higher temperature were taller on average (fixed effect of temperature).\nAll fixed effects had a p-value &lt; 0.05, calculate using Satterthwaite‚Äôs method for degrees of freedom (lmerTest package).\nIncluding plant as a random effect accounted for non-independence due to repeated measures.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#review",
    "href": "11.1-mixed-models.html#review",
    "title": "Mixed models (L11-1)",
    "section": "Review",
    "text": "Review\n\nMixed models are used when observations are grouped (non-independent).\nThey combine fixed effects (average relationships) with random effects (group-to-group variation).\nRandom intercepts model different baselines among groups; random slopes allow different responses.\nNested and crossed random effects reflect study design.\nMixed models let you keep all observations while avoiding pseudoreplication and false confidence.\n\nThink‚ÄìPair‚ÄìShare (#a_why_random_effects) What problem do random effects solve that fixed effects alone cannot?\nThink‚ÄìPair‚ÄìShare (#a_partial_pooling_intuition) How does partial pooling differ from complete pooling or no pooling at all? Hint: pooling is sharing of information.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.1-mixed-models.html#further-reading-optional-mixed-models",
    "href": "11.1-mixed-models.html#further-reading-optional-mixed-models",
    "title": "Mixed models (L11-1)",
    "section": "Further reading (optional): mixed models",
    "text": "Further reading (optional): mixed models\nIn this course, there is only a short introduction to mixed (multilevel) models. Students who are curious and would like to explore this topic further (purely for their own interest) may find the following resources useful. Material from these resources will not be examined in the final exam, unless it is also already present in the course book.\nMcElreath, R. ‚Äì Statistical Rethinking This book offers an excellent conceptual introduction to hierarchical (mixed) models, with a strong focus on understanding why they are useful. It introduces ideas such as partial pooling and shrinkage in a very intuitive way. The book uses a Bayesian perspective, but the conceptual insights are valuable even if you later apply frequentist methods. Strongly recommended by Owen for conceptual understanding.\nGelman, A. & Hill, J. ‚Äì Data Analysis Using Regression and Multilevel/Hierarchical Models A classic and very practical text on regression and mixed models, with many applied examples. It complements Statistical Rethinking well and is particularly useful if you want to understand mixed models as they are commonly used in practice.\nR documentation and tutorials for mixed-model packages If you are interested in implementation in R, the vignettes and documentation for packages such as lme4 (frequentist) or brms (Bayesian) provide hands-on examples of fitting, interpreting, and extending mixed models.\nThese resources are entirely optional and intended for students who wish to deepen their understanding beyond the scope of the course.",
    "crumbs": [
      "Mixed models (L11-1)"
    ]
  },
  {
    "objectID": "11.2-what-next.html",
    "href": "11.2-what-next.html",
    "title": "What next (L11-2)",
    "section": "",
    "text": "During this course, you have learned a variety of data analysis techniques using R, including data manipulation, visualization, and statistical modeling. You have gained a solid foundation in using R for data analysis, you can analyse data that meets the assumptions of linear models, and some types of data that do not (e.g., count data and binary data using GLMs).\nOf course, there is much more to learn! There are many opportunities for you to further develop your skills in R and data analysis. What is the next step after this course? What are the options to further improve your skills in data analysis in R? What other types of analyses could you learn about, and when might you need them?\nHere are a list of other types of problem / question that we might have, and types of analysis that could be relevant. The list is by no means exhaustive, but it should give you some ideas of what to explore next, and what to explore when you encounter specific types of data or research questions.\n\nTime series analysis: If your data are collected over time (e.g., daily, monthly, yearly), you might need to learn about time series analysis techniques such as ARIMA models, seasonal decomposition, and forecasting methods. A key feature of time series data is that observations are not independent, which violates assumptions of many standard statistical methods. This needs to be carefully handled in the analysis.\nSpatial analysis: If your data have a spatial component (e.g., locations, regions), you might need to learn about spatial statistics, geostatistics, and spatial modeling techniques. This could include methods such as kriging, spatial autocorrelation analysis, and spatial regression models. Again, spatial data often violate independence assumptions, requiring specialized methods.\nNon-linear regression: If the relationship between your explanatory variables and response variable is not linear, you might need to learn about non-linear regression techniques. These can estimate the parameters of specific non-linear functions, and to assess the goodness of fit.\nBreakpoint analysis: If you suspect that there are changes in the relationship between variables at certain points (e.g., before and after an intervention), you might need to learn about breakpoint analysis techniques, such as piecewise regression or change point detection methods.\nGeneralized Additive Models (GAMs): If you want to model complex, non-linear relationships between explanatory variables and response variables while maintaining some interpretability, you might need to learn about GAMs. These models use smooth functions to capture non-linear effects. They are rather elegant!\nStructural Equation Modeling (SEM): If you want to analyze complex relationships among multiple variables, including latent variables, you might need to learn about SEM techniques. SEM allows for the modeling of direct and indirect effects, as well as measurement error. Effectively, we can build and test complex causal models. Variables can be both explanatory variables and responses at the same time.\nMachine Learning: If you want to make predictions or classify data based on patterns, you might need to learn about machine learning techniques such as decision trees, random forests, support vector machines, and neural networks. These methods can handle large datasets and complex relationships but may sacrifice some interpretability.\nMeta-analysis: If you want to synthesize results from multiple studies to draw broader conclusions, you might need to learn about meta-analysis techniques. This involves combining effect sizes from different studies and assessing heterogeneity among them.\nSurvival analysis: If your data involve time-to-event response variables (e.g., time until failure, time until death), you might need to learn about survival analysis techniques such as Kaplan-Meier estimation, Cox proportional hazards models, and parametric survival models.\nNon-parametric methods: If your data do not meet the assumptions of parametric tests (e.g., normality, homoscedasticity), and you really can‚Äôt figure out how to make a parametric model (e.g., LM or GLM) you might need to learn about non-parametric methods such as rank-based tests, bootstrapping, and permutation tests.\nPower analysis and sample size estimation: If you want to design studies with adequate statistical power, you might need to learn about power analysis techniques. This involves calculating the required sample size based on effect sizes, significance levels, and desired power.\nBayesian statistics: If you want to incorporate prior knowledge and uncertainty into your analyses, you might need to learn about Bayesian statistical methods. This involves using Bayes‚Äô theorem to update prior beliefs based on observed data.\n\nThese are just a few examples of the many types of analyses that you might encounter in your data analysis journey. The choice of which techniques to learn next will depend on your specific research questions, data characteristics, and goals. Ideally you will plan your analyses when you design your study, so that you can collect the right type of data to answer your questions. When you don‚Äôt or when something changes, you can then explore and discussion with experts which techniques are most appropriate.\nA final word of advice‚Ä¶ try to not be driven by techniques. Instead, be driven by your research questions. After all, we are not doing data analysis for its own sake, but to answer questions about the world around us. Let your questions guide your learning journey!",
    "crumbs": [
      "What next (L11-2)"
    ]
  },
  {
    "objectID": "12.1-review.html",
    "href": "12.1-review.html",
    "title": "Review (L12)",
    "section": "",
    "text": "You need to prepare in advance for this lecture.\nIn the final lecture of the course, we will review and repeat any of the content that you find challenging. Please write in the Forum which topics you would like to revisit, so that we can focus on those during the session. There may also be chances to ask during the lecture time questions on topics that you find difficult.\nSo the preparation for this lecture is to think about which topics you would like to discuss again, and to post these in the Forum.\nHere are some ideas of topics that you might want to revisit or learn about:\n\nTypes of study designs in biology.\nFraming research questions.\nObservational studies.\nExperimental studies.\nKey modeling concepts: interactions, overfitting, degrees of freedom, variance explained, hypothesis testing, null hypothesis\n\nParametric vs non-parametric approaches: what they mean and when to use them.\nGoing through any of the practical exercises in the course.\nAI Assistant prompt creation‚Ä¶ how much can we make it do, and how much can we trust it?",
    "crumbs": [
      "Review (L12)"
    ]
  }
]