% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  letterpaper,
  8pt,
  a4paper]{book}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={BIO144 Course Book},
  pdfauthor={Owen Petchey},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{BIO144 Course Book}
\author{Owen Petchey}
\date{2025-11-22}

\begin{document}
\frontmatter
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, frame hidden, enhanced, sharp corners, interior hidden, borderline west={3pt}{0pt}{shadecolor}, breakable]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\setcounter{tocdepth}{1}
\tableofcontents
}
\mainmatter
\bookmarksetup{startatroot}

\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This book contains the content of the course BIO144 Data Analysis in
Biology at the University of Zurich. It is intended to be used as a
companion to the lectures and practical exercises of the course. All of
the required content of the course (i.e., what could be in the final
exam) is included in this book. Additional content is included for those
who want to learn more.

Beware that Owen sometimes makes updates to the book during the
semester, so the version you have may not exactly match the lectures you
attend. However, all of the required content will be the same, and any
changes will be correcting typos or improving explanations.

If you'd like a copy of this book for yourself, there are a few ways.
But beware: if you take a local copy then it will not be updated when
Owen makes changes to the online version!

\begin{itemize}
\item
  You can download a PDF version of the entire book: ðŸ“„ Download PDF
\item
  You can download a complete local copy of the HTML version of the
  BIO144 course book from here:\\
  \url{https://github.com/opetchey/BIO144_Course_Book/tree/main}. The
  html files for the book are in the \texttt{\_book} folder, and this is
  the only folder you need for your offline html copy of the book. You
  can open the \texttt{index.html} file in your web browser to read the
  book offline.
\item
  You can get all of the source code for the book from the
  \href{https://github.com/opetchey/BIO144_Course_Book}{GitHub
  repository}. However, you may find it a little complicated to do
  anything useful with it!
\end{itemize}

\hypertarget{how-this-book-was-made}{%
\section*{How this book was made}\label{how-this-book-was-made}}
\addcontentsline{toc}{section}{How this book was made}

\markright{How this book was made}

The book was written using a type of RMarkdown. It allows a script with
a mix of normal text and R code to produce chapters and a book that has
a mixture of text, R code, and R output. Rmarkdown is very useful for
making reports, books, presentations, and even websites.

This book is a Quarto book. To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\hypertarget{acknowledgements}{%
\section*{Acknowledgements}\label{acknowledgements}}
\addcontentsline{toc}{section}{Acknowledgements}

\markright{Acknowledgements}

The content was based on lectures originally written by
\href{https://www.ntnu.edu/employees/stefanie.muff}{Dr Stefanie Muff}.

The content of the book was written with the assistance of
\href{https://copilot.github.com/}{Github Copilot}, an AI tool that
helps write code and text.

\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

The first lecture of the course introduces it, gives some important
information, and sets the stage for the rest of the course. Some of the
time in the lecture will be used to create a dataset for use during the
course. It also gives an opportunity to review some of the things about
R and statistics that it is very useful to already know.

The lecture includes:

\begin{itemize}
\tightlist
\item
  Goals of the course
\item
  Course organisation
\item
  AI and the course
\item
  Making a course dataset
\item
  Using RStudio
\item
  Reviewing what you should already know
\item
  Learning objectives
\end{itemize}

\hypertarget{notation-and-some-definitions}{%
\section*{Notation and some
definitions}\label{notation-and-some-definitions}}
\addcontentsline{toc}{section}{Notation and some definitions}

\markright{Notation and some definitions}

Throughout the course, we will use the following notation:

\begin{itemize}
\tightlist
\item
  \(x\) for a variable. Typically this variable contains a set of
  observations. These observations are said to represent a sample of all
  the possible observations that could be made of a \emph{population}.
\item
  \(x_1, x_2, \ldots\) for the values of a variable
\item
  \(x_i\) for the \(i\)th value of a scalar variable. This is often
  spoken as ``x sub i'' or the ``i-th value of x''.
\item
  \(x^{(1)}\) for variable 1, \(x^{(2)}\) for variable 2, etc.
\item
  The mean of the sample \(x\) is \(\bar{x}\). This is usually spoken as
  ``x-bar''.
\item
  The mean of \(x\) is calculated as
  \(\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i\).
\item
  \(n\) is the number of observations in a sample.
\item
  The summation symbol \(\sum\) is used to indicate that the values of
  \(x\) are summed over all values of \(i\) from 1 to \(n\).
\item
  The standard deviation of the sample is \(s\). The standard deviation
  of the population is \(\sigma\).
\item
  The variance is \(s^2\). The variance of the population is
  \(\sigma^2\).
\item
  The variance of the sample is calculated as
  \(s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2\).
\item
  The standard deviation of the sample is calculated as
  \(s = \sqrt{s^2}\).
\item
  \(y\) is usually used to represent a dependent / response variable.
\item
  \(x\) is usually used to represent an independent / predictor /
  explanatory variable.
\item
  \(\beta_0\) is usually used to denote the intercept of a linear model.
\item
  \(\beta_1\), \(\beta_2\), etc. are usually used to denote the
  coefficients of the independent variables in a linear model.
\item
  Estimates are denoted with a hat, so \(\hat{\beta}_0\) is the estimate
  of the intercept of a linear model.
\item
  Hence, the estimated value of \(y_i\) in a linear regression model is
  \(\hat{y_i} = \hat{\beta}_0 + \hat{\beta}_1 x_i^{(1)}\).
\item
  \(e_i\) is the residual for the \(i\)th observation in a linear model.
  The residual is the difference between the observed value of \(y_i\)
  and the predicted value of \(y_i\) (\(\hat{y_i}\)).
\item
  Often we assume errors are normally distributed with mean 0 and
  variance \(\sigma^2\). This is written as \(e_i \sim N(0, \sigma^2)\).
\item
  SST is the total sum of squares. It is the sum of the squared
  differences between the observed values of \(y\) and the mean of
  \(y\). It is calculated as \(\sum_{i=1}^n (y_i - \bar{y})^2\).
\item
  SSM is the model sum of squares. It is the sum of the squared
  differences between the predicted values of \(y\) and the mean of
  \(y\). It is calculated as \(\sum_{i=1}^n (\hat{y_i} - \bar{y})^2\).
\item
  SSE is the error sum of squares. It is the sum of the squared
  differences between the observed values of \(y\) and the predicted
  values of \(y\). It is calculated as
  \(\sum_{i=1}^n (y_i - \hat{y_i})^2\).
\item
  The variance of \(x\) can be written as \(Var(x)\). The covariance
  between \(x\) and \(y\) can be written as \(Cov(x, y)\).
\item
  Covariance is calculated as
  \(Cov(x, y) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})\).
\item
  \(H_0\) is the null hypothesis.
\item
  \(\alpha\) is the significance level.
\item
  \emph{df} is the degrees of freedom.
\item
  \(p\) is the p-value.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{r-and-rstudio-l2}{%
\chapter*{R and RStudio (L2)}\label{r-and-rstudio-l2}}
\addcontentsline{toc}{chapter}{R and RStudio (L2)}

\markboth{R and RStudio (L2)}{R and RStudio (L2)}

\hypertarget{sec-using-r-and-rstudio}{%
\section*{Getting R and RStudio}\label{sec-using-r-and-rstudio}}
\addcontentsline{toc}{section}{Getting R and RStudio}

\markright{Getting R and RStudio}

\textbf{\emph{R}} is a programming language and software environment for
statistical computing and graphics. RStudio is an integrated development
environment (IDE) for R. \textbf{\emph{RStudio}} provides a
user-friendly interface for working with R, including a console, a
script editor, and tools for managing packages and projects.

We highly recommend using \textbf{RStudio} to work with \textbf{R}.

There are two ways to use \textbf{RStudio}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{RStudio Desktop}: a standalone application that you can
  install on your computer. If you choose this option, you will need to
  install R first, and then install RStudio. This usually requires
  administrator privileges on your computer. If you have problems
  installing add-on packages, they will have to be fixed by you or with
  our help (rarely we cannot find a solution). Follow the instructions
  on this website about how to install R and RStudio:
  https://posit.co/download/rstudio-desktop/.
\item
  \textbf{Rstudio Cloud}: a web-based version of RStudio that you can
  use in your web browser. You don't need to install anything on your
  computer, and you can access your work from any computer with an
  internet connection. The Faculty of Science has a
  \href{https://rstudio2024.mnf.uzh.ch}{RStudio Cloud here} that you can
  use (and will have to use during the final exam).
\end{enumerate}

What do we recommend? Try the cloud first. If you like it then continue
to use it.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-important-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, toprule=.15mm, breakable]

Whether you use the RStudio application on your computer, or use RStudio
on the Cloud, you are responsible for the safety and persistence of your
files (data, code, etc.). Just because you're using RStudio on the Cloud
does not mean your files are automatically saved forever. Make sure to
download and back up your important files regularly!

\end{tcolorbox}

\hypertarget{sec-getting-to-know-the-rstudio-ide}{%
\section*{Getting to know the RStudio
IDE}\label{sec-getting-to-know-the-rstudio-ide}}
\addcontentsline{toc}{section}{Getting to know the RStudio IDE}

\markright{Getting to know the RStudio IDE}

When you open RStudio, you will see a window with four main panes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Source pane}: where you can write and edit R scripts, R
  Markdown documents, and other files. This pane can have multiple tabs,
  so you can have several files open at the same time.
\item
  \textbf{Console pane}: where you can type and execute R commands
  directly. This pane has multiple tabs, including: \textbf{Console},
  \textbf{Terminal}, and \textbf{Jobs}. During this course we will
  mostly use the \textbf{Console} tab.
\item
  \textbf{Environment pane}: where you can see the objects (data frames,
  vectors, etc.) that are currently in your R session. This pane has
  multiple tabs, including: \textbf{Environment}, \textbf{History},
  \textbf{Connections}, and \textbf{Tutorial}. During this course we
  will mostly use the \textbf{Environment} tab.
\item
  \textbf{Files/Plots/Packages/Help pane}: where you can manage files,
  view plots, manage packages, and access help documentation. This pane
  has multiple tabs, including: \textbf{Files}, \textbf{Plots},
  \textbf{Packages}, \textbf{Help}, and \textbf{Viewer}. During this
  course we will mostly use the \textbf{Files}, \textbf{Plots},
  \textbf{Packages}, and \textbf{Help} tabs.
\end{enumerate}

Our \textbf{Scripts} are in the Source pane tabs. The code / script we
write in R is usually saved in a file with the extension \texttt{.R}.
This file can be opened and edited in the Source pane. Creating a new R
script: File \textgreater{} New File \textgreater{} R Script.

You can run code from the script by selecting the code and clicking the
``Run'' button, or by using the keyboard shortcut
\texttt{Ctrl\ +\ Enter} (Windows) or \texttt{Cmd\ +\ Enter} (Mac).

There is so much more to learn about the RStudio IDE, but we will cover
that as we go along in the course.

\hypertarget{sec-getting-to-know-r}{%
\section*{Getting to know R}\label{sec-getting-to-know-r}}
\addcontentsline{toc}{section}{Getting to know R}

\markright{Getting to know R}

In our newly opened script file, type the following code:

Then type the following code:

\begin{verbatim}
[1] 2
\end{verbatim}

\begin{verbatim}
[1] 7.389056
\end{verbatim}

\begin{verbatim}
[1] 4
\end{verbatim}

Select all the code and run it (using the ``Run'' button or
\texttt{Ctrl\ +\ Enter} / \texttt{Cmd\ +\ Enter}). You should see the
results of the calculations in the Console pane.

Now try assigning values to named object:

And then print the value of \texttt{c}:

\begin{verbatim}
[1] 15
\end{verbatim}

You should see the value \texttt{15} printed in the Console pane.

We can also create vectors:

\begin{verbatim}
[1] 1 2 3 4 5
\end{verbatim}

And do maths on vectors:

\begin{verbatim}
[1]  2  4  6  8 10
\end{verbatim}

\begin{verbatim}
[1] 11 12 13 14 15
\end{verbatim}

\begin{verbatim}
[1]  1  4  9 16 25
\end{verbatim}

We can vectors of strings (text):

\begin{verbatim}
[1] "apple"  "banana" "cherry"
\end{verbatim}

And can perform operations on strings:

\begin{verbatim}
[1] "I like apple"  "I like banana" "I like cherry"
\end{verbatim}

\begin{verbatim}
[1] "APPLE"  "BANANA" "CHERRY"
\end{verbatim}

And we can create data frames, which are like tables of data

\begin{verbatim}
     Name Age Height
1   Alice  25    165
2     Bob  30    180
3 Charlie  35    175
\end{verbatim}

Above we have numerous examples of functions: \texttt{exp()},
\texttt{sqrt()}, \texttt{c()}, \texttt{paste()}, \texttt{toupper()}, and
\texttt{data.frame()}. Functions are a fundamental part of R
programming. They are used to perform specific tasks, such as
calculations, data manipulation, and data analysis. All functions have a
name and can take arguments (inputs) and return values (outputs). They
are called by writing the function name followed by parentheses, with
any arguments inside the parentheses.

You likely guessed that there is much much more to learn about R, but we
will cover that as we go along in the course.

\hypertarget{sec-getting-help}{%
\section*{Getting help}\label{sec-getting-help}}
\addcontentsline{toc}{section}{Getting help}

\markright{Getting help}

R has a built-in help system that you can use to get information about
functions, packages, and other topics. To access the help system, you
can use the \texttt{?} operator followed by the name of the function or
topic you want to learn about. For example, to get help on the
\texttt{mean()} function, you would type:

This will open the help documentation for the \texttt{mean()} function
in the Help pane of RStudio. The documentation includes a description of
the function, its arguments, and examples of how to use it. Some of the
help documentation is very useful and accessible, other is less so. Over
time you will learn which functions and packages have good
documentation, and you will get better and better at understanding R
help files.

Of course you can use any other resources to get help with R, including
online forums, tutorials, and books. Some popular online resources for R
help include:

\begin{itemize}
\tightlist
\item
  \href{https://stackoverflow.com/questions/tagged/r}{Stack Overflow}
\item
  \href{https://community.rstudio.com/}{RStudio Community}
\item
  \href{https://www.r-bloggers.com/}{R-bloggers}
\item
  \href{https://www.r-graph-gallery.com/}{The R Graph Gallery}
\end{itemize}

You can also use search engines like Google to find answers to your R
questions. Just be sure to include ``R'' in your search query to get
relevant results.

AI assistants like ChatGPT can also be useful for getting help with R
programming. You can ask specific questions about R code, functions, and
packages, and get instant responses.

And of course there is always your course instructors and fellow
students to help you out when you get stuck.

\hypertarget{sec-add-on-packages}{%
\section*{Add-on packages}\label{sec-add-on-packages}}
\addcontentsline{toc}{section}{Add-on packages}

\markright{Add-on packages}

R has a vast ecosystem of add-on packages that extend its functionality.
These packages are collections of functions, data, and documentation
that can be installed and loaded into your R session. There are
thousands of packages available on CRAN (the Comprehensive R Archive
Network) and other repositories like Bioconductor and GitHub.

We will be using several packages throughout this course. To install a
package, you can use the \texttt{install.packages()} function. For
example, to install the \texttt{ggplot2} package, you would type:

You can also use the RStudio interface to install packages. Go to the
``Packages'' tab in the bottom right pane, click on ``Install'', type
the name of the package you want to install, and click ``Install''.

You can see which packages are currently installed by looking in the
``Packages'' tab.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

You only need to install a package once. After it is installed, you can
load it into your R session using the \texttt{library()} function. Do
not install packages every time you want to use them; just load them
with \texttt{library()}.

\end{tcolorbox}

\hypertarget{sec-r-versions}{%
\section*{R Version and add-on package versions}\label{sec-r-versions}}
\addcontentsline{toc}{section}{R Version and add-on package versions}

\markright{R Version and add-on package versions}

(This section concerns the Desktop version of R and RStudio, and not so
much the Cloud version, because version management is handled for you in
the Cloud.)

R and its add-on packages are constantly being updated and improved.
This can cause problems when trying to install or use packages that
depend on specific versions of R or other packages.

Imagine that the online version of a package has been updated and now
only works with the lastest version of R. If you are using an older
version of R, you may not be able to install or use that package.

Or if a package depends on another package that has been updated, you
may need to update that package as well to use the first package.

This sounds complicated, but there are some simple steps you can take to
reduce the chances of running into version-related problems:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Keep your R version up to date.} New versions of R are
  released every 6 months or so, and they often include important bug
  fixes and new features. You can check your current R version by typing
  \texttt{R.version.string} in the Console. To update R, you can
  download the latest version from the
  \href{https://cran.r-project.org/}{CRAN website}.
\item
  \textbf{Keep your add-on packages up to date.} You can update all your
  installed packages by using the \texttt{update.packages()} function.
  This will check for updates for all installed packages and install the
  latest versions. You can also use the RStudio interface to update
  packages by going to the ``Packages'' tab, clicking on ``Update'',
  selecting the packages you want to update, and clicking ``Install
  Updates''.
\item
  \textbf{Do this well before critical deadlines or important events
  (e.g., exams).} Updating R and packages can sometimes lead to
  unexpected issues, so it's best to do it well in advance of when you
  need everything to work perfectly.
\end{enumerate}

Nevertheless, even with these precautions, you may still encounter
version-related issues from time to time. When this happens, don't
panic!

A common problem you might see is an error message when trying to
install or load a package, indicating that the package requires a newer
version of R or another package. The error / warning message might look
like:

\texttt{warning:\ package\ \textquotesingle{}xyz\textquotesingle{}\ requires\ R\ version\ \textgreater{}=\ 4.2.0}

\texttt{Warning\ in\ install.packages:\ package\ â€˜XYZâ€™\ is\ not\ available\ (for\ R\ version\ 4.2.0)}

These messages indicate that the package you are trying to install or
load requires a newer version of R than the one you currently have. To
fix this, you will need to update your R installation to the required
version or higher. Then its also a good idea to update your packages as
well.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-note-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toprule=.15mm, breakable]

RStudio is also regularly updated, with new version released every
several months or so. Your version of RStudio is independent of your
version of R, so you can update RStudio without changing your R version.
Note that usually your version of RStudio is not as important as your
version of R and the packages you are using. So updating RStudio is
usually not a high priority and doesn't often help solve problems
related to add on package versions.

\end{tcolorbox}

\hypertarget{sec-r-projects}{%
\section*{R Projects}\label{sec-r-projects}}
\addcontentsline{toc}{section}{R Projects}

\markright{R Projects}

I always work within R Projects. R Projects help you to organise your
work and keep all files related to a project in one place. They also
make importing data a breeze.

But what is an R Project? An R Project is a directory (folder) that
contains all the files related to a specific project. When you open an R
Project, RStudio automatically sets the working directory to the project
directory, so you don't have to worry about setting the working
directory manually.

To see if you're working within an R Project, look at the top right of
the RStudio window. If you see the name of your project there, you're
good to go. If you see ``Project: (None)'', then you're not working
within an R Project.

If you click on the project name, a dropdown menu will appear. From
there, you can create a new project, open an existing project, or switch
between projects.

\textbf{Create a new R Project:} File \textgreater{} New Project
\textgreater{} New Directory or Existing Directory \textgreater{} New
Project \textgreater{} Choose a name and location for your project
\textgreater{} Create Project.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-important-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, toprule=.15mm, breakable]

\textbf{Get organised!} Put all files for a project in one folder. For
example, I made a folder called \texttt{BIO144\_2026} and put all files
related to this course in that folder. Within that folder, I have
subfolders for \texttt{data}, \texttt{scripts}, and \texttt{results}. I
then create an R Project in the \texttt{BIO144\_2026} folder. This way,
all files related to the course are in one place, and I can easily find
them later.

\end{tcolorbox}

Now, always open and ensure you're working within the R Project for your
project. As mentioned, you can see the project name at the top right of
the RStudio window. And if its not the correct project, click on the
name to get the drop-down list of available projects from which you can
switch to the correct one.

\hypertarget{sec-importing-data}{%
\section*{Importing data}\label{sec-importing-data}}
\addcontentsline{toc}{section}{Importing data}

\markright{Importing data}

First, get some data sets for us to work with. \textbf{XYZ} You can
download them from the course website or use your own data sets. Save
the data files in a folder called \texttt{data} within your R Project
directory.

We will use the \texttt{readr} package to import data into R. The
\texttt{readr} package provides functions to read data from various file
formats, including CSV (comma-separated values) files, tab-separated
values files, and others.

To read a CSV file, we can use the \texttt{read\_csv()} function from
the \texttt{readr} package. For example, to read a CSV file called
\texttt{data.csv}, we can use the following code:

This code will read the \texttt{data.csv} file from the \texttt{data}
folder within the current working directory (which should be the R
Project directory) and store it in a data frame called \texttt{data}.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

\textbf{Easily getting the file path} In RStudio, you can easily get the
file path by putting the cursor in the parentheses of the
\texttt{read\_csv()} function, the press the tab key. A drop-down menu
will appear with options to navigate to the file. This way, you don't
have to type the file path manually.

\end{tcolorbox}

\hypertarget{sec-viewing-data}{%
\section*{Viewing the data}\label{sec-viewing-data}}
\addcontentsline{toc}{section}{Viewing the data}

\markright{Viewing the data}

Once you've imported your data, you can view it in several ways:

\begin{itemize}
\tightlist
\item
  Click on the data frame in the Environment tab in RStudio to open it
  in a new tab.
\item
  Use the \texttt{View()} function to open the data frame in a new tab
  in RStudio.
\item
  Use the \texttt{head()} function to view the first few rows of the
  data frame.
\item
  Use the \texttt{str()} function to view the structure of the data
  frame, including the variable names and types.
\item
  Use the \texttt{summary()} function to get a summary of the data
  frame, including basic statistics for each variable.
\end{itemize}

Another useful function is \texttt{glimpse()} from the \texttt{dplyr}
package, which provides a quick overview of the data frame.

There are many checks you can do to ensure your data was imported
correctly. For example checking if there are duplicated values in a
variable when there shouldn't be:

\begin{verbatim}
[1] FALSE
\end{verbatim}

The function \texttt{any()} will return \texttt{TRUE} if there are any
duplicated values in the \texttt{Name} variable, and \texttt{FALSE}
otherwise. The function \texttt{duplicated()} returns a logical vector
indicating which values are duplicates. We use the dollar sign
\texttt{\$} to access a specific variable (column) in the data frame. A
logical vector is a vector that contains only \texttt{TRUE} or
\texttt{FALSE} values:

\begin{verbatim}
[1] FALSE FALSE FALSE
\end{verbatim}

All three logicals are \texttt{FALSE}, meaning none of the three are
duplicates. If there were duplicates, the corresponding positions in the
logical vector would be \texttt{TRUE}. For example:

What do you expect the output of \texttt{duplicated(example\_vector)} to
be?

A final check (though not the final one we could do--there are many
others). Let us check for missing values and get a count of how many
there are in each variable. We can do this with the following
\emph{tidyverse} code:

\begin{verbatim}
  Name Age Height
1    0   0      0
\end{verbatim}

Looks complicated eh! Well, that's because it is, for sure. But let's
break it down:

\begin{itemize}
\tightlist
\item
  \texttt{summarise()} creates a new data frame with summary statistics.
\item
  \texttt{across(everything(),\ \textasciitilde{}\ sum(is.na(.)))}
  applies the function \texttt{sum(is.na(.))} to every variable in the
  data frame.
\item
  The \texttt{is.na()} function returns a logical vector indicating
  which values are missing (\texttt{NA}), and the \texttt{sum()}
  function counts the number of \texttt{TRUE} values in that vector
  (i.e., the number of missing values).
\end{itemize}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-important-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-important-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-important-color}{\faExclamation}\hspace{0.5em}{Important}, toprule=.15mm, breakable]

\textbf{Let's assume your data was imported incorrectly.} This means you
have to inspect it carefully. Check that the variable names are correct,
that the data types are correct (e.g., numeric, character, factor), that
there are the correct number of rows and columns. If you find any
issues, you need to find out what caused them, fix them, and re-import
the data (see below).

\end{tcolorbox}

Common data import problems:

\begin{itemize}
\tightlist
\item
  Incorrect delimiter: If your data file uses a different delimiter
  (e.g., tab, semicolon), you need to specify it in the
  \texttt{read\_csv()} function using the \texttt{delim} argument (e.g.,
  \texttt{read\_delim("data.csv",\ delim\ =\ "\textbackslash{}t")} for
  tab-delimited files).
\item
  Missing values: If your data file uses a specific value to represent
  missing data (e.g., ``NA'', ``-999''), you need to specify it in the
  \texttt{read\_csv()} function using the \texttt{na} argument (e.g.,
  \texttt{read\_csv("data.csv",\ na\ =\ c("NA",\ "-999"))}).
\item
  Only one column: If your data file has only one column, it may be
  because the delimiter is incorrect. Check the delimiter and re-import
  the data with the correct delimiter.
\item
  You opened the downloaded file in Excel and then saved it: Excel may
  have changed the format of the file when you opened and saved it.
  Always work with the original downloaded file.
\item
  Wrong path or file name: Make sure the file path and name are correct.
  Remember, when you work in an R Project, you can place the cursor in
  the parentheses of the \texttt{read\_csv()} function and press the tab
  key to navigate to the file.
\end{itemize}

\hypertarget{sec-data-wrangling}{%
\section*{Data wrangling}\label{sec-data-wrangling}}
\addcontentsline{toc}{section}{Data wrangling}

\markright{Data wrangling}

Now we have our data imported and checked, and we're ready to start
working with it. This process is called data wrangling, and it involves
cleaning, transforming, and reshaping the data to make it suitable for
visualisation and analysis.

\hypertarget{clean-the-variable-names}{%
\subsection*{Clean the variable names}\label{clean-the-variable-names}}
\addcontentsline{toc}{subsection}{Clean the variable names}

The first thing I like to do is standardise and clean up the variable
names. I like to use the \texttt{janitor} package for this:

\begin{verbatim}

Attaching package: 'janitor'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    chisq.test, fisher.test
\end{verbatim}

The \texttt{clean\_names()} function from the \texttt{janitor} package
will convert variable names to a consistent format (lowercase, spaces
replaced by underscores, no special characters).

\hypertarget{manipulate-the-data-frame}{%
\subsection*{Manipulate the data
frame}\label{manipulate-the-data-frame}}
\addcontentsline{toc}{subsection}{Manipulate the data frame}

Functions in the \texttt{dplyr} package are used to manipulate data
frames:

\begin{itemize}
\tightlist
\item
  \texttt{select()}: select columns by position, or by name, or by other
  methods
\item
  \texttt{filter()}: select rows that meet a logical condition
\item
  \texttt{slice()}: select rows by position
\item
  \texttt{arrange()}: reorder rows
\item
  \texttt{mutate()}: add new variables
\end{itemize}

The \texttt{dplyr} package also provides functions to group data frames
and to summarize data:

\begin{itemize}
\tightlist
\item
  \texttt{group\_by()}: add to a data frame a grouping structure
\item
  \texttt{summarize()}: summarize data, respecting any grouping
  structure specified by \texttt{group\_by()}
\end{itemize}

The pipe operator \texttt{\textbar{}\textgreater{}} is used to chain
together multiple operations on a data frame.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

Note that you will often see another pipe operator
\texttt{\%\textgreater{}\%} used in examples. The pipe operator
\texttt{\textbar{}\textgreater{}} is a newer version of
\texttt{\%\textgreater{}\%} that is more efficient and easier to use.
The pipe operator \texttt{\textbar{}\textgreater{}} is available in R
version 4.1.0 and later.

\end{tcolorbox}

Lets work through some examples with a sample data frame:

Here is the same dataset with 100 rows:

\hypertarget{select-columns-select-columns}{%
\subsection*{Select columns
{[}\#select-columns{]}}\label{select-columns-select-columns}}
\addcontentsline{toc}{subsection}{Select columns {[}\#select-columns{]}}

We can select columns by name

\begin{verbatim}
# A tibble: 100 x 2
   name       score
   <chr>      <dbl>
 1 Person_001  91.9
 2 Person_002  87.3
 3 Person_003  77.8
 4 Person_004  64.5
 5 Person_005  69.8
 6 Person_006  91.2
 7 Person_007  64.3
 8 Person_008  91.9
 9 Person_009  72.6
10 Person_010  70.3
# i 90 more rows
\end{verbatim}

We can select columns by position

\begin{verbatim}
# A tibble: 100 x 2
   name       score
   <chr>      <dbl>
 1 Person_001  91.9
 2 Person_002  87.3
 3 Person_003  77.8
 4 Person_004  64.5
 5 Person_005  69.8
 6 Person_006  91.2
 7 Person_007  64.3
 8 Person_008  91.9
 9 Person_009  72.6
10 Person_010  70.3
# i 90 more rows
\end{verbatim}

We can select columns by a condition, for example select only the
numeric columns:

\begin{verbatim}
# A tibble: 100 x 2
     age score
   <int> <dbl>
 1    50  91.9
 2    34  87.3
 3    38  77.8
 4    33  64.5
 5    22  69.8
 6    29  91.2
 7    37  64.3
 8    41  91.9
 9    30  72.6
10    24  70.3
# i 90 more rows
\end{verbatim}

We can select a column by pattern matching, using helper functions, for
example select columns that contain the letter ``a'':

\begin{verbatim}
# A tibble: 100 x 2
   name         age
   <chr>      <int>
 1 Person_001    50
 2 Person_002    34
 3 Person_003    38
 4 Person_004    33
 5 Person_005    22
 6 Person_006    29
 7 Person_007    37
 8 Person_008    41
 9 Person_009    30
10 Person_010    24
# i 90 more rows
\end{verbatim}

Other helpers include \texttt{starts\_with()}, \texttt{ends\_with()},
\texttt{matches()}, and \texttt{everything()}.

\hypertarget{filter-getting-particular-rows-of-data-filter-rows}{%
\subsection*{Filter: Getting particular rows of data
{[}\#filter-rows{]}}\label{filter-getting-particular-rows-of-data-filter-rows}}
\addcontentsline{toc}{subsection}{Filter: Getting particular rows of
data {[}\#filter-rows{]}}

To get particular rows of data, we can use the \texttt{filter()}
function. This function takes a \emph{logical condition} as an argument
and returns only the rows that meet that condition. For example, to get
all rows where the Age is greater than 30:

\begin{verbatim}
# A tibble: 66 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_002    34  87.3
 3 Person_003    38  77.8
 4 Person_004    33  64.5
 5 Person_007    37  64.3
 6 Person_008    41  91.9
 7 Person_011    39  67.3
 8 Person_012    33  96.5
 9 Person_013    41  61.7
10 Person_014    44  80.0
# i 56 more rows
\end{verbatim}

Here, the logical condition is \texttt{age\ \textgreater{}\ 30}.

We can combine multiple conditions using the logical operators
\texttt{\&} (and), \texttt{\textbar{}} (or), and \texttt{!} (not). For
example, to get all rows where the Age is greater than 30 and the Score
is less than 90:

\begin{verbatim}
# A tibble: 60 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_002    34  87.3
 2 Person_003    38  77.8
 3 Person_004    33  64.5
 4 Person_007    37  64.3
 5 Person_011    39  67.3
 6 Person_013    41  61.7
 7 Person_014    44  80.0
 8 Person_015    45  87.3
 9 Person_016    46  81.3
10 Person_018    38  82.9
# i 50 more rows
\end{verbatim}

Other logical operators include \texttt{==} (equal to), \texttt{!=} (not
equal to), \texttt{\textless{}=} (less than or equal to), and
\texttt{\textgreater{}=} (greater than or equal to).

\hypertarget{slice-getting-rows-by-position-slice-rows}{%
\subsection*{Slice: Getting rows by position
{[}\#slice-rows{]}}\label{slice-getting-rows-by-position-slice-rows}}
\addcontentsline{toc}{subsection}{Slice: Getting rows by position
{[}\#slice-rows{]}}

The \texttt{slice()} function allows us to get rows by their position in
the data frame. For example, to get the first two rows:

\begin{verbatim}
# A tibble: 2 x 3
  name         age score
  <chr>      <int> <dbl>
1 Person_001    50  91.9
2 Person_002    34  87.3
\end{verbatim}

I very rarely use this function, as I prefer to use \texttt{filter()}
with logical conditions. I can't think of a good use case for this
function right now! Perhaps you can?

\hypertarget{arrange-reordering-rows-arrange-rows}{%
\subsection*{Arrange: Reordering rows
{[}\#arrange-rows{]}}\label{arrange-reordering-rows-arrange-rows}}
\addcontentsline{toc}{subsection}{Arrange: Reordering rows
{[}\#arrange-rows{]}}

The \texttt{arrange()} function allows us to reorder the rows of a data
frame based on the values in one or more columns. For example, to
reorder the rows by Age in ascending order:

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_064    20  88.8
 2 Person_056    21  79.5
 3 Person_091    21  67.4
 4 Person_005    22  69.8
 5 Person_025    22  74.9
 6 Person_033    23  67.3
 7 Person_092    23  72.4
 8 Person_010    24  70.3
 9 Person_017    24  79.1
10 Person_058    24  72.7
# i 90 more rows
\end{verbatim}

I f we want to reorder the rows by Age in descending order, we can use
the \texttt{desc()} function:

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_061    50  79.9
 3 Person_075    50  67.3
 4 Person_085    50  50.1
 5 Person_096    50  86.8
 6 Person_031    49  71.8
 7 Person_078    49  72.6
 8 Person_024    48  81.2
 9 Person_057    48  80.3
10 Person_021    47  66.0
# i 90 more rows
\end{verbatim}

It's unusual to need the rows of a dataset to be arranged in a specific
order, but it can be useful when looking at the data directly.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

Note that when you view the data in RStudio, it will always be arranged
by the row number. In the viewer you can sort by clicking on the column
headers.

\end{tcolorbox}

\hypertarget{mutate-adding-new-variables-mutate-variables}{%
\subsection*{Mutate: Adding new variables
{[}\#mutate-variables{]}}\label{mutate-adding-new-variables-mutate-variables}}
\addcontentsline{toc}{subsection}{Mutate: Adding new variables
{[}\#mutate-variables{]}}

The \texttt{mutate()} function allows us to add new variables to a data
frame. For example, to add a new variable called
\texttt{Age\_in\_5\_years} that is the Age plus 5:

\begin{verbatim}
# A tibble: 100 x 4
   name         age score age_in_5_years
   <chr>      <int> <dbl>          <dbl>
 1 Person_001    50  91.9             55
 2 Person_002    34  87.3             39
 3 Person_003    38  77.8             43
 4 Person_004    33  64.5             38
 5 Person_005    22  69.8             27
 6 Person_006    29  91.2             34
 7 Person_007    37  64.3             42
 8 Person_008    41  91.9             46
 9 Person_009    30  72.6             35
10 Person_010    24  70.3             29
# i 90 more rows
\end{verbatim}

We can add multiple new variables at once:

\begin{verbatim}
# A tibble: 100 x 5
   name         age score age_in_5_years percentage_score
   <chr>      <int> <dbl>          <dbl>            <dbl>
 1 Person_001    50  91.9             55            0.919
 2 Person_002    34  87.3             39            0.873
 3 Person_003    38  77.8             43            0.778
 4 Person_004    33  64.5             38            0.645
 5 Person_005    22  69.8             27            0.698
 6 Person_006    29  91.2             34            0.912
 7 Person_007    37  64.3             42            0.643
 8 Person_008    41  91.9             46            0.919
 9 Person_009    30  72.6             35            0.726
10 Person_010    24  70.3             29            0.703
# i 90 more rows
\end{verbatim}

\hypertarget{working-with-categorical-variables-sec-categorical-variables}{%
\subsection*{Working with categorical variables
{[}\#sec-categorical-variables{]}}\label{working-with-categorical-variables-sec-categorical-variables}}
\addcontentsline{toc}{subsection}{Working with categorical variables
{[}\#sec-categorical-variables{]}}

Variables in a data frame in R have a \emph{type}. The most common types
of variables are numeric and categorical. Numeric variables are
variables that take on numerical values, such as age or score.
Categorical variables are variables that take on a limited number of
values, often representing categories or groups. In R, categorical
variables are typically have \emph{type}
\texttt{\textless{}chr\textgreater{}} which is \texttt{character}. Or
they can be of type \texttt{\textless{}fct\textgreater{}} which is
\texttt{factor}.

When we import data categorical variable is usually imported as a
\texttt{character} variable. For example, the variable \texttt{name} in
our example dataset is a categorical variable of type
\texttt{character}. Look at the first few rows of the dataset again, and
see that below the variable name it says
\texttt{\textless{}chr\textgreater{}} for the \texttt{name} variable:

\begin{verbatim}
# A tibble: 100 x 3
   name         age score
   <chr>      <int> <dbl>
 1 Person_001    50  91.9
 2 Person_002    34  87.3
 3 Person_003    38  77.8
 4 Person_004    33  64.5
 5 Person_005    22  69.8
 6 Person_006    29  91.2
 7 Person_007    37  64.3
 8 Person_008    41  91.9
 9 Person_009    30  72.6
10 Person_010    24  70.3
# i 90 more rows
\end{verbatim}

This is all totally fine. There are, however, use cases where we might
want to convert a \texttt{character} variable to a \texttt{factor}
variable. Factors are useful when we have a categorical variable with a
fixed number of levels, and we want to specify the order of those
levels. For example, if we had a variable called
\texttt{education\_level} with the values ``High School'',
``Bachelor's'', ``Master's'', and ``PhD'', we might want to convert this
variable to a factor and specify the order of the levels.

Let's make a new dataset to illustrate this:

Look at the structure of this new dataset:

\begin{verbatim}
# A tibble: 5 x 3
  name    education_level   age
  <chr>   <chr>           <dbl>
1 Alice   Bachelor's         19
2 Bob     Master's           23
3 Charlie PhD                25
4 David   High School        16
5 Eve     Bachelor's         20
\end{verbatim}

We can see that the \texttt{education\_level} variable is of type
\texttt{\textless{}chr\textgreater{}}, which is \texttt{character}.

We can convert the \texttt{education\_level} variable to a factor:

\begin{verbatim}
# A tibble: 5 x 3
  name    education_level   age
  <chr>   <fct>           <dbl>
1 Alice   Bachelor's         19
2 Bob     Master's           23
3 Charlie PhD                25
4 David   High School        16
5 Eve     Bachelor's         20
\end{verbatim}

Now, the \texttt{education\_level} variable is of type
\texttt{\textless{}fct\textgreater{}}, which is \texttt{factor}.

Here is a graph of age by education level:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-37-1.pdf}

We have a problem here: the education levels are not in a sensible
order. The first level is ``Bachelor's'', followed by ``High School'',
``Master's'', and ``PhD''.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-note-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toprule=.15mm, breakable]

Why do you think the levels are in this order? We didn't tell R to order
them like this! The answer is that R orders factor levels alphabetically
by default. So when we convert a character variable to a factor without
specifying the order of the levels, R will order them alphabetically.

\end{tcolorbox}

It would be much better to have the levels ordered as ``High School'',
``Bachelor's'', ``Master's'', and ``PhD''.

We can fix this by specifying the order of the levels when we convert
the variable to a factor:

Now when we plot the data again, the education levels are in the correct
order:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-39-1.pdf}

Another use case is when we are making a linear model and want to
specify the reference level for a categorical variable. We will look at
this when we get to linear models. If you want to skip ahead, you can
see how this works in a section at the end of this chapter (Section
\textbf{?@sec-ref-level}).

\hypertarget{sec-visualisation}{%
\section*{Visualisation}\label{sec-visualisation}}
\addcontentsline{toc}{section}{Visualisation}

\markright{Visualisation}

There are many many many types of data visualisation. We will not
explore them all in this course! In fact, we will use only a few basic
types of visualisation, but we will use them well and critically. The
three types of visualisation we will focus on are scatter plots,
histograms, and box and whisker plots.

\hypertarget{three-basic-types-of-visualisation-three-basic-visualisations}{%
\subsection*{Three basic types of visualisation
{[}\#three-basic-visualisations{]}}\label{three-basic-types-of-visualisation-three-basic-visualisations}}
\addcontentsline{toc}{subsection}{Three basic types of visualisation
{[}\#three-basic-visualisations{]}}

\emph{Scatterplots} are used to visualise the relationship between two
continuous variables. Here is an example of a scatterplot:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-40-1.pdf}

\emph{Histograms} are used to visualise the distribution of a single
continuous variable. The axiss are different to scatterplots: the x-axis
is the variable being measured, and the y-axis is the count (or
frequency) of observations in each bin. A bin is a range of values. Here
is an esample of a histogram:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-41-1.pdf}

\emph{Box and whisker plots} are used to visualise the distribution of a
continuous variable across different categories. Here is an example of a
box and whisker plot. First we add a new variable that is age group:

The new variable \texttt{age\_group} is a categorical variable with
three levels: ``20-29'', ``30-39'', and ``40-49''. We make this using
the \texttt{case\_when()} function. This function works by checking each
condition (which are given as the arguments to the function) in turn,
and assigning the corresponding value when the condition is true. Now we
can make the box and whisker plot:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-43-1.pdf}

\hypertarget{understanding-ggplot2-syntax-understanding-ggplot2}{%
\subsection*{Understanding ggplot2 syntax
{[}\#understanding-ggplot2{]}}\label{understanding-ggplot2-syntax-understanding-ggplot2}}
\addcontentsline{toc}{subsection}{Understanding ggplot2 syntax
{[}\#understanding-ggplot2{]}}

We have used the \texttt{ggplot2} package to create visualisations. The
\texttt{ggplot2} package is based on the grammar of graphics, which
provides a consistent way to create visualisations. It is amazing, and
when it was created it revolutionised data visualisation in R.

You can see that for each of the three visualisations, we use the
\texttt{ggplot()} function to create the base plot, and then we add
layers to the plot using the \texttt{+} operator.

The first argument to the \texttt{ggplot()} function is the data frame
that we want to visualise. The layers that we add to the plot each have
two main components. The first component is the \emph{aesthetic
mappings}, which specify how the variables in the data frame are mapped
to the visual properties of the plot (e.g., x-axis, y-axis, color,
size). The second component is the \emph{geometric object}, which
defines how the data is represented in the plot (e.g., points, lines,
bars).

The \emph{aesthetic mappings} are specified using the \texttt{aes()}
function, which takes arguments that define the mappings. Inside the
\texttt{aes()} function, we specify the variables from the data frame
that we want to map to the visual properties of the plot. For example,
in the scatterplot, we map the \texttt{age} variable to the x-axis and
the \texttt{score} variable to the y-axis using
\texttt{aes(x\ =\ age,\ y\ =\ score)}.

The \emph{geometric object} is specified using functions that start with
\texttt{geom\_}, such as \texttt{geom\_point()},
\texttt{geom\_histogram()}, and \texttt{geom\_boxplot()}.

You will notice that for the scatterplot and the box and whisker plot,
we specify both an x- and a y-variable, but for the histogram we only
specify an x-variable. This is because histograms only have one
variable, which is the variable being measured. The y-axis is
automatically calculated as the count (or frequency) of observations in
each bin.

We can customise many features of the graph using additional arguments
to the \texttt{ggplot()} function and the \texttt{geom\_} functions. For
example, we can add titles and labels to the axes using the
\texttt{labs()} function:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-44-1.pdf}

We can also change the theme of the plot using the \texttt{theme\_}
functions. For example, to use a minimal theme, and add it the
customisations we already made:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-45-1.pdf}

There are a million and one ways to customise visualisations in
\texttt{ggplot2}. We will explore many of them during the course in a
rather ad-hoc way. In this course we do not \emph{assess} your skill and
competence in making clear and beautiful visualisations. We will,
however, be very happy to help you make beautiful and effective
visualisations for your assignments and projects. And please be sure
that making beautiful and effective visualisations is a skill that is
very highly valued in the workplace.

\hypertarget{saving-ggplot-visualisations-saving-ggplot}{%
\subsection*{Saving ggplot visualisations
{[}\#saving-ggplot{]}}\label{saving-ggplot-visualisations-saving-ggplot}}
\addcontentsline{toc}{subsection}{Saving ggplot visualisations
{[}\#saving-ggplot{]}}

Another feature that is very useful is to save ggplot visualisations to
objects and then save to a file (for example a pdf). First, here is how
we save a ggplot to an object:

Now we can save the plot to a file using the \texttt{ggsave()} function:

Note two things about the \texttt{ggsave()} function. First, the first
argument is the file name (including the file extension). The file
extension determines the file type (e.g., pdf, png, jpeg). Second, we
can specify the width and height of the plot in inches.

Also note that the file is saved to the current working directory. When
you're working in an R project, this is usually the base directory of
the project. If you want to save your plots in a folder named
\texttt{plots} you would first need to create the folder (if it doesn't
already exist) and then specify the path in the file name:

\begin{verbatim}
Warning in dir.create("plots"): 'plots' already exists
\end{verbatim}

\hypertarget{extras}{%
\section*{Extras}\label{extras}}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\hypertarget{making-reports-directly-using-quarto-quarto-reports}{%
\subsection*{Making reports directly using Quarto
{[}\#quarto-reports{]}}\label{making-reports-directly-using-quarto-quarto-reports}}
\addcontentsline{toc}{subsection}{Making reports directly using Quarto
{[}\#quarto-reports{]}}

We don't explicitly ask you to make reports using Quarto in this course,
but it is a very useful skill to have, and I highly recommend you
explore it further in your own time. Here are a few basics to get you
started.

One of the great features of R and RStudio is the ability to create
reports that combine text, code, and visualisations. One of the most
popular tools for this is Quarto (https://quarto.org/), which allows you
to create documents in various formats (HTML, PDF, Word, etc.) using a
combination of \emph{Markdown} and R code.

**Why is this so great???* If you want to show someone your analysis and
visualisation, say a team member or supervisor, it is often good to
prepare a report that explains what you did, perhaps shows the code you
used, and presents the results (including visualisations). One way to go
about this is to prepare a powerpoint presentation or a word document,
and then copy and paste code and visualisations into the document. Its
what I used to do. It works. But it is tedious, error prone, and when
you change something in your code or data, you have to remember to go
back and update the powerpoint or word document.

With Quarto, you can create a report that automatically includes the
code and visualisations directly from your R script. This way, if you
change the code or data, you can simply re-render the report and
everything is automatically updated. It takes away a lot of the
tediousness and potential for errors. And it makes updating reports much
easier.

If you'd like to get started with Quarto, check out the Quarto website
(https://quarto.org/) and the RStudio Quarto documentation
(https://quarto.org/docs/get-started/). There are also many tutorials
and resources available online to help you learn how to use Quarto
effectively.

If you have questions about Quarto, feel free to ask me or TAs during
the practicals, though note that any particular TAs may or may not be
experienced with Quarto themselves.

\hypertarget{combining-ggplots-with-patchwork-combining-ggplots}{%
\subsection*{Combining ggplots with patchwork
{[}\#combining-ggplots{]}}\label{combining-ggplots-with-patchwork-combining-ggplots}}
\addcontentsline{toc}{subsection}{Combining ggplots with patchwork
{[}\#combining-ggplots{]}}

We often make multiple ggplots in our analyses. Sometimes it is useful
to combine multiple plots into a single figure for easier comparison or
presentation. We can do with ggplots and the lovely add-on package
called \texttt{patchwork}. The \texttt{patchwork} package allows us to
combine multiple ggplots into a single plot layout. Here is an example
of how to use \texttt{patchwork} to combine the three plots we made
earlier (scatterplot, histogram, and boxplot):

First, load the \texttt{patchwork} package:

Next make the first plot and assign it to an object:

Now make the second plot and assign it to an object:

Now make the third plot and assign it to an object:

Now we can combine the three plots into a single layout using the
\texttt{patchwork} syntax. Here, we arrange \texttt{plot1} on the top
row, and \texttt{plot2} and \texttt{plot3} side by side on the bottom
row:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-53-1.pdf}

Amazing eh! OK, lets leave it there for now. We'll use ggplot2
throughout the course, and explore more features as we go along.

\hypertarget{sec-ref-level}{%
\subsection*{Setting a reference level in a linear
model}\label{sec-ref-level}}
\addcontentsline{toc}{subsection}{Setting a reference level in a linear
model}

Sometimes when fitting linear models with categorical explanatory
(independent) variables, it is useful to set a specific reference level
for the categorical variable. This can help in interpreting the model
coefficients. In R, we can set the reference level using the
\texttt{relevel()} function or by using the \texttt{factor()} function
with the \texttt{levels} argument.

First, let's create a simple dataset:

By default, R will set the first level of the factor (in alphabetical
order) as the reference level. In this case, ``Aspirin'' would be the
reference level. Therefore when we visualise the data:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-55-1.pdf}

It would be nicer to have the ``Control'' group as the first level on
the left of the x-axis.

Likewise, when we make a linear model:

\begin{verbatim}

Call:
lm(formula = response ~ treatment, data = my_data)

Residuals:
   1    2    3    4    5    6 
 0.5 -0.5 -0.5 -0.5  0.5  0.5 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)    
(Intercept)          7.5000     0.5000  15.000 0.000643 ***
treatmentControl    -3.0000     0.7071  -4.243 0.023981 *  
treatmentIbuprofen  -1.0000     0.7071  -1.414 0.252215    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7071 on 3 degrees of freedom
Multiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 
F-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152
\end{verbatim}

The (Intercept) term corresponds to the ``Aspirin'' group, and the
coefficients for ``Control'' and ``Ibuprofen'' are relative to
``Aspirin''. \emph{R} has done this because in the factor levels,
``Aspirin'' comes first alphabetically and was therefore set as the
reference level when the factor variable was created.

If we want to set ``Control'' as the reference level, we can do so using
\texttt{relevel()}:

Now when we visualise the data again:

\includegraphics{2.1-R-and-RStudio_files/figure-pdf/unnamed-chunk-58-1.pdf}

Magic! The ``Control'' group is now the first level on the left of the
x-axis.

And when we fit the linear model again:

\begin{verbatim}

Call:
lm(formula = response ~ treatment, data = my_data)

Residuals:
   1    2    3    4    5    6 
 0.5 -0.5 -0.5 -0.5  0.5  0.5 

Coefficients:
                   Estimate Std. Error t value Pr(>|t|)   
(Intercept)          4.5000     0.5000   9.000   0.0029 **
treatmentAspirin     3.0000     0.7071   4.243   0.0240 * 
treatmentIbuprofen   2.0000     0.7071   2.828   0.0663 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.7071 on 3 degrees of freedom
Multiple R-squared:  0.8615,    Adjusted R-squared:  0.7692 
F-statistic: 9.333 on 2 and 3 DF,  p-value: 0.05152
\end{verbatim}

The (Intercept) term now corresponds to the ``Control'' group, and the
coefficients for ``Aspirin'' and ``Ibuprofen'' are relative to
``Control''. This makes interpretation of the model coefficients more
intuitive.

\bookmarksetup{startatroot}

\hypertarget{study-design-l3}{%
\chapter*{Study design (L3)}\label{study-design-l3}}
\addcontentsline{toc}{chapter}{Study design (L3)}

\markboth{Study design (L3)}{Study design (L3)}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-note-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toprule=.15mm, breakable]

This chapter in under development.

\end{tcolorbox}

\hypertarget{different-studies-different-goals-different-designs-different-analyses}{%
\section*{Different studies, different goals, different designs,
different
analyses}\label{different-studies-different-goals-different-designs-different-analyses}}
\addcontentsline{toc}{section}{Different studies, different goals,
different designs, different analyses}

\markright{Different studies, different goals, different designs,
different analyses}

\hypertarget{paradigms-of-study-design}{%
\section*{Paradigms of study design}\label{paradigms-of-study-design}}
\addcontentsline{toc}{section}{Paradigms of study design}

\markright{Paradigms of study design}

\hypertarget{observational-studies}{%
\subsection*{Observational studies}\label{observational-studies}}
\addcontentsline{toc}{subsection}{Observational studies}

Observational studies are used to observe and analyze the effects of
interventions or exposures without manipulating the study environment.

\hypertarget{experimental-studies}{%
\subsection*{Experimental studies}\label{experimental-studies}}
\addcontentsline{toc}{subsection}{Experimental studies}

Experimental studies involve the manipulation of one or more variables
to determine their effect on a response variable. The manipulation is
typically done through random assignment of subjects to different
treatment groups, which means that on average the only thing that
differs among the subjects in the different groups is the treatment they
receive. This helps to ensure that any differences in outcomes can be
attributed to the treatment rather than other factors that might affect
the subjects.

Key aspects of experimental studies:

\begin{itemize}
\tightlist
\item
  A treatment is applied to one or more groups of subjects.
\item
  Subjects are randomly assigned to treatment groups, which helps to
  control for confounding variables.
\item
  There are typically one or more control groups that do not receive the
  treatment, allowing for comparison of outcomes between treated and
  untreated groups.
\item
  There is replication, meaning that there are multiple subjects in each
  treatment group to ensure that estimates of within group variability
  can be made.
\end{itemize}

\hypertarget{other-terms}{%
\subsection*{Other terms}\label{other-terms}}
\addcontentsline{toc}{subsection}{Other terms}

Unfortunately, the terms ``observational'' and ``experimental'' are not
always used consistently. In particular, the term ``experiment'' is
often used to refer to any study that involves a comparison of two or
more groups, regardless of whether the groups were assigned randomly or
not. This can lead to confusion, as some studies that are called
``experiments'' are actually observational studies. Some studies use the
term ``mensurative experiment'' to refer to observational studies (e.g.,
\href{https://onlinelibrary.wiley.com/doi/10.1111/gcb.15302}{Burdon
\emph{et al} 2018 \emph{Global Change Biology}}).

\bookmarksetup{startatroot}

\hypertarget{regression-part-1}{%
\chapter*{Regression Part 1}\label{regression-part-1}}
\addcontentsline{toc}{chapter}{Regression Part 1}

\markboth{Regression Part 1}{Regression Part 1}

\textbf{How Owen will structure the lecture time}

\textbf{The chapter content below is the reference for what students are
expected to know.} During the lecture time Owen will talk through and
explain hopefully most of the content of this chapter. He will write on
a tablet, show figures and other content of this chapter, and may
live-code in RStudio. He will also present questions and ask students to
\emph{Think, Pair, Share}. The \emph{Share} part will sometime be via
clicker, sometimes by telling the class. The same will happen in lecture
3-6.

\hypertarget{introduction-1}{%
\section*{Introduction}\label{introduction-1}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Linear regression is a common statistical method that models the
relationship between a dependent (response) variable and one or more
independent (explanatory) variables. The relationship is modeled with
the equation for a straight line (\(y = a + bx\)).

With linear regression we can answer questions such as:

\begin{itemize}
\tightlist
\item
  How does the dependent (response) variable change with respect to the
  independent (explanatory) variable?
\item
  What amount of variation in the dependent variable can be explained by
  the independent variable?
\item
  Is there a statistically significant relationship between the
  dependent variable and the independent variable?
\item
  Does the linear model fit the data well?
\end{itemize}

In this chapter / lesson we will explore what is linear regression and
how to use it to answer these questions. We'll cover the following
topics:

\begin{itemize}
\tightlist
\item
  Why use linear regression?
\item
  What is the linear regression model?
\item
  Fitting the regression model (= finding the intercept and the slope).
\item
  Is linear regression a good enough model to use?
\item
  What do we do when things go wrong?
\item
  Transformation of variables/the response.
\item
  Identifying and handling odd data points (aka outliers).
\end{itemize}

In this chapter / lesson we will not discuss the statistical
significance of the model. We will cover this topic in the next chapter
/ lesson.

\hypertarget{why-use-linear-regression}{%
\subsection*{Why use linear
regression?}\label{why-use-linear-regression}}
\addcontentsline{toc}{subsection}{Why use linear regression?}

\begin{itemize}
\tightlist
\item
  It's a good starting point because it is a relatively simple model.
\item
  Relationships are sometimes close enough to linear.
\item
  It's easy to interpret.
\item
  It's easy to use.
\item
  It's actually quite flexible (e.g.~can be used for non-linear
  relationships, e.g., a quadratic model is still a linear model!!! See
  \textbf{?@sec-kind-of-magic}.).
\end{itemize}

\hypertarget{an-example---blood-pressure-and-age}{%
\subsection*{An example - blood pressure and
age}\label{an-example---blood-pressure-and-age}}
\addcontentsline{toc}{subsection}{An example - blood pressure and age}

There are lots of situations in which linear regression can be useful.
For example, consider hypertension. Hypertension is a condition in which
the blood pressure in the arteries is persistently elevated.
Hypertension is a major risk factor for heart disease, stroke, and
kidney disease. It is estimated that hypertension affects about 1
billion people worldwide. Hypertension is a complex condition that is
influenced by many factors, including age. In fact, it is well known
that blood pressure increases with age. But how much does blood pressure
increase with age? This is a question that can be answered using linear
regression.

Here is an example of a study that used linear regression to answer this
question:
https://journals.lww.com/jhypertension/fulltext/2021/06000/association\_of\_age\_and\_blood\_pressure\_among\_3\_3.15.aspx

In this study, the authors used linear regression to model the
relationship between age and blood pressure. They found that systolic
blood pressure increased by 0.28--0.85 mmHg/year. This is a small
increase, but it is statistically significant. This means that the
observed relationship between age and blood pressure is unlikely to be
due to chance.

Lets look at some simulated example data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-2-1.pdf}

Well, that is pretty conclusive. We hardly need statistics. There is a
clear positive relationship between age and systolic blood pressure. But
how can we quantify this relationship? And in less clear-cut cases what
is the strength of evidence for a relationship? This is where linear
regression comes in. Linear regression models the relationship between
age and systolic blood pressure. With linear regression we can answer
the following questions:

\begin{itemize}
\tightlist
\item
  What is the value of the intercept and slope of the relationship?
\item
  Is the relationship different from what we would expect if there were
  no relationship?
\item
  How well does the mathematical representation match the observed
  values?
\item
  How much uncertainty is there in predictions?
\end{itemize}

Lets try to figure some of these out from the visualisation.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#guess-params)}, toprule=.15mm, breakable]

\begin{itemize}
\tightlist
\item
  Make a guess of the slope.
\item
  Make a guess of the intercept (hint be careful, lots of people get
  this wrong).
\end{itemize}

\end{tcolorbox}

\hypertarget{calculating-the-intercept-and-slope}{%
\section*{Calculating the intercept and
slope}\label{calculating-the-intercept-and-slope}}
\addcontentsline{toc}{section}{Calculating the intercept and slope}

\markright{Calculating the intercept and slope}

\hypertarget{regression-from-a-mathematical-perspective}{%
\subsection*{Regression from a mathematical
perspective}\label{regression-from-a-mathematical-perspective}}
\addcontentsline{toc}{subsection}{Regression from a mathematical
perspective}

Given an \textbf{independent/explanatory variable} (\(X\)) and a
\textbf{dependent/response variable} (\(Y\)) all points \((x_i,y_i)\),
\(i= 1,\ldots, n\), on a straight line follow the equation

\[y_i = \beta_0 + \beta_1 x_i\ .\]

\begin{itemize}
\tightlist
\item
  \(\beta_0\) is the \textbf{intercept} - the value of \(Y\) when
  \(x_i = 0\)
\item
  \(\beta_1\) the \textbf{slope} of the line, also known as the
  regression coefficient of \(X\).
\item
  If \(\beta_0=0\) the line goes through the origin \((x,y)=(0,0)\).
\item
  \textbf{Interpretation} of linear dependency: proportional increase in
  \(y\) with increase (decrease) in \(x\).
\end{itemize}

\hypertarget{finding-the-intercept-and-the-slope}{%
\subsection*{Finding the intercept and the
slope}\label{finding-the-intercept-and-the-slope}}
\addcontentsline{toc}{subsection}{Finding the intercept and the slope}

In a regression analysis, one task is to estimate the intercept and the
slope. These are known as the \textbf{regression coefficients}
\(\beta_0\), \(\beta_1\).

\begin{itemize}
\item
  \textbf{Problem}: For more than two points \((x_i,y_i)\),
  \(i=1,\ldots, n\), there is generally no perfectly fitting line.
\item
  \textbf{Aim:} We want to estimate the parameters \((\beta_0,\beta_1)\)
  of the \textbf{best fitting} line \(Y = \beta_0 + \beta_1 x\).
\item
  \textbf{Idea:} Find the \textbf{best fitting line} by minimizing the
  deviations between the data points \((x_i,y_i)\) and the regression
  line. I.e., minimising the residuals.
\end{itemize}

But which deviations?

These ones?

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-3-1.pdf}

Or these?

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-4-1.pdf}

Or maybe even these?

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-5-1.pdf}

Well, actually its none of these!!!

\hypertarget{least-squares}{%
\subsection*{Least squares}\label{least-squares}}
\addcontentsline{toc}{subsection}{Least squares}

For multiple reasons (theoretical aspects and mathematical convenience),
the intercept and slope are estimated using the \textbf{least squares}
approach. In this, yet something else is minimized:

The parameters \(\beta_0\) and \(\beta_1\) are estimated such that the
\textbf{sum of squared vertical distances} (sum of squared residuals /
errors) is minimised.

\textbf{SSE} means \textbf{S}um of \textbf{S}quared \textbf{E}rrors:

\[SSE = \sum_{i=1}^n e_i^2 \]

where,

\[e_i = y_i - \underbrace{(\beta_0 + \beta_1 x_i)}_{=\hat{y}_i} \]
\textbf{Note:} \(\hat y_i = \beta_0 + \beta_1 x_i\) are the
\emph{predicted values}.

In the graph just below, one of these squares is shown in red.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-6-1.pdf}

\hypertarget{least-squares-estimates}{%
\subsection*{Least squares estimates}\label{least-squares-estimates}}
\addcontentsline{toc}{subsection}{Least squares estimates}

With a linear model, we can calculate the least squares estimates of the
parameters \(\beta_0\) and \(\beta_1\) directly using the following
formulas.

For a given sample of data \((x_i,y_i), i=1,..,n\), with mean values
\(\overline{x}\) and \(\overline{y}\), the least squares estimates
\(\hat\beta_0\) and \(\hat\beta_1\) are computed as

\[ \hat\beta_1 = \frac{\sum_{i=1}^n  (y_i - \overline{y}) (x_i - \overline{x})}{ \sum_{i=1}^n (x_i - \overline{x})^2 } = \frac{cov(x,y)}{var(x)}\]

\[\hat\beta_0 = \overline{y} - \hat\beta_1 \overline{x}  \]

Moreover,

\[ \hat\sigma^2 = \frac{1}{n-2}\sum_{i=1}^n e_i^2 \quad \text{with residuals  } e_i = y_i - (\hat\beta_0 + \hat\beta_1 x_i) \]

is an unbiased estimate of the residual variance \(\sigma^2\).

(Derivations of the equations above are in the Stahel script 2.A b.
Hint: differentiate, set to zero, solve.)

\hypertarget{why-division-by-n-2-ensures-an-unbiased-estimator}{%
\subsection*{\texorpdfstring{Why division by \(n-2\) ensures an unbiased
estimator}{Why division by n-2 ensures an unbiased estimator}}\label{why-division-by-n-2-ensures-an-unbiased-estimator}}
\addcontentsline{toc}{subsection}{Why division by \(n-2\) ensures an
unbiased estimator}

When estimating parameters (\(\beta_0\) and \(\beta_1\)), the square of
the residuals is minimised. This fitting process inherently \emph{uses
up} two \emph{degrees of freedom}, as the model forces the residuals to
sum to zero and aligns the slope to best fit the data. I.e., one degree
of freedom is lost due to the estimation of the intercept, and another
due to the estimation of the slope.

The adjustment (division by \(n-2\) instead of \(n\)) compensates for
the loss of variability due to parameter estimation, ensuring the
estimator of the residual variance is unbiased. Mathematically, dividing
by n - 2 adjusts for this loss and gives an accurate estimate of the
population variance when working with sample data.

We'll look at degrees of freedom in more detail later, so don't worry if
this is a bit confusing right now.

\hypertarget{lets-do-it-in-r}{%
\subsection*{Let's do it in R}\label{lets-do-it-in-r}}
\addcontentsline{toc}{subsection}{Let's do it in R}

First we read in the dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_age\_data }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"data/Simulated\_Blood\_Pressure\_and\_Age\_Data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The we make a graph of the data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-8-1.pdf}

Then we make the linear model, using the \texttt{lm()} function:

Then we can look at the summary of the model. It contains a lot of
information, so can be a bit confusing at first.

\begin{verbatim}

Call:
lm(formula = Systolic_BP ~ Age, data = bp_age_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-13.2195  -3.4434  -0.0808   3.1383  12.6025 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 98.96874    1.46102   67.74   <2e-16 ***
Age          0.82407    0.02771   29.74   <2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.971 on 98 degrees of freedom
Multiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 
F-statistic: 884.4 on 1 and 98 DF,  p-value: < 2.2e-16
\end{verbatim}

How do our guesses of the intercept and slope compare to the guesses we
made earlier?

Recal that the units of the \emph{Age} coefficient are in mmHg per year.
This means that for each additional year of age, the systolic blood
pressure increases by Â´r round(coef(bp\_age\_model){[}2{]},2)Â´ mmHg.

\hypertarget{dealing-with-the-error}{%
\section*{Dealing with the error}\label{dealing-with-the-error}}
\addcontentsline{toc}{section}{Dealing with the error}

\markright{Dealing with the error}

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-11-1.pdf}

The line is not a perfect fit to the data. There is scatter around the
line.

Some of this scatter could be caused by other factors that influence
blood pressure, such as diet, exercise, and genetics. Also, the there
could be differences due to the measurement instrument (i.e., some
measurement error).

These other factors are not included in the model (only age is in the
model), so they create variation that can only appear in error term.

In the linear regression model the dependent variable \(Y\) is related
to the independent variable \(x\) as

\[Y = \beta_0 + \beta_1 x + \epsilon \ \] where

\begin{itemize}
\tightlist
\item
  \(\epsilon\) is the error term
\item
  \(\beta_0\) is the intercept
\item
  \(\beta_1\) is the slope
\item
  \(\epsilon\) is the error term.
\end{itemize}

The error term captures the difference between the observed value of the
dependent variable and the value predicted by the model. The error term
includes the effects of other factors that influence the dependent
variable, as well as measurement error.

\[Y \quad= \quad \underbrace{\text{expected value}}_{E(Y) = \beta_0 + \beta_1 x} \quad + \quad \underbrace{\text{random error}}_{\epsilon}  \ .\]

Graphically the error term is the vertical distance between the observed
value of the dependent variable and the value predicted by the model.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-12-1.pdf}

The error term is also known as the residual. It is the variation that
\emph{resides} (is left over / is left unexplained) after accounting for
the relationship between the dependent and independent variables.

\hypertarget{example-in-r}{%
\subsection*{Example in R}\label{example-in-r}}
\addcontentsline{toc}{subsection}{Example in R}

Let's look at observed values, expected (predicted) values, and
residuals (error) in R.

The observed values of the response (dependent) variable are already in
the dataset:

\begin{verbatim}
[1] 150.3277 170.0801 139.7174 135.4089 151.9041 122.0296
\end{verbatim}

To get the expected values, we need to find the intercept and slope of
the linear model. We can do this using the \texttt{lm()} function in R.

And we can get the intercept and slope using the \texttt{coef()}
function:

\begin{verbatim}
(Intercept)         Age 
 98.9687381   0.8240678 
\end{verbatim}

We can then use the mutate function from the dplyr package to add the
expected values to the dataset:

And we can get the residuals by subtracting the expected values from the
observed values:

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

We can also get the expected values and residuals directly from the
\texttt{lm} object using the \texttt{fitted()} (or \texttt{predicted()})
and \texttt{residuals()} functions:

\end{tcolorbox}

Now we have a model that gives the expected values (on the regression
line) and that gives us a residual. Because the expected value plus the
residual equals the observed value, if we use each of the residuals as
the error for each respective data point, we end up with a perfect fit
to the data. All we are doing is describing the observed data in a
different way. This is known as over-fitting. In fact, we have gained
very little by fitting the model. We have simply memorized / copied the
data!!!

In order to avoid this, we need to assume something about the residuals
-- we need to \emph{model} the residuals. The most common model for the
residuals is a normal distribution with mean 0 and constant variance.

\[\epsilon \sim N(0,\sigma^2)\]

\textbf{This is known as the normality assumption.} The normality
assumption is important because it allows us to make inferences about
the \emph{population parameters} based on the \emph{sample data}.

The linear regression model then becomes:

\[Y = \beta_0 + \beta_1 x + N(0,\sigma^2) \ \]

where \(\sigma^2\) is the variance of the error term. The variance of
the error term is the amount of variation in the dependent variable that
is not explained by the independent variable. The variance of the error
term is also known as the residual variance.

An alternate and equivalent formulation is that \(Y\) is a random
variable that follows a normal distribution with mean
\(\beta_0 + \beta_1 x\) and variance \(\sigma^2\).

\[Y \sim N(\beta_0 + \beta_1 x, \sigma^2)\]

So, the answer to the question ``how do we deal with the error term'' is
that we model the error term as normally distributed with mean 0 and
constant variance. Put another way, the error term is assumed to be
normally distributed with mean 0 and constant variance.

\hypertarget{back-to-blood-pressure-and-age}{%
\subsection*{Back to blood pressure and
age}\label{back-to-blood-pressure-and-age}}
\addcontentsline{toc}{subsection}{Back to blood pressure and age}

The mathematical model in this case is:

\[SystolicBP = \beta_0 + \beta_1 \times Age + \epsilon\]

where: \emph{SystolicBP} is the dependent (response) variable,
\(\beta_0\) is the intercept, \(\beta_1\) is the coefficient of Age,
\emph{Age} is the independent (explanatory) variable, \(\epsilon\) is
the error term.

Let's ensure we understand this, by thinking about the units of the
variables in this model. This can be very useful because it can help us
to understand the model better and to check that the model makes sense.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, pair, share (\#what-units)}, toprule=.15mm, breakable]

\begin{itemize}
\tightlist
\item
  What are the units of blood pressure?
\item
  What are the units of age?
\item
  What are the units of the intercept?
\item
  What are the units of the coefficient of Age?
\item
  What are the units of the error term?
\end{itemize}

\end{tcolorbox}

\hypertarget{is-the-model-good-enough-to-use}{%
\section*{Is the model good enough to
use?}\label{is-the-model-good-enough-to-use}}
\addcontentsline{toc}{section}{Is the model good enough to use?}

\markright{Is the model good enough to use?}

\begin{itemize}
\tightlist
\item
  All models are wrong, but is ours good enough to be useful?
\item
  Are the assumption of the model justified?
\item
  It would be very unwise to use the model before we know if it is good
  enough to use.
\item
  \emph{Don't jump out of an aeroplane until you know your parachute is
  good enough!}
\end{itemize}

\hypertarget{what-assumptions-do-we-make}{%
\subsection*{What assumptions do we
make?}\label{what-assumptions-do-we-make}}
\addcontentsline{toc}{subsection}{What assumptions do we make?}

We already heard about one. We assume that the residuals follow a
\(N(0,\sigma^2)\) distribution (that is, a Gaussian / Normal distrution
with mean of zero and variance of \(\sigma^2\)). We make this assumption
because it is often well enough met, and it gives great mathematical
tractability.

This assumption implies that:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\tightlist
\item
  The \(\epsilon_i\) are normally distributed.
\item
  \(\epsilon_i\) has constant variance: \(Var(\epsilon_i)=\sigma^2\).
\item
  The \(\epsilon_i\) are independent of each other.
\end{enumerate}

Furthermore:

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  we assumed a linear relationship.
\item
  implies there are no outliers (implied by (a) above)
\end{enumerate}

Lets go through each five assumptions.

\hypertarget{a-normally-distributed-residuals}{%
\subsection*{(a) Normally distributed
residuals}\label{a-normally-distributed-residuals}}
\addcontentsline{toc}{subsection}{(a) Normally distributed residuals}

Recall that we make the assumption that the residuals are normally
distributed with mean 0 and constant variance:

\[\epsilon \sim N(0,\sigma^2)\]

Here we are concerned with the first part of this assumption, that the
residuals are normally distributed.

What does this mean? How can we check it?

A normal distribution is symmetric and bell-shaped\ldots{}

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-19-1.pdf}

Lets look at the frequency distribution of the residuals of the linear
regression of blood pressure and age:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-20-1.pdf}

The normal distribution assumption (a) seems ok as well.

\hypertarget{a-normally-distributed-residuals-the-qq-plot}{%
\subsection*{(a) Normally distributed residuals: The
QQ-plot}\label{a-normally-distributed-residuals-the-qq-plot}}
\addcontentsline{toc}{subsection}{(a) Normally distributed residuals:
The QQ-plot}

Usually, not the histogram of the residuals is plotted, but the
so-called \textbf{quantile-quantile} (QQ) plot. The quantiles of the
observed distribution are plotted against the quantiles of the
respective theoretical (normal) distribution:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-21-1.pdf}

If the points lie approximately on a straight line, the data is fairly
normally distributed.

This is often ``tested'' by eye, and needs some experience.

\emph{But what on earth is a quantile???}

Imagine we make 21 measures of something, say 21 blood pressures:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-22-1.pdf}

The median of these is 127.8. The median is the 50\% or 0.5 quantile,
because half the data points are above it, and half below.

\begin{verbatim}
  50% 
127.8 
\end{verbatim}

The \emph{theoretical quantiles} come from the normal distribution. The
\emph{sample quantiles} come from the distribution of our residuals.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-24-1.pdf}

\hypertarget{how-do-i-know-if-a-qq-plot-looks-good}{%
\subsubsection*{How do I know if a QQ-plot looks
``good''?}\label{how-do-i-know-if-a-qq-plot-looks-good}}
\addcontentsline{toc}{subsubsection}{How do I know if a QQ-plot looks
``good''?}

There is \textbf{no quantitative rule} to answer this question. Instead
experience is needed. You can gain this experience from simulations. To
this end, we can generate the same number of data points of a normally
distributed variable and compare this simulated qqplot to our observed
one.

Example: Generate 100 points \(\epsilon_i \sim N(0,1)\) each time:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-25-1.pdf}

Each of the graphs above has data points that are randomly generated
from a normal distribution. In all cases the data points are close to
the line. This is what we would expect if the data were normally
distributed. The amount of deviation from the line is what we would
expect from random variation, and so seeing this amount of variation in
a QQ-plot of your model should not be cause for concern.

\hypertarget{b-constant-error-variance-homoscedasticity}{%
\subsection*{(b) Constant error variance
(homoscedasticity)}\label{b-constant-error-variance-homoscedasticity}}
\addcontentsline{toc}{subsection}{(b) Constant error variance
(homoscedasticity)}

Recall that we assume the errors are normally distributed with constant
variance \(\sigma^2\):

\[\epsilon_i \sim N(0, \sigma^2)\]

Here we're concerned with the second part of this assumption, that the
variance is constant.That is, variance of the residuals is a constant:
\(\text{Var}(\epsilon_i) = \sigma^2\). And not, for example
\(\text{Var}(\epsilon_i) = \sigma^2 \cdot x_i\).

Put another way, we're interested if the size of the residuals tends to
show a pattern with the fitted values. By \emph{size} of the residuals
we mean the \emph{absolute value} of the residuals. In fact, we often
look at the square root of the absolute value of the standardized
residuals:

\[R_i = \frac{\epsilon_i}{\hat{\sigma}}\] Where \(\hat{\sigma}\) is the
estimated standard deviation of the residuals:

\[\hat{\sigma} = \sqrt{\frac{1}{n-2} \sum_{i=1}^n \epsilon_i^2}\]

So that the full equation of the square root of the standardised
residuals is:

\[\sqrt{|R_i|} = \sqrt{\left|\frac{\epsilon_i}{\hat{\sigma}}\right|}\]

To look to see if the variance of the residuals is constant, we need to
see if there is any relationship between the size of the residuals and
the fitted values. A commonly used visualistion for this is a plot of
the size of the residuals against the fitted values.

Lets first calculated the \(\sqrt{|R_i|}\) values for our blood pressure
model:

And now visualise the relationship between the fitted values and the
size of the residuals:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-27-1.pdf}

This graph is known as the scale-location plot. It is particularly
suited to check the assumption of equal variances
(\textbf{homoscedasticity / HomoskedastizitÃ¤t}). There should be
\textbf{no trend} or pattern.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

We can also use the built-in plot function for linear models to create
this plot. It is the third plot in the set of diagnostic plots.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-28-1.pdf}

\end{tcolorbox}

\hypertarget{how-it-looks-with-the-variance-increasing-with-the-fitted-values}{%
\subsubsection*{How it looks with the variance increasing with the
fitted
values}\label{how-it-looks-with-the-variance-increasing-with-the-fitted-values}}
\addcontentsline{toc}{subsubsection}{How it looks with the variance
increasing with the fitted values}

Here's a graphical example of how it would look if the variance of the
residuals increases with the fitted values.

First here is a graph of the relationship:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-29-1.pdf}

And here the scale-location plot for a linear model of that data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-30-1.pdf}

\hypertarget{c-independence-residuals-are-independent-of-each-other}{%
\subsection*{(c) Independence (residuals are independent of each
other)}\label{c-independence-residuals-are-independent-of-each-other}}
\addcontentsline{toc}{subsection}{(c) Independence (residuals are
independent of each other)}

We assume that the residuals (\(\epsilon_i\)) are independent of each
other. This means that the value of one residual is not somehow related
to the value of another.

The dataset about blood pressure we looked at contained 100
observations, each one made from a different person. In such a study
design, we could be safe in the assumption that the people are
independent, and therefore the assumption that the residuals are
independent.

Imagine, however, if we had 100 observations of blood pressure collected
from 50 people, because we measured the blood pressure of each person
twice. In this case, the residuals would not be independent, because two
measures of the blood pressure of the same person are likely to be
similar. A person is likely to have a high blood pressure in both
measurements, or a low blood pressure in both measurements. This would
mean they have a high residual in both measurements, or a low residual
in both measurements.

In this case, we would need to account for the fact that the residuals
are not independent. We would need to use a more complex model, such as
a mixed effects model, to account for the fact that the residuals are
not independent. We will talk about this again in the last week of this
course.

In general, you should always think about the study design when you are
analysing data. You should always think about whether the residuals are
likely to be independent of each other. If they are not, you should
think about how you can account for this in your analysis.

A good way to assess if there could be dependencies in the residuals is
to be critical about what is the unit of observation in the data. In the
blood pressure example, the unit of observation is the person. Count the
number of persons in the study. If there are fewer persons than
observations, then at least some people must have been measured at least
twice. Repeating measures on the same person is a common way to get
dependent residuals.

So, to check the assumption of independence, you should:

\begin{itemize}
\tightlist
\item
  Think carefully about the study design.
\item
  Think carefully about the unit of observation in the data.
\item
  Compare the number of observations to the number of units of
  observation.
\end{itemize}

\hypertarget{d-linearity-assumption}{%
\subsection*{(d) Linearity assumption}\label{d-linearity-assumption}}
\addcontentsline{toc}{subsection}{(d) Linearity assumption}

The linearity assumption states that the relationship between the
independent variable and the dependent variable is linear. This means
that the dependent variable changes by a constant amount for a one-unit
change in the independent variable. And that this slope is does not
change with the value of the independent variable.

The blood pressure data seems to be linear:

\begin{verbatim}
Warning: `fortify(<lm>)` was deprecated in ggplot2 3.6.0.
i Please use `broom::augment(<lm>)` instead.
i The deprecated feature was likely used in the ggplot2 package.
  Please report the issue at <https://github.com/tidyverse/ggplot2/issues>.
\end{verbatim}

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-31-1.pdf}

In contrast, look at this linear regression through data that appears
non-linear:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-32-1.pdf}

And with the residuals shown as red lines:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-33-1.pdf}

At low values of \(y\), the residuals are positive, at intermediate
values of \(y\) the residuals are negative, and at high values of \(y\)
the residuals are positive. This pattern in the residuals is a sign that
the relationship between \(x\) and \(y\) is not linear.

We can plot the value of the residuals against the \(y\) value directly,
instead of looking at the pattern in the graph above. This is called a
\textbf{Tukey-Anscombe plot}. It is a graph of the residuals versus the
fitted \(y\) values:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-34-1.pdf}

We can very clearly see pattern in the residuals in this Tukey-Anscombe
plot. The residuals are positive, then negative, then positive, as the
fitted \(y\) value gets larger.

We can also make this Tukey-Anscombe plot using the built-in plot
function for linear models in R:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-35-1.pdf}

The red line in the Tukey-Anscombe plot is a loess smooth. It is
automatically added to the plot. It is a way of estimating the pattern
in the residuals. If the red line is not flat, then there is a pattern
in the residuals. However, the loess smooth is not always reliable. It
is a good idea to look at the residuals directly, without this smooth.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-36-1.pdf}

The data here is simulated to show a very clear pattern in the
residuals. In real data, the pattern might not be so clear. But if you
suspect you see a pattern in the residuals, it could be a sign that the
relationship between the independent and dependent variable is not
linear.

Here is the Tukey-Anscombe plot for the blood pressure data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-37-1.pdf}

There is very little evidence of any pattern in the residuals. This data
is simulated with a truly linear relationship, so we would not expect to
see any pattern in the residuals.

\hypertarget{e-no-outliers}{%
\subsection*{(e) No outliers}\label{e-no-outliers}}
\addcontentsline{toc}{subsection}{(e) No outliers}

An outlier is a data point that is very different from the other data
points. Outliers can have a big effect on the results of a regression
analysis. They can pull the line of best fit towards them, and make the
line of best fit a poor representation of the data.

Lets again look at the blood pressure versus age data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-38-1.pdf}

There are no obvious outliers in this data. The data points are all
close to the line of best fit. This is a good sign that the line of best
fit is a good representation of the data.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#odd-data)}, toprule=.15mm, breakable]

Where on this graph would you expect to see particularly influential
outliers? Influential in the sense that they would have a large effect
on the slope of the line of best fit.

\end{tcolorbox}

Data points that are far from the mean of the independent variable have
a large effect on the value of the slope. These data points have a large
leverage. They are data points that are far from the other data points
in the \(x\) direction.

We can think of this with the analogy of a seesaw. The slope of the line
of best fit is like the pivot point of a seesaw. Data points that are
far from the pivot point have a large effect on the slope. Data points
that are close to the pivot point have a small effect on the slope.

A measure of distance from the pivot point is called the \(leverage\) of
a data point. In simple regression, the leverage of individual \(i\) is
defined as

\(h_{i} = (1/n) + (x_i-\overline{x})^2 / SSX\).

where \(SSX = \sum_{i=1}^n (x_i - \overline{x})^2\). (\textbf{S}um of
\textbf{S}quares of \textbf{\(X\)})

So, the leverage of a data point is inversely related to \(n\) (the
number of data points). The leverage of a data point is also inversely
related to the sum of the squares of the \(x\) values. The leverage of a
data point is directly related to the square of the distance of the
\(x\) value from the mean of the \(x\) values.

More intuitively perhaps, the leverage of a data point will be greater
when the are fewer other data points. It will also be greater when the
distance from the mean value of \(x\) is greater.

Going back to the analogy of a seesaw, with data points as children on
the seesaw, the leverage of a data point is like the distance from the
pivot a child sits. But we also have children of different weights. A
lighter child will have less effect on the tilt of the seesaw. A heavier
one will have a greater effect on the tilt. A heavier child sitting far
from the pivot will have a very large effect.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#like-weight)}, toprule=.15mm, breakable]

What quantity that we already experienced is like the weight of the
child?

\end{tcolorbox}

The size of the residuals are like the weight of the child. Data points
with large residuals have a large effect on the slope of the line of
best fit. Data points with small residuals have a small effect on the
slope of the line of best fit.

So the overall effect of a data point on the slope of the line of best
fit is a combination of the leverage and the residual. This quantity is
called the \(influence\) of a data point.

Let's add a rather extreme data point to the blood pressure versus age
data:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-39-1.pdf}

This is a bit ridiculous, but it is a good example of an outlier. The
data point is far from the other data points. It has a large residual.
And it is a long way from the pivot (the middle of the \(x\) data) so
has large leverage.

We can make a histogram of the residuals and see that the outlier has a
large residual:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-40-1.pdf}

And we can see that the leverage is large.

There is a graph that we can look at to see the influence of a data
point. This is called a \(Cook's\) \(distance\) plot. The Cook's
distance of a data point is a measure of how much the slope of the line
of best fit changes when that data point is removed. The Cook's distance
of a data point is defined as

\(D_i = \sum_{j=1}^n (\hat{y}_j - \hat{y}_{j(i)})^2 / (p \times MSE)\).

where \(\hat{y}_j\) is the predicted value of the dependent variable for
data point \(j\), \(\hat{y}_{j(i)}\) is the predicted value of the
dependent variable for data point \(j\) when data point \(i\) is
removed, \(p\) is the number of parameters in the model (2 in this
case), \(MSE\) is the mean squared error of the model.

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-41-1.pdf}

But does it have a large influence on the value of the slope? In the
next graph we show the line of best fit with the outlier (blue line) and
without the outlier (red line).

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-42-1.pdf}

No, the outlier doesn't have much influence on the slope. The outlier
has a large leverage. It is far from the pivot. But it does not have
such a large effect (influence) on the slope. This is in large part
because there are a lot data points (100) that are quite tightly
arranged around the regression line.

\hypertarget{graphical-illustration-of-the-leverage-effect}{%
\subsubsection*{Graphical illustration of the leverage
effect}\label{graphical-illustration-of-the-leverage-effect}}
\addcontentsline{toc}{subsubsection}{Graphical illustration of the
leverage effect}

Data points with \(x_i\) values far from the mean have a stronger
leverage effect than when \(x_i\approx \overline{x}\):

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-43-1.pdf}

The outlier (red circle) in the middle plot ``pulls'' the regression
line in its direction and has large influence on the slope. THe outlier
(red circle) in the right plot has less influence on the slope because
it is closer to the mean of \(x\).

\hypertarget{leverage-plot-hebelarm-diagramm}{%
\subsubsection*{Leverage plot
(Hebelarm-Diagramm)}\label{leverage-plot-hebelarm-diagramm}}
\addcontentsline{toc}{subsubsection}{Leverage plot (Hebelarm-Diagramm)}

In the leverage plot, (standardized) residuals \(\tilde{R_i}\) are
plotted against the leverage \(H_{ii}\) :

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-44-1.pdf}

Critical ranges are the top and bottom right corners!!

Here, observations 71, 85, and 87 are labelled as potential outliers.

Some texts will give a rule of thumb that points with Cook's distances
greater than 1 should be considered influential, while others claim a
reasonable rule of thumb is \(4 / ( n - p - 1 )\) where \(n\) is the
sample size, and \(p\) is the number of \(beta\) parameters.

\hypertarget{what-can-go-wrong-during-the-modeling-process}{%
\section*{What can go ``wrong'' during the modeling
process?}\label{what-can-go-wrong-during-the-modeling-process}}
\addcontentsline{toc}{section}{What can go ``wrong'' during the modeling
process?}

\markright{What can go ``wrong'' during the modeling process?}

Answer: a lot of things!

\begin{itemize}
\tightlist
\item
  Non-linearity. We assumed a linear relationship between the response
  and the explanatory variables. But this is not always the case in
  practice. We might find that the relationship is curved and not well
  represtented by a straight line.
\item
  Non-normal distribution of residuals. The QQ-plot data might deviate
  from the straight line so much that we get worried!
\item
  Heteroscadisticity (non-constant variance). We assumed
  homoscadisticity, but the residuals might show a pattern.
\item
  Data point with high influence. We might have a data point that has a
  large influence on the slope of the line of best fit.
\end{itemize}

\hypertarget{what-to-do-when-things-go-wrong}{%
\subsection*{What to do when things ``go
wrong''?}\label{what-to-do-when-things-go-wrong}}
\addcontentsline{toc}{subsection}{What to do when things ``go wrong''?}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Now: Transform the response and/or explanatory variables.
\item
  Now: Take care of outliers.
\item
  Later in the course: Improve the model, e.g., by adding additional
  terms or interactions.
\item
  Later in the course: Use another model family (generalized or
  nonlinear regression model).
\end{enumerate}

\hypertarget{dealing-with-non-linearity}{%
\subsection*{Dealing with
non-linearity}\label{dealing-with-non-linearity}}
\addcontentsline{toc}{subsection}{Dealing with non-linearity}

Here's another example of \(y\) and \(x\) that are not linearly related:

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-45-1.pdf}

One way to deal with this is to transform the response variable \(Y\).
Here we try two different transformations: \(\log_{10}(Y)\) and
\(\sqrt{Y}\).

Square root transform of the response variable \(Y\):

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-46-1.pdf}

Not great.

Log transformation of the response variable \(Y\):

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-47-1.pdf}

Nope. Still some evidence of non-linearity.

What about transforming the explanatory variable \(X\) as well?

\includegraphics{4.1-regression-part1_files/figure-pdf/unnamed-chunk-48-1.pdf}

Let's look at the four diagnostic plots for the log-log-transformed
data:

\begin{figure}

{\centering \includegraphics[width=7cm,height=\textheight]{4.1-regression-part1_files/figure-pdf/unnamed-chunk-49-1.pdf}

}

\end{figure}

All looks pretty good except for the scale-location plot, which shows a
bit of a pattern. But overall, this looks much better than our original
model.

But\ldots{} how to know which transformation to use\ldots? It's a bit of
trial and error. But we can use the diagnostic plots to help us.

\textbf{Very very important} is that we do this trial and error before
we start using the model. E.g., we don't want to jump from the aeroplane
and then find out that our parachute is not working properly! And then
try to fix the parachute while we are falling\ldots.

Likewise, we must not start using the model and then try to fix it. We
need to make sure our model is in good working order before we start
using it.

One of the traps we could fall into is called ``p-hacking''. This is
when we try different transformation until we find one that gives us the
\textbf{result we want}, for example significant relationship. This is a
big no-no in statistics. We need to decide on the model (including any
transformations) before we start using it.

\hypertarget{common-transformations}{%
\subsection*{Common transformations}\label{common-transformations}}
\addcontentsline{toc}{subsection}{Common transformations}

Which transformations could be considered? There is no simple answer.
But some guidelines. E.g. if we see non-linearity and increasing
variance with increasing fitted values, then a log transform may improve
matter.

Some common and useful transformations are:

\begin{itemize}
\tightlist
\item
  The log transformation for concentrations and absolute values.
\item
  The square-root transformation for count data.
\item
  The arcsin square-root \(\arcsin(\sqrt{\cdot})\) transformation for
  proportions/percentages.
\end{itemize}

Transformations can also be applied on explanatory variables, as we saw
in the example above.

\hypertarget{outliers}{%
\subsection*{Outliers}\label{outliers}}
\addcontentsline{toc}{subsection}{Outliers}

What do we do when we identify the presence of one or more outliers?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Start by checking the ``correctness'' of the data. Is there a typo or
  a decimal point that was shifted by mistake? Check both the response
  and explanatory variables.
\item
  If not, ask whether the model could be improved. Do reasonable
  transformations of the response and/or explanatory variables eliminate
  the outlier? Do the residuals have a distribution with a long tail
  (which makes it more likely that extreme observations occur)?
\item
  Sometimes, an outlier may be the most interesting observation in a
  dataset! Was the outlier created by some interesting but different
  process from the other data points?
\item
  Consider that outliers can also occur just by chance!
\item
  Only if you decide to report the results of both scenario can you
  check if inclusion/exclusion changes the qualitative conclusion, and
  by how much it changes the quantitative conclusion.
\end{enumerate}

\hypertarget{removing-outliers}{%
\subsection*{Removing outliers}\label{removing-outliers}}
\addcontentsline{toc}{subsection}{Removing outliers}

It might seem tempting to remove observations that apparently don't fit
into the picture. However:

\begin{itemize}
\tightlist
\item
  Do this \textbf{only with greatest care} e.g., if an observation has
  extremely implausible values!\\
\item
  Before deleting outliers, check points 1-5 above.
\item
  When removing outliers, \textbf{you must mention this in your report}.
\end{itemize}

During the course we'll see many more examples of things going at least
a bit wrong. And we'll do our best to improve the model, so we can be
confident in it, and start to use it. Which we will start to do in the
next lesson. But before we wrap up, some good news\ldots{}

\hypertarget{sec-kind-of-magic}{%
\section*{Its a kind of magic\ldots{}}\label{sec-kind-of-magic}}
\addcontentsline{toc}{section}{Its a kind of magic\ldots{}}

\markright{Its a kind of magic\ldots{}}

Above, we learned about linear regression, the equation for it, how to
estimate the coefficients, and how to check the assumptions. There was a
lot of information, and it might seem a bit overwhelming.

You might also be aware that there are quite a few other types of
statistical model, such as multiple regression, t-test, ANOVA, two-way
ANOVA, and ANCOVA. It could be worrying to think that you need to learn
so much new information for each of these types of tests.

But this is where the kind-of-magic happens. The good news is that the
linear regression model is a special case of what is called a
\emph{general linear model}, or just \emph{linear model} for short. And
that all the tests mentioned above are also types of \emph{linear
model}. So, once you have learned about linear regression, you have
learned a lot about linear models, and therefore also a lot about all of
these other tests as well.

Moreover, the same function in R `lm' is used to make all those
statistical models Awesome.

\hypertarget{so-what-is-a-linear-model}{%
\subsection*{So what is a linear
model?}\label{so-what-is-a-linear-model}}
\addcontentsline{toc}{subsection}{So what is a linear model?}

A linear model is a model where the relationship between the dependent
variable and the independent variables is linear. That is, the dependent
variable can be expressed as a linear combination of the independent
variables. An example of a linear model is:

\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p + \epsilon\]

where: \(y\) is the dependent variable, \(\beta_0\) is the intercept,
\(\beta_1, \beta_2, \ldots, \beta_p\) are the coefficients of the
independent variables, \(x_1, x_2, \ldots, x_p\) are the independent
variables, \(\epsilon\) is the error term.

In contrast, a non-linear model is a model where the relationship
between the dependent variable and the independent variables is
non-linear. An example of a non-linear model is the exponential growth
model:

\[y = \beta_0 + \beta_1 e^{\beta_2 x} + \epsilon\]

where: y is the dependent variable, \(\beta_0\) is the intercept,
\(\beta_1, \beta_2\) are the coefficients of the independent variables,
\(x\) is the independent variable, \(\epsilon\) is the error term.

Keep in mind that a model with a quadratic term is still a linear model.
For example:

\[y = \beta_0 + \beta_1 x_1 + \beta_2 x^2 + \epsilon\]

is still a linear model. We can see this if we substitute \(x^2\) with a
new variable \(x_2\), where \(x_2 = x^2\). The model then becomes:

\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon\]

This is clearly a linear model.

\bookmarksetup{startatroot}

\hypertarget{regression-part-2}{%
\chapter*{Regression Part 2}\label{regression-part-2}}
\addcontentsline{toc}{chapter}{Regression Part 2}

\markboth{Regression Part 2}{Regression Part 2}

\begin{itemize}
\tightlist
\item
  r-squared
\item
  Multiple r-squared
\item
  Adjusted r-squared
\item
  Conditional r-squared
\item
  Marginal r-squared
\end{itemize}

\textbf{How Owen will structure the lecture time}

\textbf{The chapter content below is the reference for what students are
expected to know.} During the lecture time Owen will talk through and
explain hopefully most of the content of this chapter. He will write on
a tablet, show figures and other content of this chapter, and may
live-code in RStudio. He will also present questions and ask students to
\emph{Think, Pair, Share}. The \emph{Share} part will sometime be via
clicker, sometimes by telling the class. The same will happen in lecture
3-6.

\hypertarget{introduction-2}{%
\section*{Introduction}\label{introduction-2}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

Now that we have a satisfactory model, we can start to use it. In the
following material, you will learn:

\begin{itemize}
\tightlist
\item
  How to measure how good is the regression (correlation and \(R^2\)).
\item
  How to test if the parameter estimates are compatible with some
  specific value (\(t\)-test).
\item
  How to find the range of parameters values are compatible with the
  data (confidence intervals).
\item
  How to find the regression lines compatible with the data (confidence
  band).
\item
  How to calculate plausible values of newly collected data (prediction
  band).
\end{itemize}

\hypertarget{accompanying-reading-material}{%
\subsection*{Accompanying reading
material}\label{accompanying-reading-material}}
\addcontentsline{toc}{subsection}{Accompanying reading material}

\hypertarget{how-good-is-the-regression-model}{%
\section*{How good is the regression
model?}\label{how-good-is-the-regression-model}}
\addcontentsline{toc}{section}{How good is the regression model?}

\markright{How good is the regression model?}

What would a good regression model look like? What would a bad one look
like? One could say that a good regression model is one that explains
the dependent variable well. But what could we mean by ``explains the
data well''?

Take these two examples.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-2-1.pdf}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#better-model)}, toprule=.15mm, breakable]

In which of these two would you say the model is better, and in which is
it worse?

\end{tcolorbox}

The first model seems to fit the data well, while the second one does
not. But how can we quantify this?

Let's say that we will measure the goodness of the model by the amount
of variability of the dependent variable that is explained by the
independent variable. To do this we need to do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Measure the total variability of the dependent variable (total sum of
  squares, \(SST\)).
\item
  Measure the amount of variability of the dependent variable that is
  explained by the independent variable (model sum of squares, \(SSM\)).
\item
  Measure the variability of the dependent variable that is not
  explained by the independent variable (error sum of squares, \(SSE\)).
\item
  Calculate the proportion of variability of the dependent variable that
  is explained by the independent variable (\(R^2\), pronounced as
  ``r-squared'') (also known as the coefficient of determination)
  (\(R^2\) = \(SSM/SST\)).
\end{enumerate}

\textbf{Importantly, note that we will calculate \(SSM\) and \(SSE\) so
that they sum up to \(SST\). I.e., \(SST = SSM + SSE\). That is, the
total variability is the sum of what is explained by the model and what
remains unexplained.}

Let's take each in turn:

\hypertarget{sst}{%
\subsection*{\texorpdfstring{\(SST\)}{SST}}\label{sst}}
\addcontentsline{toc}{subsection}{\(SST\)}

\textbf{1. The total variability of the dependent variable is the sum of
the squared differences between the dependent variable and its mean.
This is called the total sum of squares (\(SST\)).}

\[SST = \sum_{i=1}^{n} (y_i - \bar{y})^2\]

where: \(y_i\) is the dependent variable, \(\bar{y}\) is the mean of the
dependent variable, \(n\) is the number of observations.

\textbf{Note that sometimes \(SST\) is referred to as \(SSY\) (sum of
squares of \(y\)).}

Graphically, this is the sum of the square of the blue residuals as
shown in the following graph, where the horizontal dashed line is at the
value of the mean of the dependent variable.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-3-1.pdf}

We can calculate this in R as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SST }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1 }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y1))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{ssm-and-sse}{%
\subsection*{SSM and SSE}\label{ssm-and-sse}}
\addcontentsline{toc}{subsection}{SSM and SSE}

Now the next two steps, that is getting the model sum of squares (SSM)
and the error sum of squares (SSE) are a bit more complicated. To do
this we need to fit a regression model to the data. Let's see this
graphically, and divide the data into the explained and unexplained
parts.

Make a graph with vertical lines connecting the data to the mean of the
data, but with each line two parts, one from the mean to the data, and
one from the data to the predicted value.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-5-1.pdf}

In this graph, the square of the length of the green lines is the model
sum of squares (\(SST\)). The square of the length of the red lines is
the error sum of squares (\(SSE\)).

In a better model the length of the green lines will be \textbf{longer}
(the square of these gives the \(SMM\), the variability explained by the
model). And the length of the red lines will be \textbf{shorter} (the
square of these gives the \(SSE\), the variability not explained by the
model).

\hypertarget{ssm}{%
\subsection*{\texorpdfstring{\(SSM\)}{SSM}}\label{ssm}}
\addcontentsline{toc}{subsection}{\(SSM\)}

Next we will do the second step, that is calculate the model sum of
squares (\(SSM\)).

\textbf{2. The amount of variability of the dependent variable that is
explained by the independent variable is called the model sum of squares
(\(SSM\)).}

This is the difference between the predicted value of the dependent
variable and the mean of the dependent variable, squared and summed:

\[SSE = \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2\]

where: \(\hat{y}_i\) is the predicted value of the dependent variable,

In R, we calculate this as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{m1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(y1 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ x)}
\NormalTok{y1\_predicted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(m1)}
\NormalTok{SSM }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1\_predicted }\SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(y1))}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{SSM}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 348.0177
\end{verbatim}

\hypertarget{sse}{%
\subsection*{\texorpdfstring{\(SSE\)}{SSE}}\label{sse}}
\addcontentsline{toc}{subsection}{\(SSE\)}

Third, we calculate the error sum of squares (\(SSE\)) with either of
two methods. We could calculate it as the sum of the squared residuals,
or as the difference between the total sum of squares and the model sum
of squares:

\[SSE = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = SST - SSM\] Let's calculate
this in R uses both approaches:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{((y1 }\SpecialCharTok{{-}}\NormalTok{ y1\_predicted)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15.20883
\end{verbatim}

Or\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{SSE }\OtherTok{\textless{}{-}}\NormalTok{ SST }\SpecialCharTok{{-}}\NormalTok{ SSM}
\NormalTok{SSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 15.20883
\end{verbatim}

\hypertarget{r2}{%
\subsection*{\texorpdfstring{\(R^2\)}{R\^{}2}}\label{r2}}
\addcontentsline{toc}{subsection}{\(R^2\)}

Finally, we calculate the proportion of variability of the dependent
variable that is explained by the independent variable (\(R^2\)):

\[R^2 = \frac{SSM}{SST}\]

\begin{verbatim}
[1] 0.9581285
\end{verbatim}

\hypertarget{is-my-r-squared-good}{%
\subsection*{Is my R squared good?}\label{is-my-r-squared-good}}
\addcontentsline{toc}{subsection}{Is my R squared good?}

What value of \(R^2\) is considered good? In ecological research,
\(R^2\) values are often low (less than 0.3), because ecological systems
are complex and many factors influence the dependent variable. However,
in other fields, such as physiology, \(R^2\) values are often higher.
Therefore, the answer of what values of \(R^2\) are good depends on the
field of research.

Here are the four examples and their r-squared.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-10-1.pdf}

\hypertarget{questions}{%
\subsection*{Questions}\label{questions}}
\addcontentsline{toc}{subsection}{Questions}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#what-minimised)}, toprule=.15mm, breakable]

What is minimised when we fit a regression model? And therefore what is
maximised?

\end{tcolorbox}

\hypertarget{how-unlikey-is-the-observed-data-given-the-null-hypothesis}{%
\section*{How unlikey is the observed data given the null
hypothesis?}\label{how-unlikey-is-the-observed-data-given-the-null-hypothesis}}
\addcontentsline{toc}{section}{How unlikey is the observed data given
the null hypothesis?}

\markright{How unlikey is the observed data given the null hypothesis?}

We often hear this expressed as ``is the relationship significant?'' And
maybe we heard that the relationship is significant if the p-value is
less than 0.05. But what does all this actually mean? In this section
we'll figure all this out. The first step to is to formulate a null
hypothesis.

What is a meaningful null hypothesis for a regression model?

As mentioned, often we're interested in whether there is a relationship
between the dependent (response) and independent (explanatory) variable.
Therefore, the null hypothesis is that there is no relationship between
the dependent and independent variable. This means that the null
hypothesis is that the slope of the regression line is zero.

Recall the regression model: \[y = \beta_0 + \beta_1 x + \epsilon\]

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#null-hypothesis)}, toprule=.15mm, breakable]

Write down the null hypothesis of no relationship between \(x\) and
\(y\) in terms of a \(\beta\) parameter.

\end{tcolorbox}

The null hypothesis is that the slope of the regression line is zero:
\[H_0: \beta_1 = 0\]

What is the alternative hypothesis?

\[H_1: \beta_1 \neq 0\]

So, how do we test the null hypothesis? More precisely, we are going to
calculate the probability of observing the data we have, given that the
null hypothesis is true. If this probability is very low, then we can
reject the null hypothesis.

Does that make sense? Does it seem a bit convoluted? It is a bit!!!

But this is how hypothesis testing works. We never prove the null
hypothesis is true. Instead, we calculate the probability of observing
our data given that the null hypothesis is true. If this probability is
very low, we reject the null hypothesis.

To make the calculation we can use the fact that the slope of the
regression line is an estimate of the true slope. This estimate has
uncertainty associated with it. We can use this uncertainty to calculate
the probability of observing the data we have, given the null hypothesis
is true.

We can see that the slope estimate (the \(x\) row) has uncertainty by
looking at the regression output:

\begin{verbatim}
              Estimate Std. Error
(Intercept) -0.7638353  0.6652233
x            2.1161160  0.1072104
\end{verbatim}

The estimate is the mean of the distribution of the parameter (slope)
and the standard error is a measure of the uncertainty of the estimate.

The standard error is calculated as:

\[\sigma^{(\beta_1)} = \sqrt{ \frac{\hat\sigma^2}{\sum_{i=1}^n (x_i - \bar x)^2}}\]

Where \(\hat\sigma^2\) is the expected residual variance of the model.
This is calculated as:

\[\hat\sigma^2 = \frac{\sum_{i=1}^n (y_i - \hat y_i)^2}{n-2}\]

Where \(\hat y_i\) is the predicted value of \(y_i\) from the regression
model.

OK, let's take a look at this intuitively. We have the estimate of the
slope and the standard error of the estimate.

Here is a graph of the value of the slope estimate versus the standard
error of the estimate:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-12-1.pdf}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#chance-area)}, toprule=.15mm, breakable]

In what areas of the graph is the slope estimate more likely to have
been observed by chance? And what regions is it less likely to have been
observed by chance?

Think about this before you look at the end of this chapter for an
answer (Section \textbf{?@sec-visual-p-values-regress}).

\end{tcolorbox}

When the slope estimate is larger, it is less likely to have been
observed by chance. And when the standard error is larger, it is more
likely to have been observed by chance. How can we put these together
into a single measure?

If we divide the slope estimate by the standard error, we get a measure
of how many standard errors the slope estimate is from the null
hypothesis slope of zero. This is the \(t\)-statistic:

\[t = \frac{\hat\beta_1 - \beta_{1,H_0}}{\sigma^{(\beta_1)}}\]

Where \(\beta_{1,H_0}\) is the null hypothesis value of the slope,
usually zero, so that

\[t = \frac{\hat\beta_1}{\sigma^{(\beta_1)}}\]

\textbf{The \(t\)-statistic is a measure of how many standard errors the
slope estimate is from the null hypothesis value of the slope. The
larger the \(t\)-statistic, the less likely the slope estimate was
observed by chance.}

How can we transform the value of a \(t\)-statistic into a p-value? We
can use the \textbf{\(t\)-distribution}, which quantifies the
probability of observing a value of the \(t\)-statistic under the null
hypothesis.

But what is the \(t\)-distribution? It is a distribution of the
\(t\)-statistic under the null hypothesis. It is a bell-shaped
distribution that is centered on zero. The shape of the distribution is
determined by the degrees of freedom, which is \(n-2\) for a simple
linear regression model.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-13-1.pdf}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

By the way, it is named the \(t\)-distribution by it's developer,
William Sealy Gosset, who worked for the Guinness brewery in Dublin,
Ireland. In his 1908 paper, Gosset introduced the \(t\)-distribution but
he didn't explicitly explain his choice of the letter \(t\). The choice
of the letter \(t\) could be to indicate ``Test'', as the
\(t\)-distribution was developed specifically for hypothesis testing.

\end{tcolorbox}

Now, recall that the p-value is the probability of observing the value
of the test statistic (so here the \(t\)-statistic) at least as extreme
as the one we have, given the null hypothesis is true. We can calculate
this probability by integrating the \(t\)-distribution from the observed
\(t\)-statistic to the tails of the distribution.

Here is a graph of the \(t\)-distribution with 100 degrees of freedom
with the tails of the distribution shaded so that the area of the shaded
region is 0.05 (i.e., 5\% of the total area).

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-14-1.pdf}

And here's a graph of the \(t\)-distribution with 1000 degrees of
freedom (blue line) and the normal distribution (green dashed line):

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-15-1.pdf}

So, with a large number of observations, the \(t\)-distribution
approaches the normal distribution. For the normal distribution, the
95\% area is between -1.96 and 1.96.

\begin{Shaded}
\begin{Highlighting}[]
\DocumentationTok{\#\# x value for 95\% area of normal distribution}
\NormalTok{x\_value }\OtherTok{\textless{}{-}} \FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.975}\NormalTok{)}
\NormalTok{x\_value}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.959964
\end{verbatim}

\texttt{qnorm} is a function that calculates the \(x\) value for a given
quantile (probability) of the normal distribution. In simpler terms, it
finds the value \(x\) at which the area under the normal curve (up to
\(x\)) equals the given probability \(p\) (0.975 in the example
immediately above here).

Let's go back to the age - blood pressure data and calculate the p-value
for the slope estimate.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-18-1.pdf}

Here's the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(Systolic\_BP }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Age, }\AttributeTok{data =}\NormalTok{ bp\_data)}
\end{Highlighting}
\end{Shaded}

Here we calculate the \(t\)-statistic for the slope estimate:

And here we calculate the one-tailed and two-tailed \(p\)-values:

\begin{verbatim}
         Age 
3.746958e-51 
\end{verbatim}

\begin{verbatim}
         Age 
7.493917e-51 
\end{verbatim}

We can get the \(p\)-value directly from the \texttt{summary} function:

\begin{verbatim}
    Estimate   Std. Error      t value     Pr(>|t|) 
8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 
\end{verbatim}

Conclusion: there is \textbf{very strong evidence} that the blood
pressure is associated with age, because the \(p\)-value is extremely
small (thus it is very unlikely that the observed slope value or a large
one would be seen if there was really no association). Thus, we can
reject the null hypothesis that the slope is zero.

This basically answers question 1: ``Are the parameters compatible with
some specific value?''

\hypertarget{recap-formal-definition-of-the-p-value}{%
\subsection*{\texorpdfstring{Recap: Formal definition of the
\(p\)-value}{Recap: Formal definition of the p-value}}\label{recap-formal-definition-of-the-p-value}}
\addcontentsline{toc}{subsection}{Recap: Formal definition of the
\(p\)-value}

\textbf{The formal definition of \(p\)-value is the probability to
observe a data summary (e.g., an average or a slope) that is at least as
extreme as the one observed, given that the null hypothesis is correct.}

Example (normal distribution): Assume that we calculated that
\(t\)-value = -1.96

\(\Rightarrow\) \(Pr(|t|\geq 1.96)=0.05\) (two-tailed) and
\(Pr(t\leq-1.96)=0.025\) (one-tailed).

And here is a graph showing this:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-23-1.pdf}

\hypertarget{a-cautionary-note-on-the-use-of-p-values}{%
\subsection*{\texorpdfstring{A cautionary note on the use of
\(p\)-values}{A cautionary note on the use of p-values}}\label{a-cautionary-note-on-the-use-of-p-values}}
\addcontentsline{toc}{subsection}{A cautionary note on the use of
\(p\)-values}

Maybe you have seen that in statistical testing, often the criterion
\(p\leq 0.05\) is used to test whether \(H_0\) should be rejected. This
is often done in a black-or-white manner. However, we will put a lot of
attention to a more reasonable and cautionary interpretation of
\(p\)-values in this course!

\hypertarget{how-strong-is-the-relationship}{%
\section*{How strong is the
relationship?}\label{how-strong-is-the-relationship}}
\addcontentsline{toc}{section}{How strong is the relationship?}

\markright{How strong is the relationship?}

The actual value of the slope has practical meaning. The slope of the
regression line tells us how much the dependent variable changes when
the independent variable changes by one unit. The slope is one measure
of the strength of the relationship between the two variables.

We can ask what values of a parameter estimate are compatible with the
data (confidence intervals)? To answer this question, we can determine
the confidence intervals of the regression parameters.

The confidence interval of a parameter estimate is defined as the
interval that contains the true parameter value with a certain
probability. So the 95\% confidence interval of the slope is the
interval that contains the true slope with a probability of 95\%.

We can then imagine two cases. The 95\% confidence interval of the slope
includes 0:

\begin{verbatim}
Warning: `geom_errobarh()` was deprecated in ggplot2 4.0.0.
i Please use the `orientation` argument of `geom_errorbar()` instead.
\end{verbatim}

\begin{verbatim}
`height` was translated to `width`.
\end{verbatim}

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-24-1.pdf}

Or where the confidence interval does not include zero:

\begin{verbatim}
`height` was translated to `width`.
\end{verbatim}

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-25-1.pdf}

How do we calculate the lower and upper limits of the 95\% confidence
interval of the slope?

Recall that the \(t\)-value for a null hypothesis of slope of zero is
defined as:

\[t = \frac{\hat\beta_1}{\hat\sigma^{(\beta_1)}}\]

The first step is to calculate the \(t\)-value that corresponds to a
p-value of 0.05. This is the \(t\)-value that corresponds to the 97.5\%
quantile of the \(t\)-distribution with \(n-2\) degrees of freedom.

\(t_{0.975} = t_{0.025} = 1.96\), for large \(n\).

The 95\% confidence interval of the slope is then given by:

\[\hat\beta_1 \pm t_{0.975} \cdot \hat\sigma^{(\beta_1)}\]

In our blood pressure example the estimated slope is 0.8240678 and the
standard error of the slope is 0.0277096. We can calculate the 95\%
confidence interval of the slope in \emph{R} as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{100}
\NormalTok{t\_0975 }\OtherTok{\textless{}{-}} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{, }\AttributeTok{df =}\NormalTok{ n }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{)}
\NormalTok{half\_interval }\OtherTok{\textless{}{-}}\NormalTok{ t\_0975 }\SpecialCharTok{*} \FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coef[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{]}
\NormalTok{lower\_limit }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}}\NormalTok{ half\_interval}
\NormalTok{upper\_limit }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{+}\NormalTok{ half\_interval}
\NormalTok{ci\_slope }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(lower\_limit, upper\_limit)}
\NormalTok{slope }\OtherTok{\textless{}{-}} \FunctionTok{coef}\NormalTok{(mod1)[}\DecValTok{2}\NormalTok{]}
\NormalTok{slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Age 
0.8240678 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ci\_slope}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
      Age       Age 
0.7690791 0.8790565 
\end{verbatim}

Or, using the \texttt{confint} function:

\begin{verbatim}
    2.5 %    97.5 % 
0.7690791 0.8790565 
\end{verbatim}

Or we can do it using values from the coefficients table:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coefs }\OtherTok{\textless{}{-}} \FunctionTok{summary}\NormalTok{(mod1)}\SpecialCharTok{$}\NormalTok{coef}
\NormalTok{beta }\OtherTok{\textless{}{-}}\NormalTok{ coefs[}\DecValTok{2}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\NormalTok{sdbeta }\OtherTok{\textless{}{-}}\NormalTok{ coefs[}\DecValTok{2}\NormalTok{,}\DecValTok{2}\NormalTok{] }
\NormalTok{beta }\SpecialCharTok{+} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{,}\DecValTok{1}\NormalTok{) }\SpecialCharTok{*} \FunctionTok{qt}\NormalTok{(}\FloatTok{0.975}\NormalTok{,}\DecValTok{241}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ sdbeta }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.7694840 0.8786516
\end{verbatim}

\emph{Interpretation}: for an increase in the age by one year, roughly
0.82 mmHg increase in blood pressure is expected, and all true values
for \(\beta_1\) between 0.77 and 0.88 are compatible with the observed
data.

\hypertarget{confidence-and-prediction-bands}{%
\section*{Confidence and Prediction
Bands}\label{confidence-and-prediction-bands}}
\addcontentsline{toc}{section}{Confidence and Prediction Bands}

\markright{Confidence and Prediction Bands}

\begin{itemize}
\item
  Remember: If another sample from the same population was taken, the
  regression line would look slightly different.
\item
  There are two questions to be asked:
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Which other regression lines are compatible with the observed data?
  This leads to the \emph{confidence band}.
\item
  Where do future observations (\(y\)) with a given \(x\) coordinate
  lie? This leads to the \emph{prediction band}.
\end{enumerate}

Note: The prediction band is much broader than the confidence band.

\hypertarget{calculation-of-the-confidence-band}{%
\section*{Calculation of the confidence
band}\label{calculation-of-the-confidence-band}}
\addcontentsline{toc}{section}{Calculation of the confidence band}

\markright{Calculation of the confidence band}

Given a fixed value of \(x\), say \(x_0\). The question is:

Where does \(\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0\) lie with a
certain confidence (i.e., 95\%)?

This question is not trivial, because both \(\hat\beta_0\) and
\(\hat\beta_1\) are estimates from the data and contain uncertainty.

The details of the calculation are given in Stahel 2.4b.

Plotting the confidence interval around all \(\hat y_0\) values one
obtains the \emph{confidence band} or \emph{confidence band for the
expected values} of \(y\).

Note: For the confidence band, only the uncertainty in the estimates
\(\hat\beta_0\) and \(\hat\beta_1\) matters.

Here is the confidence band for the blood pressure data:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-29-1.pdf}

Very narrow confidence bands indicate that the estimates are very
precise. In this case the estimated intercept and slope are precise
because the sample size is large and the data points are close to the
regression line.

\hypertarget{calculations-of-the-prediction-band}{%
\section*{Calculations of the prediction
band}\label{calculations-of-the-prediction-band}}
\addcontentsline{toc}{section}{Calculations of the prediction band}

\markright{Calculations of the prediction band}

We can easily predicted an expected value of \(y\) for a given \(x\)
value. But we can also ask w where does a \emph{future observation} lie
with a certain confidence (i.e., 95\%)?

To answer this question, we have to \emph{consider not only the
uncertainty in the predicted value caused by uncertainty in the
parameter estimates} \(\hat y_0 = \hat\beta_0 + \hat\beta_1 x_0\), but
also the \emph{error term} \(\epsilon_i \sim N(0,\sigma^2)\)\}.

This is the reason why the \textbf{prediction band} is wider than the
confidence band.

Here's a graph showing the prediction band for the blood pressure data:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-31-1.pdf}

Another way to think of the 95\% confidence band is that it is where we
would expect 95\% of the regression lines to lie if we were to collect
many samples from the same population. The 95\% prediction band is where
we would expect 95\% of the future observations to lie.

\hypertarget{that-is-regression-done-at-least-for-our-current-purposes}{%
\section*{That is regression done (at least for our current
purposes)}\label{that-is-regression-done-at-least-for-our-current-purposes}}
\addcontentsline{toc}{section}{That is regression done (at least for our
current purposes)}

\markright{That is regression done (at least for our current purposes)}

\begin{itemize}
\tightlist
\item
  Why use (linear) regression?
\item
  Fitting the line (= parameter estimation)
\item
  Is linear regression good enough model to use?
\item
  What to do when things go wrong?
\item
  Transformation of variables/the response.
\item
  Handling of outliers.
\item
  Goodness of the model: Correlation and \(R^2\)
\item
  Tests and confidence intervals
\item
  Confidence and prediction bands
\end{itemize}

\hypertarget{additional-reading-material}{%
\section*{Additional reading
material}\label{additional-reading-material}}
\addcontentsline{toc}{section}{Additional reading material}

\markright{Additional reading material}

If you'd like another perspective and a deeper delve into some of the
mathematical details, please look at Chapter 2 of \emph{Lineare
Regression}, p.7-20 (Stahel script), Chapters 3.1, 3.2a-q of
\emph{Lineare Regression}, and Chapters 4.1 4.2f, 4.3a-e of
\emph{Lineare Regression}

\hypertarget{extras-1}{%
\section*{Extras}\label{extras-1}}
\addcontentsline{toc}{section}{Extras}

\markright{Extras}

\hypertarget{randomisation-test-for-the-slope-of-a-regression-line}{%
\subsection*{Randomisation test for the slope of a regression
line}\label{randomisation-test-for-the-slope-of-a-regression-line}}
\addcontentsline{toc}{subsection}{Randomisation test for the slope of a
regression line}

Let's use randomisation as another method to understand how likely we
are to observe the data we have, given the null hypothesis is true.

If the null hypothesis is true, we expect no relationship between \(x\)
and \(y\). Therefore, we can shuffle the \(y\) values and fit a
regression model to the shuffled data. We can repeat this many times and
calculate the slope of the regression line each time. This will give us
a distribution of slopes we would expect to observe if the null
hypothesis is true.

First, we'll make some data and get the slope of the regression line.
Here is the observed slope and relationship:

\begin{verbatim}
        x 
0.1251108 
\end{verbatim}

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-33-1.pdf}

Now we'll use randomisation to test the null hypothesis. We can create
lots of examples where the relationship is expected to have a slope of
zero by shuffling randomly the \(y\) values. Here are 20:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-34-1.pdf}

Now let's create 19 and put the real one in there somewhere random.
Here's a case where the real data has a quite strong relationship:

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-35-1.pdf}

We can confidently find the real data amount the shuffled data. But what
if the relationship is weaker?

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-36-1.pdf}

Now its less clear which is the real data. We can use this idea to test
the null hypothesis.

We do the same procedure of but instead of just looking at the graphs,
we calculate the slope of the regression line each time. This gives us a
distribution of slopes we would expect to observe if the null hypothesis
is true. We can then see where the observed slope lies in this
distribution of null hypothesis slopes.

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-37-1.pdf}

We can now calculate the probability of observing the data we have,
given the null hypothesis is true.

\begin{verbatim}
[1] 0.0172
\end{verbatim}

\hypertarget{sec-visual-p-values-regress}{%
\subsection*{Visualising p-values for regression
slopes}\label{sec-visual-p-values-regress}}
\addcontentsline{toc}{subsection}{Visualising p-values for regression
slopes}

\includegraphics{5.1-regression-part2_files/figure-pdf/unnamed-chunk-39-1.pdf}

\bookmarksetup{startatroot}

\hypertarget{anova-l6}{%
\chapter*{ANOVA (L6)}\label{anova-l6}}
\addcontentsline{toc}{chapter}{ANOVA (L6)}

\markboth{ANOVA (L6)}{ANOVA (L6)}

\begin{itemize}
\tightlist
\item
  One-way ANOVA
\item
  Post-hoc tests and contrasts
\item
  Two-way ANOVA
\end{itemize}

ANOVA = ANalysis Of VAriance (Varianzanalyse)

\hypertarget{introduction-3}{%
\section*{Introduction}\label{introduction-3}}
\addcontentsline{toc}{section}{Introduction}

\markright{Introduction}

The previous two chapters were about linear regression. \emph{Linear
regression} is a type of \emph{linear model} -- recall that in \emph{R}
we used the function \texttt{lm()} to make the regression model. In this
chapter we will look at a different type of linear model: analysis of
variance (ANOVA).

Recall that linear regression is a linear model with one continuous
explanatory (independent) variable. A continuous explanatory variable is
a variable in which values can take any value within a range (e.g.,
height, weight, temperature).

In contrast, analysis of variance (ANOVA) is a linear model with one or
more categorical explanatory variables. We will first look at a one-way
ANOVA, which has one categorical explanatory variable. Later (in a
following chapter) we will look at two-way ANOVA, which has two
categorical explanatory variables.

What is a categorical variable? A categorical explanatory variable is a
variable that contains values that fall into distinct groups or
categories. For example, habitat type (e.g., forest, grassland,
wetland), treatment group (e.g., control, low dose, high dose), or diet
type (e.g., vegetarian, vegan, omnivore).

This means that each observation belongs to one of a limited number of
categories or groups. For example, in a study of how blood pressure
varies with diet type, diet type is a categorical variable with several
levels (e.g., vegetarian, vegan, omnivore). A person can only belong to
one diet type category.

Here are the first several rows of a dataset that contains blood
pressure measurements for individuals following different diet types:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(bp\_data\_diet, bp, diet, person\_ID)}
\NormalTok{bp\_data\_diet}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 3
      bp diet          person_ID
   <dbl> <chr>         <chr>    
 1   120 meat heavy    person_1 
 2    89 vegan         person_2 
 3    86 vegetarian    person_3 
 4   116 meat heavy    person_4 
 5   115 Mediterranean person_5 
 6   134 meat heavy    person_6 
 7    99 vegetarian    person_7 
 8   104 vegetarian    person_8 
 9   110 Mediterranean person_9 
10    97 Mediterranean person_10
# i 40 more rows
\end{verbatim}

There are three variables: - \texttt{bp}: blood pressure (continuous
response variable) - \texttt{diet}: diet type (categorical explanatory
variable) - \texttt{person\_ID}: unique identifier for each individual
(not used in the analysis)

Note that the \texttt{diet} variable is of type
\texttt{\textless{}chr\textgreater{}} which is short for
\texttt{character}. In \emph{R}, categorical variables are often
represented as factors.

As usual, its a really good idea to visualise the data in as close to
``raw'' form as possible before doing any analysis. We'll make a
scatterplot of blood pressure versus diet type.

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-4-1.pdf}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Tip}, toprule=.15mm, breakable]

We just used \texttt{geom\_jitter()} instead of \texttt{geom\_point()}
to make a scatterplot. This is because \texttt{geom\_jitter()} adds a
small amount of random noise to the points, which helps to prevent
overplotting when multiple points have the same value (which is common
when the x-axis is categorical).

When we use \texttt{geom\_jitter()}, we can specify the amount of noise
to add in the x and y directions using the \texttt{width} and
\texttt{height} arguments, respectively. We must be very careful to not
add noise to the y direction if we care about the actual y values (e.g.,
blood pressure). In this case, we only added noise in the x direction by
setting \texttt{height\ =\ 0} to separate the points just enough, but
not so much that we could get confused about which of the diets they
belong to.

\end{tcolorbox}

Looking at this graph it certainly looks like diet type has an effect on
blood pressure. But is this effect statistically significant? In other
words, are the differences in mean blood pressure between diet types
larger than we would expect due to random variation alone?

Analysis of variance (ANOVA) is a statistical method that can help us
answer this question, and also others.

\hypertarget{how-does-it-look-like-in-r}{%
\section*{How does it look like in
R?}\label{how-does-it-look-like-in-r}}
\addcontentsline{toc}{section}{How does it look like in R?}

\markright{How does it look like in R?}

We can fit a one-way ANOVA model in \emph{R} using the same
\texttt{lm()} function that we used for linear regression. The only
difference is that the explanatory variable is categorical.

Then instead of using \texttt{summary()} to look at the results, we use
the \texttt{anova()} function.

\begin{verbatim}
Analysis of Variance Table

Response: bp
          Df Sum Sq Mean Sq F value    Pr(>F)    
diet       3 5274.2 1758.08  20.728 1.214e-08 ***
Residuals 46 3901.5   84.82                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

This is an ANOVA table. It shows us the sources of variation in the
data, along with their associated degrees of freedom (Df), sum of
squares (Sum Sq), mean square (Mean Sq), F value, and p-value
(Pr(\textgreater F)) associate with a getting a F value the same as or
greater than the observed F value if the null hypothesis were true.

The challenge now is to understand what all of these values mean! Let's
take it step by step.

\hypertarget{what-is-anova}{%
\section*{What is ANOVA?}\label{what-is-anova}}
\addcontentsline{toc}{section}{What is ANOVA?}

\markright{What is ANOVA?}

Analysis of variance is a method to compare whether the observations
(e.g., of blood pressure) differ according to some grouping (e.g., diet)
that the subjects (e.g., people) belong to.

We already know a lot about analysing variance: we compared the total
sum of squares (SST), model sum of squares (SSM) and the residual sum of
squares (SSE) in the context of linear regression. We used these to
calculated the \(R^2\) value. The \(R^2\) value tells us how much of the
total variance in the response variable (e.g., blood pressure) is
explained by the explanatory variable (e.g., diet).

The same applies to analysis of variance (ANOVA) (as well as regression)
because ANOVA is a special case of a linear model, just like regression
is also a special case of a linear model.

The defining characteristic of ANOVA is that we are comparing the means
of groups by analysing variances. Put another way, we will have a single
categorical explanatory variable with two or more levels. We will test
whether the means of the response variable are the same across all
levels of the explanatory variable, and we test this by analysing the
variances.

When we have only one categorical explanatory variable, we use a
\emph{one-way} ANOVA. When we have two categorical explanatory
variables, we will use a \emph{two-way} ANOVA (we'll look at this in a
subsequent chapter).

\hypertarget{anova-as-a-linear-model}{%
\section*{ANOVA as a linear model}\label{anova-as-a-linear-model}}
\addcontentsline{toc}{section}{ANOVA as a linear model}

\markright{ANOVA as a linear model}

Just like linear regression, ANOVA can be expressed as a linear model.
The key difference is that in ANOVA, the explanatory variable is
categorical rather than continuous.We formulate the linear model as
follows:

\[y_{ij} = \mu_j + \epsilon_{i}\]

where:

\begin{itemize}
\tightlist
\item
  \(y_{ij}\) = Blood pressure of individual \(i\) with diet \(j\)
\item
  \(\mu_i\) = Mean blood pressure of an individual with diet \(j\)
\item
  \(\epsilon_{i}\sim N(0,\sigma^2)\) is an independent error term.
\end{itemize}

Graphically, with the blood pressure and diet data, this looks like:

\begin{verbatim}
`summarise()` has grouped output by 'diet'. You can override using the
`.groups` argument.
\end{verbatim}

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-9-1.pdf}

::: \{.callout-note\}

There is lots of hidden code used to create the data used in the graph
above, and to make the graph itself. You can see the code by going to
the \href{https://github.com/opetchey/BIO144_Course_Book}{Github
repository for this book}.

\hypertarget{rewrite-the-model}{%
\subsection*{Rewrite the model}\label{rewrite-the-model}}
\addcontentsline{toc}{subsection}{Rewrite the model}

We usually use a different formulation of the linear model for ANOVA.
This is because we usually prefer to express the estimated parameters in
terms of \emph{differences between means} (rather than the means
themselves). The reason for this is that then the null hypothesis can be
that the differences are zero.

To proceed with this formulation, we define one of the groups as the
reference group, and make the mean of that equal to the intercept of the
model. For example, if we choose the ``meat heavy'' diet as the
reference group, we can write:

\[\mu_{meat} = \beta_0\]

And then to express the other group means as deviations from the
reference group mean:

\[\mu_{Med} = \beta_0 + \beta_1\] \[\mu_{vegan} = \beta_0 + \beta_2\]
\[\mu_{veggi} = \beta_0 + \beta_3\]

When we write out the entire model, we get:

\[y_i = \beta_0 + \beta_1 x_i^{1} + \beta_2 x_i^{2} + \beta_3 x_i^{3} + \epsilon_i\]
where: \(y_i\) is the blood pressure of individual \(i\). \(x_i^{1}\) is
a binary variable indicating whether individual \(i\) is on the
Mediterranean diet. \(x_i^{2}\) is a binary variable indicating whether
individual \(i\) is on the vegan diet. \(x_i^{3}\) is a binary variable
indicating whether individual \(i\) is on the vegetarian diet.

Graphically, the model now looks like this:

\begin{verbatim}
`summarise()` has grouped output by 'diet'. You can override using the
`.groups` argument.
\end{verbatim}

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-10-1.pdf}

\hypertarget{the-anova-test-the-f-test}{%
\subsection*{\texorpdfstring{The ANOVA test: The
\(F\)-test}{The ANOVA test: The F-test}}\label{the-anova-test-the-f-test}}
\addcontentsline{toc}{subsection}{The ANOVA test: The \(F\)-test}

\textbf{Aim of ANOVA}: to test \emph{globally} if the groups differ.
That is we want to test the null hypothesis that all of the group means
are equal:

\[H_0: \mu_1=\mu_2=\ldots = \mu_g\] This is equivalent to testing if all
\(\beta\)s that belong to a categorical variable are = 0.

\[H_0: \beta_1 = \ldots = \beta_{g-1} = 0\] The alternate hypothesis is
that \({H_1}\): The group means are not all the same.

A key point is that we are testing a null hypothesis that concerns all
the groups. We are not testing if one group is different from another
group (which we could do with a \(t\)-test on one of the non-intercept
\(\beta\)s).

Because we are testing a null hypothesis that concerns all the groups,
we need to use an \(F\)-test. It asks if the model with the group means
is better than a model with just the overall mean.

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-note-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toprule=.15mm, breakable]

The \(F\)-test is called the ``\(F\)-test'' because it is based on the
\(F\)-distribution, which was named after the statistician
\href{https://en.wikipedia.org/wiki/Ronald_Fisher}{Sir Ronald A.
Fisher}. Fisher developed this statistical method as part of his
pioneering work in analysis of variance (ANOVA) and other fields of
experimental design and statistical inference.

\end{tcolorbox}

Actually, the \(F\)-test does not directly test the null hypothesis that
all the group means are equal. Instead, it tests whether the model that
includes the \emph{group means} explains significantly more variance in
the data than a model that only includes the overall mean (i.e., without
considering group differences).

The \(F\)-test does this by comparing two variance estimates: the
variance explained by the group means (between-group variance) and the
variance that remains unexplained within each group (within-group
variance).

\hypertarget{interpretation-of-the-f-statistic}{%
\subsection*{\texorpdfstring{Interpretation of the \(F\)
statistic}{Interpretation of the F statistic}}\label{interpretation-of-the-f-statistic}}
\addcontentsline{toc}{subsection}{Interpretation of the \(F\) statistic}

The \(F\)-test involves calculating from the observed data the value of
the \(F\) statistic, and then computing if that value is large enough to
reject the null hypothesis.

The \(F\) statistic is a ratio of two variances: the variance
\textbf{between} groups, and the variance \textbf{within} groups.

Here is an example with very low within group variability, and high
between group variability:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-11-1.pdf}

And here's an example with very high within group variability, and low
between group variability:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-12-1.pdf}

So, when the ratio of between group variance to within group variance is
large, the group means are very different compared to the variability
within groups. This suggests that the groups are different.

When the ratio is small, the group means are similar compared to the
variability within groups. This suggests that the groups are not
different.

\begin{itemize}
\tightlist
\item
  \textbf{\(F\) increases}

  \begin{itemize}
  \tightlist
  \item
    when the group means become more different, or
  \item
    when the variability within groups decreases.
  \end{itemize}
\item
  \textbf{\(F\) decreases}

  \begin{itemize}
  \tightlist
  \item
    when the group means become more similar, or
  \item
    when the variability within groups increases.
  \end{itemize}
\end{itemize}

\(\rightarrow\) The larger \(F\), the less likely are the data seen
under \(H_0\).

\hypertarget{calculating-the-f-statistic}{%
\subsection*{\texorpdfstring{Calculating the \(F\)
statistic}{Calculating the F statistic}}\label{calculating-the-f-statistic}}
\addcontentsline{toc}{subsection}{Calculating the \(F\) statistic}

Recall that the \(F\) statistic is a ratio of two variances.
Specifically, it is the ratio of two mean squares (MS):

\begin{itemize}
\tightlist
\item
  \(MS_{model}\): the variability \textbf{between} groups.
\item
  \(MS_{residual}\): the variability \textbf{within} groups.
\end{itemize}

\(MS\) stands for Mean Square, and is a variance estimate.

The \(F\) statistic is calculated as:

\[F = \frac{MS_{model}}{MS_{residual}}\]

To find the mean squares, we need to calculate the within and the
between group sums of squares, and the corresponding degrees of freedom.
Let's go though this step by step.

\hypertarget{calculating-the-sums-of-squares}{%
\subsection*{Calculating the sums of
squares}\label{calculating-the-sums-of-squares}}
\addcontentsline{toc}{subsection}{Calculating the sums of squares}

First we get the total sum of squares (SST), which quantifies the total
variability in the data. This is then split into the explained
variability (SSM), and the residual variability (SSE).

\textbf{Total variability:} SST =
\(\sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij}-\overline{y})^2\)

where:

\begin{itemize}
\tightlist
\item
  \(y_{ij}\) is the blood pressure of individual \(j\) in group \(i\)
\item
  \(\overline{y}\) is the overall mean blood pressure
\item
  \(n_i\) is the number of individuals in group \(i\)
\item
  \(k\) is the number of groups
\end{itemize}

\textbf{Explained variability (between group variability)}: == SSM =
\(\sum_{i=1}^k n_i (\overline{y}_{i} - \overline{y})^2\)

where:

\begin{itemize}
\tightlist
\item
  \(\overline{y}_{i}\) is the mean blood pressure of group \(i\)
\end{itemize}

\textbf{Residual variability (within group variability)}: = SSE =
\(\sum_{i=1}^k \sum_{j=1}^{n_i} (y_{ij} - \overline{y}_{i} )^2\)

\hypertarget{calculating-the-degrees-of-freedom}{%
\subsection*{Calculating the degrees of
freedom}\label{calculating-the-degrees-of-freedom}}
\addcontentsline{toc}{subsection}{Calculating the degrees of freedom}

And now we need the degrees of freedom for each sum of squares:

\textbf{SST degrees of freedom}: \(n - 1\) (total degrees of freedom is
number of observations \(n\) minus 1)

\textbf{SSM degrees of freedom}: \(k - 1\) (model degrees of freedom is
number of groups \(k\) minus 1)

\textbf{SSE degrees of freedom}: \(n - k\) (residual degrees of freedom
is total degrees of freedom \(n - 1\) minus model degrees of freedom
\(k - 1\))

\hypertarget{total-degrees-of-freedom}{%
\subsubsection*{Total degrees of
freedom}\label{total-degrees-of-freedom}}
\addcontentsline{toc}{subsubsection}{Total degrees of freedom}

The total degrees of freedom are the degrees of freedom associated with
the total sum of squares (\(SST\)).

In order to calculate the \(SST\), we need to calculate the mean of the
response variable. This implies that we estimate one parameter (the mean
of the response variable). As a consequence, we lose one degree of
freedom and so there remain \(n-1\) degrees of freedom associated with
the total sum of squares (where \(n\) is the number of observations).

What do we mean by ``lose one degree of freedom''? Imagine we have ten
observations. We can calculate the mean of these ten observations. But
if we know the mean and nine of the observations, we can calculate the
tenth observation. So, in a sense, once we calculate the mean, the value
of one of the ten observations is fixed. This is what we mean by
``losing one degree of freedom''. When we calculate and use the mean,
one of the observations ``loses its freedom''.

For example, take the numbers 1, 3, 5, 7, 9. The mean is 5. The sum of
the squared differences between the observations and the mean is
\((1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2 = 20\). This is the
total sum of squares. The degrees of freedom are \(5-1 = 4\).

The total degrees of freedom are the total number of observations minus
one. That is, the total sum of squares is associated with \(n-1\)
degrees of freedom.

Another perspective in which to think about the total sum of squares and
total degrees of freedom is to consider the intercept only model. The
intercept only model is a model that only includes the intercept term.
The equation of this model would be:

\[y_i = \beta_0 + \epsilon_i\] The sum of the square of the residuals
for this model is minimised when the predicted value of the response
variable is the mean of the response variable. That is, the least
squares estimate of \(\beta_0\) is the mean of the response variable:

\[\hat{\beta}_0 = \bar{y}\]

Hence, the predicted value of the response variable is the mean of the
response variable. The equation is:

\[\hat{y}_i = \bar{y} + \epsilon_i\]

The error term is therefore:

\[\epsilon_i = y_i - \bar{y}\] And the total sum of squares is:

\[SST = \sum_{i=1}^n (y_i - \bar{y})^2\]

where \(\hat{y}_i\) is the predicted value of the response variable for
the \(i\)th observation, \(\bar{y}\) is the mean of the response
variable, and \(\epsilon_i\) is the residual for the \(i\)th
observation.

The intercept only model involves estimating only one parameter, so the
total degrees of freedom are the total number of observations minus one
\(n - 1\).

Therefore, the total degrees of freedom are the total number of
observations minus one.

Bottom line: \(SST\) is the residual sum of squares when we fit the
intercept only model. The total degrees of freedom are the total number
of observations minus one.

\hypertarget{model-degrees-of-freedom}{%
\subsubsection*{Model degrees of
freedom}\label{model-degrees-of-freedom}}
\addcontentsline{toc}{subsubsection}{Model degrees of freedom}

The model degrees of freedom are the degrees of freedom associated with
the model sum of squares (\(SSM\)).

In the case of the intercept only model, we estimated one parameter, the
mean of the response variable.

In the case of a categorical variable with \(k\) groups, we need \(k-1\)
parameters (non intercept \(\beta\) parameters), so we lose \(k-1\)
degrees of freedom. Put another way, when we fit a model with a
categorical explanatory variable with \(k\) groups, we estimate \(k-1\)
parameters in addition to the intercept. That is, we estimate the
difference between each group and the reference group.

Each time we estimate a new parameter, we lose a degree of freedom.

\hypertarget{residual-degrees-of-freedom}{%
\subsubsection*{Residual degrees of
freedom}\label{residual-degrees-of-freedom}}
\addcontentsline{toc}{subsubsection}{Residual degrees of freedom}

The residual degrees of freedom are the total degrees of freedom
(\(n-1\)) minus the model degrees of freedom (\(k-1\)).

Therefore, the residual degrees of freedom are the degrees of freedom
remaining after we estimate the intercept and the other \(\beta\)
parameters. There is one intercept and \(k-1\) other \(\beta\)
parameters, so the residual degrees of freedom are
\(n-1- ( k-1) = n - k\).

\hypertarget{calculating-the-mean-square-and-f-statistic}{%
\subsection*{\texorpdfstring{Calculating the mean square and \(F\)
statistic}{Calculating the mean square and F statistic}}\label{calculating-the-mean-square-and-f-statistic}}
\addcontentsline{toc}{subsection}{Calculating the mean square and \(F\)
statistic}

From these sums of squares and degrees of freedom we can calculate the
mean squares and \(F\)-statistic:

\[MS_{model} = \frac{SS_{\text{between}}}{k-1} = \frac{SSM}{k-1}\]

\[MS_{residual} = \frac{SS_{\text{within}}}{n-k} = \frac{SSE}{n-k}\]

\[F = \frac{MS_{model}}{MS_{residual}}\]

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-note-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-note-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-note-color}{\faInfo}\hspace{0.5em}{Note}, toprule=.15mm, breakable]

\textbf{Why divide by the degrees of freedom?} The more observations we
have, the greater will be the total sum of squares. The more
observations we have, the greater will be the residual sum of squares.
So it is not very informative to compare totals. Rather, we need to
compare the mean of the sums of squares. Except we don't calculate the
mean by dividing by the number of observations. Rather we divide by the
degrees of freedom. The total mean square is an estimate of the variance
of the response variable. And the residual mean square is an estimate of
the variance of the residuals.

\end{tcolorbox}

\hypertarget{sst-ssm-sse-and-degrees-of-freedom}{%
\subsection*{\texorpdfstring{\(SST\), \(SSM\), \(SSE\), and degrees of
freedom}{SST, SSM, SSE, and degrees of freedom}}\label{sst-ssm-sse-and-degrees-of-freedom}}
\addcontentsline{toc}{subsection}{\(SST\), \(SSM\), \(SSE\), and degrees
of freedom}

Just a reminder and a summary of some of the material above:

\begin{itemize}
\tightlist
\item
  \(SST\): degrees of freedom = \(n-1\)
\item
  \(SSM\): degrees of freedom = \(k-1\)
\item
  \(SSE\): degrees of freedom = \(n-k\)
\end{itemize}

The sum of squares add up:

\[SST = SSM + SSE\]

and the degrees of freedom add up

\[(n-1) = (k-1) + (n - k)\]

\hypertarget{source-of-variance-table}{%
\subsection*{Source of variance table}\label{source-of-variance-table}}
\addcontentsline{toc}{subsection}{Source of variance table}

Now we have nearly everything we need. We often express all of this (and
a few more quantities) in a convenient table called the \textbf{sources
of variance table} (or ANOVA table).

The \textbf{sources of variance table} is a table that conveniently and
clearly gives all of the quantities mentioned above. It breaks down the
total sum of squares into the sum of squares explained by the model and
the sum of squares due to error. The source of variance table is used to
calculate the \(F\)-statistic.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.0702}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1404}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1754}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3070}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.3070}}@{}}
\caption{Sources of variance table}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F-statistic
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Source
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sum of squares
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Degrees of freedom
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Mean square
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
F-statistic
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Model & \(SSM\) & \(k-1\) & \(MSE_{model} = SSM / k-1\) &
\(\frac{MSE_{model}}{MSE_{error}}\) \\
Error & \(SSE\) & \(n - 1 - (k-1)\) &
\(MSE_{error} = SSE / (n - 1 - (k-1))\) & \\
Total & \(SST\) & \(n - 1\) & & \\
\end{longtable}

\hypertarget{back-to-the-f-test}{%
\subsection*{\texorpdfstring{Back to the
\(F\)-test}{Back to the F-test}}\label{back-to-the-f-test}}
\addcontentsline{toc}{subsection}{Back to the \(F\)-test}

OK, so we have calculated the \(F\) statistic. But how do we use it to
test our hypothesis?

We can use the \(F\) statistic to calculate a \(p\)-value, which tells
us how likely our data is under the null hypothesis.

Some key points:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(F\)-Distribution: The test statistic of the \(F\)-test (that is, the
  \(F\)-statistic) follows the \(F\)-distribution under the null
  hypothesis. This distribution arises when comparing the ratio of two
  independent sample variances (or mean squares).
\item
  Ronald Fisher's Contribution: Fisher introduced the \(F\)-distribution
  in the early 20th century as a way to test hypotheses about the
  equality of variances and to analyze variance in regression and
  experimental designs. The ``\(F\)'' in \(F\)-distribution honours him.
\item
  Variance Ratio: The test statistic for the \(F\)-test is the ratio of
  two variances (termed mean squares in this case), making the
  \(F\)-distribution the natural choice for modeling this ratio when the
  null hypothesis is true.
\end{enumerate}

The \(F\)-test is widely used, including when comparing variances,
assessing the significance of multiple regression models (see later
chapter), conducting ANOVA to test for differences among group means,
and for comparing different models.

Recall that ``The \(F\)-statistic is calculated as the ratio of the mean
square error of the model to the mean square error of the residuals.''
And that a large \(F\)-statistic is evidence against the null hypothesis
that the slopes of the explanatory variables are zero. And that a small
\(F\)-statistic is evidence to not reject the null hypothesis that the
slopes of the explanatory variables are zero.

But how big does the F-statistic need to be in order to confidently
reject the null hypothesis?

The null hypothesis that the explained variance of the model is no
greater than would be expected by chance. Here, ``by chance'' means that
the slopes of the explanatory variables are zero.

\[H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0\]

The alternative hypothesis is that the explained variance of the model
is greater than would be expected by chance. This would occur if the
slopes of some or all of the explanatory variables are not zero.

\[H_1: \beta_1 \neq 0 \text{ or } \beta_2 \neq 0 \text{ or } \ldots \text{ or } \beta_p \neq 0\]

To test this hypothesis we are going to, as usual, calculate a
\(p\)-value. The \(p\)-value is the probability of observing a test
statistic as or more extreme as the one we observed, assuming the null
hypothesis is true. To do this, we need to know the distribution of the
test statistic under the null hypothesis. The distribution of the test
statistic under the null hypothesis is known as the \(F\)-distribution.

The \(F\)-distribution has two degrees of freedom values associated with
it: the degrees of freedom of the model and the degrees of freedom of
the residuals. The degrees of freedom of the model are the number of
parameters estimated by the model corresponding to the null hypothesis.
The degrees of freedom of the residuals are the total degrees of freedom
minus the degrees of freedom of the model.

Here is the \(F\)-distribution with 2 and 99 degrees of freedom:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-13-1.pdf}

The F-distribution is skewed to the right and has a long tail. The area
to the right of 3.89 is shaded in red. This area represents the
probability of observing an F-statistic as or more extreme as 3.89,
assuming the null hypothesis is true. This probability is the
\(p\)-value of the hypothesis test.

The \(F\)-statistic and \(F\)-test is briefly recaptured in 3.1.f) of
the Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact
that

\[\frac{MSE_{model}}{MSE_{residual}} =  \frac{SSM/p}{SSE/(n-1-p)} \sim F_{p,n-1-p}\]

follows an \(F\)-distribution with \(p\) and \((n-1-p)\) degrees of
freedom, where \(p\) are the number of continuous variables, \(n\) the
number of data points.

\begin{itemize}
\tightlist
\item
  \(SSE=\sum_{i=1} ^n(y_i-\hat{y}_i)^2\) is the residual sum of squares
\item
  \(SSM = SST - SSE\) is the sum of squares of the model
\item
  \(SST=\sum_{i=1}^n(y_i-\overline{y})^2\) is the total sum of squares
\item
  \(n\) is the number of data points
\item
  \(p\) is the number of explanatory variables in the regression model
\end{itemize}

Well, that is ANOVA conceptually. But how does it actually look like in
R?

\hypertarget{doing-anova-in-r}{%
\section*{Doing ANOVA in R}\label{doing-anova-in-r}}
\addcontentsline{toc}{section}{Doing ANOVA in R}

\markright{Doing ANOVA in R}

Let's go back again the question of how diet effects blood pressure.
Here is the data:

\begin{verbatim}
# A tibble: 6 x 3
     bp diet          person_ID
  <dbl> <chr>         <chr>    
1   120 meat heavy    person_1 
2    89 vegan         person_2 
3    86 vegetarian    person_3 
4   116 meat heavy    person_4 
5   115 Mediterranean person_5 
6   134 meat heavy    person_6 
\end{verbatim}

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-15-1.pdf}

And here is how we fit a linear model to this data:

\textbf{IMPORTANT}: Since ANOVA is a linear model, it is important to
check the assumptions of linear models before interpreting the results.
These are some of the same assumptions we checked for simple linear
regression, including: independence of errors, normality of residuals,
and homoscedasticity (constant variance of residuals).

As with linear regression, we check the assumptions are not too badly
broken by looking at diagnostic plots:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-17-1.pdf}

Nothing looks too bad.

** Think-pair-share**: Which of the four plots above would you use to
check each of the three assumptions listed above?

** Think-pair-share**: Before we look at the ANOVA table, lets figure
out the total degrees of freedom, the degrees of freedom for the model,
and the degrees of freedom for the residuals. Have a think-pair-share
about each of these. Write you ideas down. Chat with you neighbour. Then
share with the class.

Now we can look at the ANOVA table:

\begin{verbatim}
Analysis of Variance Table

Response: bp
          Df Sum Sq Mean Sq F value    Pr(>F)    
diet       3 5274.2 1758.08  20.728 1.214e-08 ***
Residuals 46 3901.5   84.82                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The ANOVA table shows the sum of squares, degrees of freedom, mean
square, F value, and p-value for the model and residuals. As we know,
the \(F\) value (\(F\) statistics) is calculated as the mean square of
the model divided by the mean square of the residuals. The p-value is
calculated based on the F-distribution with the appropriate degrees of
freedom.

A suitable sentence to report our findings would be: ``Diet has a
significant effect on blood pressure
(\(F(2, 27) = 20.7, p < 0.0001\))''. This means that the probability of
observing such a large \(F\) value under the null hypothesis is less
than 0.01\%.

\emph{Think-pair-share}: You know that the \(R^2\) value is a measure of
how much variance in the response variable is explained by the model.
How would you calculate the \(R^2\) value from the ANOVA table above?

\hypertarget{difference-between-pairs-of-groups}{%
\section*{Difference between pairs of
groups}\label{difference-between-pairs-of-groups}}
\addcontentsline{toc}{section}{Difference between pairs of groups}

\markright{Difference between pairs of groups}

Recall that the \(F\) test is a global test. It tests the null
hypothesis that all group means are equal. It does not tell us which
groups are different from each other. It just tells us that at least one
group mean is different. Sometimes researchers are interested in more
specific questions such as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  finding the actual group(s) that deviate(s) from the others.
\item
  in estimates of the pairwise differences.
\end{enumerate}

The summary table in R provides some of these comparison, specifically
it contains the estimates for \(\beta_1\), \(\beta_2\), \(\beta_3\)
(while the reference was set to \(\beta_0 = 0\)).

For example, here is the summary table for our diet data:

\begin{verbatim}

Call:
lm(formula = bp ~ diet, data = bp_data_diet)

Residuals:
     Min       1Q   Median       3Q      Max 
-17.9375  -5.9174  -0.4286   5.2969  22.3750 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)        122.625      2.302  53.260  < 2e-16 ***
dietMediterranean  -12.688      3.256  -3.897 0.000314 ***
dietvegan          -26.768      4.173  -6.414 6.92e-08 ***
dietvegetarian     -23.625      3.607  -6.549 4.33e-08 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.21 on 46 degrees of freedom
Multiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 
F-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08
\end{verbatim}

In this table we have the intercept (\(\beta_0\)) and the three
\(\beta\) values for the diet groups: ``dietMeat'', ``dietVegetarian'',
and ``dietVegan''. The Estimate column shows the estimated coefficients
for each group. The intercept (\(\beta_0\)) represents the mean blood
pressure for the reference group (in this case, the ``meat'' diet
group). The other three coefficients represent \emph{the difference} in
mean blood pressure between each diet group and the reference group.

All well and good up to a point. But there are two issues with using the
results from this table:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The greater the number of individual tests, the more likely one will
  be significant just by chance. This is called the problem of multiple
  comparisons. Many test can result in a type-I error: rejecting the
  null hypothesis when it is actually true. The more tests one does, the
  more likely one is to make a type-I error.
\end{enumerate}

** Think-pair-share**: Imagine that when our threshold p-value each
individual test is 0.05 (5\%) so that if it is less than 0.05 we call it
``significant'' and if it is greater than 0.05 we call it ``not
significant'' (this is the standard practice in many fields). When we
make 20 hypothesis tests, how many would we expect to be ``significant''
just by chance (i.e., when we assume that all null hypotheses is true.)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The summary table does not provide all the possible pairwise
  comparisons. It does not, for example, provide the comparison between
  the ``vegan'' and the ``vegetarian'' group.
\end{enumerate}

Several methods to circumvent the problem of too many ``significant''
test results (type-I error) have been proposed. The most prominent ones
are:

\begin{itemize}
\tightlist
\item
  Bonferroni correction
\item
  Tukey \textbf{H}onest \textbf{S}ignificant \textbf{D}ifferences (HSD)
  approach
\item
  Fisher \textbf{L}east \textbf{S}ignificant \textbf{D}ifferences (LSD)
  approach
\end{itemize}

The second two when implemented in R also provide all possible pairwise
comparisons.

\hypertarget{bonferroni-correction}{%
\subsection*{Bonferroni correction}\label{bonferroni-correction}}
\addcontentsline{toc}{subsection}{Bonferroni correction}

\textbf{Idea:} If a total of \(m\) tests are carried out, simply divide
the type-I error level \(\alpha_0\) (often 5\%) such that

\[\alpha = \alpha_0 / m \ .\]

But this still leaves the problem of how to efficiently get all of the
possible pairwise comparisons. We can do this using the
\texttt{pairwise.t.test} function in R:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairwise.t.test}\NormalTok{(bp\_data\_diet}\SpecialCharTok{$}\NormalTok{bp,}
\NormalTok{                bp\_data\_diet}\SpecialCharTok{$}\NormalTok{diet,}
                \AttributeTok{p.adjust.method =} \StringTok{"bonferroni"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pairwise comparisons using t tests with pooled SD 

data:  bp_data_diet$bp and bp_data_diet$diet 

              meat heavy Mediterranean vegan 
Mediterranean 0.0019     -             -     
vegan         4.2e-07    0.0091        -     
vegetarian    2.6e-07    0.0239        1.0000

P value adjustment method: bonferroni 
\end{verbatim}

Here we can see that all pairwise comparisons have a p-value less than
0.05, except for the comparison of vegan versus vegetarian, which has a
p-value that rounds to 1.0000.

We also see in the output the note that ``P value adjustment method:
bonferroni'', indicating that the Bonferroni correction has been applied
to the p-values.

\hypertarget{tukey-hsd-approach}{%
\subsection*{Tukey HSD approach}\label{tukey-hsd-approach}}
\addcontentsline{toc}{subsection}{Tukey HSD approach}

\textbf{Idea:} Take into account the distribution of \emph{ranges}
(max-min) and design a new test.

In R we can use the \texttt{multcomp} package to do Tukey HSD tests:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bp\_data\_diet }\OtherTok{\textless{}{-}}\NormalTok{ bp\_data\_diet }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diet =} \FunctionTok{as.factor}\NormalTok{(diet))}
\NormalTok{fit }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\FunctionTok{library}\NormalTok{(multcomp)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Loading required package: mvtnorm
\end{verbatim}

\begin{verbatim}
Loading required package: survival
\end{verbatim}

\begin{verbatim}
Loading required package: TH.data
\end{verbatim}

\begin{verbatim}
Loading required package: MASS
\end{verbatim}

\begin{verbatim}

Attaching package: 'MASS'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:patchwork':

    area
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:dplyr':

    select
\end{verbatim}

\begin{verbatim}

Attaching package: 'TH.data'
\end{verbatim}

\begin{verbatim}
The following object is masked from 'package:MASS':

    geyser
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tukey\_test }\OtherTok{\textless{}{-}} \FunctionTok{glht}\NormalTok{(fit, }\AttributeTok{linfct =} \FunctionTok{mcp}\NormalTok{(}\AttributeTok{diet =} \StringTok{"Tukey"}\NormalTok{))}
\FunctionTok{summary}\NormalTok{(tukey\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lm(formula = bp ~ diet, data = bp_data_diet)

Linear Hypotheses:
                                Estimate Std. Error t value Pr(>|t|)    
Mediterranean - meat heavy == 0  -12.688      3.256  -3.897  0.00168 ** 
vegan - meat heavy == 0          -26.768      4.173  -6.414  < 0.001 ***
vegetarian - meat heavy == 0     -23.625      3.607  -6.549  < 0.001 ***
vegan - Mediterranean == 0       -14.080      4.173  -3.374  0.00759 ** 
vegetarian - Mediterranean == 0  -10.938      3.607  -3.032  0.01951 *  
vegetarian - vegan == 0            3.143      4.453   0.706  0.89305    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Adjusted p values reported -- single-step method)
\end{verbatim}

We get all the pairwise comparisons, along with their estimates,
standard errors, t-values, and p-values. We also get a note
\texttt{Adjusted\ p\ values\ reported\ -\/-\ single-step\ method},
indicating that the Tukey HSD adjustment has been applied to the
p-values.

Again, all pairwise comparisons have a p-value less than 0.05, except
for the comparison of vegan versus vegetarian, which has a p-value of
0.89305.

\hypertarget{fishers-lsd-approach}{%
\subsection*{Fisher's LSD approach}\label{fishers-lsd-approach}}
\addcontentsline{toc}{subsection}{Fisher's LSD approach}

\textbf{Idea:} Adjust the idea of a two-sample test, but use a larger
variance (namely the pooled variance of all groups).

\hypertarget{other-contrasts}{%
\subsection*{Other contrasts}\label{other-contrasts}}
\addcontentsline{toc}{subsection}{Other contrasts}

A contrast is a specific comparison between groups. So far we have only
considered pairwise contrasts (i.e., comparing two groups at a time).
But we can also design more complex contrasts. For example: are diets
that contain meat different from diets that do not contain meat?

\begin{verbatim}
# A tibble: 6 x 4
     bp diet          person_ID meat_or_no_meat
  <dbl> <fct>         <chr>     <chr>          
1   120 meat heavy    person_1  no meat        
2    89 vegan         person_2  no meat        
3    86 vegetarian    person_3  no meat        
4   116 meat heavy    person_4  no meat        
5   115 Mediterranean person_5  meat           
6   134 meat heavy    person_6  no meat        
\end{verbatim}

Here we defined a new explanatory variable that groups the meat heavy
and Mediterranean diet together into a single ``meat'' group and
vegetarian and vegan into a single ``no meat'' group. We then fit a
model with this explanatory variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit\_mnm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ meat\_or\_no\_meat, }\AttributeTok{data =}\NormalTok{ bp\_data\_diet)}
\end{Highlighting}
\end{Shaded}

(We should not look at model diagnostics here, before using the model.
But let us continue as if the assumptions are sufficiently met.)

We now do something a bit more complicated: we compare the variance
explained by the model with four diets to the model with two diets. This
is done by comparing the two models using an \(F\)-test. We are testing
the null hypothesis that the two models are equally good at explaining
the data, in which case the two diet model will explain as much variance
as the four diet model.

Let's look at the ANOVA table of the model comparison:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{anova}\NormalTok{(fit, fit\_mnm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Analysis of Variance Table

Model 1: bp ~ diet
Model 2: bp ~ meat_or_no_meat
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     46 3901.5                                  
2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

We see the residual sum of squares of the model with meat or no meat is
over 9'000, while that of the four diet model is less than 4'000. That
is, the four diet model explains much more variance in the data than the
two diet model. The \(F\)-test is highly significant, so we reject the
null hypothesis that the two models are equally good at explaining the
data. And we conclude that its not just whether people eat meat or not,
but rather what kind of diet they eat that affects their blood pressure.

Ideally we do not make a lot of contrasts after we have collected and
looked at our data. Rather, we would specify the contrasts we are
interested in before we collect the data. This is called a priori
contrasts. But sometimes we do exploratory data analysis and then we can
make post hoc contrasts. In this case we should be careful to adjust for
multiple comparisons.

\hypertarget{choosing-the-reference-category}{%
\subsection*{Choosing the reference
category}\label{choosing-the-reference-category}}
\addcontentsline{toc}{subsection}{Choosing the reference category}

\textbf{Question}: Why was the ``heavy meat'' diet chosen as the
reference (intercept) category?

\textbf{Answer}: Because R orders the categories alphabetically and
takes the first level alphabetically as reference category.

Sometimes we may want to override this, for example if we have a
treatment that is experimentally the control, then it will usually be
useful to set this as the reference / intercept level.

In R we can set the reference level using the \texttt{relevel} function:

And now make the model and look at the estimated coefficients:

\begin{verbatim}

Call:
lm(formula = bp ~ diet, data = bp_data_diet)

Residuals:
     Min       1Q   Median       3Q      Max 
-17.9375  -5.9174  -0.4286   5.2969  22.3750 

Coefficients:
                  Estimate Std. Error t value Pr(>|t|)    
(Intercept)         95.857      3.481  27.538  < 2e-16 ***
dietmeat heavy      26.768      4.173   6.414 6.92e-08 ***
dietMediterranean   14.080      4.173   3.374  0.00151 ** 
dietvegetarian       3.143      4.453   0.706  0.48386    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 9.21 on 46 degrees of freedom
Multiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 
F-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08
\end{verbatim}

Now we see the estimated coefficients for all diets except the vegan
diet. The intercept is the mean individuals with vegan diet.

\hypertarget{communicating-the-results-of-anova}{%
\section*{Communicating the results of
ANOVA}\label{communicating-the-results-of-anova}}
\addcontentsline{toc}{section}{Communicating the results of ANOVA}

\markright{Communicating the results of ANOVA}

When communicating the results of an ANOVA, we usually report the
\(F\)-statistic, the degrees of freedom of the numerator and
denominator, and the p-value. For example, we could say:

\begin{quote}
Blood pressure differed significantly between groups, with the mean of a
meat heavy diet being 123 mmHg, while the mean blood pressure of the
vegan group was 27 mmHg lower (One-way ANOVA, \(F(3, 46) = 20.7\),
\(p < 0.0001\).
\end{quote}

And we would make a nice graph, in this case showing each individual
observation since there are not too many to cause overplotting. We can
also add the estimated means of each group if we like:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure by Diet"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood Pressure (mmHg)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-27-1.pdf}

}

\end{figure}

Some people like to see error bars as well, for example showing the 95\%
confidence intervals of the means:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(bp\_data\_diet, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ diet, }\AttributeTok{y =}\NormalTok{ bp)) }\SpecialCharTok{+}
  \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.1}\NormalTok{, }\AttributeTok{height =} \DecValTok{0}\NormalTok{, }\AttributeTok{col =} \StringTok{"grey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun =}\NormalTok{ mean, }\AttributeTok{geom =} \StringTok{"point"}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{, }\AttributeTok{size =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{stat\_summary}\NormalTok{(}\AttributeTok{fun.data =}\NormalTok{ mean\_cl\_normal, }\AttributeTok{geom =} \StringTok{"errorbar"}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.2}\NormalTok{, }\AttributeTok{color =} \StringTok{"black"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Blood Pressure by Diet}\SpecialCharTok{\textbackslash{}n}\StringTok{Black points and error bars show mean Â± 95\% CI"}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"Diet"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{"Blood Pressure (mmHg)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-28-1.pdf}

}

\end{figure}

There are many many plotting styles and preferences. The important thing
is to clearly communicate the results, and to not mislead the reader. I
find that plotting the individual data points is often a good idea,
especially when the sample size is not too large.

\hypertarget{summary-of-what-you-have-learned}{%
\section*{Summary of what you have
learned}\label{summary-of-what-you-have-learned}}
\addcontentsline{toc}{section}{Summary of what you have learned}

\markright{Summary of what you have learned}

Please referring to the learning objectives document and the learning
objectives for this chapter.

\hypertarget{end}{%
\section*{end}\label{end}}
\addcontentsline{toc}{section}{end}

\markright{end}

\hypertarget{end-1}{%
\section*{end}\label{end-1}}
\addcontentsline{toc}{section}{end}

\markright{end}

\hypertarget{two-way-anova-zweiweg-varianzanalyse}{%
\section*{Two-way ANOVA
(Zweiweg-Varianzanalyse)}\label{two-way-anova-zweiweg-varianzanalyse}}
\addcontentsline{toc}{section}{Two-way ANOVA (Zweiweg-Varianzanalyse)}

\markright{Two-way ANOVA (Zweiweg-Varianzanalyse)}

Two-way ANOVA is used to analyse a specific type of study design. When
we have a study with two categorical treatments and all possible
combinations of them, we can use a two-way ANOVA.

For example, take the question of how diet and exercise affect blood
pressure. Let's say we can have three levels of diet: meat heavy,
Mediterranean, and vegetarian. And that we have two levels of exercise:
low and high. And that we have all possible combinations of these two
treatments: i.e., we have a total of \(3 \times 2 = 6\)
\textbf{treatment combinations}.

We can also represent this study design in a table:

Exercise (G)

Diet (B)

Low (1)

High (2)

Meat heavy (1)

Mediterranean (2)

Vegetarian (3)

The six empty cells in the table represent the six treatment
combinations.

This type of study, with all possible combinations, is known as a
\emph{factorial design}. The two treatments are called factors, and the
levels of the factors are called factor levels. A \emph{fully factorial
design} is one where all possible combinations of the factor levels are
present.

Let's look at example data:

\begin{verbatim}
# A tibble: 6 x 5
  diet          exercise  reps    bp  error
  <chr>         <fct>    <int> <dbl>  <dbl>
1 Mediterranean high         1  110.  4.59 
2 Mediterranean low          1  119.  6.79 
3 meat heavy    high         1  102. -3.13 
4 meat heavy    low          1  128.  7.56 
5 vegetarian    high         1  104. -0.823
6 vegetarian    low          1  111.  1.99 
\end{verbatim}

We can use the \texttt{xtabs} function to create a table of the data, by
cross-tabulating the two treatments diet and exercise:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{xtabs}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{diet }\SpecialCharTok{+}\NormalTok{ exercise, }\AttributeTok{data =}\NormalTok{ bp\_data\_2cat)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
               exercise
diet            low high
  meat heavy     10   10
  Mediterranean  10   10
  vegetarian     10   10
\end{verbatim}

This tells us there are 10 replicates in each of the six treatment
combinations.

And a visualisation of the data:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-31-1.pdf}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#twoway-plot)}, toprule=.15mm, breakable]

What do you conclude from this plot?

\end{tcolorbox}

\hypertarget{the-model-for-2-way-anova}{%
\subsection*{The model for 2-way
ANOVA}\label{the-model-for-2-way-anova}}
\addcontentsline{toc}{subsection}{The model for 2-way ANOVA}

Assume we have a factorial design with two treatments (factors), factor
\(B\) and factor \(G\).

And that we can label the levels of factor \(B\) as \(i=1,2...\) and
factor \(G\) as \(j=1,2...\).

Then we can denote a particular treatment combination as \(B_iG_j\).

And let us set the one of the treatment combinations as the intercept of
the model, and the let the intercept be equal to the mean of the
observations in the treatment combination \(B1G1\).

\[intercept = \frac{1}{n_{11}}\sum_{k=1}^{n_{11}} y_{1,1,k}\]

where:

\begin{itemize}
\tightlist
\item
  \(y_{1,1,k}\) is the \(k\)th observation in the treatment combination
  \(B1G1\)
\item
  \(n_{11}\) is the number of observations in the treatment combination
  \(B1G1\).
\end{itemize}

And we will let all of the other treatment combinations be represented
by the \textbf{effects} \(\beta_i\) and \(\gamma_j\).

The resulting linear model is:

\[y_{ijk} = intercept + \beta_i + \gamma_j + (\beta\gamma)_{ij} + \epsilon_{ijk} \quad \text{with} \quad \epsilon_{ijk} \sim N(0,\sigma^2)\]

where

\begin{itemize}
\tightlist
\item
  \(y_{ijk}\) is the \(k\)th observation in the treatment combination of
  \(i\) and \(j\).
\item
  \((\beta\gamma)_{ij}\) is the interaction effect between the \(i\)th
  level of factor \(\beta\) and the \(j\)th level of factor \(\gamma\).
\end{itemize}

In this model, we set \(\beta_1=\gamma_1=0\) and
\((\beta\gamma)_{11}=0\) because they are already included in the
intercept.

\hypertarget{using-r-for-2-way-anova}{%
\subsection*{Using R for 2-way ANOVA}\label{using-r-for-2-way-anova}}
\addcontentsline{toc}{subsection}{Using R for 2-way ANOVA}

In R, a two-way ANOVA is as simple as one-way ANOVA, just add another
variable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod1 }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(bp }\SpecialCharTok{\textasciitilde{}}\NormalTok{ diet }\SpecialCharTok{*}\NormalTok{ exercise, }\AttributeTok{data =}\NormalTok{ bp\_data\_2cat)}
\end{Highlighting}
\end{Shaded}

Note that, as we saw in the chapter about interactions, we include the
main effects of diet and exercise and the interaction term with the
short hand \texttt{diet\ *\ exercise}.

Of course we next check the model diagnostics:

\includegraphics{6.1-anova_files/figure-pdf/unnamed-chunk-33-1.pdf}

No clear patterns: all is good.

\hypertarget{hypothesis-testing}{%
\subsection*{Hypothesis testing}\label{hypothesis-testing}}
\addcontentsline{toc}{subsection}{Hypothesis testing}

As is implied by the name ``Analysis of variance'' we analyse variances,
here mean squares, to test hypotheses. And as before we use an
\(F\)-test to do this. Remember that the \(F\)-test is a ratio of two
mean squares (where mean squares are a kind of variance).

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#full-degrees)}, toprule=.15mm, breakable]

How many degrees of freedom for error will there be when we fit this
model with both main effects and the interaction term? Hint: remember
that the degrees of freedom for error is the number of observations
minus the number of parameters estimated.

\end{tcolorbox}

We can have have a null hypothesis of no effect for each of the two main
effects and for the interaction. So we can do an \(F\)-test for each of
these null hypotheses.

Here is the ANOVA table:

\begin{verbatim}
Analysis of Variance Table

Response: bp
              Df  Sum Sq Mean Sq F value    Pr(>F)    
diet           2  390.74  195.37  9.9672 0.0002069 ***
exercise       1 1297.52 1297.52 66.1966 5.952e-11 ***
diet:exercise  2  340.67  170.33  8.6901 0.0005346 ***
Residuals     54 1058.46   19.60                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

\textbf{Interpretation}: All three of the null hypotheses are rejected.
Importantly, we see that the interaction term is significant, which
means that the effect of one treatment is different depending on the
level of the other treatment. This means that even though the main
effects are significant, we cannot interpret them without considering
the interaction term. I.e., we cannot say anything general about the
effect of diet or exercise alone on blood pressure. We have to qualify
any statement about the effect of diet or exercise with ``depending on
the level of the other treatment''. Or state something like ``the
exercise reduces blood pressure greatly for people with a meat heavy
diet, but reduces blood pressure only slightly for people with a
vegetarian diet or Mediterranean diet''.

\hypertarget{interpreting-coefficients}{%
\subsection*{Interpreting
coefficients}\label{interpreting-coefficients}}
\addcontentsline{toc}{subsection}{Interpreting coefficients}

We can look at the estimated coefficients to see the size of the
effects, but be aware that it contains only a subset of the possible
effects; it would contain different values if a different treatment
combination were set to be the intercept of the model. Also be aware
that often we are mostly interested in the \(F\)-test and their
hypotheses test, and are less interest in the coefficients and their
\(t\)-tests (unlike in a regression model where we are often interested
in the coefficients and their \(t\)-tests).

Finally, be aware that it needs a bit of work to interpret the
coefficients, because they are relative to the intercept. Let's try to
figure out what that means. First, back to the table of the experimental
design. This time we will put in the cells an expression for the mean of
that treatment combination:

Exercise (G)

Diet (B)

Low (1)

High (2)

Meat heavy (1)

\(B_1G_1\)

\(B_1G_2\)

Mediterranean (2)

\(B_2G_1\)

\(B_2G_2\)

Vegetarian (3)

\(B_2G_1\)

\(B_3G_2\)

So, for example, the mean of the treatment combination ``Meat heavy,
Low'' is \(B_1G_1\). And the mean of the treatment combination
``Mediterranean, High'' is \(B_2G_2\).

However, the coefficients in the summary table given by R are not like
this. They are coefficients relative to an intercept / reference
treatment combination. The reference treatment combination chosen by R
is the first level of the first factor and the first level of the second
factor. In this case, that is ``\textbf{Meat heavy, Low}'' --
\(B_1G_1\).

All of the other coefficients are about differences from this reference
treatment combination.

So, for example, the coefficient for ``High'' in the ``Exercise'' factor
(appearing as \texttt{exercisehigh} in the summary table) is the
difference in mean blood pressure between the treatment combination
``Meat heavy, High'' (\(B_1G_2\)) and the treatment combination ``Meat
heavy, Low''. Put another way, \(B_1G_2 = B_1G_1 + \gamma_2\) where
\(\gamma_2\) is the coefficient for ``High'' in the ``Exercise'' factor.

And the coefficient for ``Mediterranean'' in the ``Diet'' factor
(appearing as \texttt{dietMediterranean} in the summary table) is the
difference in mean blood pressure between the treatment combination
``Mediterranean, Low'' and the treatment combination ``Meat heavy,
Low''. Put another way, \(B_2G_1 = B_1G_1 + \beta_2\) where \(\beta_2\)
is the coefficient for ``Mediterranean'' in the ``Diet'' factor.

\textbf{Let us for a moment assume that the effects of diet and exercise
are additive.} If this is the case, then the mean for \(B_2G_2\) =
\(B_1G_1 + \beta_2 + \gamma_2\). That is, the mean for ``Mediterranean,
High'' is the mean for ``Meat heavy, Low'' plus the effect of
``Mediterranean'' plus the effect of ``High''.

However, if the effects are not additive, then the mean for \(B_2G_2\)
is not \(B_1G_1 + \beta_2 + \gamma_2\). Rather, it is
\(B_2G_2 = B_1G_1 + \beta_2 + \gamma_2 + (\beta\gamma)_{22}\). That is,
the mean for ``Mediterranean, High'' is the mean for ``Meat heavy, Low''
\(B_1G_1\) plus the effect of ``Mediterranean'' \(\beta_2\) plus the
effect of ``High'' \(\gamma_2\) plus the non-additive effect between
``Mediterranean'' and ``High'' \((\beta\gamma)_{22}\).

Non-additivity implies an interaction, therefore the non-additive effect
is the interaction effect. In the summary table these interaction
effects are those that contain a colon (:), e.g.,
\texttt{dietMediterranean:exercisehigh}.

Here's a graphical representation of how the coefficients in the summary
table relate to the means of the treatment combinations:

\begin{figure}

{\centering \includegraphics[width=3.125in,height=\textheight]{assets/twowayanova1.png}

}

\caption{Understanding coefficients}

\end{figure}

\begin{tcolorbox}[enhanced jigsaw, arc=.35mm, bottomtitle=1mm, opacityback=0, toptitle=1mm, rightrule=.15mm, left=2mm, colframe=quarto-callout-tip-color-frame, colback=white, leftrule=.75mm, colbacktitle=quarto-callout-tip-color!10!white, coltitle=black, titlerule=0mm, opacitybacktitle=0.6, bottomrule=.15mm, title=\textcolor{quarto-callout-tip-color}{\faLightbulb}\hspace{0.5em}{Think, Pair, Share (\#veghigh-estimate)}, toprule=.15mm, breakable]

From the values in the coefficients table, calculate the estimated mean
of the treatment combination ``vegetarian, High''.

\end{tcolorbox}

\hypertarget{summing-up}{%
\section*{Summing up}\label{summing-up}}
\addcontentsline{toc}{section}{Summing up}

\markright{Summing up}

\begin{itemize}
\tightlist
\item
  ANOVA is just another linear model.
\item
  It is used when we have categorical explanatory variables.
\item
  We use \(F\)-tests to test the null hypothesis of no difference among
  the means of the groups (categories).
\item
  We can use contrasts and post-hoc tests to test specific hypotheses
  about the means of the groups.
\item
  Two-way ANOVA is used when we have two categorical explanatory
  variables and can be used to test for interactions between them.
\end{itemize}

\hypertarget{additional-reading}{%
\section*{Additional reading}\label{additional-reading}}
\addcontentsline{toc}{section}{Additional reading}

\markright{Additional reading}

Please feel free to look at the follow resources for a slightly
different perspective and some more information on ANOVA:

\begin{itemize}
\tightlist
\item
  Chapter 12 from Stahel book \emph{Statistische Datenenalyse}
\item
  \emph{Getting Started with R} chapters 5.6 and 6.2
\end{itemize}


\backmatter

\end{document}
