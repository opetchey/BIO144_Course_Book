[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BIO144 Course Book",
    "section": "",
    "text": "Preface\nThis book contains content of the first six Monday lectures of the course BIO144 Data Analysis in Biology at the University of Zurich. Information for the remaining lectures are available on the OLAT course website."
  },
  {
    "objectID": "index.html#making-a-copy-for-yourself",
    "href": "index.html#making-a-copy-for-yourself",
    "title": "BIO144 Course Book",
    "section": "Making a copy for yourself",
    "text": "Making a copy for yourself\nIf you’d like a copy of this book for yourself, there are a couple of ways:\n\nFrom your web browser print a PDF of a chapter of the book. Sometime graphs will be displayed across a page, which is not so nice. Sorry. But it should be ok for your learning and to make notes on.\nYou can get all of the source code for the book from the GitHub repository. However, you may find it a little complicated to do anything useful with it!"
  },
  {
    "objectID": "index.html#how-this-book-was-made",
    "href": "index.html#how-this-book-was-made",
    "title": "BIO144 Course Book",
    "section": "How this book was made",
    "text": "How this book was made\nThe book was written using a type of RMarkdown. It allows a script with a mix of normal text and R code to produce chapters and a book that has a mixture of text, R code, and R output. Rmarkdown is very useful for making reports, books, presentations, and even websites.\nThis book is a Quarto book. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "BIO144 Course Book",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThe content was based on lectures originally written by Dr Stefanie Muff.\nThe content of the book was written with the assistance of Github Copilot, an AI tool that helps write code and text."
  },
  {
    "objectID": "1-intro.html#notation-and-some-definitions",
    "href": "1-intro.html#notation-and-some-definitions",
    "title": "Introduction (L1)",
    "section": "Notation and some definitions",
    "text": "Notation and some definitions\nThroughout the course, we will use the following notation:\n\n\\(x\\) for a variable. Typically this variable contains a set of observations. These observations are said to represent a sample of all the possible observations that could be made of a population.\n\\(x_1, x_2, \\ldots\\) for the values of a variable\n\\(x_i\\) for the \\(i\\)th value of a scalar variable. This is often spoken as “x sub i” or the “i-th value of x”.\n\\(x^{(1)}\\) for variable 1, \\(x^{(2)}\\) for variable 2, etc.\nThe mean of the sample \\(x\\) is \\(\\bar{x}\\). This is usually spoken as “x-bar”.\nThe mean of \\(x\\) is calculated as \\(\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\\).\n\\(n\\) is the number of observations in a sample.\nThe summation symbol \\(\\sum\\) is used to indicate that the values of \\(x\\) are summed over all values of \\(i\\) from 1 to \\(n\\).\nThe standard deviation of the sample is \\(s\\). The standard deviation of the population is \\(\\sigma\\).\nThe variance is \\(s^2\\). The variance of the population is \\(\\sigma^2\\).\nThe variance of the sample is calculated as \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})^2\\).\nThe standard deviation of the sample is calculated as \\(s = \\sqrt{s^2}\\).\n\\(y\\) is usually used to represent a dependent / response variable.\n\\(x\\) is usually used to represent an independent / predictor / explanatory variable.\n\\(\\beta_0\\) is usually used to denote the intercept of a linear model.\n\\(\\beta_1\\), \\(\\beta_2\\), etc. are usually used to denote the coefficients of the independent variables in a linear model.\nEstimates are denoted with a hat, so \\(\\hat{\\beta}_0\\) is the estimate of the intercept of a linear model.\nHence, the estimated value of \\(y_i\\) in a linear regression model is \\(\\hat{y_i} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i^{(1)}\\).\n\\(e_i\\) is the residual for the \\(i\\)th observation in a linear model. The residual is the difference between the observed value of \\(y_i\\) and the predicted value of \\(y_i\\) (\\(\\hat{y_i}\\)).\nOften we assume errors are normally distributed with mean 0 and variance \\(\\sigma^2\\). This is written as \\(e_i \\sim N(0, \\sigma^2)\\).\nSST is the total sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\bar{y})^2\\).\nSSM is the model sum of squares. It is the sum of the squared differences between the predicted values of \\(y\\) and the mean of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (\\hat{y_i} - \\bar{y})^2\\).\nSSE is the error sum of squares. It is the sum of the squared differences between the observed values of \\(y\\) and the predicted values of \\(y\\). It is calculated as \\(\\sum_{i=1}^n (y_i - \\hat{y_i})^2\\).\nThe variance of \\(x\\) can be written as \\(Var(x)\\). The covariance between \\(x\\) and \\(y\\) can be written as \\(Cov(x, y)\\).\nCovariance is calculated as \\(Cov(x, y) = \\frac{1}{n-1} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\\).\n\\(H_0\\) is the null hypothesis.\n\\(\\alpha\\) is the significance level.\n\\(df\\) is the degrees of freedom.\n\\(p\\) is the p-value."
  },
  {
    "objectID": "2-data-wrangling.html",
    "href": "2-data-wrangling.html",
    "title": "Data wrangling (L2)",
    "section": "",
    "text": "Note\n\n\n\nThere is no in-person lecture this week, and so there is no content here. Instead, please do the homework listed on OLAT."
  },
  {
    "objectID": "3-regression.html#introduction",
    "href": "3-regression.html#introduction",
    "title": "Regression (L3&4)",
    "section": "Introduction",
    "text": "Introduction\nLinear regression is a common statistical method that models the relationship between a dependent (response) variable and one or more independent (explanatory) variables. The relationship is modeled with the equation for a straight line (\\(y = a + bx\\)).\nWith linear regression we can answer questions such as:\n\nHow does the dependent (response) variable change with respect to the independent (explanatory) variable?\nWhat amount of variation in the dependent variable can be explained by the independent variable?\nIs there a statistically significant relationship between the dependent variable and the independent variable?\nDoes the linear model fit the data well?\n\nIn this chapter / lesson we will explore what is linear regression and how to use it to answer these questions. We’ll cover the following topics:\n\nWhy use linear regression?\nWhat is the linear regression model?\nFitting the regression model (= finding the intercept and the slope).\nIs linear regression a good enough model to use?\nWhat do we do when things go wrong?\nTransformation of variables/the response.\nIdentifying and handling odd data points (aka outliers).\n\nIn this chapter / lesson we will not discuss the statistical significance of the model. We will cover this topic in the next chapter / lesson.\n\nWhy use linear regression?\n\nIt’s a good starting point because it is a relatively simple model.\nRelationships are sometimes close enough to linear.\nIt’s easy to interpret.\nIt’s easy to use.\nIt’s actually quite flexible (e.g. can be used for non-linear relationships, e.g., a quadratic model is still a linear model!!! See Section 5.).\n\n\n\nAn example - blood pressure and age\nThere are lots of situations in which linear regression can be useful. For example, consider hypertension. Hypertension is a condition in which the blood pressure in the arteries is persistently elevated. Hypertension is a major risk factor for heart disease, stroke, and kidney disease. It is estimated that hypertension affects about 1 billion people worldwide. Hypertension is a complex condition that is influenced by many factors, including age. In fact, it is well known that blood pressure increases with age. But how much does blood pressure increase with age? This is a question that can be answered using linear regression.\nHere is an example of a study that used linear regression to answer this question: https://journals.lww.com/jhypertension/fulltext/2021/06000/association_of_age_and_blood_pressure_among_3_3.15.aspx\nIn this study, the authors used linear regression to model the relationship between age and blood pressure. They found that systolic blood pressure increased by 0.28–0.85 mmHg/year. This is a small increase, but it is statistically significant. This means that the observed relationship between age and blood pressure is unlikely to be due to chance.\nLets look at some simulated example data:\n Download the dataset. \n\n\n\n\n\nWell, that is pretty conclusive. We hardly need statistics. There is a clear positive relationship between age and systolic blood pressure. But how can we quantify this relationship? And in less clear-cut cases what is the strength of evidence for a relationship? This is where linear regression comes in. Linear regression models the relationship between age and systolic blood pressure. With linear regression we can answer the following questions:\n\nWhat is a good mathematical representation of the relationship?\nIs the relationship different from what we would expect if there were no relationship?\nHow well does the mathematical representation match the observed values?\nHow much uncertainty is there in any predictions?\n\nLets try to figure some of these out from the visualisation.\n\n\n\n\n\n\nThink, Pair, Share (#guess-params)\n\n\n\n\nMake a guess of the slope.\nMake a guess of the intercept (hint be careful, lots of people get this wrong).\n\n\n\n\n\nRegression from a mathematical perspective\nGiven an independent/explanatory variable (\\(X\\)) and a dependent/response variable (\\(Y\\)) all points \\((x_i,y_i)\\), \\(i= 1,\\ldots, n\\), on a straight line follow the equation\n\\[y_i = \\beta_0 + \\beta_1 x_i\\ .\\]\n\n\\(\\beta_0\\) is the intercept - the value of \\(Y\\) when \\(x_i = 0\\)\n\\(\\beta_1\\) the slope of the line, also known as the regression coefficient of \\(X\\).\nIf \\(\\beta_0=0\\) the line goes through the origin \\((x,y)=(0,0)\\).\nInterpretation of linear dependency: proportional increase in \\(y\\) with increase (decrease) in \\(x\\).\n\n\n\nBut a straight line cannot perfectly fit the data\n\n\n\n\n\nThe line is not a perfect fit to the data. There is scatter around the line.\nSome of this scatter could be caused by other factors that influence blood pressure, such as diet, exercise, and genetics. Also, the there could be differences due to the measurement instrument (i.e., some measurement error).\nThese other factors are not included in the model (only age is in the model), so they create variation that can only appear in error term.\nIn the linear regression model the dependent variable \\(Y\\) is related to the independent variable \\(x\\) as\n\\[Y = \\beta_0 + \\beta_1 x + \\epsilon \\ \\] where\n\n\\(\\epsilon\\) is the error term\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope\n\\(\\epsilon\\) is the error term.\n\nThe error term captures the difference between the observed value of the dependent variable and the value predicted by the model. The error term includes the effects of other factors that influence the dependent variable, as well as measurement error.\n\\[Y \\quad= \\quad \\underbrace{\\text{expected value}}_{E(Y) = \\beta_0 + \\beta_1 x} \\quad + \\quad \\underbrace{\\text{random error}}_{\\epsilon}  \\ .\\]\nGraphically the error term is the vertical distance between the observed value of the dependent variable and the value predicted by the model.\n\n\n\n\n\nThe error term is also known as the residual. It is the variation that resides (is left over / is left unexplained) after accounting for the relationship between the dependent and independent variables.\n\n\nDealing with the error\nIf we use each of the observed residuals as the error for the respective data point, we end up with a perfect fit to the data. This is because the model is fitted to the data, so the residuals are the difference between the observed value of the dependent variable and the value predicted by the model. We then have a model with no error – everything is explained by the model. This is known as over-fitting. In fact, we have gained nothing by fitting the model, because we have not learned anything about the relationship between the dependent and independent variables. We have simply memorized / copied the data!!!\nIn order to avoid this, we need to assume something about the residuals – we need to model the error term. The most common model for the error term is normally distributed with mean 0 and constant variance.\n\\[\\epsilon \\sim N(0,\\sigma^2)\\]\nThis is known as the normality assumption. The normality assumption is important because it allows us to make inferences about the population parameters based on the sample data.\nThe linear regression model then becomes:\n\\[Y = \\beta_0 + \\beta_1 x + N(0,\\sigma^2) \\ \\]\nwhere \\(\\sigma^2\\) is the variance of the error term. The variance of the error term is the amount of variation in the dependent variable that is not explained by the independent variable. The variance of the error term is also known as the residual variance.\nAn alternate and equivalent formulation is that \\(Y\\) is a random variable that follows a normal distribution with mean \\(\\beta_0 + \\beta_1 x\\) and variance \\(\\sigma^2\\).\n\\[Y \\sim N(\\beta_0 + \\beta_1 x, \\sigma^2)\\]\nSo, the answer to the question “how do we deal with the error term” is that we model the error term as normally distributed with mean 0 and constant variance. Put another way, the error term is assumed to be normally distributed with mean 0 and constant variance.\n\n\nBack to blood pressure and age\nThe mathematical model in this case is:\n\\[SystolicBP = \\beta_0 + \\beta_1 \\times Age + \\epsilon\\]\nwhere: SystolicBP is the dependent (response) variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1\\) is the coefficient of Age, Age is the independent (explanatory) variable, \\(\\epsilon\\) is the error term.\nLet’s ensure we understand this, by thinking about the units of the variables in this model. This can be very useful because it can help us to understand the model better and to check that the model makes sense.\n\n\n\n\n\n\nThink, pair, share (#what-units)\n\n\n\n\nWhat are the units of blood pressure?\nWhat are the units of age?\nWhat are the units of the intercept?\nWhat are the units of the coefficient of Age?\nWhat are the units of the error term?\n\n\n\nWe can easily guess that the coefficient of Age is positive, because blood pressure tends to increase with age. But what is the most likely value of this coefficient? We need to estimate the coefficient of Age from the data. This is what linear regression does. It estimates the coefficients (intercept and slope) of the independent variables in the model."
  },
  {
    "objectID": "3-regression.html#finding-the-intercept-and-the-slope",
    "href": "3-regression.html#finding-the-intercept-and-the-slope",
    "title": "Regression (L3&4)",
    "section": "Finding the intercept and the slope",
    "text": "Finding the intercept and the slope\nIn a regression analysis, one task is to estimate the intercept and the slope. These are known as the regression coefficients \\(\\beta_0\\), \\(\\beta_1\\). (We also estimate the residual variance \\(\\sigma^2\\).)\n\nProblem: For more than two points \\((x_i,y_i)\\), \\(i=1,\\ldots, n\\), there is generally no perfectly fitting line.\nAim: We want to estimate the parameters \\((\\beta_0,\\beta_1)\\) of the best fitting line \\(Y = \\beta_0 + \\beta_1 x\\).\nIdea: Find the best fitting line by minimizing the deviations between the data points \\((x_i,y_i)\\) and the regression line. I.e., minimising the residuals.\n\nBut which deviations?\nThese ones?\n\n\n\n\n\nOr these?\n\n\n\n\n\nOr maybe even these?\n\n\n\n\n\nWell, actually its none of these!!!\n\nLeast squares\nFor multiple reasons (theoretical aspects and mathematical convenience), the intercept and slope are estimated using the least squares approach. In this, yet something else is minimized:\nThe parameters \\(\\beta_0\\) and \\(\\beta_1\\) are estimated such that the sum of squared vertical distances (sum of squared residuals / errors) is minimised.\nSSE means Sum of Squared Errors:\n\\[SSE = \\sum_{i=1}^n e_i^2 \\]\nwhere,\n\\[e_i = y_i - \\underbrace{(\\beta_0 + \\beta_1 x_i)}_{=\\hat{y}_i} \\] Note: \\(\\hat y_i = \\beta_0 + \\beta_1 x_i\\) are the predicted values.\nIn the graph just below, one of these squares is shown in red.\n\n\n\n\n\n\n\nLeast squares estimates\nWith a linear model, we can calculate the least squares estimates of the parameters \\(\\beta_0\\) and \\(\\beta_1\\) directly using the following formulas.\nFor a given sample of data \\((x_i,y_i), i=1,..,n\\), with mean values \\(\\overline{x}\\) and \\(\\overline{y}\\), the least squares estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are computed as\n\\[ \\hat\\beta_1 = \\frac{\\sum_{i=1}^n  (y_i - \\overline{y}) (x_i - \\overline{x})}{ \\sum_{i=1}^n (x_i - \\overline{x})^2 } = \\frac{cov(x,y)}{var(x)}\\]\n\\[\\hat\\beta_0 = \\overline{y} - \\hat\\beta_1 \\overline{x}  \\]\nMoreover,\n\\[ \\hat\\sigma^2 = \\frac{1}{n-2}\\sum_{i=1}^n e_i^2 \\quad \\text{with residuals  } e_i = y_i - (\\hat\\beta_0 + \\hat\\beta_1 x_i) \\]\nis an unbiased estimate of the residual variance \\(\\sigma^2\\).\n(Derivations of the equations above are in the Stahel script 2.A b. Hint: differentiate, set to zero, solve.)\n\n\nWhy division by \\(n-2\\) ensures an unbiased estimator\nWhen estimating parameters (\\(\\beta_0\\) and \\(\\beta_1\\)), the square of the residuals is minimised. This fitting process inherently uses up two degrees of freedom, as the model forces the residuals to sum to zero and aligns the slope to best fit the data. I.e., one degree of freedom is lost due to the estimation of the intercept, and another due to the estimation of the slope.\nThe adjustment (division by \\(n-2\\) instead of \\(n\\)) compensates for the loss of variability due to parameter estimation, ensuring the estimator of the residual variance is unbiased. Mathematically, dividing by n - 2 adjusts for this loss and gives an accurate estimate of the population variance when working with sample data.\nWe’ll look at degrees of freedom in more detail later, so don’t worry if this is a bit confusing right now.\n\n\nLet’s do it in R\nFirst we read in the dataset:\n Download the dataset. \n\nbp_age_data &lt;- read.csv(\"data/Simulated_Blood_Pressure_and_Age_Data.csv\")\n\nThe we make a graph of the data:\n\n\n\n\n\nThen we make the linear model, using the lm() function:\nThen we can look at the summary of the model. It contains a lot of information, so can be a bit confusing at first.\n\n\n\nCall:\nlm(formula = Systolic_BP ~ Age, data = bp_age_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.2195  -3.4434  -0.0808   3.1383  12.6025 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 98.96874    1.46102   67.74   &lt;2e-16 ***\nAge          0.82407    0.02771   29.74   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.971 on 98 degrees of freedom\nMultiple R-squared:  0.9002,    Adjusted R-squared:  0.8992 \nF-statistic: 884.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nHow do our guesses of the intercept and slope compare to the guesses we made earlier?\nRecal that the units of the Age coefficient are in mmHg per year. This means that for each additional year of age, the systolic blood pressure increases by ´r round(coef(bp_age_model)[2],2)´ mmHg."
  },
  {
    "objectID": "3-regression.html#is-the-model-good-enough-to-use",
    "href": "3-regression.html#is-the-model-good-enough-to-use",
    "title": "Regression (L3&4)",
    "section": "Is the model good enough to use?",
    "text": "Is the model good enough to use?\n\nAll models are wrong, but is ours good enough to be useful?\nAre the assumption of the model justified?\nIt would be very unwise to use the model before we know if it is good enough to use.\nDon’t jump out of an aeroplane until you know your parachute is good enough!\n\n\nWhat assumptions do we make?\nWe already heard about one. We assume that the residuals follow a \\(N(0,\\sigma^2)\\) distribution. We make this assumption because it is often well enough met, and it gives great mathematical tractability.\nThis assumption implies that:\n\nThe \\(\\epsilon_i\\) are normally distributed.\nAll \\(\\epsilon_i\\) have the same variance: \\(Var(\\epsilon_i)=\\sigma^2\\).\nThe \\(\\epsilon_i\\) are independent of each other.\n\nFurthermore:\n\nwe assumed a linear relationship.\nimplies there are no outliers (implied by (a) above)\n\nLets go through each five assumptions.\n\n\n(a) Normally distributed residuals\nWhat does this mean? How can we check it?\nA normal distribution is symmetric and bell-shaped…\n\n\n\n\n\nLets look at the frequency distribution of the residuals of the linear regression of blood pressure and age:\n\n\n\n\n\nThe normal distribution assumption (a) seems ok as well.\n\n\n(a) Normally distributed residuals: The QQ-plot\nUsually, not the histogram of the residuals is plotted, but the so-called quantile-quantile (QQ) plot. The quantiles of the observed distribution are plotted against the quantiles of the respective theoretical (normal) distribution:\n\n\n\n\n\nIf the points lie approximately on a straight line, the data is fairly normally distributed.\nThis is often “tested” by eye, and needs some experience.\nBut what on earth is a quantile???\nImagine we make 21 measures of something, say 21 blood pressures:\n\n\n\n\n\nThe median of these is 127.8. The median is the 50% or 0.5 quantile, because half the data points are above it, and half below.\n\n\n  50% \n127.8 \n\n\nThe theoretical quantiles come from the normal distribution. The sample quantiles come from the distribution of our residuals.\n\n\n\n\n\n\nHow do I know if a QQ-plot looks “good”?\nThere is no quantitative rule to answer this question. Instead experience is needed. You can gain this experience from simulations. To this end, we can generate the same number of data points of a normally distributed variable and compare this simulated qqplot to our observed one.\nExample: Generate 100 points \\(\\epsilon_i \\sim N(0,1)\\) each time:\n\n\n\n\n\nEach of the graphs above has data points that are randomly generated from a normal distribution. In all cases the data points are close to the line. This is what we would expect if the data were normally distributed. The amount of deviation from the line is what we would expect from random variation, and so seeing this amount of variation in a QQ-plot of your model should not be cause for concern.\n\n\n\n(b) Equal variance (homoscedasticity)\nBasically, we’re interested if the size of the residuals tends to show a pattern with the fitted values. By size of the residuals we mean the absolute value of the residuals.\nWe have assumed that there is no relationship between the residuals and the fitted values.\nThat is, variance of the residuals is a constant: \\(\\text{Var}(\\epsilon_i) = \\sigma^2\\). And not, for example \\(\\text{Var}(\\epsilon_i) = \\sigma^2 \\cdot x_i\\).\nSo we’re going to plot the size of the residuals against the fitted values.\nThis graph is known as the scale-location plot. It is particularly suited to check the assumption of equal variances (homoscedasticity / Homoskedastizität).\nThe idea is to plot the square root of the (standardized) residuals \\(\\sqrt{|R_i|}\\) against the fitted values \\(\\hat{y_i}\\). There should be no trend:\n\n\n\n\n\n\nHow it looks with the variance increasing with the fitted values\nHere’s a graphical example of how it would look if the variance of the residuals increases with the fitted values.\nFirst here is a graph of the relationship:\n\n\n\n\n\nAnd here the scale-location plot for a linear model of that data:\n\n\n\n\n\n\n\n\n(c) Independence (the \\(\\epsilon_i\\) are independent of each other)\nWe assume that the residuals are independent of each other. This means that the value of one residual is not somehow related to the value of another.\nThe dataset about blood pressure we looked at contained 100 observations, each one made from a different person. In such a study design, we could be safe in the assumption that the people are independent, and therefore the assumption that the residuals are independent.\nImagine, however, if we had 100 observations of blood pressure collected from 50 people, because we measured the blood pressure of each person twice. In this case, the residuals would not be independent, because two measures of the blood pressure of the same person are likely to be similar. A person is likely to have a high blood pressure in both measurements, or a low blood pressure in both measurements. This would mean they have a high residual in both measurements, or a low residual in both measurements.\nIn this case, we would need to account for the fact that the residuals are not independent. We would need to use a more complex model, such as a mixed effects model, to account for the fact that the residuals are not independent. We will talk about this again in the last week of this course.\nIn general, you should always think about the study design when you are analysing data. You should always think about whether the residuals are likely to be independent of each other. If they are not, you should think about how you can account for this in your analysis.\nA good way to assess if there could be dependencies in the residuals is to be critical about what is the unit of observation in the data. In the blood pressure example, the unit of observation is the person. Count the number of persons in the study. If there are fewer persons than observations, then at least some people must have been measured at least twice. Repeating measures on the same person is a common way to get dependent residuals.\nSo, to check the assumption of independence, you should:\n\nThink carefully about the study design.\nThink carefully about the unit of observation in the data.\nCompare the number of observations to the number of units of observation.\n\n\n\n(d) Linearity assumption\nThe linearity assumption states that the relationship between the independent variable and the dependent variable is linear. This means that the dependent variable changes by a constant amount for a one-unit change in the independent variable. And that this slope is does not change with the value of the independent variable.\nThe blood pressure data seems to be linear:\n\n\n\n\n\nIn contrast, look at this linear regression through data that appears non-linear:\n\n\n\n\n\nAnd with the residuals shown as red lines:\n\n\n\n\n\nAt low values of \\(y\\), the residuals are positive, at intermediate values of \\(y\\) the residuals are negative, and at high values of \\(y\\) the residuals are positive. This pattern in the residuals is a sign that the relationship between \\(x\\) and \\(y\\) is not linear.\nWe can plot the value of the residuals against the \\(y\\) value directly, instead of looking at the pattern in the graph above. This is called a Tukey-Anscombe plot. It is a graph of the residuals versus the fitted \\(y\\) values.\n\n\n\n\n\nWe can very clearly see pattern in the residuals in this Tukey-Anscombe plot. The residuals are positive, then negative, then positive, as the fitted \\(y\\) value gets larger.\nThe red line in the Tukey-Anscombe plot is a loess smooth. It is automatically added to the plot. It is a way of estimating the pattern in the residuals. If the red line is not flat, then there is a pattern in the residuals. However, the loess smooth is not always reliable. It is a good idea to look at the residuals directly, without this smooth.\n\n\n\n\n\nThe data here is simulated to show a very clear pattern in the residuals. In real data, the pattern might not be so clear. But if you suspect you see a pattern in the residuals, it could be a sign that the relationship between the independent and dependent variable is not linear.\nHere is the Tukey-Anscombe plot for the blood pressure data:\n\n\n\n\n\nThere is very little evidence of any pattern in the residuals. This data is simulated with a truly linear relationship, so we would not expect to see any pattern in the residuals.\n\n\n(e) No outliers\nAn outlier is a data point that is very different from the other data points. Outliers can have a big effect on the results of a regression analysis. They can pull the line of best fit towards them, and make the line of best fit a poor representation of the data.\nLets again look at the blood pressure versus age data:\n\n\n\n\n\nThere are no obvious outliers in this data. The data points are all close to the line of best fit. This is a good sign that the line of best fit is a good representation of the data.\n\n\n\n\n\n\nThink, Pair, Share (#odd-data)\n\n\n\nWhere on this graph would you expect to see particularly influential outliers? Influential in the sense that they would have a large effect on the slope of the line of best fit.\n\n\nData points that are far from the mean of the independent variable have a large effect on the value of the slope. These data points have a large leverage. They are data points that are far from the other data points in the \\(x\\) direction.\nWe can think of this with the analogy of a seesaw. The slope of the line of best fit is like the pivot point of a seesaw. Data points that are far from the pivot point have a large effect on the slope. Data points that are close to the pivot point have a small effect on the slope.\nA measure of distance from the pivot point is called the \\(leverage\\) of a data point. In simple regression, the leverage of individual \\(i\\) is defined as\n\\(h_{i} = (1/n) + (x_i-\\overline{x})^2 / SSX\\).\nwhere \\(SSX = \\sum_{i=1}^n (x_i - \\overline{x})^2\\). (Sum of Squares of \\(X\\))\nSo, the leverage of a data point is inversely related to \\(n\\) (the number of data points). The leverage of a data point is also inversely related to the sum of the squares of the \\(x\\) values. The leverage of a data point is directly related to the square of the distance of the \\(x\\) value from the mean of the \\(x\\) values.\nMore intuitively perhaps, the leverage of a data point will be greater when the are fewer other data points. It will also be greater when the distance from the mean value of \\(x\\) is greater.\nGoing back to the analogy of a seesaw, with data points as children on the seesaw, the leverage of a data point is like the distance from the pivot a child sits. But we also have children of different weights. A lighter child will have less effect on the tilt of the seesaw. A heavier one will have a greater effect on the tilt. A heavier child sitting far from the pivot will have a very large effect.\n\n\n\n\n\n\nThink, Pair, Share (#like-weight)\n\n\n\nWhat quantity that we already experienced is like the weight of the child?\n\n\nThe size of the residuals are like the weight of the child. Data points with large residuals have a large effect on the slope of the line of best fit. Data points with small residuals have a small effect on the slope of the line of best fit.\nSo the overall effect of a data point on the slope of the line of best fit is a combination of the leverage and the residual. This quantity is called the \\(influence\\) of a data point.\nLets add a rather extreme data point to the blood pressure versus age data:\n\n\n\n\n\nThis is a bit ridiculous, but it is a good example of an outlier. The data point is far from the other data points. It has a large residual. And it is a long way from the pivot (the middle of the \\(x\\) data) so has large leverage.\nWe can make a histogram of the residuals and see that the outlier has a large residual:\n\n\n\n\n\nAnd we can see that the leverage is large.\nThere is a graph that we can look at to see the influence of a data point. This is called a \\(Cook's\\) \\(distance\\) plot. The Cook’s distance of a data point is a measure of how much the slope of the line of best fit changes when that data point is removed. The Cook’s distance of a data point is defined as\n\\(D_i = \\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2 / (p \\times MSE)\\).\nwhere \\(\\hat{y}_j\\) is the predicted value of the dependent variable for data point \\(j\\), \\(\\hat{y}_{j(i)}\\) is the predicted value of the dependent variable for data point \\(j\\) when data point \\(i\\) is removed, \\(p\\) is the number of parameters in the model (2 in this case), \\(MSE\\) is the mean squared error of the model.\n\n\n\n\n\nBut does it have a large influence on the value of the slope?\n\n\n\n\n\nThe outlier has a large leverage. It is far from the pivot. But it does not have such a large effect (influence) on the slope. This is in large part because there are a lot data points (100) that are quite tightly arranged around the regression line.\n\nGraphical illustration of the leverage effect\nData points with \\(x_i\\) values far from the mean have a stronger leverage effect than when \\(x_i\\approx \\overline{x}\\):\n\n\n\n\n\nThe outlier in the middle plot “pulls” the regression line in its direction and has large influence on the slope.\n\n\nLeverage plot (Hebelarm-Diagramm)\nIn the leverage plot, (standardized) residuals \\(\\tilde{R_i}\\) are plotted against the leverage \\(H_{ii}\\) :\n\n\n\n\n\nCritical ranges are the top and bottom right corners!!\nHere, observations 71, 85, and 87 are labelled as potential outliers.\nSome texts will give a rule of thumb that points with Cook’s distances greater than 1 should be considered influential, while others claim a reasonable rule of thumb is \\(4 / ( n - p - 1 )\\) where \\(n\\) is the sample size, and \\(p\\) is the number of \\(beta\\) parameters."
  },
  {
    "objectID": "3-regression.html#what-can-go-wrong-during-the-modeling-process",
    "href": "3-regression.html#what-can-go-wrong-during-the-modeling-process",
    "title": "Regression (L3&4)",
    "section": "What can go “wrong” during the modeling process?",
    "text": "What can go “wrong” during the modeling process?\nAnswer: a lot of things!\n\nNon-linearity. We assumed a linear relationship between the response and the explanatory variables. But this is not always the case in practice. We might find that the relationship is curved and not well represtented by a straight line.\nNon-normal distribution of residuals. The QQ-plot data might deviate from the straight line so much that we get worried!\nHeteroscadisticity (non-constant variance). We assumed homoscadisticity, but the residuals might show a pattern.\nData point with high influence. We might have a data point that has a large influence on the slope of the line of best fit.\n\n\nWhat to do when things “go wrong”?\n\nNow: Transform the response and/or explanatory variables.\nNow: Take care of outliers.\nLater in the course: Improve the model, e.g., by adding additional terms or interactions.\nLater in the course: Use another model family (generalized or nonlinear regression model).\n\n\n\nDealing with non-linearity\nHere’s another example of \\(y\\) and \\(x\\) that are not linearly related:\n\n\n\n\n\nOne way to deal with this is to transform the response variable \\(Y\\). Here we try two different transformations: \\(\\log_{10}(Y)\\) and \\(\\sqrt{Y}\\).\nSquare root transform of the response variable \\(Y\\):\n\n\n\n\n\nNot great.\nLog transformation of the response variable \\(Y\\):\n\n\n\n\n\nNope. Still some evidence of non-linearity.\nWhat about transforming the explanatory variable \\(X\\) as well?\n\n\n\n\n\nLet’s look at the four diagnostic plots for the log-log-transformed data:\n\n\n\n\n\n\n\n\n\nAll looks pretty good. Pat on the back for us!\nBut… how to know which transformation to use…? It’s a bit of trial and error. But we can use the diagnostic plots to help us.\nVery very important is that we do this trial and error before we start using the model. E.g., we don’t want to jump from the aeroplane and then find out that our parachute is not working properly! And then try to fix the parachute while we are falling….\nLikewise, we must not start using the model and then try to fix it. We need to make sure our model is in good working order before we start using it.\nOne of the traps we could fall into is called “p-hacking”. This is when we try different transformation until we find one that gives us the result we want, for example significant relationship. This is a big no-no in statistics. We need to decide on the model (including any transformations) before we start using it.\n\n\nCommon transformations\nWhich transformations could be considered? There is no simple answer. But some guidelines. E.g. if we see non-linearity and increasing variance with increasing fitted values, then a log transform may improve matter.\nSome common and useful transformations are:\n\nThe log transformation for concentrations and absolute values.\nThe square-root transformation for count data.\nThe arcsin square-root \\(\\arcsin(\\sqrt{\\cdot})\\) transformation for proportions/percentages.\n\nTransformations can also be applied on explanatory variables, as we saw in the example above.\n\n\nOutliers\nWhat do we do when we identify the presence of one or more outliers?\n\nStart by checking the “correctness” of the data. Is there a typo or a decimal point that was shifted by mistake? Check both the response and explanatory variables.\nIf not, ask whether the model could be improved. Do reasonable transformations of the response and/or explanatory variables eliminate the outlier? Do the residuals have a distribution with a long tail (which makes it more likely that extreme observations occur)?\nSometimes, an outlier may be the most interesting observation in a dataset! Was the outlier created by some interesting but different process from the other data points?\nConsider that outliers can also occur just by chance!\nOnly if you decide to report the results of both scenario can you check if inclusion/exclusion changes the qualitative conclusion, and by how much it changes the quantitative conclusion.\n\n\n\nRemoving outliers\nIt might seem tempting to remove observations that apparently don’t fit into the picture. However:\n\nDo this only with greatest care e.g., if an observation has extremely implausible values!\n\nBefore deleting outliers, check points 1-5 above.\nWhen removing outliers, you must mention this in your report.\n\nDuring the course we’ll see many more examples of things going at least a bit wrong. And we’ll do our best to improve the model, so we can be confident in it, and start to use it. Which we will start to do in the next lesson. But before we wrap up, some good news…"
  },
  {
    "objectID": "3-regression.html#sec-kind-of-magic",
    "href": "3-regression.html#sec-kind-of-magic",
    "title": "Regression (L3&4)",
    "section": "Its a kind of magic…",
    "text": "Its a kind of magic…\nAbove, we learned about linear regression, the equation for it, how to estimate the coefficients, and how to check the assumptions. There was a lot of information, and it might seem a bit overwhelming.\nYou might also be aware that there are quite a few other types of statistical model, such as multiple regression, t-test, ANOVA, two-way ANOVA, and ANCOVA. It could be worrying to think that you need to learn so much new information for each of these types of tests.\nBut this is where the kind-of-magic happens. The good news is that the linear regression model is a special case of what is called a general linear model, or just linear model for short. And that all the tests mentioned above are also types of linear model. So, once you have learned about linear regression, you have learned a lot about linear models, and therefore also a lot about all of these other tests as well.\nMoreover, the same function in R ‘lm’ is used to make all those statistical models Awesome.\n\nSo what is a linear model?\nA linear model is a model where the relationship between the dependent variable and the independent variables is linear. That is, the dependent variable can be expressed as a linear combination of the independent variables. An example of a linear model is:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p + \\epsilon\\]\nwhere: \\(y\\) is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) are the coefficients of the independent variables, \\(x_1, x_2, \\ldots, x_p\\) are the independent variables, \\(\\epsilon\\) is the error term.\nIn contrast, a non-linear model is a model where the relationship between the dependent variable and the independent variables is non-linear. An example of a non-linear model is the exponential growth model:\n\\[y = \\beta_0 + \\beta_1 e^{\\beta_2 x} + \\epsilon\\]\nwhere: y is the dependent variable, \\(\\beta_0\\) is the intercept, \\(\\beta_1, \\beta_2\\) are the coefficients of the independent variables, \\(x\\) is the independent variable, \\(\\epsilon\\) is the error term.\nKeep in mind that a model with a quadratic term is still a linear model. For example:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x^2 + \\epsilon\\]\nis still a linear model. We can see this if we substitute \\(x^2\\) with a new variable \\(x_2\\), where \\(x_2 = x^2\\). The model then becomes:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\\]\nThis is clearly a linear model."
  },
  {
    "objectID": "3-regression.html#end-of-l3-start-of-l4",
    "href": "3-regression.html#end-of-l3-start-of-l4",
    "title": "Regression (L3&4)",
    "section": "End of L3 / Start of L4",
    "text": "End of L3 / Start of L4"
  },
  {
    "objectID": "3-regression.html#overview-of-this-week-l4",
    "href": "3-regression.html#overview-of-this-week-l4",
    "title": "Regression (L3&4)",
    "section": "Overview of this week (L4)",
    "text": "Overview of this week (L4)\nNow that we have a satisfactory model, we can start to use it. In the following material, you will learn:\n\nHow to measure how good is the regression (correlation and \\(R^2\\)).\nHow to test if the parameter estimates are compatible with some specific value (\\(t\\)-test).\nHow to find the range of parameters values are compatible with the data (confidence intervals).\nHow to find the regression lines compatible with the data (confidence band).\nHow to calculate plausible values of newly collected data (prediction band)."
  },
  {
    "objectID": "3-regression.html#how-good-is-the-regression-model",
    "href": "3-regression.html#how-good-is-the-regression-model",
    "title": "Regression (L3&4)",
    "section": "How good is the regression model?",
    "text": "How good is the regression model?\nWhat would a good regression model look like? What would a bad one look like? One could say that a good regression model is one that explains the dependent variable well. But what could we mean by “explains the data well”?\nTake these two examples.\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#better-model)\n\n\n\nIn which of these two would you say the model is better, and in which is it worse?\n\n\nThe first model seems to fit the data well, while the second one does not. But how can we quantify this?\nLet’s say that we will measure the goodness of the model by the amount of variability of the dependent variable that is explained by the independent variable. To do this we need to do the following:\n\nMeasure the total variability of the dependent variable (total sum of squares, \\(SST\\)).\nMeasure the amount of variability of the dependent variable that is explained by the independent variable (model sum of squares, \\(SSM\\)).\nMeasure the variability of the dependent variable that is not explained by the independent variable (error sum of squares, \\(SSE\\)).\nCalculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\), pronounced as “r-squared”) (also known as the coefficient of determination) (\\(R^2\\) = \\(SSM/SST\\)).\n\nImportantly, note that we will calculate \\(SSM\\) and \\(SSE\\) so that they sum up to \\(SST\\). I.e., \\(SST = SSM + SSE\\). That is, the total variability is the sum of what is explained by the model and what remains unexplained.\nLet’s take each in turn:\n\n\\(SST\\)\n1. The total variability of the dependent variable is the sum of the squared differences between the dependent variable and its mean. This is called the total sum of squares (\\(SST\\)).\n\\[SST = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\\]\nwhere: \\(y_i\\) is the dependent variable, \\(\\bar{y}\\) is the mean of the dependent variable, \\(n\\) is the number of observations.\nNote that sometimes \\(SST\\) is referred to as \\(SSY\\) (sum of squares of \\(y\\)).\nGraphically, this is the sum of the square of the blue residuals as shown in the following graph, where the horizontal dashed line is at the value of the mean of the dependent variable.\n\n\n\n\n\nWe can calculate this in R as follows:\n\nSST &lt;- sum((y1 - mean(y1))^2)\n\n\n\nSSM and SSE\nNow the next two steps, that is getting the model sum of squares (SSM) and the error sum of squares (SSE) are a bit more complicated. To do this we need to fit a regression model to the data. Let’s see this graphically, and divide the data into the explained and unexplained parts.\n\n\n\n\n\nIn this graph, the square of the length of the green lines is the model sum of squares (\\(SSM\\)). The square of the length of the red lines is the error sum of squares (\\(SSE\\)).\nIn a better model the length of the green lines will be longer (the square of these gives the \\(SMM\\), the variability explained by the model). And the length of the red lines will be shorter (the square of these gives the \\(SSE\\), the variability not explained by the model).\n\n\n\\(SSM\\)\nNext we will do the second step, that is calculate the model sum of squares (\\(SSM\\)).\n2. The amount of variability of the dependent variable that is explained by the independent variable is called the model sum of squares (\\(SSM\\)).\nThis is the difference between the predicted value of the dependent variable and the mean of the dependent variable, squared and summed:\n\\[SSE = \\sum_{i=1}^{n} (\\hat{y}_i - \\bar{y})^2\\]\nwhere: \\(\\hat{y}_i\\) is the predicted value of the dependent variable,\nIn R, we calculate this as follows:\n\nm1 &lt;- lm(y1 ~ x)\ny1_predicted &lt;- predict(m1)\nSSM &lt;- sum((y1_predicted - mean(y1))^2)\nSSM\n\n[1] 306.4576\n\n\n\n\n\\(SSE\\)\nThird, we calculate the error sum of squares (\\(SSE\\)) with either of two methods. We could calculate it as the sum of the squared residuals, or as the difference between the total sum of squares and the model sum of squares:\n\\[SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = SST - SSM\\] Let’s calculate this in R uses both approaches:\n\nSSE &lt;- sum((y1 - y1_predicted)^2)\nSSE\n\n[1] 8.163797\n\n\nOr…\n\nSSE &lt;- SST - SSM\nSSE\n\n[1] 8.163797\n\n\n\n\n\\(R^2\\)\nFinally, we calculate the proportion of variability of the dependent variable that is explained by the independent variable (\\(R^2\\)):\n\\[R^2 = \\frac{SSM}{SST}\\]\n\n\n[1] 0.974052\n\n\n\n\nIs my R squared good?\nWhat value of \\(R^2\\) is considered good? In ecological research, \\(R^2\\) values are often low (less than 0.3), because ecological systems are complex and many factors influence the dependent variable. However, in other fields, such as physiology, \\(R^2\\) values are often higher. Therefore, the answer of what values of \\(R^2\\) are good depends on the field of research.\nHere are the four examples and their r-squared.\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#what-minimised)\n\n\n\nWhat is minimised when we fit a regression model? And therefore what is maximised?"
  },
  {
    "objectID": "3-regression.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "href": "3-regression.html#how-unlikey-is-the-observed-data-given-the-null-hypothesis",
    "title": "Regression (L3&4)",
    "section": "How unlikey is the observed data given the null hypothesis?",
    "text": "How unlikey is the observed data given the null hypothesis?\n(We often hear this expressed as “is the relationship significant?”)\nWhat is a meaningful null hypothesis for a regression model?\nOften we are interested in whether there is a relationship between the dependent and independent variable. Therefore, the null hypothesis is that there is no relationship between the dependent and independent variable. This means that the null hypothesis is that the slope of the regression line is zero.\nRecall the regression model: \\[y = \\beta_0 + \\beta_1 x + \\epsilon\\]\n\n\n\n\n\n\nThink, Pair, Share (#null-hypothesis)\n\n\n\nWrite down the null hypothesis of no relationship between \\(x\\) and \\(y\\).\n\n\nThe null hypothesis is that the slope of the regression line is zero: \\[H_0: \\beta_1 = 0\\]\nWhat is the alternative hypothesis?\n\\[H_1: \\beta_1 \\neq 0\\]\nSo, how do we test the null hypothesis? We can use the fact that the the slope of the regression line is an estimate of the true slope. The slope estimate has uncertainty associated with it. We can use this uncertainty to calculate the probability of observing the data we have, given the null hypothesis is true.\nWe can see that the slope estimate (the \\(x\\) row) has uncertainty by looking at the regression output:\n\n\n              Estimate Std. Error\n(Intercept) -13.006987  0.8338264\nx             7.629066  0.1356499\n\n\nThe estimate is the mean of the distribution of the parameter (slope) and the standard error is a measure of the uncertainty of the estimate.\nThe standard error is calculated as:\n\\[\\sigma^{(\\beta_1)} = \\sqrt{ \\frac{\\hat\\sigma^2}{\\sum_{i=1}^n (x_i - \\bar x)^2}}\\]\nWhere \\(\\hat\\sigma^2\\) is the expected residual variance of the model. This is calculated as:\n\\[\\hat\\sigma^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat y_i)^2}{n-2}\\]\nWhere \\(\\hat y_i\\) is the predicted value of \\(y_i\\) from the regression model.\nOK, let’s take a look at this intuitively. We have the the estimate of the slope and the standard error of the estimate.\nHere is a graph of the value of the slope estimate versus the standard error of the estimate:\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#chance-area)\n\n\n\nIn what areas of the graph is the slope estimate more likely to have been observed by chance? And what regions is it less likely to have been observed by chance?\n\n\nWhen the slope estimate is larger, it is less likely to have been observed by chance. And when the standard error is larger, it is more likely to have been observed by chance. How can we put these together into a single measure?\nIf we divide the slope estimate by the standard error, we get a measure of how many standard errors the slope estimate is from the null hypothesis slope of zero. This is the \\(t\\)-statistic:\n\\[t = \\frac{\\hat\\beta_1 - \\beta_{1,H_0}}{\\sigma^{(\\beta_1)}}\\]\nWhere \\(\\beta_{1,H_0}\\) is the null hypothesis value of the slope, usually zero, so that\n\\[t = \\frac{\\hat\\beta_1}{\\sigma^{(\\beta_1)}}\\]\nThe \\(t\\)-statistic is a measure of how many standard errors the slope estimate is from the null hypothesis value of the slope. The larger the \\(t\\)-statistic, the less likely the slope estimate was observed by chance.\nHow can we transform the value of a \\(t\\)-statistic into a p-value? We can use the \\(t\\)-distribution, which quantifies the probability of observing a value of the \\(t\\)-statistic under the null hypothesis.\nBut what is the \\(t\\)-distribution? It is a bell-shaped distribution that is centered on zero. The spread of the distribution is determined by the degrees of freedom, which is \\(n-2\\) for a simple linear regression model. Fewer degrees of freedom ( = fewer data points) makes for a more “spread” distribution.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy the way, it is named the \\(t\\)-distribution by it’s developer, William Sealy Gosset, who worked for the Guinness brewery in Dublin, Ireland. In his 1908 paper, Gosset introduced the \\(t\\)-distribution but he didn’t explicitly explain his choice of the letter \\(t\\). The choice of the letter \\(t\\) could be to indicate “Test”, as the \\(t\\)-distribution was developed specifically for hypothesis testing.\n\n\nNow, recall that the p-value is the probability of observing the value of the test statistic (here the \\(t\\)-statistic) at least as extreme as the one we have, given the null hypothesis is true. We can calculate this probability by integrating the \\(t\\)-distribution from the observed \\(t\\)-statistic to the tails of the distribution.\nHere is a graph of the \\(t\\)-distribution with the tails of the distribution shaded so that the area of the shaded region is 0.05 (i.e., 5% of the total area).\n\n\n\n\n\nLet’s go back to the age - blood pressure data and calculate the p-value for the slope estimate.\n\n\n\n\n\nHere’s the model:\n\nmod1 &lt;- lm(Systolic_BP ~ Age, data = bp_data)\n\nHere we calculate the \\(t\\)-statistic for the slope estimate:\nWe can get the \\(p\\)-value directly from the summary function:\n\n\n    Estimate   Std. Error      t value     Pr(&gt;|t|) \n8.240678e-01 2.770955e-02 2.973948e+01 7.493917e-51 \n\n\nConclusion: there is very strong evidence that the blood pressure is associated with age, because the \\(p\\)-value is extremely small (thus it is very unlikely that the observed slope value or a large one would be seen if there was really no association). Thus, we can reject the null hypothesis that the slope is zero.\nThis basically answers question 1: “Are the parameters compatible with some specific value?”\n\nRecap: Formal definition of the \\(p\\)-value\nThe formal definition of \\(p\\)-value is the probability to observe a data summary (e.g., an average or a slope) that is at least as extreme as the one observed, given that the null hypothesis is correct.\n\n\nA cautionary note on the use of \\(p\\)-values\nMaybe you have seen that in statistical testing, often the criterion \\(p\\leq 0.05\\) is used to test whether \\(H_0\\) should be rejected. This is often done in a black-or-white manner. However, we will put a lot of attention to a more reasonable and cautionary interpretation of \\(p\\)-values in this course!"
  },
  {
    "objectID": "3-regression.html#confidence-interval-for-the-slope",
    "href": "3-regression.html#confidence-interval-for-the-slope",
    "title": "Regression (L3&4)",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nWe are not only interested in whether the slope differs from zero. The value of the parameter has practical meaning. The slope of the regression line tells us how much the dependent variable changes when the independent variable changes by one unit. The slope is one measure of the strength of the relationship between the two variables.\nWe can ask what values of a parameter estimate are compatible with the data (confidence intervals)? To answer this question, we can determine the confidence intervals of the regression parameters.\nThe confidence interval of a parameter estimate is defined as the interval that contains the true parameter value with a certain probability. So the 95% confidence interval of the slope is the interval that contains the true slope with a probability of 95%.\nWe can then imagine two cases. The 95% confidence interval of the slope includes 0:\n\n\n\n\n\nOr where the confidence interval does not include zero:\n\n\n\n\n\nWhen the confidence interval includes 0 then we conclude that a slope of 0 is compatible with the data. When the confidence interval doesn’t include zero, we conclude that a slope of 0 is not compatible with the data.\nHow do we calculate the lower and upper limits of the 95% confidence interval of the slope?\nRecall that the \\(t\\)-value for a null hypothesis of slope of zero is defined as:\n\\[t = \\frac{\\hat\\beta_1}{\\hat\\sigma^{(\\beta_1)}}\\]\nThe first step is to calculate the \\(t\\)-value that corresponds to a p-value of 0.05. This is the \\(t\\)-value that corresponds to the 97.5% quantile of the \\(t\\)-distribution with \\(n-2\\) degrees of freedom.\n\\(t_{0.975} = t_{0.025} = 1.96\\), for large \\(n\\).\nThe 95% confidence interval of the slope is then given by:\n\\[\\hat\\beta_1 \\pm t_{0.975} \\cdot \\hat\\sigma^{(\\beta_1)}\\]\nIn our blood pressure example the estimated slope is 0.8240678 and the standard error of the slope is 0.0277096. We can calculate the 95% confidence interval of the slope as follows:\n\nn &lt;- 100\nt_0975 &lt;- qt(0.975, df = n - 2)\nhalf_interval &lt;- t_0975 * summary(mod1)$coef[2,2]\nlower_limit &lt;- coef(mod1)[2] - half_interval\nupper_limit &lt;- coef(mod1)[2] + half_interval\nci_slope &lt;- c(lower_limit, upper_limit)\nslope &lt;- coef(mod1)[2]\nslope\n\n      Age \n0.8240678 \n\nci_slope\n\n      Age       Age \n0.7690791 0.8790565 \n\n\nOr we can do it using values from the coefficients table:\n\ncoefs &lt;- summary(mod1)$coef\nbeta &lt;- coefs[2,1]\nsdbeta &lt;- coefs[2,2] \nbeta + c(-1,1) * qt(0.975,241) * sdbeta \n\n[1] 0.7694840 0.8786516\n\n\nOr we can have R do all the hard work for us:\n\n\n    2.5 %    97.5 % \n0.7690791 0.8790565 \n\n\nInterpretation: for an increase in the age by one year, roughly 0.82 mmHg increase in blood pressure is expected, and all true values for \\(\\beta_1\\) between 0.77 and 0.88 are compatible with the observed data."
  },
  {
    "objectID": "3-regression.html#confidence-and-prediction-bands",
    "href": "3-regression.html#confidence-and-prediction-bands",
    "title": "Regression (L3&4)",
    "section": "Confidence and Prediction Bands",
    "text": "Confidence and Prediction Bands\n\nRemember: If another sample from the same population was taken, the regression line would look slightly different.\nThere are two questions to be asked:\n\n\nWhich other regression lines are compatible with the observed data? This leads to the confidence band.\nWhere do future observations (\\(y\\)) with a given \\(x\\) coordinate lie? This leads to the prediction band.\n\nNote: The prediction band is broader than the confidence band."
  },
  {
    "objectID": "3-regression.html#calculation-of-the-confidence-band",
    "href": "3-regression.html#calculation-of-the-confidence-band",
    "title": "Regression (L3&4)",
    "section": "Calculation of the confidence band",
    "text": "Calculation of the confidence band\nGiven a fixed value of \\(x\\), say \\(x_0\\). The question is:\nWhere does \\(\\hat y_0 = \\hat\\beta_0 + \\hat\\beta_1 x_0\\) lie with a certain confidence (i.e., 95%)?\nThis question is not trivial, because both \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) are estimates from the data and contain uncertainty.\nThe details of the calculation are given in Stahel 2.4b.\nPlotting the confidence interval around all \\(\\hat y_0\\) values one obtains the confidence band or confidence band for the expected values of \\(y\\).\nNote: For the confidence band, only the uncertainty in the estimates \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) matters.\nHere is the confidence band for the blood pressure data:\n\n\n\n\n\nVery narrow confidence bands indicate that the estimates are very precise. In this case the estimated intercept and slope are precise because the sample size is large and the data points are close to the regression line."
  },
  {
    "objectID": "3-regression.html#calculations-of-the-prediction-band",
    "href": "3-regression.html#calculations-of-the-prediction-band",
    "title": "Regression (L3&4)",
    "section": "Calculations of the prediction band",
    "text": "Calculations of the prediction band\nWe can easily predicted an expected value of \\(y\\) for a given \\(x\\) value. But we can also ask w where does a future observation lie with a certain confidence (i.e., 95%)?\nTo answer this question, we have to consider not only the uncertainty in the predicted value caused by uncertainty in the parameter estimates \\(\\hat y_0 = \\hat\\beta_0 + \\hat\\beta_1 x_0\\), but also the error term \\(\\epsilon_i \\sim N(0,\\sigma^2)\\)}.\nThis is the reason why the prediction band is wider than the confidence band.\nHere’s a graph showing the prediction band for the blood pressure data:\n\n\n\n\n\nAnother way to think of the 95% confidence band is that it is where we would expect 95% of the regression lines to lie if we were to collect many samples from the same population. The 95% prediction band is where we would expect 95% of the future observations to lie."
  },
  {
    "objectID": "3-regression.html#that-is-regression-done-at-least-for-our-current-purposes",
    "href": "3-regression.html#that-is-regression-done-at-least-for-our-current-purposes",
    "title": "Regression (L3&4)",
    "section": "That is regression done (at least for our current purposes)",
    "text": "That is regression done (at least for our current purposes)\n\nWhy use (linear) regression?\nFitting the line (= parameter estimation)\nIs linear regression good enough model to use?\nWhat to do when things go wrong?\nTransformation of variables/the response.\nHandling of outliers.\nGoodness of the model: Correlation and \\(R^2\\)\nTests and confidence intervals\nConfidence and prediction bands"
  },
  {
    "objectID": "3-regression.html#additional-reading-material",
    "href": "3-regression.html#additional-reading-material",
    "title": "Regression (L3&4)",
    "section": "Additional reading material",
    "text": "Additional reading material\nIf you’d like another perspective and a deeper delve into some of the mathematical details, please look at Chapter 2 of Lineare Regression, p.7-20 (Stahel script), Chapters 3.1, 3.2a-q of Lineare Regression, and Chapters 4.1 4.2f, 4.3a-e of Lineare Regression"
  },
  {
    "objectID": "3-regression.html#additional-information",
    "href": "3-regression.html#additional-information",
    "title": "Regression (L3&4)",
    "section": "Additional information",
    "text": "Additional information\n\nRandomisation-based approaches to hypothesis testing\nLet’s use randomisation as a method to understand how likely we are to observe the data we have, given the null hypothesis is true.\nIf the null hypothesis is true, we expect no relationship between \\(x\\) and \\(y\\). Therefore, we can shuffle the \\(y\\) values and fit a regression model to the shuffled data. We can repeat this many times and calculate the slope of the regression line each time. This will give us a distribution of slopes we would expect to observe if the null hypothesis is true.\nFirst, we’ll make some data and get the slope of the regression line. Here is the observed slope and relationship:\n Download the dataset. \n\n\nx \n0 \n\n\n\n\n\n\n\nNow we’ll use randomisation to test the null hypothesis. We can create lots of examples where the relationship is expected to have a slope of zero by shuffling randomly the \\(y\\) values. Here are 20:\n\n\n\n\n\nNow let’s create 19 and put the real one in there somewhere random. Here’s a case where the real data has a quite strong relationship:\n\n\n\n\n\nWe can confidently find the real data in the shuffled data. But what if the relationship is weaker?\n\n\n\n\n\nNow its less clear which is the real data. We can use this idea to test the null hypothesis.\nWe do the same procedure of but instead of just looking at the graphs, we calculate the slope of the regression line each time. This gives us a distribution of slopes we would expect to observe if the null hypothesis is true. We can then see where the observed slope lies in this distribution of null hypothesis slopes.\n\n\n\n\n\nWe can now calculate the probability of observing the data we have, given the null hypothesis is true.\n\n\n[1] 1\n\n\nWhen we do linear regression we usually don’t use this randomisation approach. Instead,\n\n\nOne- and two-sided tests\nMost often we are interested in whether there is a relationship between \\(x\\) and \\(y\\). This is a two-sided test. We are interested in whether the slope of the regression line is different from zero.\nAssume that we calculated that \\(t\\)-value = -1.96\nA two sided test would be written as, with vertical bars surrounding the \\(t\\) value, which mean we are here concerned with the absolute value of the \\(t\\) value, and don’t care about the sign:\n\\(Pr(|t|\\geq 1.96)=0.05\\)\nIn contrast, a one-sided test would be written as, with not vertical bars surrounding the \\(t\\) value, which mean we are here concerned with the sign of the \\(t\\) value:\n\\(Pr(t\\leq-1.96)=0.025\\).\nGraphically these would look like this:\n\n\n\n\n\nAnd here we calculate the one-tailed and two-tailed \\(p\\)-values:\n\n\n         Age \n3.746958e-51 \n\n\n         Age \n7.493917e-51"
  },
  {
    "objectID": "4-multiple-regression.html#introduction",
    "href": "4-multiple-regression.html#introduction",
    "title": "Multiple regression (L4)",
    "section": "Introduction",
    "text": "Introduction\nWe previously looked at whether blood pressure is associated with age. This is an important question, because blood pressure has many health implications. However, blood pressure is not only associated with age, but also with other factors, such as weight, height, and lifestyle. In this chapter, we will look at how to investigate the association between blood pressure and multiple explanatory variables.\nWhen we have multiple explanatory variables, we are often interested in questions such as:\n\nQuestion 1: As an ensemble, are the explanatory variables associated with the response?\nQuestion 2: Are each of the explanatory variables associated with the response?\nQuestion 3: What proportion of variability is explained?\nQuestion 4: Are some explanatory variables more important than others?\n\nLets get an example of blood pressure data, with age and lifestyle as two explanatory variables. Lifestyle will be a continuous variable that is the number of minutes of exercise per week.\n Download the dataset. \nHere is a look at the dataset:\n\n\n  X age mins_exercise  bp\n1 1  38           180  84\n2 2  68           100 117\n3 3  45           147  96\n4 4  73           287  95\n5 5  77           145 105\n6 6  23           268  75\n\n\nWe can get insights by visualising the data by looking at three graphs:\n\nAge vs blood pressure. This is the graph of the response variable (blood pressure) against one of the explanatory variables (age). It looks like there is evidence of a positive relationship.\n\n\n\n\n\n\n\nMinutes of exercise vs blood pressure. This is a graph of the response variable (blood pressure) against the other explanatory variable (minutes of exercise). It looks like there is evidence of a negative relationship.\n\n\n\n\n\n\n\nAge vs minutes of exercise. This is a graph of the two explanatory variables against each other. It looks like there is no relationship.\n\n\n\n\n\n\nWe can also look at this in a plot with three axes. The y-axis is blood pressure, the x-axis is age, and the z-axis is minutes of exercise. Here is a 3d plot that we can interactive with and rotate:\n\n\n\n\n\n\nAll very good, and this makes sense. We can see that blood pressure is positively associated with age and negatively associated with minutes of exercise. But we need to be more quantitative about this. We need to build a model that relates blood pressure to age and minutes of exercise."
  },
  {
    "objectID": "4-multiple-regression.html#the-multiple-linear-regression-model",
    "href": "4-multiple-regression.html#the-multiple-linear-regression-model",
    "title": "Multiple regression (L4)",
    "section": "The multiple linear regression model",
    "text": "The multiple linear regression model\n\nThe model\nThe multiple linear regression model is an extension of the simple linear regression model. Recall the simple linear regression model is:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\]\nwhere:\n\n\\(y_i\\) is the response variable\n\\(x_i\\) is the explanatory variable\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope\n\\(\\epsilon_i\\) is the error term.\n\nThe multiple linear regression model with two explanatory variables is:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\epsilon_i\\]\nwhere:\n\n\\(x_i^{(1)}\\) and \\(x_i^{(2)}\\) are the two explanatory variables\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope for the first explanatory variable\n\\(\\beta_2\\) is the slope for the second explanatory variable\n\nNote that the intercept \\(\\beta_0\\) is the value of the response variable when all explanatory variables are zero. In this example, it would be the blood pressure for someone that is 0 years old and does 0 minutes of exercise per week. This is not a particularly useful scenario, but it is a necessary mathematical construct that helps us to build the model.\nWe can extend the multiple regression model to have an arbitrary number of explanatory variables:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\ldots + \\beta_p x_i^{(p)} + \\epsilon_i\\]\nWhere:\n\\(x_i^{(1)}, x_i^{(2)}, \\ldots, x_i^{(p)}\\) are the \\(p\\) explanatory variables and all else is as before.\nor with summation notation:\n\\[y_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_i^{(j)} + \\epsilon_i\\]\nJust like in simple linear regression, we can estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the method of least squares. The least squares method minimizes the sum of the squared residuals:\n\\[\\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\\]\nwhere \\(\\hat{y}_i\\) is the predicted value of the response variable for the \\(i\\)th observation:\n\\[\\hat{y}_i = \\hat{\\beta}_0 + \\sum_{j=1}^p \\hat{\\beta}_j x_i^{(j)}\\] where:\n\\(\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_p\\) are the estimated parameters.\n\n\n\n\n\n\nThink, Pair, Share (#two-shape)\n\n\n\nGraphically, a linear regression with one explanatory variable is a line. What is a geometric representation of linear regression with two explanatory variables?\n\n\n\n\n\n\n\n\n\n\nThe blood pressure example\nLet’s write the equation for the blood pressure data:\n\\[bp_i = \\beta_0 + \\beta_1 \\cdot age_i + \\beta_2 \\cdot mins\\_exercise_i + \\epsilon_i\\]\nwhere:\n\n\\(bp_i\\) is the blood pressure for the \\(i\\)th observation\n\\(age_i\\) is the age for the \\(i\\)th observation\n\\(mins\\_exercise_i\\) is the minutes of exercise for the \\(i\\)th observation\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the slope for age\n\\(\\beta_2\\) is the slope for minutes of exercise\n\\(\\epsilon_i\\) is the error term\n\nand the error term is assumed to be normally distributed with mean 0 and constant variance, just as was the case for simple linear regression:\n\\[\\epsilon_i \\sim N(0, \\sigma^2)\\]\n\n\nWe already know a lot!\nFirst, the five assumptions of the multiple linear regression model are the same as for the simple linear regression model:\n\nNormality of residuals.\nHomoscedasticity = constant variance of residuals.\nIndependence of residuals.\nLinearity.\nNo outliers.\n\n\n\n\n\n\n\nThink, Pair, Share (#assump-match)\n\n\n\nReview how we can check these assumptions in the simple linear regression model:\nMatch the following to the assumptions above:\n\nGraph of size of residuals vs. fitted values.\nQQ-plot.\nGraph of residuals vs. fitted values.\nGraph of leverage vs. standardized residuals.\n\nAnd what is missing?\n\n\nSecond, we know how to estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) using the method of least squares.\nThird, we know how to test the significance of the parameters using the \\(t\\)-test.\nFourth, we know how to calculate the confidence intervals for the parameters, and to make a confidence band.\nFifth, we know how to calculate the \\(R^2\\) value to measure the goodness of fit of the model.\nSixth, we know how to infer the strength of the relationship between the response variable and an explanatory variable.\nSeventh, we know how to make predictions using the model, and to make a prediction band.\nWhat we don’t know is how to answer the four questions already mentioned above:\n\nQuestion 1: As an ensemble, are the explanatory variables associated with the response?\nQuestion 2: Are each of the explanatory variables associated with the response?\nQuestion 3: What proportion of variability is explained?\nQuestion 4: Are some explanatory variables more important than others?\n\nLet’s answer these questions using the blood pressure example.\n\n\nFitting the model\nIn R, we can fit a multiple linear regression model using the lm() function in a very similar way to the simple linear regression model. Here is the code for the blood pressure example. To fit two explanatory variables, we simply add the second variable to the formula using the + sign:\n\nm1 &lt;- lm(bp ~ age + mins_exercise, data = bp_data)\n\n\n\nChecking the assumptions\nWe can check the assumptions of the multiple linear regression model using the same methods as for the simple linear regression model. Here is the code for the blood pressure example:\n\n# Check the assumptions\npar(mfrow=c(2,2))\nplot(m1, which = c(1,2,3,5), add.smooth = FALSE)\n\n\n\n\nWe see that the assumptions are met for the blood pressure example:\n\nNormality of residuals: The QQ-plot shows that the residuals are normally distributed.\nHomoscedasticity: The scale-location plot shows that the residuals have constant variance.\nIndependence of residuals: No evidence of pattern or clustering. But also need to know about study design to properly assess independence.\nLinearity: The residuals vs. fitted values plot shows no clear pattern in the residuals.\nNo outliers: No points with high leverage or high residuals."
  },
  {
    "objectID": "4-multiple-regression.html#question-1-as-an-ensemble-are-the-explanatory-variables-associated-with-the-response",
    "href": "4-multiple-regression.html#question-1-as-an-ensemble-are-the-explanatory-variables-associated-with-the-response",
    "title": "Multiple regression (L4)",
    "section": "Question 1: As an ensemble, are the explanatory variables associated with the response?",
    "text": "Question 1: As an ensemble, are the explanatory variables associated with the response?\nTo answer this question, we can use the \\(F\\)-test. The null hypothesis is that the residual error variance is not significantly different from what would be expected by chance. The alternative hypothesis is that the residuals error variance is significantly lower than what would be expected by chance.\nThis hypothesis is similar to the null hypothesis that all of the slopes (regression coefficients) are equal to zero. If the null hypothesis is rejected, then at least one of the regression coefficients is significantly different from zero, and we can conclude that the explanatory variables are associated with the response.\nOK, back to the \\(F\\)-test.\n\n\n\n\n\n\nNote\n\n\n\nThe \\(F\\)-test is called the “\\(F\\)-test” because it is based on the \\(F\\)-distribution, which was named after the statistician Sir Ronald A. Fisher. Fisher developed this statistical method as part of his pioneering work in analysis of variance (ANOVA) and other fields of experimental design and statistical inference.\n\n\nKey Points Behind the Name:\n\n\\(F\\)-Distribution: The test statistics of the \\(F\\)-test (that is, the \\(F\\)-statistic) follows the \\(F\\)-distribution under the null hypothesis. This distribution arises when comparing the ratio of two independent sample variances or mean squares.\nRonald Fisher’s Contribution: Fisher introduced the \\(F\\)-distribution in the early 20th century as a way to test hypotheses about the equality of variances and to analyze variance in regression and experimental designs. The “\\(F\\)” in \\(F\\)-distribution honors him.\nVariance Ratio: The test statistic for the \\(F\\)-test is the ratio of two variances (termed mean squares in this case), making the \\(F\\)-distribution the natural choice for modeling this ratio when the null hypothesis is true.\n\nThe \\(F\\)-test is widely used in various contexts, including comparing variances, assessing the significance of regression models, conducting ANOVA to test for differences among group means, and comparing different models.\n\nBut what is the \\(F\\)-statistic?\nThe F-statistic is calculated as the ratio of the mean square error of the model to the mean square error of the residuals.\nHere’s the formula for the F-statistic: \\[\\frac{MSE_{model}}{MSE_{residual}}\\]\nwhere: * \\(MSE_{model}\\) is the mean square error of the model. * \\(MSE_{residual}\\) is the mean square error of the residuals.\nWhat on earth does that mean? Put simply, the F-statistic is a measure of how well the model fits the data. It compares the how much variability (variance) is explained to how much is not explained.\nThe mean square of the model (\\(MSE_{model}\\)) is a measure of how much variability is explained.\nThe mean square of the residuals (\\(MSE_{residual}\\)) is a measure of how much variability is not explained.\n\nIf there are strong relationships in our data, a lot of variance will be explained (large \\(MSE_{model}\\)) and not much will remain unexplained (low \\(MES_{residual}\\)). The \\(F\\)-statistic will be large.\nIf there are weak relationships in our data, not much variance will be explained (low \\(MSE_{model}\\)) and a lot will remain unexplained (large \\(MES_{residual}\\)). The F-statistic will be small.\n\nSo, if the \\(F\\)-statistic is large, we suspect that we can reject the null hypothesis that the explained explained variance is greater than we would expect by chance (i.e., when the slopes are zero). If the \\(F\\)-statistic is small, we suspect that we cannot reject the null hypothesis that the explained variance is no greater than we would expect by chance, and we suspect that we cannot reject the null hypothesis that the slopes are zero.\nThe \\(F\\)-statistic is then compared to the \\(F\\)-distribution with \\(p\\) and \\(n-p-1\\) degrees of freedom, where \\(p\\) is the number of explanatory variables and \\(n\\) is the number of observations.\n\n\nCalculating the F-statistic\nThe mean square error of a model is the sum of squares of the model divided by the degrees of freedom of the model:\n\\[MSE_{model} = \\frac{SSM}{df_{model}}\\]\nwhere \\(SSM\\) is the sum of squares of the model and \\(df_{model}\\) is the degrees of freedom of the model.\nThe mean square error of the residuals is the sum of squares of the residuals divided by the degrees of freedom of the residuals:\n\\[MSE_{residuals} = \\frac{SSE}{df_{residuals}}\\]\nwhere \\(SSE\\) is the sum of squares of the residuals and \\(df_{residuals}\\) is the degrees of freedom of the residuals.\n\nSSM, SSE, and SST\nThis is a recap, since its the same as we learned in the simple linear regression model.\n\\(SST\\): The total sum of squares is the sum of the squared differences between the observed values of the response variable and the mean of the response variable. The \\(SST\\) is a measure of the total variability in the response variable.\nThe formula for the \\(SST\\) is:\n\\[SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\]\nwhere \\(y_i\\) is the observed value of the response variable for the \\(i\\)th observation, \\(\\bar{y}\\) is the mean of the response variable, and \\(n\\) is the number of observations.\n\\(SSE\\): The residual sum of squares (\\(SSE\\)) is the sum of the squared differences between the observed values of the response variable and the predicted values of the response variable. The equation is:\n\\[SSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\]\nwhere \\(y_i\\) is the observed value of the response variable, \\(\\hat{y}_i\\) is the predicted value of the response variable, and \\(n\\) is the number of observations.\n\\(SSM\\): The model sum of squares is the sum of squares explained by the model. The model sum of squares can be calculated as:\n\\[SSM = SST - SSE\\]\nThat is, the model sum of squares is the difference between the total sum of squares and the residual sum of squares.\n\n\nTotal degrees of freedom\nThe total degrees of freedom are the degrees of freedom associated with the total sum of squares (\\(SST\\)).\nIn order to calculate the \\(SST\\), we need to calculate the mean of the response variable. This implies that we estimate one parameter (the mean of the response variable). As a consequence, we lose one degree of freedom and so there remain \\(n-1\\) degrees of freedom associated with the total sum of squares (where \\(n\\) is the number of observations).\nWhat do we mean by “lose one degree of freedom”? Imagine we have ten observations. We can calculate the mean of these ten observations. But if we know the mean and nine of the observations, we can calculate the tenth observation. So, in a sense, once we calculate the mean, the value of one of the ten observations is fixed. This is what we mean by “losing one degree of freedom”. When we calculate and use the mean, one of the observations “loses its freedom”.\nFor example, take the numbers 1, 3, 5, 7, 9. The mean is 5. The sum of the squared differences between the observations and the mean is \\((1-5)^2 + (3-5)^2 + (5-5)^2 + (7-5)^2 + (9-5)^2 = 20\\). This is the total sum of squares. The degrees of freedom are \\(5-1 = 4\\).\nThe total degrees of freedom are the total number of observations minus one. That is, the total sum of squares is associated with \\(n-1\\) degrees of freedom.\nAnother perspective in which to think about the total sum of squares and total degrees of freedom is to consider the intercept only model. The intercept only model is a model that only includes the intercept term. The equation of this model would be:\n\\[y_i = \\beta_0 + \\epsilon_i\\] The sum of the square of the residuals for this model is minimised when the predicted value of the response variable is the mean of the response variable. That is, the least squares estimate of \\(\\beta_0\\) is the mean of the response variable:\n\\[\\hat{\\beta}_0 = \\bar{y}\\]\nHence, the predicted value of the response variable is the mean of the response variable. The equation is:\n\\[\\hat{y}_i = \\bar{y} + \\epsilon_i\\]\nThe error term is therefore:\n\\[\\epsilon_i = y_i - \\bar{y}\\] And the total sum of squares is:\n\\[SST = \\sum_{i=1}^n (y_i - \\bar{y})^2\\]\nwhere \\(\\hat{y}_i\\) is the predicted value of the response variable for the \\(i\\)th observation, \\(\\bar{y}\\) is the mean of the response variable, and \\(\\epsilon_i\\) is the residual for the \\(i\\)th observation.\nThe intercept only model involves estimating only one parameter, so the total degrees of freedom are the total number of observations minus one \\(n - 1\\).\nTherefore, the total degrees of freedom are the total number of observations minus one.\nBottom line: \\(SST\\) is the residual sum of squares when we fit the intercept only model. The total degrees of freedom are the total number of observations minus one.\n\n\nModel degrees of freedom\nThe model degrees of freedom are the degrees of freedom associated with the model sum of squares (\\(SSM\\)).\nIn the case of the intercept only model, we estimated one parameter, the mean of the response variable.\nIn the case of a regression model with one continuous explanatory variable, we estimate one parameter in addition to the intercept. That is, we estimate the slope of the explanatory variable.\nIn the case of a regression model with two continuous explanatory variables, we estimate two parameters in addition to the intercept. That is, we estimate the slopes of the two explanatory variables.\nMore generally, in the case of a regression model with \\(p\\) continuous explanatory variables, we estimate \\(p\\) parameters in addition to the intercept. That is, we estimate the slopes of the \\(p\\) explanatory variables.\nEach time we estimate a new parameter, we lose a degree of freedom.\n\n\n\n\n\n\nThink, Pair, Share (#lost-degrees)\n\n\n\nHow many degrees of freedom are lost when this model is fit?:\n\\[y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i\\]\n\n\nThe model has three parameters: \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\). So, three degrees of freedom are lost. These three degrees of freedom are one for the intercept (\\(\\beta_0\\)) and two for the slopes of each of the two (\\(p\\)) continuous explanatory variables (\\(\\beta_1\\) and \\(\\beta_2\\)).\nIn the case of a regression model with \\(p\\) continuous explanatory variables, we estimate \\(p+1\\) parameters when we include the degree of freedom used by the intercept. But recall that the hypothesis we are testing is that the slopes of the explanatory variables are zero. This means we are interested in the sum of squares explained by the model with the two slopes, compared to the sum of squares explained by the model with only the intercept. Hence the relevant degrees of freedom of the model are the difference between the degrees of freedom of the model with the two slopes and the degrees of freedom of the model with only the intercept. That is, the model degrees of freedom are \\(p\\).\n\n\nResidual degrees of freedom\nThe residual degrees of freedom are the total degrees of freedom (\\(n-1\\)) minus the degrees of freedom used by the regression model (\\(p\\)).\nTherefore, the residual degrees of freedom are the degrees of freedom remaining after we estimate the intercept and the slopes of the explanatory variables. There is one intercept and \\(p\\) slopes, so the residual degrees of freedom are \\(n-1-p\\).\n\n\nFinally…\nWe have everything we need to calculate the two mean squares:\n\\[MSE_{\\text{model}} = \\frac{SSM}{p}\\] \\[MSE_{\\text{residual}} = \\frac{SSE}{n-1-p}\\]\nWhy divide by the degrees of freedom? The more observations we have, the greater will be the total sum of squares. The more observations we have, the greater will be the residual sum of squares. So it is not very informative to compare totals. Rather, we need to compare the mean of the sums of squares. Except we don’t calculate the mean by dividing by the number of observations. Rather we divide by the degrees of freedom. The total mean square is an estimate of the variance of the response variable. And the residual mean square is an estimate of the variance of the residuals.\n\n\n\n\\(SST\\), \\(SSM\\), \\(SSE\\), and degrees of freedom\nJust a reminder and a summary of some of the material above:\n\n\\(SST\\): degrees of freedom = \\(n-1\\)\n\\(SSM\\): degrees of freedom = \\(p\\) *\\(SSE\\): degrees of freedom = \\(n-1-p\\)\n\nThe sum of squares add up:\n\\[SST = SSM + SSE\\]\nand the degrees of freedom add up\n\\[(n-1) = (p) + (n-1-p)\\]\n\n\nIs my \\(F\\)-statistic large or small?\nRecall that “The \\(F\\)-statistic is calculated as the ratio of the mean square error of the model to the mean square error of the residuals.” And that a large \\(F\\)-statistic is evidence against the null hypothesis that the slopes of the explanatory variables are zero. And that a small \\(F\\)-statistic is evidence to not reject the null hypothesis that the slopes of the explanatory variables are zero.\nBut how big does the F-statistic need to be in order to confidently reject the null hypothesis?\nThe null hypothesis that the explained variance of the model is no greater than would be expected by chance. Here, “by chance” means that the slopes of the explanatory variables are zero.\n\\[H_0: \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0\\] The alternative hypothesis is that the explained variance of the model is greater than would be expected by chance. This would occur if the slopes of some or all of the explanatory variables are not zero.\n\\[H_1: \\beta_1 \\neq 0 \\text{ or } \\beta_2 \\neq 0 \\text{ or } \\ldots \\text{ or } \\beta_p \\neq 0\\] To test this hypothesis we are going to, as usual, calculate a \\(p\\)-value. The \\(p\\)-value is the probability of observing a test statistic as or more extreme as the one we observed, assuming the null hypothesis is true. To do this, we need to know the distribution of the test statistic under the null hypothesis. The distribution of the test statistic under the null hypothesis is known as the \\(F\\)-distribution.\nThe \\(F\\)-distribution has two degrees of freedom values associated with it: the degrees of freedom of the model and the degrees of freedom of the residuals. The degrees of freedom of the model are the number of parameters estimated by the model corresponding to the null hypothesis. The degrees of freedom of the residuals are the total degrees of freedom minus the degrees of freedom of the model.\nHere is the \\(F\\)-distribution with 2 and 99 degrees of freedom:\n\n\n\n\n\nThe F-distribution is skewed to the right and has a long tail. The area to the right of 3.89 is shaded in red. This area represents the probability of observing an F-statistic as or more extreme as 3.89, assuming the null hypothesis is true. This probability is the \\(p\\)-value of the hypothesis test.\nThe \\(F\\)-statistic and \\(F\\)-test is briefly recaptured in 3.1.f) of the Stahel script, but see also Mat183 chapter 6.2.5. It uses the fact that\n\\[\\frac{MSE_{model}}{MSE_{residual}} =  \\frac{SSM/p}{SSE/(n-1-p)} \\sim F_{p,n-1-p}\\]\nfollows an \\(F\\)-distribution with \\(p\\) and \\((n-1-p)\\) degrees of freedom, where \\(p\\) are the number of continuous variables, \\(n\\) the number of data points.\n\n\\(SSE=\\sum_{i=1} ^n(y_i-\\hat{y}_i)^2\\) is the residual sum of squares\n\\(SSM = SST - SSE\\) is the sum of squares of the model\n\\(SST=\\sum_{i=1}^n(y_i-\\overline{y})^2\\) is the total sum of squares\n\\(n\\) is the number of data points\n\\(p\\) is the number of explanatory variables in the regression model\n\n\n\nSource of variance table\nThe sources of variance table is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the \\(F\\)-statistic.\n\nSources of variance table\n\n\n\n\n\n\n\n\n\nSource\nSum of squares\nDegrees of freedom\nMean square\nF-statistic\n\n\n\n\nModel\n\\(SSM\\)\n\\(p\\)\n\\(MSE_{model} = SSM / p\\)\n\\(\\frac{MSE_{model}}{MSE_{error}}\\)\n\n\nError\n\\(SSE\\)\n\\(n - 1 - p\\)\n\\(MSE_{error} = SSE / (n - 1 - p)\\)\n\n\n\nTotal\n\\(SST\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\nThe blood pressure example\nHow many observations?\n\n\n[1] 100\n\n\n\n\n\n\n\n\nThink, Pair, Share (#three-degrees)\n\n\n\nWe have two explanatory variables (age and minutes of exercise per week).\n\nHow many total degrees of freedom?\nHow many degrees of freedom for the regression model?\nHow many degrees of freedom for the residuals?\n\n\n\nFit the model in R:\n\nm1 &lt;- lm(bp ~ age + mins_exercise, data = bp_data)\n\nGet the F-statistic:\n\nsummary(m1)$fstatistic\n\n   value    numdf    dendf \n79.80361  2.00000 97.00000 \n\n\nGet the p-value:\n\np_value &lt;- pf(summary(m1)$fstatistic[1],\n              summary(m1)$fstatistic[2],\n              summary(m1)$fstatistic[3],\n              lower.tail = FALSE)\np_value\n\n       value \n3.227666e-21 \n\n\n(The R-function pf calculates the cumulative probability distribution function of the F-distribution.)\nAnd here is how we can get that information from the summary function. Look at the final line of the output:\n\nsummary(m1)\n\n\nCall:\nlm(formula = bp ~ age + mins_exercise, data = bp_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.211  -7.032  -0.853   5.769  33.586 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   88.65837    3.75256  23.626  &lt; 2e-16 ***\nage            0.51764    0.05768   8.974  2.2e-14 ***\nmins_exercise -0.10086    0.01246  -8.094  1.7e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.773 on 97 degrees of freedom\nMultiple R-squared:  0.622, Adjusted R-squared:  0.6142 \nF-statistic:  79.8 on 2 and 97 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "4-multiple-regression.html#question-2-which-variables-are-associated-with-the-response",
    "href": "4-multiple-regression.html#question-2-which-variables-are-associated-with-the-response",
    "title": "Multiple regression (L4)",
    "section": "Question 2: Which variables are associated with the response?",
    "text": "Question 2: Which variables are associated with the response?\nAs we did for simple linear regression, we can perform a \\(t\\)-test for each explanatory variable to determine if it is associated with the response. As before, the null hypothesis for each \\(t\\)-test is that the slope of the explanatory variable is zero. The alternative hypothesis is that the slope of the explanatory variable is not zero.\n\nsummary(m1)$coef\n\n                Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   88.6583701 3.75256098 23.626097 5.081665e-42\nage            0.5176392 0.05767978  8.974362 2.195855e-14\nmins_exercise -0.1008620 0.01246203 -8.093547 1.697396e-12\n\n\nAgain, a 95% CI for each slope estimate \\(\\hat\\beta_j\\) can be calculated with R:\n\n\nconfint(m1)\n\n                   2.5 %      97.5 %\n(Intercept)   81.2105754 96.10616493\nage            0.4031609  0.63211765\nmins_exercise -0.1255957 -0.07612834\n\n\nReminder: The 95% confidence interval is \\([\\hat\\beta - c \\cdot \\sigma^{(\\beta)} ; \\hat\\beta + c \\cdot \\sigma^{(\\beta)}]\\), where \\(c\\) is the 97.5% quantile of the \\(t\\)-distribution with \\(n-p\\) degrees of freedom).\nWe can also use the tbl_regression function within the gtsummary package to get a publication ready table of the coefficients and confidence intervals:\n\nlibrary(gtsummary)\ntbl_regression(m1)\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    age\n0.52\n0.40, 0.63\n&lt;0.001\n    mins_exercise\n-0.10\n-0.13, -0.08\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nHowever Please insert a note into your brain that we are dealing here with an ideal case of uncorrelated explanatory variables. You’ll learn later in the course about what happens when explanatory variables are correlated."
  },
  {
    "objectID": "4-multiple-regression.html#question-3-what-proportion-of-variability-is-explained",
    "href": "4-multiple-regression.html#question-3-what-proportion-of-variability-is-explained",
    "title": "Multiple regression (L4)",
    "section": "Question 3: What proportion of variability is explained?",
    "text": "Question 3: What proportion of variability is explained?\n\nMultiple \\(R^2\\)\nWe can calculate the \\(R^2\\) value for the multiple linear regression model just like we already did for a simple linear regression model. The \\(R^2\\) value is the proportion of variability in the response variable that is explained by the model. As before, the \\(R^2\\) value ranges from 0 to 1, where 0 indicates that the model does not explain any variability in the response variable, and 1 indicates that the model explains all the variability in the response variable.\nFor multiple linear regression, we often use the term “multiple \\(R^2\\)” to distinguish it from the \\(R^2\\) value for simple linear regression. The multiple \\(R^2\\) is the proportion of variability in the response variable that is explained by the model, taking into account all the explanatory variables in the model.\nAs before, for simple linear regression, the multiple \\(R^2\\) value is calculated as the sum of squares explained by the model divided by the total sum of squares:\n\\[R^2 = \\frac{SSM}{SST}\\]\nwhere \\(SSM\\) is the sum of squares explained by the model and \\(SST\\) is the total sum of squares, and \\(SSM = SST - SSE\\).\nFor the blood pressure data:\n\n\n[1] 0.6219904\n\n\n\\(R^2\\) for multiple linear regression can also be calculated as the squared correlation between \\((y_1,\\ldots,y_n)\\) and \\((\\hat{y}_1,\\ldots,\\hat{y}_n)\\), where the \\(\\hat y\\) are the fitted values from the model. The fitted values are calculated as:\n\\[\\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x^{(1)} + \\ldots + \\hat\\beta_m x^{(m)}\\]\nIn R:\n\nr_squared &lt;- cor(m1$fitted.values, bp_data$bp)^2\nr_squared\n\n[1] 0.6219904\n\n\nOr:\n\nsss &lt;- anova(m1)\nSSM &lt;- sss$`Sum Sq`[1] + sss$`Sum Sq`[2]\nSST &lt;- sum(sss$`Sum Sq`)\nR_squared &lt;- SSM / SST\nR_squared\n\n[1] 0.6219904\n\n\n\n\nAdjusted \\(R^2\\)\nHowever, we have a little problem to address. The \\(R^2\\) value increases as we add more explanatory variables to the model, even if the additional variables are not associated with the response. This is because the \\(R^2\\) value is calculated as the proportion of variability in the response variable that is explained by the model. As we add more explanatory variables to the model, the model will always explain more variability in the response variable, even if the additional variables are not associated with the response. Some of the variance will be explained by chance.\nHere is an example of this problem. First, here’s the explanatory power of the model with only age and minutes of exercise as the explanatory variables:\n\n\n[1] 0.6219904\n\n\nNow, we can add a new explanatory variable to the blood pressure model that is not associated with the response:\n\nbp_data$random_variable &lt;- rnorm(nrow(bp_data))\nm2 &lt;- lm(bp ~ age + mins_exercise + random_variable, data = bp_data)\nsummary(m2)$r.squared\n\n[1] 0.6219905\n\n\nThe \\(R^2\\) value for the model with the random variable is higher than the \\(R^2\\) value for the model without the random variable. This is because the model with the random variable explains more variability in the response variable, even though the random variable is not associated with the response.\nTo address this problem, we can use the adjusted \\(R^2\\) value. The adjusted \\(R^2\\) value is calculated as:\n\\[R^2_{\\text{adj}} = 1 - \\frac{SSE / (n - p - 1)}{SST / (n - 1)}\\]\nwhere * \\(SSE\\) is the sum of squared errors * \\(SST\\) is the total sum of squares * \\(n\\) is the number of observations * \\(p\\) is the number of explanatory variables in the model.\nOr put another way:\n\\[R^2_{adj} = 1-(1-R^2 )\\frac{n-1}{n-p-1}\\] In this form, we can see that as \\(p\\) increases (as we add explanatory variables) the term \\((n-1)/(n-p-1)\\) increases, and the adjusted \\(R^2\\) value will decrease if the additional variables are not associated with the response.\nTake home: when we want to compare the explanatory power of models that differ in the number of explanatory variables, we should use the adjusted \\(R^2\\) value."
  },
  {
    "objectID": "4-multiple-regression.html#question-4-are-some-explanatory-variables-more-important-than-others",
    "href": "4-multiple-regression.html#question-4-are-some-explanatory-variables-more-important-than-others",
    "title": "Multiple regression (L4)",
    "section": "Question 4: Are some explanatory variables more important than others?",
    "text": "Question 4: Are some explanatory variables more important than others?\nHow important are the explanatory variables and how important are they relative to each other?\n\n\n\n\n\n\nThink, Pair, Share (#variable-importance)\n\n\n\nHow might we assess how important is each of the explanatory variables, and how important they are relative to each other?\n\n\nThe importance of an explanatory variable can be assessed by looking at the size of the coefficient for that variable. The larger the coefficient, the more important the variable is in explaining the response variable.\nIt is, however, important to remember that the size of the coefficient depends on the scale of the explanatory variable. If the explanatory variables are on different scales, then the coefficients will be on different scales and cannot be directly compared.\nIn our example, the age variable is measured in years, so the coefficient is in units mmHg (pressure) per year. The mins_exercise variable is measured in minutes, so the coefficient is in units mmHg per minute. The coefficients are on different scales and cannot be directly compared. Furthermore, the value of the coefficients would change if we measured age in months or minutes of exercise in hours.\nThere are other perspectives we can take when we’re assessing importance. For example, we cannot change our age, but we can change the number of minutes of exercise. So, the practical importance of the two variables is quite different in that sense also.\nTo compare the importance of the explanatory variables that are measured on different scales, we can standardize the variables before fitting the model. This means that we subtract the mean of the variable and divide by the standard deviation. This puts all the variables on the same scale, so the coefficients can be directly compared. The coefficients are then in units of the response variable per standard deviation of the explanatory variable.\nHowever, the coefficients are then not in the original units of the explanatory variables, so it is not always easy to interpret the coefficients. So while we can compare the coefficients, they have lost a bit of their original meaning and are not so easy to interpret.\nOne way to relate the coefficients in this case is to realise that to compensate for the blood pressure increase associated with one year of age, one would need to exercise for a certain number of minutes more.\n\n\n\n\n\n\nThink, Pair, Share (#exercise-age)\n\n\n\nHow many minutes of exercise per week would we need to add to our fitness schedule to compensate for the blood pressure increase associated with one year of age?"
  },
  {
    "objectID": "4-multiple-regression.html#an-elephant-in-the-room-collinearity",
    "href": "4-multiple-regression.html#an-elephant-in-the-room-collinearity",
    "title": "Multiple regression (L4)",
    "section": "An elephant in the room: collinearity",
    "text": "An elephant in the room: collinearity\nIn the blood pressure data there is no evidence of correlation (collinearity) between the explanatory variables. Recall that we can check for correlation between the explanatory variables by making a scatter plot of the two explanatory variables.\n\n\n\n\n\n\n\n\n\nThe lack of pattern is good, because with no evidence of collinearity between the explanatory variables the model coefficients are stable and easy to interpret. They are stable in the sense that including or excluding one of the explanatory variables does not change the coefficients of the other variables much.\nIf there is correlation between the explanatory variables, then the model coefficients can be unstable and difficult to interpret. This is because the model cannot distinguish between the effects of the correlated variables.\nHere is a new version of the blood pressure data in which the age and minutes of exercise variables are correlated:\n Download the dataset. \n\n\n\n\n\n\n\n\n\nNow we fit a multiple linear regression model to the correlated data:\n\n\n                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   114.3902120 15.3868489  7.434284 4.187345e-11\nage             0.3433312  0.1708701  2.009310 4.728268e-02\nmins_exercise  -0.2277340  0.1469848 -1.549371 1.245493e-01\n\n\nThere is a positive effect of age on blood pressure, and a negative effect of minutes of exercise. The age coefficient is just significant, with \\(p = 0.473\\). The minutes of exercise coefficient is not significant, with \\(p = 0.125\\).\nHere is the model with only age included:\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 91.551025 4.44272391 20.606958 2.124161e-37\nage          0.574799 0.08352313  6.881914 5.627696e-10\n\n\nThis gives quite different results for age. The coefficient is much larger (0.57 compared to 0.34) and it is very very unlikely that the observed relationship could have occurred if the null hypothesis were true (\\(p &lt; 0.001\\)).\nAnd here is the model with only minutes of exercise:\n\n\n                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   144.3629954 3.83158396 37.677106 4.104736e-60\nmins_exercise  -0.4859533 0.07243686 -6.708646 1.273477e-09\n\n\nThe same thing happens again. The coefficient for exercise is larger (-0.49 compared to -0.23) and it is very very unlikely that the observed relationship could have occurred if the null hypothesis were true (\\(p &lt; 0.001\\)).\nWhen each of the explanatory variables is included in the model separately, the coefficients are quite different from when both variables are included in the model. And when they are included in the model separately, their p-values are very low (reject null hypothesis of slope of zero), but when they are included in the model together, their p-values are much larger and give quite weak or no evidence to reject the null hypothesis of slope of zero.\nThese phenomena occur due to the correlation/collinearity between the explanatory variables. The model cannot distinguish between the effects of the correlated variables, so the coefficients are unstable and difficult to interpret.\nUnfortunately, collinearity is a common feature of real data, and it can make the interpretation of multiple linear regression models difficult. It is sometimes possible to design observational studies to reduced or avoid collinearity, but in practice it is often difficult to escape from.\nIn designed experiments, it is standard to make the design of the experiments so that explanatory variables are completely independent (zero collinearity). A fully-factorial design with balanced replication is a good way to ensure that explanatory variables are independent. Two-way analysis of variance is then an appropriate statistical model. We will look at this design and analysis in a later lecture.\nCollinearity also affects the interpretation of the \\(R^2\\) values. Collinearity will cause the collinear explanatory variables to share some of the explained variance. The \\(R^2\\) value of the multiple regression will then be less than the sum of the \\(R^2\\) values of the individual regressions of the response variable on each of the explanatory variables separately.\n\\(R^2\\) of the age only model:\n\nsummary(m2_age)$r.squared\n\n[1] 0.3258152\n\n\n\\(R^2\\) of the mins_exercise only model:\n\nsummary(m2_mins_exercise)$r.squared\n\n[1] 0.3147137\n\n\n\\(R^2\\) of the model with both age and mins_exercise:\n\nsummary(m2_both)$r.squared\n\n[1] 0.3420969\n\n\nIn this case the two explanatory variables are strongly correlated and so share a lot of the explained variance. The \\(R^2\\) value of the model with both explanatory variables is much less than the sum of the \\(R^2\\) values of the models with each explanatory variable separately. In fact, either of the models with only one explanatory variable is nearly as good as the model with both explanatory variables. We don’t gain much from including another explanatory variable in the model when we already include one explanatory variable that is strongly correlated with the other."
  },
  {
    "objectID": "4-multiple-regression.html#recap",
    "href": "4-multiple-regression.html#recap",
    "title": "Multiple regression (L4)",
    "section": "Recap",
    "text": "Recap\nSimple regression:\n\nHow well does the model describe the data: Correlation and \\(R^2\\)\nAre the parameter estimates compatible with some specific value (\\(t\\)-test)?\nWhat range of parameters values are compatible with the data (confidence intervals)?\nWhat regression lines are compatible with the data (confidence band)?\nWhat are plausible values of other data (prediction band)?\n\nMultiple regression:\n\nMultiple linear regression \\(x_1\\), \\(x_2\\), , \\(x_m\\)\nChecking assumptions.\n\\(R^2\\) in multiple linear regression\n\\(t\\)-tests, \\(F\\)-tests and \\(p\\)-values"
  },
  {
    "objectID": "5-categorical-explanatory-variables.html#introduction",
    "href": "5-categorical-explanatory-variables.html#introduction",
    "title": "Categorical explanatory variables (L5)",
    "section": "Introduction",
    "text": "Introduction\nWe so far only considered continuous explanatory variables. Today we will look at the following topics:\n\nBinary explanatory variables.\nCategorical explanatory variables.\n\n\nThe good news\nThe good news is that the mathematical model of a linear regression does not change when we include binary or categorical explanatory variables. The model is still:\n\\[y_i = \\beta_0 + \\beta_1 x^{(1)}_i + \\beta_2 x^{(2)}_i + \\ldots + \\epsilon_i\\]\nThis means that things like the \\(R^2\\), \\(F\\)-test, \\(t\\)-test, confidence intervals, etc. are still applicable. We also carry out model diagnostics mostly in the same way. So a lot of what you already learned is useful here. I suppose this is a good thing, since you don’t have to learn it again. But it also means that you have to have learned and understood it by now. If you haven’t, please reach out for assistance.\n\n\nThe bad news\nThere isn’t any."
  },
  {
    "objectID": "5-categorical-explanatory-variables.html#binary-explanatory-variables",
    "href": "5-categorical-explanatory-variables.html#binary-explanatory-variables",
    "title": "Categorical explanatory variables (L5)",
    "section": "Binary explanatory variables",
    "text": "Binary explanatory variables\nImagine the question “do people who smoke have higher blood pressure?” Though we could quantify the amount of smoking, we could also just ask if someone smokes or not. This is a binary variable. We can put that information into a variable \\(x\\) where values \\(x_i\\) can take on the values 0 or 1, where 0 denotes non-smoker and 1 denotes smoker. The linear model is then:\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\] Huh? This is exactly the same model as when the explanatory variable \\(x\\) is continuous. The only difference is that \\(x_i\\) can only be a zero or a one. The interpretation of the \\(\\beta_1\\) is also the same: it is the change in the response variable \\(y\\) when \\(x\\) changes by one unit. In this case, the change is from non-smoker to smoker.\n\n\n\n\n\n\nThink, Pair, Share (#bp-nonsmoker)\n\n\n\nWhat is the predicted value of \\(y\\) for a non-smoker? And for a smoker?\n\n\nThe \\(\\beta_1\\) can be thought of as the effect on blood pressure of smoking. If \\(\\beta_1\\) is positive, smoking increases blood pressure. If \\(\\beta_1\\) is negative, smoking decreases blood pressure.\nFor the question of whether smoking influences blood pressure we can formulate a null and an alternative hypothesis.\n\nNull hypothesis: \\(\\beta_1 = 0\\). That is, smoking has no effect on blood pressure.\nAlternative hypothesis: \\(\\beta_1 \\neq 0\\). That is, smoking has an effect on blood pressure.\n\n\n\n\n\n\n\nThink, Pair, Share (#null-test)\n\n\n\nWhat statistical test can we use to test the null hypothesis here?\n\n\nLet’s take a graphical view of the case of a binary response variable.\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#draw-coefficients)\n\n\n\nDraw on the graph a representation of the model \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_0 + \\beta_1\\).\n\n\nHere is a graph showing the mean of the two groups (non-smoker and smoker) and a vertical line for the difference between the two means:\n\n\n\n\n\n\nExample: Smoking and blood pressure\nLet’s do a real example. We have data on blood pressure and smoking status. We want to know if smoking has an effect on blood pressure.\nTo fit the linear model in R, we use the lm function as before. The only difference is that the explanatory variable is binary. But we do not need to tell this to R. It can handle things just fine. So, the code to fit the model is:\n\nmod_smoking &lt;- lm(bp ~ smoke_01, data = bp_data_smoking)\n\nOf course, we next look at the model diagnostic plots to see if the model assumptions are met.\n\n\n\n\n\nThe QQ-plot shows that the residuals are close enough to the normal distribution.\nThe other plots are a little different from before, because we have only two fitted, one for non-smokers and one for smokers. But we can still see that the residuals show no clear difference between the two groups. Also there is no leverage plot, due to all leverages being equal (all data points are equal distance from \\(\\hat{x}\\))\nHere is how it could have looked if smoking also increased the variability in the blood pressure among individuals:\n\n\n\n\n\nIt is clear that not only does smoking increase the mean blood pressure, but it also increases the inter-individuals (between individual) variability in blood pressure.\n\n\n\n\n\nWe can see in the residuals vs. fitted plot that the residuals of the non-smokers are less spread out than the residuals of the smokers. This is a sign of heteroscedasticity (difference in variance among groups).\n\n\nDoes smoking have an effect on blood pressure?\nWe address this question by testing the null hypothesis of no effect of smoking on blood pressure? This null hypothesis corresponds to \\(\\beta_1 = 0\\). This is the same as testing if the slope of the line is different from zero, for which we use a \\(t\\)-test.\nHere is the table of estimates and the \\(t\\)-test:\n\n\n\n\n\n\n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    smoke_01\n14\n13, 15\n&lt;0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nOr for more information:\n\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 80.85535  0.3536468 228.63305 1.448104e-74\nsmoke_01    14.30890  0.5001321  28.61023 8.198275e-32\n\n\nThere is very clear evidence that smoking increases blood pressure. The \\(p\\)-value is very small.\n\n\nReporting our findings\nAn appropriate graph would be a scatter plot of blood pressure against smoking status, just like we made above. You may see graphs of such data that are bar graphs with error bars. This is not a good way to present the data, because it hides the individual data points and their distribution. It is better to show the individual data points. If there are very many data points, however, a box and whisker plot may be more informative.\n\n\n\n\n\nAn appropriate sentence to write about our results would be. “We found that smoking increases blood pressure by 15 units (95% CI: 14 to 17 units, \\(t = 25.8\\), \\(p &lt; 0.001\\), \\(df = 48\\)).” This sentence gives the effect size, the confidence interval, and the \\(t\\)-test results. It is a sentence about the the effect of smoking.\nNot so good would be to write: “There was a statistically significant effect of smoking on blood pressure (\\(t = 25.8\\), \\(p &lt; 0.001\\), \\(df = 48\\)).” – This is not informative; it does not even give the direction of the effect. It is a sentence about statistical significance..\nGenerally speaking, put the focus on the effect/relationship, not on the statistical significance.\n\n\nWords instead of 0/1\nIn the smoking explanatory variable we denoted a non-smoker with a 0 and a smoker with a 1. Could we instead have denoted a non-smoker with the word “non-smoker” and a smoker with the word “smoker”? Lets see how the lm function feels about that.\n\n\n        bp     status\n1 83.30404 non-smoker\n2 76.67931 non-smoker\n3 80.79058 non-smoker\n4 96.60230     smoker\n5 80.10957 non-smoker\n6 83.63842 non-smoker\n\n\nThe data now has the variable that describes smoker status as a word. Here’s a graph of that data:\n\n\n\n\n\nVery nice. Now we can fit the model:\n\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)  80.85535  0.3536468 228.63305 1.448104e-74\nstatussmoker 14.30890  0.5001321  28.61023 8.198275e-32\n\n\nThe numbers in this table are exactly the same as before, when we used the 0/1 coding. The lm function is pretty smart. It recodes the two levels (non-smoker and smoker) in the status variable to be a binary variable. The summary function then gives the estimates for the two levels.\nWe can also get the \\(R^2\\) value, and it has the same meaning and is calculated in the same way.\n\n\n[1] 0.9446076\n\n\nSo, a linear model with a binary explanatory variable very similiar to a linear model with a continuous explanatory variable. A difference is that the estimates are not slopes, but differences in means between the two groups.\n\n\n\n\n\n\nNote\n\n\n\nA linear model with a binary explanatory variable is the same as a two-sample \\(t\\)-test:\n\n\n\n    Two Sample t-test\n\ndata:  bp by status\nt = -28.61, df = 48, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group non-smoker and group smoker is not equal to 0\n95 percent confidence interval:\n -15.31448 -13.30331\nsample estimates:\nmean in group non-smoker     mean in group smoker \n                80.85535                 95.16424 \n\n\nThe value of \\(t\\)-statistic is the same as exactly the value of the \\(t\\)-statistic in the linear output. It is exactly the same test.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may hear the term “binary regression” or “logistic regression” in the future. This is a different type of regression model that is used when the response variable is binary. The model we are talking about here is called “linear regression with binary explanatory variables”.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn all that went above, we assumed that observations were independent. I.e., we assumed there were 50 truly independent observation. This is a very strong assumption. In reality, we might have made more than one measure of blood pressure from one individual. This is called “clustering” or “repeated measures”. A consequence would be that we not then have 50 truly independent observation, we would have fewer than 50, and so we would have fewer than 48 degrees of freedom for error. We will talk about this again later in the course."
  },
  {
    "objectID": "5-categorical-explanatory-variables.html#categorical-explanatory-variables",
    "href": "5-categorical-explanatory-variables.html#categorical-explanatory-variables",
    "title": "Categorical explanatory variables (L5)",
    "section": "Categorical explanatory variables",
    "text": "Categorical explanatory variables\nImage we have a new question: “Is a person’s blood pressure related to their diet?” We have data on the diet of 50 people and we have coded their diets as: “meat heavy”, “Mediterranean”, “vegetarian”, and “vegan”.\nHere is an example dataset:\n\n\n# A tibble: 6 × 3\n  diet             bp person_ID\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;    \n1 meat heavy      120 person_1 \n2 vegan            89 person_2 \n3 vegetarian       86 person_3 \n4 meat heavy      116 person_4 \n5 Mediterranean   115 person_5 \n6 meat heavy      134 person_6 \n\n\nAnd here’s a graph of the dataset:\n\n\n\n\n\nWe can see quite a lot from this, for example that the “meat heavy” diet is associated with higher blood pressure.\nBut how can a linear model be used with such an explanatory variable?\n\nDummy variables\nThe trick is to convert the categorical variable into a set of binary variables. For a categorical variable with \\(k\\) levels, we create \\(k - 1\\) binary variables \\(x_i^{(j)}\\).\nWe already did this with the smoking status variable. It contained two levels, “non-smoker” and “smoker”. We created a binary variable \\(x_i^{(1)}\\) that was 1 if the \\(i\\)-th person was a smoker and 0 if they were a non-smoker. That is, the categorical variable smoking status had \\(k = 2\\) levels which were recoded into \\(k - 1 = 1\\) binary variables.\nFor the diet variable, we have \\(k = 4\\) levels. So we should create \\(k - 1 = 3\\) binary variables. We create one less than the number of levels because the last level is the “baseline” level. The baseline level is the level that the other levels are compared to. In this case, the baseline level is “meat heavy”. The other levels are compared to “meat heavy”.\nAnother way to think about why we need one less binary variable than the number of levels is that when meat_heavy = 0 & Mediterranean = 0 & vegetarian = 0, we there is no other possibility than the person has a vegan diet. We infer the vegan diet from a process of elimination of the other possibilities. This is why we only need three binary variables, and more generally only need \\(k - 1\\) binary variables for a categorical variable with \\(k\\) levels.\nHere is the dataset with four binary variables:\n\n\n# A tibble: 6 × 7\n  diet             bp person_ID meat_heavy Mediterranean vegetarian vegan\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 meat heavy      120 person_1           1             0          0     0\n2 vegan            89 person_2           0             0          0     1\n3 vegetarian       86 person_3           0             0          1     0\n4 meat heavy      116 person_4           1             0          0     0\n5 Mediterranean   115 person_5           0             1          0     0\n6 meat heavy      134 person_6           1             0          0     0\n\n\nAnd here it is with three:\n\n\n# A tibble: 6 × 6\n  diet             bp person_ID Mediterranean vegetarian vegan\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1 meat heavy      120 person_1              0          0     0\n2 vegan            89 person_2              0          0     1\n3 vegetarian       86 person_3              0          1     0\n4 meat heavy      116 person_4              0          0     0\n5 Mediterranean   115 person_5              1          0     0\n6 meat heavy      134 person_6              0          0     0\n\n\nThe point is that the second table with one less binary variable has exactly the same information as the first table with four binary variables. It has the same information because we know that there are only four possible diets in the dataset.\nNow we can use the following linear model, which has three binary explanatory variables:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{(1)} + \\beta_2 x_i^{(2)} + \\beta_3 x_i^{(3)} + \\epsilon_i\\]\nwhere \\(x_i^{(1)}\\) is the binary variable for the Mediterranean diet, \\(x_i^{(2)}\\) is the binary variable for the vegetarian diet, and \\(x_i^{(3)}\\) is the binary variable for the vegan diet.\n\n\n\n\n\n\nThink, Pair, Share (#beta0-meaning)\n\n\n\nQuestion: What is the interpretation of \\(\\beta_0\\) in this model?\n\n\nIt is the expected value of the response variable when all the explanatory variables are zero. I.e., it is the intercept. In this case, it is the expected blood pressure of a person on a meat heavy diet because the meat heavy diet is the baseline diet.\nMathematically, \\(\\hat{y} = \\beta_0\\) when \\(x_i^{(1)} = x_i^{(2)} = x_i^{(3)} = 0\\). This is the case when the person is on a meat heavy diet, because \\(x_i^{(1)} = 0\\) means not on a Mediterranean diet, \\(x_i^{(2)} = 0\\) means not on a vegetarian diet, and \\(x_i^{(3)} = 0\\) means not on a vegan diet.\n\n\n\n\n\n\nThink, Pair, Share (#beta1-meaning)\n\n\n\nWhat is the interpretation of \\(\\beta_1\\) in this model?\n\n\nRecall the meaning of the \\(\\beta_j\\) coefficients in the model of smoking effects on blood pressure. It was the expected difference in blood pressure between a smoker and a non-smoker. In this model, \\(\\beta_1\\) is the expected difference in blood pressure between a person on a Mediterranean diet and a person on a meat heavy diet.\nQuestion: What is the interpretation of \\(\\beta_2\\) in this model?\nAnswer: \\(\\beta_2\\) is the expected difference in blood pressure between a person on a vegetarian diet and a person on a meat heavy diet.\nQuestion: What is the interpretation of \\(\\beta_3\\) in this model?\nAnswer: \\(\\beta_3\\) is the expected difference in blood pressure between a person on a vegan diet and a person on a meat heavy diet.\nSo, each of the three coefficients \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) is the expected difference in blood pressure between a person on a particular diet and a person on the meat heavy diet. Meat heavy is the reference / intercept diet.\n\n\nIn a visualisation\nHere is a visualisation of the model:\n\n\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\nDegrees of freedom of a categorical variable\nWhen we fit a single categorical variable to the data we are fitting a model with \\(k\\) levels. However, the degrees of freedom of a categorical variable with \\(k\\) levels is \\(k-1\\). This is because one degree of freedom corresponds to the intercept, and the remaining \\(k-1\\) degrees of freedom correspond to the \\(k-1\\) coefficients of the \\(k-1\\) dummy variables.\nSo, generally speaking, the degrees of freedom used up a categorical variable with \\(k\\) levels is \\(k-1\\).\n\n\nDoing it in R\nIn R, we can fit a linear model with a categorical variable using the lm() function. And we don’t need to make the dummy binary variables. That is all very conveniently done in the inner workings of the lm() function. Here we go:\n\nbp_model_diet &lt;- lm(bp ~ diet, data = bp_data_diet)\n\nNo news is good news!\nCheck the diagnostic plots to assess if the model assumptions are met.\n\n\n\n\n\nNo patterns in the residuals, QQ-plot data close to the line, and the residuals are homoscedastic. No evidence of outliers. The model assumptions are met.\nLet’s look at the estimates of the coefficients and their \\(t\\)-statistics and \\(p\\)-values.\n\n\n                   Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)       122.62500   2.302393 53.259813 5.494274e-43\ndietMediterranean -12.68750   3.256075 -3.896563 3.144677e-04\ndietvegan         -26.76786   4.173441 -6.413858 6.922791e-08\ndietvegetarian    -23.62500   3.607156 -6.549481 4.328870e-08\n\n\n\n\n\n\n\n\nThink, Pair, Share (#beta-conclusions)\n\n\n\nWhat can we conclude from the values of the three \\(\\beta\\) coefficients?\n\n\nThe three non-intercept coefficients are the \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) coefficients. The value of each is negative, which means that the blood pressure is lower on the Mediterranean, vegetarian, and vegan diets compared to the meat heavy diet.\nThe \\(t\\)-statistics and \\(p\\)-values are also given. The \\(p\\)-values are all less than 0.05, so we can reject the null hypothesis that the coefficients are zero. This means that the blood pressure is significantly lower on the Mediterranean, vegetarian, and vegan diets compared to the meat heavy diet.\nJust for fun, let’s fit the model using the three binary dummy variables:\n\nbp_model_diet_bin &lt;- lm(bp ~ Mediterranean + vegetarian + vegan, data = bp_data_diet)\nsummary(bp_model_diet_bin)$coefficients\n\n               Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)   122.62500   2.302393 53.259813 5.494274e-43\nMediterranean -12.68750   3.256075 -3.896563 3.144677e-04\nvegetarian    -23.62500   3.607156 -6.549481 4.328870e-08\nvegan         -26.76786   4.173441 -6.413858 6.922791e-08\n\n\nExactly the same :)\nAnd for even more fun, lets see what happens if we fit a model with four binary variables, one for each diet.\n\nbp_data_diet_bin &lt;- bp_data_diet %&gt;%\n  select(bp, meat_heavy, Mediterranean, vegetarian, vegan)\nbp_model_diet_bin_all &lt;- lm(bp ~ - Mediterranean + vegetarian + vegan + meat_heavy,\n                            data = bp_data_diet_bin)\nsummary(bp_model_diet_bin_all)$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 109.93750   2.302393 47.749241 7.652542e-41\nvegetarian  -10.93750   3.607156 -3.032167 3.980698e-03\nvegan       -14.08036   4.173441 -3.373800 1.513076e-03\nmeat_heavy   12.68750   3.256075  3.896563 3.144677e-04\n\n\nThe lm function has dropped the meat_heavy variable because it is perfectly collinear with the other three variables. This is because the sum of the four binary variables is always 1. This is called the dummy variable trap. The lm function is smart enough to drop one of the variables to avoid the trap.\n\n\nIs diet important for blood pressure?\nWhen we want to answer the question “is diet important for blood pressure” we likely want to understand if there are any differences between diets. This is very similar to when we asked in multiple regression if any of the slopes are different from zero. Recall that then we used an \\(F\\)-test. We can do the same here.\nThe null hypothesis is that all the coefficients are zero, which means that the diet has no effect on blood pressure. The alternative hypothesis is that at least one of the coefficients is different from zero, which means that at least one diet has an effect on blood pressure.\nAnother way to express the null hypothesis is that diet does not explain a significant amount of the variance in blood pressure. The alternative hypothesis is that diet explains a significant amount of the variance in blood pressure.\nVariance explained by diet is quantified by the sum of squares associated with the diet variable. This is the model sum of squares, SSM.\nAs before, the \\(F\\)-statistic is the ratio the mean square of the model to the mean square of the residuals:\n\\[F = \\frac{MSE_{model}}{MSE_{residual}} = \\frac{SSM/(k-1)}{SSE/(n -1 - (k -1))}\\]\nwhere \\(k\\) is the number of levels of the categorical variable and \\(n\\) is the number of observations.\n\n\n\n\n\n\nNote\n\n\n\n\\((n - 1)\\) is the total number of degrees of freedom in the model. \\((k - 1)\\) is the number of degrees of freedom used up by the categorical variable. \\((n - 1 - (k - 1))\\) is the number of degrees of freedom left over for the residuals. This simplifies to \\(n - k\\).\n\n\nThe \\(F\\)-statistic is compared to the \\(F\\)-distribution with \\(k-1\\) and \\((n -1 - (k -1)) = n-k\\)) degrees of freedom.\nLet’s do the \\(F\\)-test for the diet variable:\n\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe \\(p\\)-value is less than 0.05, so we can reject the null hypothesis that diet has no effect on blood pressure.\n\n\n\n\n\n\nThink, Pair, Share (#table-rsquared)\n\n\n\nCalculate the \\(R^2\\) for the model from the ANOVA table.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLet’s use the lovely gtsummary package to summarize the model.\n\n\n\n\nANOVA Table: Linear Model Analysis\n\n\nTerm\nDF\nSum of Squares\nMean Square\nF Statistic\nP Value\n\n\n\n\ndiet\n3\n5274.235\n1758.078\n20.728\n0\n\n\nResiduals\n46\n3901.545\n84.816\nNA\nNA"
  },
  {
    "objectID": "5-categorical-explanatory-variables.html#additional-reading",
    "href": "5-categorical-explanatory-variables.html#additional-reading",
    "title": "Categorical explanatory variables (L5)",
    "section": "Additional reading",
    "text": "Additional reading\nIf you’d like to read another perspective please feel free to take a look at Chapters 3.2u-x, 3.3, 4.1-4.5 in Lineare Regression."
  },
  {
    "objectID": "6-interactions.html#introduction",
    "href": "6-interactions.html#introduction",
    "title": "Interactions (L5)",
    "section": "Introduction",
    "text": "Introduction\nInteractions are some of the most interesting phenomena in science, including biology. We are not talking about interactions between species, like predation, though these are also very interesting. We are talking about effects of one thing, like diet, depending on another thing, like exercise. Lets break that down a bit…\nImagine we make a study of the effect of exercise (minutes per week) on blood pressure for people with a meat heavy diet. We might find the following:\n\n\n         bp mins_per_week       diet\n1  94.12854      53.10173 meat heavy\n2  90.99957      74.42478 meat heavy\n3  73.83541     114.57067 meat heavy\n4  77.05434     181.64156 meat heavy\n5 100.14578      40.33639 meat heavy\n6  95.61900     179.67794 meat heavy\n\n\n\n\n\n\n\nWe see that exercise seems to lower blood pressure. But what if we look at the effect of exercise on blood pressure for people with a vegetarian diet?\n\n\n         bp mins_per_week       diet\n1  77.56704     122.92899 vegetarian\n2  84.30684     111.43191 vegetarian\n3  75.23323      65.75546 vegetarian\n4  85.67402      90.62629 vegetarian\n5  77.45327     100.08819 vegetarian\n6 102.31114      36.17327 vegetarian\n\n\n\n\n\n\n\nWe see that exercise seems to lower blood pressure for vegetarians too, but the effect seems to be weaker.\nTo summarise this finding, we can say that the effect of exercise on blood pressure is stronger for people with a meat heavy diet than for people with a vegetarian diet. This means that the effect of exercise on blood pressure depends on diet.\nThis is very clear when we look at the both diets in the same graph:\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#general-diet)\n\n\n\nCan we say anything general about the effect of diet on blood pressure?\n\n\nNo, we can’t. We cannot, for example, state that a vegetarian diet lowers blood pressure. We can say, however, that a vegatarian diet lowers blood pressure of people that do little exercise.\n\n\n\n\n\n\nThink, Pair, Share (#general-exercise)\n\n\n\nCan we say anything general about the effect of exercise on blood pressure?\n\n\nWell, we can say that exercise lowers blood pressure, but we have to be careful. We have to say that exercise lowers blood pressure of people with a vegetarian diet more than of people with a meat heavy diet.\nI think it is clear that the interaction was easier to see when we plotted all the data in one graph… it is much easier to visually compare the slopes of the two regression lines when they are on the same graph.\nAs we will see later in this chapter, the same holds true for statistical tests of interactions: it is much easier to make a statistical test of the interaction when we have all the data in one data frame. It is harder and is not recommended to make a separate regression for each level of the second variable (diet) and then compare the slopes of the regression lines (it is possible, just not at all efficient).\n\nParallel and non-parallel effects\nIn the example above, the effect of exercise on blood pressure was stronger for people with a meat heavy diet than for people with a vegetarian diet. That is, the slope of the regression line was steeper for the meat heavy diet than for the vegetarian diet. Put another way, the regression lines are not parallel.\nParallel regression lines are evidence of no interaction. This means that the effect of one variable (exercise) is the same for all levels of another variable (diet).\nWhen the regression lines are not parallel, there is evidence of an interaction. This means that the effect of one variable (exercise) depends on the level of another variable (diet).\n\n\nAnother example of an interaction\nTwo categorical variables: diet and exercise\n\n\n# A tibble: 6 × 5\n  diet       exercise  reps    bp error\n  &lt;chr&gt;      &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 meat heavy high         1  83.7 -6.26\n2 meat heavy low          1 115.  15.1 \n3 vegetarian high         1  84.2  9.19\n4 vegetarian low          1 109.  13.6 \n5 meat heavy high         2  91.8  1.84\n6 meat heavy low          2 104.   3.90\n\n\n\n\n\n\n\n\n\nAnd another example of an interaction\nTwo continuous explanatory variables (age and exercise minutes) and one continuous response variable (blood pressure)\n\n\n         bp      age mins_per_week\n1  61.58319 35.93052     130.94479\n2  83.86716 42.32743      70.63945\n3  93.01438 54.37120      54.05203\n4  82.14413 74.49247     198.53681\n5  60.13102 32.10092     126.69865\n6 102.00306 73.90338      42.64163\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractions and additivity of effects\nAnother way of thinking about this is that the effects of diet and exercise on blood pressure are not additive. If in one study we estimated the effect of diet on blood pressure we might find an effect size of 10. If in another study we estimated the effect of exercise on blood pressure we might find an effect size of 5.\n\n\n\n\n\n\nThink, Pair, Share (#adding-effects)\n\n\n\nIf the effects of diet and exercise were additive, what would we expect the effect size to be if we estimated the effect of diet and exercise on blood pressure in the same study?\n\n\nIf the effects are non-additive, we would expect the effect size to be different from additive. For example, if we found the combined effect of diet and exercise on blood pressure to be 30, we would say that the effects are non-additive. Their combined effect is more than the sum of their individual effects. This example is of a synergistic interaction because the combined effect (30) is greater than the sum of the individual effects (15).\n\n\nDrug interactions\nWhen we are prescribed a medication, we are often asked if we are taking any other medications. This is because the effects of drugs can interact. For example, if we take two drugs that both lower blood pressure and they interfere with each other, the combined effect might be less than the sum of their individual effects. This is an antagonistic interaction. It could be worse than that though, the interaction might actually be harmful, which is why doctors are so careful about known and potential drug interactions."
  },
  {
    "objectID": "6-interactions.html#the-maths-bit",
    "href": "6-interactions.html#the-maths-bit",
    "title": "Interactions (L5)",
    "section": "The maths bit",
    "text": "The maths bit\nLet us return to the example of the effects of number of minutes of exercise and diet on blood pressure:\n\n\n\n\n\nWe have one continuous explanatory variable (minutes of exercise) and one binary explanatory variable (diet) and one continuous response variable (blood pressure).\n\n\n\n\n\n\nThink, Pair, Share (#without-interaction)\n\n\n\nWhat would a linear model without an interaction term be?\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\epsilon_i\\]\nwhere:\n\n\\(y_i\\) is the blood pressure of the \\(i\\)th participant\n$x_1^{(i)} is the number of minutes of exercise of the \\(i\\)th participant\n\\(x_2^{(i)}\\) is the diet of the \\(i\\)th participant\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1\\) is the effect of exercise on blood pressure\n\\(\\beta_2\\) is the effect of diet on blood pressure\n\\(\\epsilon_i\\) is the error term for the \\(i\\)th participant.\n\nThis model is a multiple regression model in which one of the explanatory variables is binary.\n\n\n\n\n\n\nThink, Pair, Share (#with-interaction)\n\n\n\nWhat might the model look like if we wanted to include an interaction between diet and exercise?\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\beta_3 (x_{i}^{(1)} x_{i}^{(2)}) + \\epsilon_i\\]\nwhere:\n\n\\(x_{i}^{(1)}x_{i}^{(2)}\\) is the product of the number of minutes of exercise and the diet of the \\(i\\)th participant.\n\\(\\beta_3\\) is the coefficient of the interaction term between diet and exercise.\n\nWe could also write this model as:\n\\[y_i = \\beta_0 + \\beta_1 x_{i}^{(1)} + \\beta_2 x_{i}^{(2)} + \\beta_3 x_{i}^{(3)} + \\epsilon_i\\]\nwhere:\n\n\\(x_{i}^{(3)} = x_{i}^{(1)} x_{i}^{(2)}\\)\n\nThis is again a multiple regression model, but now with three explanatory variables.\n\n\n\n\n\n\nThink, Pair, Share (#sketch-interaction)\n\n\n\nMake sketches of the possible relationships between diet, exercise and blood pressure. Make a sketch compatible with \\(\\beta_3 = 0\\). Make a sketch compatible with \\(\\beta_3 \\neq 0\\)."
  },
  {
    "objectID": "6-interactions.html#hypothesis-testing",
    "href": "6-interactions.html#hypothesis-testing",
    "title": "Interactions (L5)",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nIf we want to test whether the effect of minutes of exercise on blood pressure is different for people with different diets, we need a null hypothesis to test.\n\n\n\n\n\n\nThink, Pair, Share (#interaction-null)\n\n\n\nWhat is the null hypothesis in this case, verbally, and in terms of the coefficients of the model?\n\n\nThe null hypothesis is that the effect of minutes of exercise on blood pressure is the same for people with different diets. This is a null hypothesis of no interaction between diet and exercise. In terms of the coefficients of the model, the null hypothesis is that \\(\\beta_3 = 0\\).\nIf we reject the null hypothesis, we conclude that the effect of minutes of exercise on blood pressure is different for people with different diets. This is a non-additive effect."
  },
  {
    "objectID": "6-interactions.html#doing-it-in-r",
    "href": "6-interactions.html#doing-it-in-r",
    "title": "Interactions (L5)",
    "section": "Doing it in R",
    "text": "Doing it in R\nLet us fit the model with the interaction term in R. There are two methods to do this and they are equivalent:\n\nmod1 &lt;- lm(bp ~ mins_per_week + diet + mins_per_week:diet, data=bp_diet)\nmod2 &lt;- lm(bp ~ mins_per_week * diet, data=bp_diet)\n\nThe second is a shorthand for the first. The * operator includes the main effects (main effects are terms in the model that don’t include interactions) and the interaction term. The : operator includes only the interaction term.\nOf course, we check the model diagnostics before we interpret the results:\n\n\n\n\n\nAll of the plots look good.\nNow, let us look at the coefficients of the model:\n\n\n                                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)                  100.62716134 2.87228930 35.033783 1.714199e-56\nmins_per_week                 -0.09661054 0.02406014 -4.015376 1.177969e-04\ndietvegetarian               -15.27591304 4.09882726 -3.726898 3.275630e-04\nmins_per_week:dietvegetarian   0.06907583 0.03433487  2.011828 4.704076e-02\n\n\nAs expected, there are four coefficients.\nThe first is (Intercept), which is the expected blood pressure for a person who does 0 minutes of exercise and is on diet “meat heavy”.\nThe second is mins_per_week, which is the effect (slope) of minutes of exercise on blood pressure for a person on diet “meat heavy”.\nThe third is dietvegetarian, which is the effect of being on a vegetarian diet on blood pressure for a person who does 0 minutes of exercise. This can be thought of as the change in the intercept for a person on a vegetarian diet compared to a person on a “meat heavy” diet.\nThe fourth is the interaction term mins_per_week:dietvegetarian, which is the difference in the effect (slope) of minutes of exercise on blood pressure for a person on a vegetarian diet compared to a person on a “meat heavy” diet.\n\n\n\n\n\n\nThink, Pair, Share (#two-equations)\n\n\n\nWrite two equations, one for each of the two diets. They would look something like this: \\(y_i = 0.1 - 0.1 x_{i}^{(1)}\\), but will have other numbers."
  },
  {
    "objectID": "6-interactions.html#reporting-our-findings",
    "href": "6-interactions.html#reporting-our-findings",
    "title": "Interactions (L5)",
    "section": "Reporting our findings",
    "text": "Reporting our findings\nOf course a nice graph is always helpful. We already have quite a nice one:\n\n\n\n\n\nWe also might want some tables summarizing the model results. Here is a table of the coefficients:\n\n\n\n\n\n\nEstimate\nStd. Error\nt value\nPr(&gt;|t|)\n\n\n\n\n(Intercept)\n100.6271613\n2.8722893\n35.033783\n0.0000000\n\n\nmins_per_week\n-0.0966105\n0.0240601\n-4.015377\n0.0001178\n\n\ndietvegetarian\n-15.2759130\n4.0988273\n-3.726898\n0.0003276\n\n\nmins_per_week:dietvegetarian\n0.0690758\n0.0343349\n2.011828\n0.0470408\n\n\n\n\n\n\n\nWe could also report the \\(R^2\\) of the model:\n\n\n[1] 0.2732093\n\n\nAnd also a table of the variances of the terms in the model:\n\n\n\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n\nmins_per_week\n1\n1133.554\n1133.55446\n13.47998\n0.0003964\n\n\ndiet\n1\n1560.753\n1560.75255\n18.56012\n0.0000398\n\n\nmins_per_week:diet\n1\n340.357\n340.35702\n4.04745\n0.0470408\n\n\nResiduals\n96\n8072.804\n84.09171\nNA\nNA\n\n\n\n\n\n\n\nWe also might use a sentence like this to report the results: “The effect of minutes of exercise is generally negative, but the effect is stronger for people on a meat heavy diet than for people on a vegetarian diet (\\(t\\)-statistics of interaction term = 2.01, degree of freedom = 2, p = 0.05). The greater beneficial effect of exercise on blood pressure for people on a meat heavy diet is largely caused by the high blood pressure of people on a meat heavy diet that do not exercise.”"
  },
  {
    "objectID": "6-interactions.html#multiple-regression-vs.-many-single-regressions",
    "href": "6-interactions.html#multiple-regression-vs.-many-single-regressions",
    "title": "Interactions (L5)",
    "section": "Multiple regression vs. many single regressions",
    "text": "Multiple regression vs. many single regressions\nQuestion: Why not just fit a separate simple regression model and then compare the two separately estimated slopes? That is, why not fit the two models:\n\\[y_i = \\beta_{0,veg} + \\beta_{1,veg} x_i^{(1)} + \\epsilon_i\\]\n\\[y_i = \\beta_{0,meat} + \\beta_{1,meat} x_i^{(2)} + \\epsilon_i\\]\nand compare the estimate of \\(\\beta_{1,veg}\\) to the estimate of \\(\\beta_{1,meat}\\)?\nWell, you could do that, and could probably find a way to test for whether the difference in the slopes is different from 0. This would be a test of the null hypothesis that the effect of minutes of exercise on blood pressure is the same for people with different diets. But, this would be a more complicated way to do it, and would not be as general as the model with the interaction term. The model with the interaction term is more general, more flexible, and more elegant."
  },
  {
    "objectID": "6-interactions.html#recap",
    "href": "6-interactions.html#recap",
    "title": "Interactions (L5)",
    "section": "Recap",
    "text": "Recap\n\nBinary and categorical explanatory variables.\nInteractions: a categorical explanatory variables allows for group-specific intercepts and slopes.\nThe \\(F\\)-test is used to test if \\(\\beta_1=\\beta_2=...=\\beta_k=0\\) at the same time for a categorical explanatory variable with \\(k\\) levels. Use the anova function in R to carry out this test.\nThe \\(F\\)-test is a generalization of the \\(t\\)-test, because the latter is used to test \\(\\beta_j = 0\\) for one single variable \\(x^{(j)}\\).\n\nTest for a single \\(\\beta_j=0\\) \\(\\rightarrow\\) \\(t\\)-test.\nTest for several \\(\\beta_2 = ... = \\beta_{k}=0\\) simultaneously \\(\\rightarrow\\) \\(F\\)-test.\n\nThus you will always need the \\(F\\)-test anova() to obtain a \\(p\\)-value for a categorial explanatory variable with more than 2 levels. This is because such a variable is represented by more than one coefficient in the model (remember the dummy variables)."
  },
  {
    "objectID": "7-ANOVA.html#introduction",
    "href": "7-ANOVA.html#introduction",
    "title": "ANOVA (L6)",
    "section": "Introduction",
    "text": "Introduction\nAnalysis of variance is a method to compare the means of more than two groups. We already know a lot about analysing variance: we compared the total sum of squares (SST), model sum of squares (SSM) and the residual sum of squares (SSE) in the context of linear regression. We used these to calculated the \\(R^2\\) value and the \\(F\\)-statistic. To calculate the F-statistic we used the formula \\(F = \\frac{MSM}{MSE}\\), where \\(MSM\\) is the mean square of the model and \\(MSE\\) is the mean square of the residuals. The mean square is a measure of variance.\nAnalysis of variance is a special case of a linear model, so much of what we already learned about linear models still holds.\nThe defining characteristic of ANOVA is that we are comparing the means of more than two groups. Put another way, we will have a single categorical explanatory variable with more than two levels. We will test whether the means of the response variable are the same across all levels of the explanatory variable.\nWhen we have only one categorical explanatory variable, we will use a one-way ANOVA. When we have two categorical explanatory variables, we will use a two-way ANOVA (we’ll look at this in the second half of this chapter).\nWe have already looked at categorical variables with more than two groups. Let us recap that material from lecture 5."
  },
  {
    "objectID": "7-ANOVA.html#anova",
    "href": "7-ANOVA.html#anova",
    "title": "ANOVA (L6)",
    "section": "ANOVA",
    "text": "ANOVA\n\nUnderstanding anlysis of variance (ANOVA)\nIn ANOVA, we often talk of within-group variance, between-group variance, and total variance. These are not new things:\n\nWithin-group variance is the variance of the residuals, \\(MS_{residual}\\).\nBetween-group variance is the variance of the group means, \\(MS_{model}\\).\n\nWe formulate a model as follows: \\[y_{ij} = \\mu_j + \\epsilon_{i}\\]\nwhere:\n\n\\(y_{ij}\\) = Blood pressure of individual \\(i\\) with diet \\(j\\)\n\\(\\mu_i\\) = Mean blood pressure of an individual with diet \\(j\\)\n\\(\\epsilon_{i}\\sim N(0,\\sigma^2)\\) is an independent error term.\n\nGraphically, with the blood pressure and diet data, this looks like:\n\n\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nRewrite the model\nOne common way to rewrite the model is to define one of the groups as the reference group, and make the mean of that equal to the intercept of the model:\n\\[\\mu_{meat} = \\beta_0\\]\nAnd then to express the other group means as deviations from the reference group mean:\n\\[\\mu_{Med} = \\beta_0 + \\beta_1\\] \\[\\mu_{vegan} = \\beta_0 + \\beta_2\\] \\[\\mu_{veggi} = \\beta_0 + \\beta_3\\]\nWhen we write out the entire model, we get:\n\\[y_i = \\beta_0 + \\beta_1 x_i^{1} + \\beta_2 x_i^{2} + \\beta_3 x_i^{3} + \\epsilon_i\\] where: \\(y_i\\) is the blood pressure of individual \\(i\\). \\(x_i^{1}\\) is a binary variable indicating whether individual \\(i\\) is on the Mediterranean diet. \\(x_i^{2}\\) is a binary variable indicating whether individual \\(i\\) is on the vegan diet. \\(x_i^{3}\\) is a binary variable indicating whether individual \\(i\\) is on the vegetarian diet.\nGraphically, the model now looks like this:\n\n\n`summarise()` has grouped output by 'diet'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThis looks exactly like the graph and model we made before while learning about categorical variables in Lecture 5. It is! There is nothing different here.\n\n\n\n\nThe ANOVA test: The \\(F\\)-test\nAim of ANOVA: to test globally if the groups differ. That is we want to test the null hypothesis that all of the group means are equal:\n\\[H_0: \\mu_1=\\mu_2=\\ldots = \\mu_g\\] This is equivalent to testing if all \\(\\beta\\)s that belong to a categorical variable are =0.\n\\[H_0: \\beta_1 = \\ldots = \\beta_{g-1} = 0\\] The alternate hypothesis is that \\({H_1}\\): The group means are not all the same.\nA key point is that we are testing a null hypothesis that concerns all the groups. We are not testing if one group is different from another group (which we could do with a \\(t\\)-test on one of the non-intercept \\(\\beta\\)s).\nBecause we are testing a null hypothesis that concerns all the groups, we need to use an \\(F\\)-test. It asks if the model with the group means is better than a model with just the overall mean.\n\n\nCalculating and analysing the variances\nTo derive the ingredients of the \\(F\\)-test, we look at the variances :\nTotal variability: SST = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij}-\\overline{y})^2\\)\nwhere:\n\n\\(y_{ij}\\) is the blood pressure of individual \\(j\\) in group \\(i\\)\n\\(\\overline{y}\\) is the overall mean blood pressure\n\\(n_i\\) is the number of individuals in group \\(i\\)\n\\(k\\) is the number of groups\n\nExplained variability (between group variability): == SSM = \\(\\sum_{i=1}^k n_i (\\overline{y}_{i} - \\overline{y})^2\\)\nwhere:\n\n\\(\\overline{y}_{i}\\) is the mean blood pressure of group \\(i\\)\n\nResidual variability (within group variability): = SSE = \\(\\sum_{i=1}^k \\sum_{j=1}^{n_i} (y_{ij} - \\overline{y}_{i} )^2\\)\nSST degrees of freedom: \\(n - 1\\) (total degrees of freedom is number of observations \\(n\\) minus 1)\nSSM degrees of freedom: \\(k - 1\\) (model degrees of freedom is number of groups \\(k\\) minus 1)\nSSE degrees of freedom: \\(n - k\\) (residual degrees of freedom is total degrees of freedom \\(n - 1\\) minus model degrees of freedom \\(k - 1\\))\nFrom these sums of squares and degrees of freedom we can calculate the mean squares and \\(F\\)-statistic:\n\\[MS_{model} = \\frac{SS_{\\text{between}}}{k-1} = \\frac{SSM}{k-1}\\]\n\\[MS_{residual} = \\frac{SS_{\\text{within}}}{n-k} = \\frac{SSE}{n-k}\\]\n\\[F = \\frac{MS_{model}}{MS_{residual}}\\]\n\n\nInterpretation of the \\(F\\) statistic\n\n\\(MS_{model}\\): Quantifies the variability between groups.\n\\(MS_{residual}\\): Quantifies the variability within groups.\n\nHere is an example with very low within group variability, and high between group variability:\n\n\n\n\n\nAnd here’s an example with very high within group variability, and low between group variability:\n\n\n\n\n\n\n\nInterpretation of the \\(F\\) statistic II\n\n\\(F\\) increases\n\nwhen the group means become more different, or\nwhen the variability within groups decreases.\n\n\\(F\\) decreases\n\nwhen the group means become more similar, or\nwhen the variability within groups increases.\n\n\n\\(\\rightarrow\\) The larger \\(F\\), the less likely are the data seen under \\(H_0\\).\n\n\nSource of variance table\nThe sources of variance table is a table that conveniently and clearly gives all of the quantities mentioned above. It breaks down the total sum of squares into the sum of squares explained by the model and the sum of squares due to error. The source of variance table is used to calculate the \\(F\\)-statistic.\n\nSources of variance table\n\n\n\n\n\n\n\n\n\nSource\nSum of squares\nDegrees of freedom\nMean square\nF-statistic\n\n\n\n\nModel\n\\(SSM\\)\n\\(k-1\\)\n\\(MSE_{model} = SSM / k-1\\)\n\\(\\frac{MSE_{model}}{MSE_{error}}\\)\n\n\nError\n\\(SSE\\)\n\\(n - 1 - (k-1)\\)\n\\(MSE_{error} = SSE / (n - 1 - (k-1))\\)\n\n\n\nTotal\n\\(SST\\)\n\\(n - 1\\)\n\n\n\n\n\n\n\nDoing ANOVA in R\nLet’s go back again the question of how diet effects blood pressure. Here is the data:\n\n\n# A tibble: 6 × 3\n     bp diet          person_ID\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;    \n1   120 meat heavy    person_1 \n2    89 vegan         person_2 \n3    86 vegetarian    person_3 \n4   116 meat heavy    person_4 \n5   115 Mediterranean person_5 \n6   134 meat heavy    person_6 \n\n\n\n\n\n\n\nAnd here is how we fit a linear model to this data:\nNext we check the diagnostic plots:\n\n\n\n\n\nNothing looks too bad.\nNow we can look at the ANOVA table:\n\n\nAnalysis of Variance Table\n\nResponse: bp\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet       3 5274.2 1758.08  20.728 1.214e-08 ***\nResiduals 46 3901.5   84.82                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA suitable sentence to report our findings would be: “Diet has a significant effect on blood pressure (\\(F(2, 27) = 20.7, p &lt; 0.0001\\))”. This means that the probability of observing such a large \\(F\\) value under the null hypothesis is less than 0.01%."
  },
  {
    "objectID": "7-ANOVA.html#difference-between-pairs-of-groups",
    "href": "7-ANOVA.html#difference-between-pairs-of-groups",
    "title": "ANOVA (L6)",
    "section": "Difference between pairs of groups",
    "text": "Difference between pairs of groups\nIf the \\(F\\)-test of the null hypothesis that \\(\\beta_1=\\ldots= \\beta_{g-1}=0\\) is rejected, a researcher might then be interested:\n\nin finding the actual group(s) that deviate(s) from the others.\nin estimates of the pairwise differences.\n\nThe summary table in R provides some of these comparison, specifically it contains the estimates for \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) (while the reference was set to \\(\\beta_0 = 0\\)). These three \\(\\beta\\) values are the differences between the group means and the reference group mean. We can test if these differences are significantly different from zero using a \\(t\\)-test, as you’ve see before.\nHowever, there are two issues:\n\nThe more tests you do, the more likely you are to find a significant result just by chance. This is called the problem of multiple comparisons. Many test can result in a type-I error: rejecting the null hypothesis when it is actually true. The more tests one does, the more likely one is to make a type-I error.\nThe summary table does not provide all the possible pairwise comparisons. It does not, for example, provide the comparison between the “vegan” and the “vegetarian” group.\n\nSeveral methods to circumvent the problem of too many “significant” test results (type-I error) have been proposed. The most prominent ones are:\n\nBonferroni correction\nTukey Honest Significant Differences (HSD) approach\nFisher Least Significant Differences (LSD) approach\n\n\nBonferroni correction\nIdea: If a total of \\(m\\) tests are carried out, simply divide the type-I error level \\(\\alpha_0\\) (often 5%) such that\n\\[\\alpha = \\alpha_0 / m \\ .\\]\n\n\nTukey HSD approach\nIdea: Take into account the distribution of (max-min) and design a new test.\n\n\nFisher’s LSD approach\nIdea: Adjust the idea of a two-sample test, but use a larger variance (namely the pooled variance of all groups).\n\n\nOther contrasts\nWe can design other contrasts, for example: are diets that contain meat different from diets that do not contain meat?\n\n\n# A tibble: 6 × 4\n     bp diet          person_ID meat_or_no_meat\n  &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;     &lt;chr&gt;          \n1   120 meat heavy    person_1  no meat        \n2    89 vegan         person_2  no meat        \n3    86 vegetarian    person_3  no meat        \n4   116 meat heavy    person_4  no meat        \n5   115 Mediterranean person_5  meat           \n6   134 meat heavy    person_6  no meat        \n\n\nHere we defined a new explanatory variable that groups the meat heavy and Mediterranean diet together into a single “meat” group and vegetarian and vegan into a single “no meat” group. We then fit a model with this explanatory variable:\n\nfit_mnm &lt;- lm(bp ~ meat_or_no_meat, data = bp_data_diet)\n\n(We should not look at model diagnostics here, before using the model. But let us continue as if the assumptions are sufficiently met.)\nWe now do something a bit more complicated: we compare the variance explained by the model with four diets to the model with two diets. This is done by comparing the two models using an \\(F\\)-test. We are testing the null hypothesis that the two models are equally good at explaining the data, in which case the two diet model will explain as much variance as the four diet model.\nLet’s look at the ANOVA table of the model comparison:\n\nanova(fit, fit_mnm)\n\nAnalysis of Variance Table\n\nModel 1: bp ~ diet\nModel 2: bp ~ meat_or_no_meat\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     46 3901.5                                  \n2     48 9173.4 -2   -5271.9 31.078 2.886e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe see the residual sum of squares of the model with meat or no meat is over 9’000, while that of the four diet model is less than 4’000. That is, the four diet model explains much more variance in the data than the two diet model. The \\(F\\)-test is highly significant, so we reject the null hypothesis that the two models are equally good at explaining the data. And we conclude that its not just whether people eat meat or not, but rather what kind of diet they eat that affects their blood pressure.\nIdeally we do not make a lot of contrasts after we have collected and looked at our data. Rather, we would specify the contrasts we are interested in before we collect the data. This is called a priori contrasts. But sometimes we do exploratory data analysis and then we can make post hoc contrasts. In this case we should be careful to adjust for multiple comparisons.\n\n\nChoosing the reference category\nQuestion: Why was the “heavy meat” diet chosen as the reference (intercept) category?\nAnswer: Because R orders the categories alphabetically and takes the first level alphabetically as reference category.\nSometimes we may want to override this, for example if we have a treatment that is experimentally the control, then it will usually be useful to set this as the reference / intercept level.\nIn R we can set the reference level using the relevel function:\nAnd now make the model and look at the estimated coefficients:\n\n\n\nCall:\nlm(formula = bp ~ diet, data = bp_data_diet)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-17.9375  -5.9174  -0.4286   5.2969  22.3750 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         95.857      3.481  27.538  &lt; 2e-16 ***\ndietmeat heavy      26.768      4.173   6.414 6.92e-08 ***\ndietMediterranean   14.080      4.173   3.374  0.00151 ** \ndietvegetarian       3.143      4.453   0.706  0.48386    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.21 on 46 degrees of freedom\nMultiple R-squared:  0.5748,    Adjusted R-squared:  0.5471 \nF-statistic: 20.73 on 3 and 46 DF,  p-value: 1.214e-08\n\n\nNow we see the estimated coefficients for all diets except the vegan diet. The intercept is the mean individuals with vegan diet."
  },
  {
    "objectID": "7-ANOVA.html#two-way-anova-zweiweg-varianzanalyse",
    "href": "7-ANOVA.html#two-way-anova-zweiweg-varianzanalyse",
    "title": "ANOVA (L6)",
    "section": "Two-way ANOVA (Zweiweg-Varianzanalyse)",
    "text": "Two-way ANOVA (Zweiweg-Varianzanalyse)\nTwo-way ANOVA is used to analyse a specific type of study design. When we have a study with two categorical treatments and all possible combinations of them, we can use a two-way ANOVA.\nFor example, take the question of how diet and exercise affect blood pressure. Let’s say we can have three levels of diet: meat heavy, Mediterranean, and vegetarian. And that we have two levels of exercise: low and high. And that we have all possible combinations of these two treatments: i.e., we have a total of \\(3 \\times 2 = 6\\) treatment combinations.\nWe can also represent this study design in a table:\n\n\n\n\n\n\n\n\nExercise (G)\n\n\n\n\n\n\nDiet (B)\n\n\nLow (1)\n\n\nHigh (2)\n\n\n\n\nMeat heavy (1)\n\n\n\n\n\n\n\n\nMediterranean (2)\n\n\n\n\n\n\n\n\nVegetarian (3)\n\n\n\n\n\n\n\n\n\nThe six empty cells in the table represent the six treatment combinations.\nThis type of study, with all possible combinations, is known as a factorial design. The two treatments are called factors, and the levels of the factors are called factor levels. A fully factorial design is one where all possible combinations of the factor levels are present.\nLet’s look at example data:\n\n\n# A tibble: 6 × 4\n  diet          exercise  reps    bp\n  &lt;chr&gt;         &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 Mediterranean high         1  110.\n2 Mediterranean low          1  119.\n3 meat heavy    high         1  102.\n4 meat heavy    low          1  128.\n5 vegetarian    high         1  104.\n6 vegetarian    low          1  111.\n\n\nWe can use the xtabs function to create a table of the data, by cross-tabulating the two treatments diet and exercise:\n\nxtabs(~diet + exercise, data = bp_data_2cat)\n\n               exercise\ndiet            low high\n  meat heavy     10   10\n  Mediterranean  10   10\n  vegetarian     10   10\n\n\nThis tells us there are 10 replicates in each of the six treatment combinations.\nAnd a visualisation of the data:\n\n\n\n\n\n\n\n\n\n\n\nThink, Pair, Share (#twoway-plot)\n\n\n\nWhat do you conclude from this plot?\n\n\n\nThe model for 2-way ANOVA\nAssume we have a factorial design with two treatments (factors), factor \\(B\\) and factor \\(G\\).\nAnd that we can label the levels of factor \\(B\\) as \\(i=1,2...\\) and factor \\(\\G\\) as \\(j=1,2...\\).\nThen we can denote a particular treatment combination as \\(B_iG_j\\).\nAnd let us set the one of the treatment combinations as the intercept of the model, and the let the intercept be equal to the mean of the observations in the treatment combination \\(B1G1\\).\n\\[intercept = \\frac{1}{n_{11}}\\sum_{k=1}^{n_{11}} y_{1,1,k}\\]\nwhere:\n\n\\(y_{1,1,k}\\) is the \\(k\\)th observation in the treatment combination \\(B1G1\\)\n\\(n_{11}\\) is the number of observations in the treatment combination \\(B1G1\\).\n\nAnd we will let all of the other treatment combinations be represented by the effects \\(\\beta_i\\) and \\(\\gamma_j\\).\nThe resulting linear model is:\n\\[y_{ijk} = intercept + \\beta_i + \\gamma_j + (\\beta\\gamma)_{ij} + \\epsilon_{ijk} \\quad \\text{with} \\quad \\epsilon_{ijk} \\sim N(0,\\sigma^2)\\]\nwhere\n\n\\(y_{ijk}\\) is the \\(k\\)th observation in the treatment combination of \\(i\\) and \\(j\\).\n\\((\\beta\\gamma)_{ij}\\) is the interaction effect between the \\(i\\)th level of factor \\(\\beta\\) and the \\(j\\)th level of factor \\(\\gamma\\).\n\nIn this model, we set \\(\\beta_1=\\gamma_1=0\\) and \\((\\beta\\gamma)_{11}=0\\) because they are already included in the intercept.\n\n\nUsing R for 2-way ANOVA\nIn R, a two-way ANOVA is as simple as one-way ANOVA, just add another variable:\n\nmod1 &lt;- lm(bp ~ diet * exercise, data = bp_data_2cat)\n\nNote that, as we saw in the chapter about interactions, we include the main effects of diet and exercise and the interaction term with the short hand diet * exercise.\nOf course we next check the model diagnostics:\n\n\n\n\n\nNo clear patterns: all is good.\n\n\nHypothesis testing\nAs is implied by the name “Analysis of variance” we analyse variances, here mean squares, to test hypotheses. And as before we use an \\(F\\)-test to do this. Remember that the \\(F\\)-test is a ratio of two mean squares (where mean squares are a kind of variance).\n\n\n\n\n\n\nThink, Pair, Share (#full-degrees)\n\n\n\nHow many degrees of freedom for error will there be when we fit this model with both main effects and the interaction term? Hint: remember that the degrees of freedom for error is the number of observations minus the number of parameters estimated.\n\n\nWe can have have a null hypothesis of no effect for each of the two main effects and for the interaction. So we can do an \\(F\\)-test for each of these null hypotheses.\nHere is the ANOVA table:\n\n\nAnalysis of Variance Table\n\nResponse: bp\n              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ndiet           2  390.74  195.37  9.9672 0.0002069 ***\nexercise       1 1297.52 1297.52 66.1966 5.952e-11 ***\ndiet:exercise  2  340.67  170.33  8.6901 0.0005346 ***\nResiduals     54 1058.46   19.60                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation: All three of the null hypotheses are rejected. Importantly, we see that the interaction term is significant, which means that the effect of one treatment is different depending on the level of the other treatment. This means that even though the main effects are significant, we cannot interpret them without considering the interaction term. I.e., we cannot say anything general about the effect of diet or exercise alone on blood pressure. We have to qualify any statement about the effect of diet or exercise with “depending on the level of the other treatment”. Or state something like “the exercise reduces blood pressure greatly for people with a meat heavy diet, but reduces blood pressure only slightly for people with a vegetarian diet or Mediterranean diet”.\n\n\nInterpreting coefficients\nWe can look at the estimated coefficients to see the size of the effects, but be aware that it contains only a subset of the possible effects; it would contain different values if a different treatment combination were set to be the intercept of the model. Also be aware that often we are mostly interested in the \\(F\\)-test and their hypotheses test, and are less interest in the coefficients and their \\(t\\)-tests (unlike in a regression model where we are often interested in the coefficients and their \\(t\\)-tests).\nFinally, be aware that it needs a bit of work to interpret the coefficients, because they are relative to the intercept. Let’s try to figure out what that means. First, back to the table of the experimental design. This time we will put in the cells an expression for the mean of that treatment combination:\n\n\n\n\n\n\n\n\nExercise (G)\n\n\n\n\n\n\nDiet (B)\n\n\nLow (1)\n\n\nHigh (2)\n\n\n\n\nMeat heavy (1)\n\n\n\\(B_1G_1\\)\n\n\n\\(B_1G_2\\)\n\n\n\n\nMediterranean (2)\n\n\n\\(B_2G_1\\)\n\n\n\\(B_2G_2\\)\n\n\n\n\nVegetarian (3)\n\n\n\\(B_2G_1\\)\n\n\n\\(B_3G_2\\)\n\n\n\n\n\nSo, for example, the mean of the treatment combination “Meat heavy, Low” is \\(B_1G_1\\). And the mean of the treatment combination “Mediterranean, High” is \\(B_2G_2\\).\nHowever, the coefficients in the summary table given by R are not like this. They are coefficients relative to an intercept / reference treatment combination. The reference treatment combination chosen by R is the first level of the first factor and the first level of the second factor. In this case, that is “Meat heavy, Low” – \\(B_1G_1\\).\nAll of the other coefficients are about differences from this reference treatment combination.\nSo, for example, the coefficient for “High” in the “Exercise” factor (appearing as exercisehigh in the summary table) is the difference in mean blood pressure between the treatment combination “Meat heavy, High” (\\(B_1G_2\\)) and the treatment combination “Meat heavy, Low”. Put another way, \\(B_1G_2 = B_1G_1 + \\gamma_2\\) where \\(\\gamma_2\\) is the coefficient for “High” in the “Exercise” factor.\nAnd the coefficient for “Mediterranean” in the “Diet” factor (appearing as dietMediterranean in the summary table) is the difference in mean blood pressure between the treatment combination “Mediterranean, Low” and the treatment combination “Meat heavy, Low”. Put another way, \\(B_2G_1 = B_1G_1 + \\beta_2\\) where \\(\\beta_2\\) is the coefficient for “Mediterranean” in the “Diet” factor.\nLet us for a moment assume that the effects of diet and exercise are additive. If this is the case, then the mean for \\(B_2G_2\\) = \\(B_1G_1 + \\beta_2 + \\gamma_2\\). That is, the mean for “Mediterranean, High” is the mean for “Meat heavy, Low” plus the effect of “Mediterranean” plus the effect of “High”.\nHowever, if the effects are not additive, then the mean for \\(B_2G_2\\) is not \\(B_1G_1 + \\beta_2 + \\gamma_2\\). Rather, it is \\(B_2G_2 = B_1G_1 + \\beta_2 + \\gamma_2 + (\\beta\\gamma)_{22}\\). That is, the mean for “Mediterranean, High” is the mean for “Meat heavy, Low” \\(B_1G_1\\) plus the effect of “Mediterranean” \\(\\beta_2\\) plus the effect of “High” \\(\\gamma_2\\) plus the non-additive effect between “Mediterranean” and “High” \\((\\beta\\gamma)_{22}\\).\nNon-additivity implies an interaction, therefore the non-additive effect is the interaction effect. In the summary table these interaction effects are those that contain a colon (:), e.g., dietMediterranean:exercisehigh.\nHere’s a graphical representation of how the coefficients in the summary table relate to the means of the treatment combinations:\n\n\n\nUnderstanding coefficients\n\n\n\n\n\n\n\n\nThink, Pair, Share (#veghigh-estimate)\n\n\n\nFrom the values in the coefficients table, calculate the estimated mean of the treatment combination “vegetarian, High”."
  },
  {
    "objectID": "7-ANOVA.html#why-not-perform-multiple-t-tests",
    "href": "7-ANOVA.html#why-not-perform-multiple-t-tests",
    "title": "ANOVA (L6)",
    "section": "Why not perform multiple \\(t\\)-tests?",
    "text": "Why not perform multiple \\(t\\)-tests?\nWhy not carry out pairwise \\(t\\)-tests between every pair of groups?\n\nHow many tests would this imply?\nWhy is this not a very clever idea?"
  },
  {
    "objectID": "7-ANOVA.html#summing-up",
    "href": "7-ANOVA.html#summing-up",
    "title": "ANOVA (L6)",
    "section": "Summing up",
    "text": "Summing up\n\nANOVA is just another linear model.\nIt is used when we have categorical explanatory variables.\nWe use \\(F\\)-tests to test the null hypothesis of no difference among the means of the groups (categories).\nWe can use contrasts and post-hoc tests to test specific hypotheses about the means of the groups.\nTwo-way ANOVA is used when we have two categorical explanatory variables and can be used to test for interactions between them."
  },
  {
    "objectID": "7-ANOVA.html#additional-reading",
    "href": "7-ANOVA.html#additional-reading",
    "title": "ANOVA (L6)",
    "section": "Additional reading",
    "text": "Additional reading\nPlease feel free to look at the follow resources for a slightly different perspective and some more information on ANOVA:\n\nChapter 12 from Stahel book Statistische Datenenalyse\nGetting Started with R chapters 5.6 and 6.2"
  },
  {
    "objectID": "8-ANCOVA.html",
    "href": "8-ANCOVA.html",
    "title": "ANCOVA (L7)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "9-linear-algebra.html",
    "href": "9-linear-algebra.html",
    "title": "Linear algebra (L7)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "10-variable-selection.html",
    "href": "10-variable-selection.html",
    "title": "Variable selection (L8)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "11-interpretation.html",
    "href": "11-interpretation.html",
    "title": "Intepretation, causality, and cautionary notes (L9)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "12-count-data.html",
    "href": "12-count-data.html",
    "title": "Count data (L10)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "13-binary-data.html",
    "href": "13-binary-data.html",
    "title": "Binary data (L11)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "14-measurement-error.html",
    "href": "14-measurement-error.html",
    "title": "Measurement error (L12)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "15-mixed-models.html",
    "href": "15-mixed-models.html",
    "title": "Mixed models (L12)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  },
  {
    "objectID": "16-what-next.html",
    "href": "16-what-next.html",
    "title": "What next (L12)",
    "section": "",
    "text": "Lecture content not available here in 2025. Please see the OLAT course website for the content of this lecture."
  }
]